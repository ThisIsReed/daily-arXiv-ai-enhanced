{"id": "2509.09729", "categories": ["cs.CL", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.09729", "abs": "https://arxiv.org/abs/2509.09729", "authors": ["Gerard Sant", "Zifan Jiang", "Carlos Escolano", "Amit Moryossef", "Mathias M\u00fcller", "Rico Sennrich", "Sarah Ebling"], "title": "MultimodalHugs: Enabling Sign Language Processing in Hugging Face", "comment": null, "summary": "In recent years, sign language processing (SLP) has gained importance in the\ngeneral field of Natural Language Processing. However, compared to research on\nspoken languages, SLP research is hindered by complex ad-hoc code,\ninadvertently leading to low reproducibility and unfair comparisons. Existing\ntools that are built for fast and reproducible experimentation, such as Hugging\nFace, are not flexible enough to seamlessly integrate sign language\nexperiments. This view is confirmed by a survey we conducted among SLP\nresearchers.\n  To address these challenges, we introduce MultimodalHugs, a framework built\non top of Hugging Face that enables more diverse data modalities and tasks,\nwhile inheriting the well-known advantages of the Hugging Face ecosystem. Even\nthough sign languages are our primary focus, MultimodalHugs adds a layer of\nabstraction that makes it more widely applicable to other use cases that do not\nfit one of the standard templates of Hugging Face. We provide quantitative\nexperiments to illustrate how MultimodalHugs can accommodate diverse modalities\nsuch as pose estimation data for sign languages, or pixel data for text\ncharacters.", "AI": {"tldr": "MultimodalHugs\u662f\u4e00\u4e2a\u57fa\u4e8eHugging Face\u6784\u5efa\u7684\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u624b\u8bed\u5904\u7406\u7814\u7a76\u4e2d\u5b58\u5728\u7684\u4ee3\u7801\u590d\u6742\u3001\u53ef\u590d\u73b0\u6027\u4f4e\u548c\u6bd4\u8f83\u4e0d\u516c\u5e73\u7b49\u95ee\u9898\uff0c\u652f\u6301\u66f4\u591a\u6837\u5316\u7684\u6570\u636e\u6a21\u6001\u548c\u4efb\u52a1\u3002", "motivation": "\u624b\u8bed\u5904\u7406\u7814\u7a76\u76f8\u6bd4\u53e3\u8bed\u8bed\u8a00\u7814\u7a76\u9762\u4e34\u66f4\u591a\u6311\u6218\uff0c\u5305\u62ec\u590d\u6742\u7684\u4e34\u65f6\u4ee3\u7801\u3001\u4f4e\u53ef\u590d\u73b0\u6027\u548c\u4e0d\u516c\u5e73\u6bd4\u8f83\u3002\u73b0\u6709\u5de5\u5177\u5982Hugging Face\u5728\u624b\u8bed\u5b9e\u9a8c\u96c6\u6210\u65b9\u9762\u4e0d\u591f\u7075\u6d3b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7814\u7a76\u9700\u6c42\u3002", "method": "\u5728Hugging Face\u57fa\u7840\u4e0a\u6784\u5efaMultimodalHugs\u6846\u67b6\uff0c\u589e\u52a0\u62bd\u8c61\u5c42\u4ee5\u652f\u6301\u66f4\u591a\u6570\u636e\u6a21\u6001\u548c\u4efb\u52a1\uff0c\u7279\u522b\u662f\u9488\u5bf9\u624b\u8bed\u5904\u7406\u7684\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u548c\u6587\u672c\u5b57\u7b26\u7684\u50cf\u7d20\u6570\u636e\u7b49\u591a\u6837\u5316\u6a21\u6001\u3002", "result": "\u901a\u8fc7\u5b9a\u91cf\u5b9e\u9a8c\u8bc1\u660eMultimodalHugs\u80fd\u591f\u6709\u6548\u9002\u5e94\u624b\u8bed\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u548c\u6587\u672c\u5b57\u7b26\u50cf\u7d20\u6570\u636e\u7b49\u591a\u79cd\u6a21\u6001\uff0c\u63d0\u9ad8\u4e86\u5b9e\u9a8c\u7684\u53ef\u590d\u73b0\u6027\u548c\u516c\u5e73\u6027\u3002", "conclusion": "MultimodalHugs\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u624b\u8bed\u5904\u7406\u7814\u7a76\u4e2d\u7684\u6280\u672f\u969c\u788d\uff0c\u4e0d\u4ec5\u9002\u7528\u4e8e\u624b\u8bed\u7814\u7a76\uff0c\u8fd8\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5176\u4ed6\u4e0d\u7b26\u5408Hugging Face\u6807\u51c6\u6a21\u677f\u7684\u7528\u4f8b\uff0c\u4fc3\u8fdb\u4e86\u591a\u6a21\u6001\u7814\u7a76\u7684\u53ef\u590d\u73b0\u6027\u548c\u516c\u5e73\u6bd4\u8f83\u3002"}}
{"id": "2509.09716", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09716", "abs": "https://arxiv.org/abs/2509.09716", "authors": ["Jun Zhan", "Mingyang Han", "Yuxuan Xie", "Chen Wang", "Dong Zhang", "Kexin Huang", "Haoxiang Shi", "DongXiao Wang", "Tengtao Song", "Qinyuan Cheng", "Shimin Li", "Jun Song", "Xipeng Qiu", "Bo Zheng"], "title": "VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions", "comment": null, "summary": "Spoken language models (SLMs) have emerged as a unified paradigm for speech\nunderstanding and generation, enabling natural human machine interaction.\nHowever, while most progress has focused on semantic accuracy and instruction\nfollowing, the ability of SLMs to adapt their speaking style based on spoken\ninstructions has received limited attention. We introduce Voice Style\nAdaptation (VSA), a new task that examines whether SLMs can modify their\nspeaking style, such as timbre, prosody, or persona following natural language\nspoken commands. To study this task, we present VStyle, a bilingual (Chinese &\nEnglish) benchmark covering four categories of speech generation: acoustic\nattributes, natural language instruction, role play, and implicit empathy. We\nalso introduce the Large Audio Language Model as a Judge (LALM as a Judge)\nframework, which progressively evaluates outputs along textual faithfulness,\nstyle adherence, and naturalness, ensuring reproducible and objective\nassessment. Experiments on commercial systems and open source SLMs demonstrate\nthat current models face clear limitations in controllable style adaptation,\nhighlighting both the novelty and challenge of this task. By releasing VStyle\nand its evaluation toolkit, we aim to provide the community with a foundation\nfor advancing human centered spoken interaction. The dataset and code are\npublicly available at\n\\href{https://junzhan2000.github.io/VStyle.github.io/}{project's homepage}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u8bed\u97f3\u98ce\u683c\u9002\u5e94(VSA)\u65b0\u4efb\u52a1\uff0c\u7814\u7a76\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u6839\u636e\u81ea\u7136\u8bed\u8a00\u8bed\u97f3\u6307\u4ee4\u8c03\u6574\u8bf4\u8bdd\u98ce\u683c\uff0c\u5e76\u53d1\u5e03\u4e86\u53cc\u8bed\u57fa\u51c6VStyle\u548c\u8bc4\u4f30\u6846\u67b6LALM as a Judge\u3002", "motivation": "\u5f53\u524d\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u8bed\u4e49\u51c6\u786e\u6027\u548c\u6307\u4ee4\u8ddf\u968f\uff0c\u4f46\u5728\u6839\u636e\u8bed\u97f3\u6307\u4ee4\u8c03\u6574\u8bf4\u8bdd\u98ce\u683c\u65b9\u9762\u7684\u80fd\u529b\u7814\u7a76\u6709\u9650\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u81ea\u7136\u7684\u4eba\u673a\u8bed\u97f3\u4ea4\u4e92\u3002", "method": "\u63d0\u51faVSA\u4efb\u52a1\uff0c\u6784\u5efa\u53cc\u8bed\u57fa\u51c6VStyle\u8986\u76d64\u7c7b\u8bed\u97f3\u751f\u6210\uff0c\u5f00\u53d1LALM as a Judge\u6846\u67b6\u4ece\u6587\u672c\u5fe0\u5b9e\u5ea6\u3001\u98ce\u683c\u9075\u5faa\u5ea6\u548c\u81ea\u7136\u5ea6\u4e09\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u6e10\u8fdb\u5f0f\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5f53\u524d\u5546\u4e1a\u7cfb\u7edf\u548c\u5f00\u6e90SLM\u5728\u53ef\u63a7\u98ce\u683c\u9002\u5e94\u65b9\u9762\u5b58\u5728\u660e\u663e\u5c40\u9650\u6027\uff0c\u9a8c\u8bc1\u4e86\u8be5\u4efb\u52a1\u7684\u65b0\u9896\u6027\u548c\u6311\u6218\u6027\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03VStyle\u57fa\u51c6\u548c\u8bc4\u4f30\u5de5\u5177\u5305\uff0c\u4e3a\u63a8\u8fdb\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8bed\u97f3\u4ea4\u4e92\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2509.09699", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09699", "abs": "https://arxiv.org/abs/2509.09699", "authors": ["Mingyang Li", "Viktor Schlegel", "Tingting Mu", "Warren Del-Pinto", "Goran Nenadic"], "title": "Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs", "comment": null, "summary": "Mapping clinical documents to standardised clinical vocabularies is an\nimportant task, as it provides structured data for information retrieval and\nanalysis, which is essential to clinical research, hospital administration and\nimproving patient care. However, manual coding is both difficult and\ntime-consuming, making it impractical at scale. Automated coding can\npotentially alleviate this burden, improving the availability and accuracy of\nstructured clinical data. The task is difficult to automate, as it requires\nmapping to high-dimensional and long-tailed target spaces, such as the\nInternational Classification of Diseases (ICD). While external knowledge\nsources have been readily utilised to enhance output code representation, the\nuse of external resources for representing the input documents has been\nunderexplored. In this work, we compute a structured representation of the\ninput documents, making use of document-level knowledge graphs (KGs) that\nprovide a comprehensive structured view of a patient's condition. The resulting\nknowledge graph efficiently represents the patient-centred input documents with\n23\\% of the original text while retaining 90\\% of the information. We assess\nthe effectiveness of this graph for automated ICD-9 coding by integrating it\ninto the state-of-the-art ICD coding architecture PLM-ICD. Our experiments\nyield improved Macro-F1 scores by up to 3.20\\% on popular benchmarks, while\nimproving training efficiency. We attribute this improvement to different types\nof entities and relationships in the KG, and demonstrate the improved\nexplainability potential of the approach over the text-only baseline.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u77e5\u8bc6\u56fe\u8c31(KG)\u7ed3\u6784\u5316\u8868\u793a\u4e34\u5e8a\u6587\u6863\uff0c\u7528\u4e8e\u81ea\u52a8\u5316ICD\u7f16\u7801\u4efb\u52a1\uff0c\u5728\u4fdd\u630190%\u4fe1\u606f\u7684\u540c\u65f6\u51cf\u5c1123%\u6587\u672c\u91cf\uff0c\u5728PLM-ICD\u67b6\u6784\u4e0a\u5b9e\u73b0Macro-F1\u5206\u6570\u63d0\u53473.20%", "motivation": "\u4e34\u5e8a\u6587\u6863\u6807\u51c6\u5316\u7f16\u7801\u662f\u91cd\u8981\u4f46\u8017\u65f6\u7684\u4efb\u52a1\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5229\u7528\u5916\u90e8\u77e5\u8bc6\u589e\u5f3a\u8f93\u51fa\u8868\u793a\uff0c\u4f46\u5bf9\u8f93\u5165\u6587\u6863\u7684\u5916\u90e8\u77e5\u8bc6\u8868\u793a\u5229\u7528\u4e0d\u8db3", "method": "\u6784\u5efa\u6587\u6863\u7ea7\u77e5\u8bc6\u56fe\u8c31\u6765\u7ed3\u6784\u5316\u8868\u793a\u60a3\u8005\u72b6\u51b5\uff0c\u5c06KG\u96c6\u6210\u5230\u6700\u5148\u8fdb\u7684PLM-ICD\u7f16\u7801\u67b6\u6784\u4e2d", "result": "\u77e5\u8bc6\u56fe\u8c31\u752823%\u7684\u539f\u59cb\u6587\u672c\u4fdd\u7559\u4e8690%\u7684\u4fe1\u606f\uff0c\u5728ICD-9\u7f16\u7801\u4efb\u52a1\u4e0aMacro-F1\u5206\u6570\u63d0\u5347\u8fbe3.20%\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387", "conclusion": "\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u5316\u6587\u6863\u8868\u793a\u80fd\u6709\u6548\u63d0\u5347\u81ea\u52a8\u5316\u4e34\u5e8a\u7f16\u7801\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u6bd4\u7eaf\u6587\u672c\u57fa\u7ebf\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027"}}
{"id": "2509.09717", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09717", "abs": "https://arxiv.org/abs/2509.09717", "authors": ["Jorge E. Le\u00f3n", "Miguel Carrasco"], "title": "Testing chatbots on the creation of encoders for audio conditioned image generation", "comment": null, "summary": "On one hand, recent advances in chatbots has led to a rising popularity in\nusing these models for coding tasks. On the other hand, modern generative image\nmodels primarily rely on text encoders to translate semantic concepts into\nvisual representations, even when there is clear evidence that audio can be\nemployed as input as well. Given the previous, in this work, we explore whether\nstate-of-the-art conversational agents can design effective audio encoders to\nreplace the CLIP text encoder from Stable Diffusion 1.5, enabling image\nsynthesis directly from sound. We prompted five publicly available chatbots to\npropose neural architectures to work as these audio encoders, with a set of\nwell-explained shared conditions. Each valid suggested encoder was trained on\nover two million context related audio-image-text observations, and evaluated\non held-out validation and test sets using various metrics, together with a\nqualitative analysis of their generated images. Although almost all chatbots\ngenerated valid model designs, none achieved satisfactory results, indicating\nthat their audio embeddings failed to align reliably with those of the original\ntext encoder. Among the proposals, the Gemini audio encoder showed the best\nquantitative metrics, while the Grok audio encoder produced more coherent\nimages (particularly, when paired with the text encoder). Our findings reveal a\nshared architectural bias across chatbots and underscore the remaining coding\ngap that needs to be bridged in future versions of these models. We also\ncreated a public demo so everyone could study and try out these audio encoders.\nFinally, we propose research questions that should be tackled in the future,\nand encourage other researchers to perform more focused and highly specialized\ntasks like this one, so the respective chatbots cannot make use of well-known\nsolutions and their creativity/reasoning is fully tested.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u5229\u7528\u5bf9\u8bdd\u673a\u5668\u4eba\u8bbe\u8ba1\u97f3\u9891\u7f16\u7801\u5668\u66ff\u4ee3Stable Diffusion 1.5\u4e2d\u7684CLIP\u6587\u672c\u7f16\u7801\u5668\uff0c\u5b9e\u73b0\u4ece\u58f0\u97f3\u76f4\u63a5\u751f\u6210\u56fe\u50cf\uff0c\u4f46\u6240\u6709\u5bf9\u8bdd\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u5747\u672a\u8fbe\u5230\u6ee1\u610f\u6548\u679c\u3002", "motivation": "\u9274\u4e8e\u804a\u5929\u673a\u5668\u4eba\u5728\u7f16\u7801\u4efb\u52a1\u4e2d\u7684\u6d41\u884c\uff0c\u4ee5\u53ca\u73b0\u6709\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u7f16\u7801\u5668\uff0c\u672c\u7814\u7a76\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u5229\u7528\u6700\u5148\u8fdb\u7684\u5bf9\u8bdd\u4ee3\u7406\u8bbe\u8ba1\u6709\u6548\u7684\u97f3\u9891\u7f16\u7801\u5668\uff0c\u5b9e\u73b0\u4ece\u58f0\u97f3\u76f4\u63a5\u5408\u6210\u56fe\u50cf\u3002", "method": "\u63d0\u793a\u4e94\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u804a\u5929\u673a\u5668\u4eba\u63d0\u51fa\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4f5c\u4e3a\u97f3\u9891\u7f16\u7801\u5668\uff0c\u5728\u8d85\u8fc7200\u4e07\u4e2a\u76f8\u5173\u97f3\u9891-\u56fe\u50cf-\u6587\u672c\u89c2\u6d4b\u6570\u636e\u4e0a\u8bad\u7ec3\u6bcf\u4e2a\u6709\u6548\u5efa\u8bae\u7684\u7f16\u7801\u5668\uff0c\u5e76\u4f7f\u7528\u591a\u79cd\u6307\u6807\u5728\u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u867d\u7136\u51e0\u4e4e\u6240\u6709\u804a\u5929\u673a\u5668\u4eba\u90fd\u751f\u6210\u4e86\u6709\u6548\u7684\u6a21\u578b\u8bbe\u8ba1\uff0c\u4f46\u6ca1\u6709\u4e00\u4e2a\u8fbe\u5230\u4ee4\u4eba\u6ee1\u610f\u7684\u7ed3\u679c\uff0c\u8868\u660e\u5b83\u4eec\u7684\u97f3\u9891\u5d4c\u5165\u65e0\u6cd5\u4e0e\u539f\u59cb\u6587\u672c\u7f16\u7801\u5668\u7684\u5d4c\u5165\u53ef\u9760\u5bf9\u9f50\u3002Gemini\u97f3\u9891\u7f16\u7801\u5668\u5728\u5b9a\u91cf\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800cGrok\u97f3\u9891\u7f16\u7801\u5668\u751f\u6210\u66f4\u8fde\u8d2f\u7684\u56fe\u50cf\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u804a\u5929\u673a\u5668\u4eba\u5b58\u5728\u5171\u4eab\u7684\u67b6\u6784\u504f\u89c1\uff0c\u5e76\u5f3a\u8c03\u4e86\u8fd9\u4e9b\u6a21\u578b\u672a\u6765\u7248\u672c\u9700\u8981\u5f25\u5408\u7684\u7f16\u7801\u5dee\u8ddd\u3002\u5efa\u8bae\u672a\u6765\u7814\u7a76\u5e94\u4e13\u6ce8\u4e8e\u66f4\u4e13\u4e1a\u5316\u7684\u4efb\u52a1\uff0c\u4ee5\u5145\u5206\u6d4b\u8bd5\u804a\u5929\u673a\u5668\u4eba\u7684\u521b\u9020\u529b\u548c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2509.09700", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09700", "abs": "https://arxiv.org/abs/2509.09700", "authors": ["Malavika Suresh", "Rahaf Aljundi", "Ikechukwu Nkisi-Orji", "Nirmalie Wiratunga"], "title": "Cross-Layer Attention Probing for Fine-Grained Hallucination Detection", "comment": "To be published at the TRUST-AI workshop, ECAI 2025", "summary": "With the large-scale adoption of Large Language Models (LLMs) in various\napplications, there is a growing reliability concern due to their tendency to\ngenerate inaccurate text, i.e. hallucinations. In this work, we propose\nCross-Layer Attention Probing (CLAP), a novel activation probing technique for\nhallucination detection, which processes the LLM activations across the entire\nresidual stream as a joint sequence. Our empirical evaluations using five LLMs\nand three tasks show that CLAP improves hallucination detection compared to\nbaselines on both greedy decoded responses as well as responses sampled at\nhigher temperatures, thus enabling fine-grained detection, i.e. the ability to\ndisambiguate hallucinations and non-hallucinations among different sampled\nresponses to a given prompt. This allows us to propose a detect-then-mitigate\nstrategy using CLAP to reduce hallucinations and improve LLM reliability\ncompared to direct mitigation approaches. Finally, we show that CLAP maintains\nhigh reliability even when applied out-of-distribution.", "AI": {"tldr": "\u63d0\u51faCross-Layer Attention Probing (CLAP)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5904\u7406LLM\u6574\u4e2a\u6b8b\u5dee\u6d41\u7684\u6fc0\u6d3b\u4f5c\u4e3a\u8054\u5408\u5e8f\u5217\uff0c\u6539\u8fdb\u5e7b\u89c9\u68c0\u6d4b\u6548\u679c\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u68c0\u6d4b\u548c\u68c0\u6d4b\u540e\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u7684\u5927\u89c4\u6a21\u91c7\u7528\uff0c\u5176\u751f\u6210\u4e0d\u51c6\u786e\u6587\u672c\uff08\u5e7b\u89c9\uff09\u7684\u503e\u5411\u5f15\u53d1\u4e86\u53ef\u9760\u6027\u62c5\u5fe7\uff0c\u9700\u8981\u6709\u6548\u7684\u68c0\u6d4b\u548c\u7f13\u89e3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCLAP\u6fc0\u6d3b\u63a2\u6d4b\u6280\u672f\uff0c\u5c06LLM\u6574\u4e2a\u6b8b\u5dee\u6d41\u7684\u6fc0\u6d3b\u4f5c\u4e3a\u8054\u5408\u5e8f\u5217\u8fdb\u884c\u5904\u7406\uff0c\u7528\u4e8e\u5e7b\u89c9\u68c0\u6d4b\u3002\u652f\u6301\u5bf9\u8d2a\u5a6a\u89e3\u7801\u548c\u9ad8\u6e29\u91c7\u6837\u54cd\u5e94\u7684\u68c0\u6d4b\uff0c\u5e76\u80fd\u533a\u5206\u4e0d\u540c\u91c7\u6837\u54cd\u5e94\u4e2d\u7684\u5e7b\u89c9\u548c\u975e\u5e7b\u89c9\u3002", "result": "\u5728\u4e94\u4e2aLLM\u548c\u4e09\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0cCLAP\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u5e7b\u89c9\u68c0\u6d4b\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\uff0c\u5373\u4f7f\u5728\u5206\u5e03\u5916\u5e94\u7528\u65f6\u4ecd\u4fdd\u6301\u9ad8\u53ef\u9760\u6027\u3002", "conclusion": "CLAP\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u548c\u7f13\u89e3LLM\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u68c0\u6d4b\u540e\u7f13\u89e3\u7b56\u7565\u76f8\u6bd4\u76f4\u63a5\u7f13\u89e3\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u63d0\u9ad8LLM\u53ef\u9760\u6027\uff0c\u4e14\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.09746", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09746", "abs": "https://arxiv.org/abs/2509.09746", "authors": ["Ning Ma", "Bahman Mirheidari", "Guy J. Brown", "Minyoi M. Maimbolwa", "Nsala Sanjase", "Solomon Chifwamba", "Seke Muzazu", "Monde Muyoyeta", "Mary Kagujje"], "title": "AI-enabled tuberculosis screening in a high-burden setting using cough sound analysis and speech foundation models", "comment": "submitted to The Lancet Digital Health", "summary": "Background\n  Artificial intelligence (AI) can detect disease-related acoustic patterns in\ncough sounds, offering a scalable approach to tuberculosis (TB) screening in\nhigh-burden, low-resource settings. Previous studies have been limited by small\ndatasets, under-representation of symptomatic non-TB patients, reliance on\nsimple models, and recordings collected under idealised conditions.\n  Methods\n  We enrolled 512 participants at two hospitals in Zambia, grouped as\nbacteriologically confirmed TB (TB+), symptomatic patients with other\nrespiratory diseases (OR), and healthy controls (HC). Usable cough recordings\nplus demographic and clinical data were obtained from 500 participants. Deep\nlearning classifiers based on speech foundation models were trained on cough\nrecordings. The best-performing model, trained on 3-second segments, was\nfurther evaluated with demographic and clinical features.\n  Findings\n  The best audio-only classifier achieved an AUROC of 85.2% for distinguishing\nTB+ from all others (TB+/Rest) and 80.1% for TB+ versus OR. Adding demographic\nand clinical features improved performance to 92.1% (TB+/Rest) and 84.2%\n(TB+/OR). At a threshold of 0.38, the multimodal model reached 90.3%\nsensitivity and 73.1% specificity for TB+/Rest, and 80.6% and 73.1% for TB+/OR.\n  Interpretation\n  Cough analysis using speech foundation models, especially when combined with\ndemographic and clinical data, showed strong potential as a TB triage tool,\nmeeting WHO target product profile benchmarks. The model was robust to\nconfounding factors including background noise, recording time, and device\nvariability, indicating detection of genuine disease-related acoustic patterns.\nFurther validation across diverse regions and case definitions, including\nsubclinical TB, is required before clinical use.", "AI": {"tldr": "\u4f7f\u7528\u57fa\u4e8e\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u7684\u6df1\u5ea6\u5b66\u4e60\u5206\u6790\u54b3\u55fd\u58f0\uff0c\u7ed3\u5408\u4eba\u53e3\u7edf\u8ba1\u5b66\u548c\u4e34\u5e8a\u6570\u636e\uff0c\u80fd\u591f\u6709\u6548\u7b5b\u67e5\u7ed3\u6838\u75c5\uff0c\u8fbe\u5230WHO\u76ee\u6807\u6027\u80fd\u6807\u51c6\u3002", "motivation": "\u5728\u7ed3\u6838\u75c5\u9ad8\u8d1f\u62c5\u3001\u4f4e\u8d44\u6e90\u5730\u533a\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u7b5b\u67e5\u65b9\u6cd5\u3002\u4eba\u5de5\u667a\u80fd\u53ef\u4ee5\u901a\u8fc7\u5206\u6790\u54b3\u55fd\u58f0\u4e2d\u7684\u75be\u75c5\u76f8\u5173\u58f0\u5b66\u6a21\u5f0f\u6765\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u4f46\u4e4b\u524d\u7684\u7814\u7a76\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u5c0f\u3001\u6a21\u578b\u7b80\u5355\u7b49\u95ee\u9898\u3002", "method": "\u5728\u8d5e\u6bd4\u4e9a\u4e24\u5bb6\u533b\u9662\u62db\u52df512\u540d\u53c2\u4e0e\u8005\uff0c\u5206\u4e3a\u786e\u8bca\u7ed3\u6838\u75c5\u7ec4\u3001\u5176\u4ed6\u547c\u5438\u9053\u75be\u75c5\u7ec4\u548c\u5065\u5eb7\u5bf9\u7167\u7ec4\u3002\u4f7f\u7528\u57fa\u4e8e\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u7684\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u5668\u5206\u6790\u54b3\u55fd\u5f55\u97f3\uff0c\u5e76\u8fdb\u4e00\u6b65\u7ed3\u5408\u4eba\u53e3\u7edf\u8ba1\u5b66\u548c\u4e34\u5e8a\u7279\u5f81\u8fdb\u884c\u591a\u6a21\u6001\u5efa\u6a21\u3002", "result": "\u4ec5\u97f3\u9891\u5206\u7c7b\u5668\u5bf9TB+/Rest\u7684AUROC\u8fbe\u523085.2%\uff0cTB+/OR\u8fbe\u523080.1%\u3002\u52a0\u5165\u4eba\u53e3\u7edf\u8ba1\u5b66\u548c\u4e34\u5e8a\u7279\u5f81\u540e\uff0c\u6027\u80fd\u5206\u522b\u63d0\u5347\u81f392.1%\u548c84.2%\u3002\u591a\u6a21\u6001\u6a21\u578b\u5728TB+/Rest\u4e0a\u8fbe\u523090.3%\u654f\u611f\u6027\u548c73.1%\u7279\u5f02\u6027\u3002", "conclusion": "\u54b3\u55fd\u58f0\u5206\u6790\u7ed3\u5408\u4eba\u53e3\u7edf\u8ba1\u5b66\u548c\u4e34\u5e8a\u6570\u636e\u663e\u793a\u51fa\u4f5c\u4e3a\u7ed3\u6838\u75c5\u5206\u8bca\u5de5\u5177\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u6ee1\u8db3WHO\u76ee\u6807\u4ea7\u54c1\u6807\u51c6\u3002\u6a21\u578b\u5bf9\u80cc\u666f\u566a\u58f0\u3001\u5f55\u97f3\u65f6\u95f4\u548c\u8bbe\u5907\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u68c0\u6d4b\u5230\u771f\u5b9e\u7684\u75be\u75c5\u76f8\u5173\u58f0\u5b66\u6a21\u5f0f\u3002"}}
{"id": "2509.09701", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09701", "abs": "https://arxiv.org/abs/2509.09701", "authors": ["JungHo Jung", "Junhyun Lee"], "title": "Optimal Multi-Task Learning at Regularization Horizon for Speech Translation Task", "comment": null, "summary": "End-to-end speech-to-text translation typically suffers from the scarcity of\npaired speech-text data. One way to overcome this shortcoming is to utilize the\nbitext data from the Machine Translation (MT) task and perform Multi-Task\nLearning (MTL). In this paper, we formulate MTL from a regularization\nperspective and explore how sequences can be regularized within and across\nmodalities. By thoroughly investigating the effect of consistency\nregularization (different modality) and R-drop (same modality), we show how\nthey respectively contribute to the total regularization. We also demonstrate\nthat the coefficient of MT loss serves as another source of regularization in\nthe MTL setting. With these three sources of regularization, we introduce the\noptimal regularization contour in the high-dimensional space, called the\nregularization horizon. Experiments show that tuning the hyperparameters within\nthe regularization horizon achieves near state-of-the-art performance on the\nMuST-C dataset.", "AI": {"tldr": "\u672c\u6587\u4ece\u6b63\u5219\u5316\u89d2\u5ea6\u7814\u7a76\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u7684\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u63a2\u7d22\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u6b63\u5219\u5316\u548c\u540c\u6a21\u6001R-drop\u6b63\u5219\u5316\uff0c\u63d0\u51fa\u6b63\u5219\u5316\u5730\u5e73\u7ebf\u6982\u5ff5\u6765\u4f18\u5316\u8d85\u53c2\u6570\u914d\u7f6e\u3002", "motivation": "\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\u9762\u4e34\u914d\u5bf9\u8bed\u97f3-\u6587\u672c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e0c\u671b\u901a\u8fc7\u5229\u7528\u673a\u5668\u7ffb\u8bd1\u7684\u53cc\u8bed\u6570\u636e\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u6765\u514b\u670d\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u4ece\u6b63\u5219\u5316\u89c6\u89d2\u6784\u5efa\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u7814\u7a76\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u6b63\u5219\u5316\u548c\u540c\u6a21\u6001R-drop\u6b63\u5219\u5316\uff0c\u5206\u6790MT\u635f\u5931\u7cfb\u6570\u4f5c\u4e3a\u6b63\u5219\u5316\u6e90\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u6b63\u5219\u5316\u5730\u5e73\u7ebf\u6982\u5ff5\u6765\u6307\u5bfc\u8d85\u53c2\u6570\u8c03\u4f18\u3002", "result": "\u5728MuST-C\u6570\u636e\u96c6\u4e0a\uff0c\u901a\u8fc7\u5728\u6b63\u5219\u5316\u5730\u5e73\u7ebf\u5185\u8c03\u4f18\u8d85\u53c2\u6570\uff0c\u8fbe\u5230\u4e86\u63a5\u8fd1\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u4e09\u79cd\u6b63\u5219\u5316\u6e90\uff08\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u3001\u540c\u6a21\u6001R-drop\u3001MT\u635f\u5931\u7cfb\u6570\uff09\u5171\u540c\u6784\u6210\u4e86\u6709\u6548\u7684\u6b63\u5219\u5316\u673a\u5236\uff0c\u6b63\u5219\u5316\u5730\u5e73\u7ebf\u4e3a\u8d85\u53c2\u6570\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u6846\u67b6\u3002"}}
{"id": "2509.09748", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09748", "abs": "https://arxiv.org/abs/2509.09748", "authors": ["Yanru Huo", "Ziyue Jiang", "Zuoli Tang", "Qingyang Hong", "Zhou Zhao"], "title": "DiTReducio: A Training-Free Acceleration for DiT-Based TTS via Progressive Calibration", "comment": null, "summary": "While Diffusion Transformers (DiT) have advanced non-autoregressive (NAR)\nspeech synthesis, their high computational demands remain an limitation.\nExisting DiT-based text-to-speech (TTS) model acceleration approaches mainly\nfocus on reducing sampling steps through distillation techniques, yet they\nremain constrained by training costs. We introduce DiTReducio, a training-free\nacceleration framework that compresses computations in DiT-based TTS models via\nprogressive calibration. We propose two compression methods, Temporal Skipping\nand Branch Skipping, to eliminate redundant computations during inference.\nMoreover, based on two characteristic attention patterns identified within DiT\nlayers, we devise a pattern-guided strategy to selectively apply the\ncompression methods. Our method allows flexible modulation between generation\nquality and computational efficiency through adjustable compression thresholds.\nExperimental evaluations conducted on F5-TTS and MegaTTS 3 demonstrate that\nDiTReducio achieves a 75.4% reduction in FLOPs and improves the Real-Time\nFactor (RTF) by 37.1%, while preserving generation quality.", "AI": {"tldr": "DiTReducio\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u8df3\u8fc7\u548c\u5206\u652f\u8df3\u8fc7\u4e24\u79cd\u538b\u7f29\u65b9\u6cd5\uff0c\u7ed3\u5408\u6a21\u5f0f\u5f15\u5bfc\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4eDiT-based TTS\u6a21\u578b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u6269\u6563\u53d8\u6362\u5668(DiT)\u5728\u975e\u81ea\u56de\u5f52\u8bed\u97f3\u5408\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u9700\u6c42\u9ad8\u3002\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u84b8\u998f\u6280\u672f\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\uff0c\u4f46\u4ecd\u53d7\u8bad\u7ec3\u6210\u672c\u9650\u5236\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u52a0\u901f\u65b9\u6848\u3002", "method": "\u63d0\u51faDiTReducio\u6846\u67b6\uff1a1) \u65f6\u95f4\u8df3\u8fc7\u548c\u5206\u652f\u8df3\u8fc7\u4e24\u79cd\u8ba1\u7b97\u538b\u7f29\u65b9\u6cd5\uff1b2) \u57fa\u4e8eDiT\u5c42\u4e2d\u53d1\u73b0\u7684\u4e24\u79cd\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u8bbe\u8ba1\u6a21\u5f0f\u5f15\u5bfc\u7b56\u7565\u9009\u62e9\u6027\u5e94\u7528\u538b\u7f29\uff1b3) \u901a\u8fc7\u53ef\u8c03\u538b\u7f29\u9608\u503c\u7075\u6d3b\u5e73\u8861\u751f\u6210\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728F5-TTS\u548cMegaTTS 3\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1aFLOPs\u51cf\u5c1175.4%\uff0c\u5b9e\u65f6\u56e0\u5b50(RTF)\u63d0\u534737.1%\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "DiTReducio\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u52a0\u901f\u65b9\u6848\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u6821\u51c6\u548c\u6a21\u5f0f\u611f\u77e5\u538b\u7f29\uff0c\u663e\u8457\u63d0\u5347\u4e86DiT-based TTS\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2509.09702", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.09702", "abs": "https://arxiv.org/abs/2509.09702", "authors": ["Ninad Bhat", "Kieran Browne", "Pip Bingemann"], "title": "Creativity Benchmark: A benchmark for marketing creativity for LLM models", "comment": "30 Pages, 14 figures", "summary": "We introduce Creativity Benchmark, an evaluation framework for large language\nmodels (LLMs) in marketing creativity. The benchmark covers 100 brands (12\ncategories) and three prompt types (Insights, Ideas, Wild Ideas). Human\npairwise preferences from 678 practising creatives over 11,012 anonymised\ncomparisons, analysed with Bradley-Terry models, show tightly clustered\nperformance with no model dominating across brands or prompt types: the\ntop-bottom spread is $\\Delta\\theta \\approx 0.45$, which implies a head-to-head\nwin probability of $0.61$; the highest-rated model beats the lowest only about\n$61\\%$ of the time. We also analyse model diversity using cosine distances to\ncapture intra- and inter-model variation and sensitivity to prompt reframing.\nComparing three LLM-as-judge setups with human rankings reveals weak,\ninconsistent correlations and judge-specific biases, underscoring that\nautomated judges cannot substitute for human evaluation. Conventional\ncreativity tests also transfer only partially to brand-constrained tasks.\nOverall, the results highlight the need for expert human evaluation and\ndiversity-aware workflows.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Creativity Benchmark\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8425\u9500\u521b\u610f\u751f\u6210\u65b9\u9762\u7684\u8868\u73b0\u3002\u901a\u8fc7678\u540d\u4e13\u4e1a\u521b\u610f\u4eba\u5458\u768411,012\u6b21\u533f\u540d\u6bd4\u8f83\uff0c\u53d1\u73b0\u5404\u6a21\u578b\u8868\u73b0\u76f8\u8fd1\uff0c\u6ca1\u6709\u660e\u663e\u4f18\u52bf\u6a21\u578b\uff0c\u4e14\u81ea\u52a8\u8bc4\u4f30\u65e0\u6cd5\u66ff\u4ee3\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u4f30\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u521b\u610f\u751f\u6210\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u9700\u8981\u5efa\u7acb\u53ef\u9760\u7684\u8bc4\u4f30\u6807\u51c6\u6765\u6d4b\u8bd5\u6a21\u578b\u5728\u8425\u9500\u521b\u610f\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u54c1\u724c\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u521b\u610f\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b100\u4e2a\u54c1\u724c\uff0812\u4e2a\u7c7b\u522b\uff09\u548c\u4e09\u79cd\u63d0\u793a\u7c7b\u578b\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6536\u96c6678\u540d\u4e13\u4e1a\u521b\u610f\u4eba\u5458\u768411,012\u6b21\u533f\u540d\u6210\u5bf9\u504f\u597d\u6570\u636e\uff0c\u4f7f\u7528Bradley-Terry\u6a21\u578b\u5206\u6790\uff0c\u5e76\u8ba1\u7b97\u4f59\u5f26\u8ddd\u79bb\u6765\u8bc4\u4f30\u6a21\u578b\u591a\u6837\u6027\u548c\u5bf9\u63d0\u793a\u91cd\u6784\u7684\u654f\u611f\u6027\u3002", "result": "\u5404\u6a21\u578b\u8868\u73b0\u7d27\u5bc6\u805a\u96c6\uff0c\u6ca1\u6709\u5355\u4e00\u6a21\u578b\u5728\u6240\u6709\u54c1\u724c\u6216\u63d0\u793a\u7c7b\u578b\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff08\u6700\u9ad8\u4e0e\u6700\u4f4e\u6a21\u578b\u7684\u5bf9\u6218\u80dc\u7387\u4ec5\u4e3a61%\uff09\u3002\u81ea\u52a8\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u6392\u540d\u76f8\u5173\u6027\u5f31\u4e14\u4e0d\u4e00\u81f4\uff0c\u4f20\u7edf\u521b\u9020\u529b\u6d4b\u8bd5\u5728\u54c1\u724c\u7ea6\u675f\u4efb\u52a1\u4e2d\u4ec5\u90e8\u5206\u6709\u6548\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u4e13\u5bb6\u4eba\u7c7b\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u53ca\u9700\u8981\u91c7\u7528\u591a\u6837\u6027\u611f\u77e5\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u76ee\u524d\u65e0\u6cd5\u66ff\u4ee3\u4eba\u7c7b\u4e13\u4e1a\u5224\u65ad\u3002"}}
{"id": "2509.09752", "categories": ["cs.SD", "cs.CY", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09752", "abs": "https://arxiv.org/abs/2509.09752", "authors": ["Abdullah All Tanvir", "Chenyu Huang", "Moe Alahmad", "Chuyang Yang", "Xin Zhong"], "title": "Combining Textual and Spectral Features for Robust Classification of Pilot Communications", "comment": null, "summary": "Accurate estimation of aircraft operations, such as takeoffs and landings, is\ncritical for effective airport management, yet remains challenging, especially\nat non-towered facilities lacking dedicated surveillance infrastructure. This\npaper presents a novel dual pipeline machine learning framework that classifies\npilot radio communications using both textual and spectral features. Audio data\ncollected from a non-towered U.S. airport was annotated by certified pilots\nwith operational intent labels and preprocessed through automatic speech\nrecognition and Mel-spectrogram extraction. We evaluate a wide range of\ntraditional classifiers and deep learning models, including ensemble methods,\nLSTM, and CNN across both pipelines. To our knowledge, this is the first system\nto classify operational aircraft intent using a dual-pipeline ML framework on\nreal-world air traffic audio. Our results demonstrate that spectral features\ncombined with deep architectures consistently yield superior classification\nperformance, with F1-scores exceeding 91%. Data augmentation further improves\nrobustness to real-world audio variability. The proposed approach is scalable,\ncost-effective, and deployable without additional infrastructure, offering a\npractical solution for air traffic monitoring at general aviation airports.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53cc\u7ba1\u9053\u673a\u5668\u5b66\u4e60\u6846\u67b6\u7684\u98de\u673a\u64cd\u4f5c\u610f\u56fe\u5206\u7c7b\u7cfb\u7edf\uff0c\u7ed3\u5408\u6587\u672c\u548c\u9891\u8c31\u7279\u5f81\u5206\u6790\u98de\u884c\u5458\u65e0\u7ebf\u7535\u901a\u4fe1\uff0c\u5728\u975e\u5854\u53f0\u673a\u573a\u5b9e\u73b091%\u4ee5\u4e0aF1\u5206\u6570\u7684\u51c6\u786e\u5206\u7c7b", "motivation": "\u975e\u5854\u53f0\u673a\u573a\u7f3a\u4e4f\u4e13\u7528\u76d1\u63a7\u57fa\u7840\u8bbe\u65bd\uff0c\u98de\u673a\u8d77\u964d\u64cd\u4f5c\u4f30\u8ba1\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u4e14\u53ef\u6269\u5c55\u7684\u76d1\u63a7\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u7528\u53cc\u7ba1\u9053\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff1a\u6587\u672c\u7ba1\u9053\uff08\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff09\u548c\u9891\u8c31\u7ba1\u9053\uff08\u6885\u5c14\u9891\u8c31\u56fe\u63d0\u53d6\uff09\uff0c\u8bc4\u4f30\u4f20\u7edf\u5206\u7c7b\u5668\u3001\u96c6\u6210\u65b9\u6cd5\u3001LSTM\u548cCNN\u6a21\u578b", "result": "\u9891\u8c31\u7279\u5f81\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u8868\u73b0\u6700\u4f73\uff0cF1\u5206\u6570\u8d85\u8fc791%\uff0c\u6570\u636e\u589e\u5f3a\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u5bf9\u771f\u5b9e\u97f3\u9891\u53d8\u5316\u7684\u9c81\u68d2\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u57fa\u7840\u8bbe\u65bd\u5373\u53ef\u90e8\u7f72\uff0c\u4e3a\u901a\u7528\u822a\u7a7a\u673a\u573a\u7a7a\u4e2d\u4ea4\u901a\u76d1\u63a7\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09703", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09703", "abs": "https://arxiv.org/abs/2509.09703", "authors": ["Zhenhua Xu", "Xixiang Zhao", "Xubin Yue", "Shengwei Tian", "Changting Lin", "Meng Han"], "title": "CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor", "comment": "Accepted by EMNLP2025 MainConference", "summary": "The widespread deployment of large language models (LLMs) has intensified\nconcerns around intellectual property (IP) protection, as model theft and\nunauthorized redistribution become increasingly feasible. To address this,\nmodel fingerprinting aims to embed verifiable ownership traces into LLMs.\nHowever, existing methods face inherent trade-offs between stealthness,\nrobustness, and generalizability, being either detectable via distributional\nshifts, vulnerable to adversarial modifications, or easily invalidated once the\nfingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven\nfingerprinting framework that encodes contextual correlations across multiple\ndialogue turns, such as counterfactual, rather than relying on token-level or\nsingle-turn triggers. CTCC enables fingerprint verification under black-box\naccess while mitigating false positives and fingerprint leakage, supporting\ncontinuous construction under a shared semantic rule even if partial triggers\nare exposed. Extensive experiments across multiple LLM architectures\ndemonstrate that CTCC consistently achieves stronger stealth and robustness\nthan prior work. Our findings position CTCC as a reliable and practical\nsolution for ownership verification in real-world LLM deployment scenarios. Our\ncode and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.", "AI": {"tldr": "CTCC\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u8bed\u8a00\u6a21\u578b\u6307\u7eb9\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8f6e\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u5173\u8054\u7f16\u7801\u6765\u5b9e\u73b0\u9690\u853d\u4e14\u9c81\u68d2\u7684\u6240\u6709\u6743\u9a8c\u8bc1", "motivation": "\u89e3\u51b3\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u6307\u7eb9\u65b9\u6cd5\u5728\u9690\u853d\u6027\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u9632\u6b62\u6a21\u578b\u88ab\u76d7\u7528\u548c\u672a\u7ecf\u6388\u6743\u7684\u91cd\u65b0\u5206\u53d1", "method": "\u91c7\u7528\u89c4\u5219\u9a71\u52a8\u7684\u6307\u7eb9\u6846\u67b6\uff0c\u7f16\u7801\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u4e0a\u4e0b\u6587\u5173\u8054\uff08\u5982\u53cd\u4e8b\u5b9e\u5173\u7cfb\uff09\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u8bcd\u7ea7\u6216\u5355\u8f6e\u89e6\u53d1\u673a\u5236", "result": "\u5728\u591a\u79cdLLM\u67b6\u6784\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCTCC\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u5f3a\u7684\u9690\u853d\u6027\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u8bef\u62a5\u548c\u6307\u7eb9\u6cc4\u9732", "conclusion": "CTCC\u4e3a\u73b0\u5b9e\u4e16\u754cLLM\u90e8\u7f72\u573a\u666f\u4e2d\u7684\u6240\u6709\u6743\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9760\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09823", "categories": ["cs.SD", "cs.AI", "cs.ET", "cs.HC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09823", "abs": "https://arxiv.org/abs/2509.09823", "authors": ["Yixuan Gao", "Tanvir Ahmed", "Shuang He", "Zhongqi Cheng", "Rajalakshmi Nandakumar"], "title": "SoilSound: Smartphone-based Soil Moisture Estimation", "comment": "12 pages, 8 figures", "summary": "Soil moisture monitoring is essential for agriculture and environmental\nmanagement, yet existing methods require either invasive probes disturbing the\nsoil or specialized equipment, limiting access to the public. We present\nSoilSound, an ubiquitous accessible smartphone-based acoustic sensing system\nthat can measure soil moisture without disturbing the soil. We leverage the\nbuilt-in speaker and microphone to perform a vertical scan mechanism to\naccurately measure moisture without any calibration. Unlike existing work that\nuse transmissive properties, we propose an alternate model for acoustic\nreflections in soil based on the surface roughness effect to enable moisture\nsensing without disturbing the soil. The system works by sending acoustic\nchirps towards the soil and recording the reflections during a vertical scan,\nwhich are then processed and fed to a convolutional neural network for\non-device soil moisture estimation with negligible computational, memory, or\npower overhead. We evaluated the system by training with curated soils in boxes\nin the lab and testing in the outdoor fields and show that SoilSound achieves a\nmean absolute error (MAE) of 2.39% across 10 different locations. Overall, the\nevaluation shows that SoilSound can accurately track soil moisture levels\nranging from 15.9% to 34.0% across multiple soil types, environments, and\nusers; without requiring any calibration or disturbing the soil, enabling\nwidespread moisture monitoring for home gardeners, urban farmers, citizen\nscientists, and agricultural communities in resource-limited settings.", "AI": {"tldr": "SoilSound\u662f\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u7684\u58f0\u5b66\u4f20\u611f\u7cfb\u7edf\uff0c\u901a\u8fc7\u5185\u7f6e\u626c\u58f0\u5668\u548c\u9ea6\u514b\u98ce\u8fdb\u884c\u5782\u76f4\u626b\u63cf\uff0c\u65e0\u9700\u6821\u51c6\u5373\u53ef\u975e\u4fb5\u5165\u5f0f\u6d4b\u91cf\u571f\u58e4\u6e7f\u5ea6\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a2.39%\u3002", "motivation": "\u73b0\u6709\u571f\u58e4\u6e7f\u5ea6\u76d1\u6d4b\u65b9\u6cd5\u9700\u8981\u4fb5\u5165\u5f0f\u63a2\u5934\u6216\u4e13\u4e1a\u8bbe\u5907\uff0c\u9650\u5236\u4e86\u516c\u4f17\u4f7f\u7528\u3002\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u6270\u52a8\u571f\u58e4\u3001\u6613\u4e8e\u83b7\u53d6\u7684\u76d1\u6d4b\u65b9\u6848\u3002", "method": "\u5229\u7528\u667a\u80fd\u624b\u673a\u5185\u7f6e\u626c\u58f0\u5668\u548c\u9ea6\u514b\u98ce\uff0c\u901a\u8fc7\u53d1\u9001\u58f0\u5b66\u5541\u557e\u4fe1\u53f7\u5e76\u8bb0\u5f55\u53cd\u5c04\uff0c\u57fa\u4e8e\u8868\u9762\u7c97\u7cd9\u5ea6\u6548\u5e94\u7684\u58f0\u5b66\u53cd\u5c04\u6a21\u578b\uff0c\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u8bbe\u5907\u7aef\u6e7f\u5ea6\u4f30\u8ba1\u3002", "result": "\u572810\u4e2a\u4e0d\u540c\u5730\u70b9\u6d4b\u8bd5\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a2.39%\uff0c\u80fd\u591f\u51c6\u786e\u8ddf\u8e2a15.9%\u523034.0%\u7684\u571f\u58e4\u6e7f\u5ea6\u8303\u56f4\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u571f\u58e4\u7c7b\u578b\u548c\u73af\u5883\u3002", "conclusion": "SoilSound\u65e0\u9700\u6821\u51c6\u6216\u6270\u52a8\u571f\u58e4\u5373\u53ef\u5b9e\u73b0\u51c6\u786e\u7684\u571f\u58e4\u6e7f\u5ea6\u76d1\u6d4b\uff0c\u4e3a\u5bb6\u5ead\u56ed\u4e01\u3001\u57ce\u5e02\u519c\u6c11\u548c\u8d44\u6e90\u6709\u9650\u5730\u533a\u7684\u519c\u4e1a\u793e\u533a\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u53ef\u7528\u7684\u76d1\u6d4b\u65b9\u6848\u3002"}}
{"id": "2509.09704", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.09704", "abs": "https://arxiv.org/abs/2509.09704", "authors": ["Ali Mazyaki", "Mohammad Naghizadeh", "Samaneh Ranjkhah Zonouzaghi", "Hossein Setareh"], "title": "Temporal Preferences in Language Models for Long-Horizon Assistance", "comment": null, "summary": "We study whether language models (LMs) exhibit future- versus\npresent-oriented preferences in intertemporal choice and whether those\npreferences can be systematically manipulated. Using adapted human experimental\nprotocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them\nagainst a sample of human decision makers. We introduce an operational metric,\nthe Manipulability of Time Orientation (MTO), defined as the change in an LM's\nrevealed time preference between future- and present-oriented prompts. In our\ntests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini)\nchoose later options under future-oriented prompts but only partially\npersonalize decisions across identities or geographies. Moreover, models that\ncorrectly reason about time orientation internalize a future orientation for\nthemselves as AI decision makers. We discuss design implications for AI\nassistants that should align with heterogeneous, long-horizon goals and outline\na research agenda on personalized contextual calibration and socially aware\ndeployment.", "AI": {"tldr": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u671f\u9009\u62e9\u4e2d\u662f\u5426\u8868\u73b0\u51fa\u672a\u6765\u5bfc\u5411\u504f\u597d\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u504f\u597d\u662f\u5426\u53ef\u88ab\u7cfb\u7edf\u6027\u64cd\u7eb5\u3002\u901a\u8fc7\u5f15\u5165MTO\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u7684\u65f6\u95f4\u504f\u597d\u53ef\u64cd\u7eb5\u6027\uff0c\u53d1\u73b0\u63a8\u7406\u578b\u6a21\u578b\u5728\u672a\u6765\u5bfc\u5411\u63d0\u793a\u4e0b\u66f4\u503e\u5411\u4e8e\u9009\u62e9\u5ef6\u8fdf\u9009\u9879\u3002", "motivation": "\u63a2\u7a76\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u504f\u597d\u7279\u5f81\u53ca\u5176\u53ef\u64cd\u7eb5\u6027\uff0c\u4e3aAI\u52a9\u624b\u4e0e\u4eba\u7c7b\u5f02\u8d28\u6027\u957f\u671f\u76ee\u6807\u7684\u5bf9\u9f50\u63d0\u4f9b\u8bbe\u8ba1\u4f9d\u636e\uff0c\u63a8\u52a8\u4e2a\u6027\u5316\u60c5\u5883\u6821\u51c6\u548c\u793e\u4f1a\u610f\u8bc6\u90e8\u7f72\u7684\u7814\u7a76\u8bae\u7a0b\u3002", "method": "\u91c7\u7528\u6539\u7f16\u7684\u4eba\u7c7b\u5b9e\u9a8c\u534f\u8bae\uff0c\u5728\u65f6\u95f4\u6743\u8861\u4efb\u52a1\u4e2d\u8bc4\u4f30\u591a\u4e2a\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u4e0e\u4eba\u7c7b\u51b3\u7b56\u8005\u8fdb\u884c\u57fa\u51c6\u6bd4\u8f83\u3002\u5b9a\u4e49MTO\uff08\u65f6\u95f4\u5bfc\u5411\u53ef\u64cd\u7eb5\u6027\uff09\u6307\u6807\u6765\u8861\u91cf\u6a21\u578b\u5728\u9762\u5411\u672a\u6765\u548c\u9762\u5411\u5f53\u524d\u63d0\u793a\u4e0b\u65f6\u95f4\u504f\u597d\u7684\u53d8\u5316\u3002", "result": "\u63a8\u7406\u578b\u6a21\u578b\uff08\u5982DeepSeek-Reasoner\u548cgrok-3-mini\uff09\u5728\u672a\u6765\u5bfc\u5411\u63d0\u793a\u4e0b\u66f4\u503e\u5411\u4e8e\u9009\u62e9\u5ef6\u8fdf\u9009\u9879\uff0c\u4f46\u5728\u8de8\u8eab\u4efd\u6216\u5730\u7406\u4f4d\u7f6e\u7684\u4e2a\u6027\u5316\u51b3\u7b56\u65b9\u9762\u8868\u73b0\u6709\u9650\u3002\u80fd\u591f\u6b63\u786e\u63a8\u7406\u65f6\u95f4\u5bfc\u5411\u7684\u6a21\u578b\u4f1a\u5185\u5316\u4f5c\u4e3aAI\u51b3\u7b56\u8005\u7684\u672a\u6765\u5bfc\u5411\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u65f6\u95f4\u504f\u597d\u7684\u53ef\u64cd\u7eb5\u6027\u7279\u5f81\uff0c\u5f3a\u8c03\u4e86AI\u52a9\u624b\u8bbe\u8ba1\u9700\u8981\u4e0e\u5f02\u8d28\u6027\u957f\u671f\u76ee\u6807\u5bf9\u9f50\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e2a\u6027\u5316\u60c5\u5883\u6821\u51c6\u548c\u793e\u4f1a\u610f\u8bc6\u90e8\u7f72\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2509.09836", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09836", "abs": "https://arxiv.org/abs/2509.09836", "authors": ["Marco Pasini", "Stefan Lattner", "George Fazekas"], "title": "CoDiCodec: Unifying Continuous and Discrete Compressed Representations of Audio", "comment": "Accepted to ISMIR 2025", "summary": "Efficiently representing audio signals in a compressed latent space is\ncritical for latent generative modelling. However, existing autoencoders often\nforce a choice between continuous embeddings and discrete tokens. Furthermore,\nachieving high compression ratios while maintaining audio fidelity remains a\nchallenge. We introduce CoDiCodec, a novel audio autoencoder that overcomes\nthese limitations by both efficiently encoding global features via summary\nembeddings, and by producing both compressed continuous embeddings at ~ 11 Hz\nand discrete tokens at a rate of 2.38 kbps from the same trained model,\noffering unprecedented flexibility for different downstream generative tasks.\nThis is achieved through Finite Scalar Quantization (FSQ) and a novel\nFSQ-dropout technique, and does not require additional loss terms beyond the\nsingle consistency loss used for end-to-end training. CoDiCodec supports both\nautoregressive decoding and a novel parallel decoding strategy, with the latter\nachieving superior audio quality and faster decoding. CoDiCodec outperforms\nexisting continuous and discrete autoencoders at similar bitrates in terms of\nreconstruction audio quality. Our work enables a unified approach to audio\ncompression, bridging the gap between continuous and discrete generative\nmodelling paradigms.", "AI": {"tldr": "CoDiCodec\u662f\u4e00\u79cd\u65b0\u9896\u7684\u97f3\u9891\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u6709\u9650\u6807\u91cf\u91cf\u5316\u548cFSQ-dropout\u6280\u672f\uff0c\u5728\u540c\u4e00\u6a21\u578b\u4e2d\u540c\u65f6\u751f\u6210\u538b\u7f29\u8fde\u7eed\u5d4c\u5165\u548c\u79bb\u6563\u6807\u8bb0\uff0c\u5728\u4fdd\u6301\u97f3\u9891\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u538b\u7f29\u6bd4\u3002", "motivation": "\u73b0\u6709\u7684\u97f3\u9891\u81ea\u7f16\u7801\u5668\u5f80\u5f80\u9700\u8981\u5728\u8fde\u7eed\u5d4c\u5165\u548c\u79bb\u6563\u6807\u8bb0\u4e4b\u95f4\u505a\u51fa\u9009\u62e9\uff0c\u4e14\u5728\u4fdd\u6301\u97f3\u9891\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u538b\u7f29\u6bd4\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u91c7\u7528\u6709\u9650\u6807\u91cf\u91cf\u5316(FSQ)\u548c\u65b0\u578bFSQ-dropout\u6280\u672f\uff0c\u901a\u8fc7\u5355\u4e00\u4e00\u81f4\u6027\u635f\u5931\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u652f\u6301\u81ea\u56de\u5f52\u89e3\u7801\u548c\u5e76\u884c\u89e3\u7801\u7b56\u7565\u3002", "result": "\u5728\u76f8\u4f3c\u6bd4\u7279\u7387\u4e0b\uff0cCoDiCodec\u5728\u91cd\u5efa\u97f3\u9891\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u8fde\u7eed\u548c\u79bb\u6563\u81ea\u7f16\u7801\u5668\uff0c\u5e76\u884c\u89e3\u7801\u7b56\u7565\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u97f3\u9891\u8d28\u91cf\u548c\u66f4\u5feb\u7684\u89e3\u7801\u901f\u5ea6\u3002", "conclusion": "CoDiCodec\u4e3a\u97f3\u9891\u538b\u7f29\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u65b9\u6cd5\uff0c\u5f25\u5408\u4e86\u8fde\u7eed\u548c\u79bb\u6563\u751f\u6210\u5efa\u6a21\u8303\u5f0f\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.09705", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09705", "abs": "https://arxiv.org/abs/2509.09705", "authors": ["Claudio Pinhanez", "Paulo Cavalin", "Cassia Sanctos", "Marcelo Grave", "Yago Primerano"], "title": "The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in Repetition Trials of Standard Multiple-Choice Benchmarks", "comment": null, "summary": "This work explores the consistency of small LLMs (2B-8B parameters) in\nanswering multiple times the same question. We present a study on known,\nopen-source LLMs responding to 10 repetitions of questions from the\nmultiple-choice benchmarks MMLU-Redux and MedQA, considering different\ninference temperatures, small vs. medium models (50B-80B), finetuned vs. base\nmodels, and other parameters. We also look into the effects of requiring\nmulti-trial answer consistency on accuracy and the trade-offs involved in\ndeciding which model best provides both of them. To support those studies, we\npropose some new analytical and graphical tools. Results show that the number\nof questions which can be answered consistently vary considerably among models\nbut are typically in the 50%-80% range for small models at low inference\ntemperatures. Also, accuracy among consistent answers seems to reasonably\ncorrelate with overall accuracy. Results for medium-sized models seem to\nindicate much higher levels of answer consistency.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5c0f\u578bLLM\uff082B-8B\u53c2\u6570\uff09\u5728\u591a\u6b21\u56de\u7b54\u76f8\u540c\u95ee\u9898\u65f6\u7684\u7b54\u6848\u4e00\u81f4\u6027\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u6e29\u5ea6\u8bbe\u7f6e\u3001\u6a21\u578b\u5927\u5c0f\u3001\u5fae\u8c03\u72b6\u6001\u7b49\u56e0\u7d20\u5bf9\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u8bc4\u4f30\u5c0f\u578bLLM\u5728\u91cd\u590d\u56de\u7b54\u76f8\u540c\u95ee\u9898\u65f6\u7684\u8868\u73b0\u4e00\u81f4\u6027\uff0c\u4e3a\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u53c2\u8003\u4f9d\u636e\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7a33\u5b9a\u8f93\u51fa\u7684\u5e94\u7528\u573a\u666f\u4e2d\u3002", "method": "\u4f7f\u7528\u5f00\u6e90LLM\u5bf9MMLU-Redux\u548cMedQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u95ee\u9898\u8fdb\u884c10\u6b21\u91cd\u590d\u56de\u7b54\uff0c\u5206\u6790\u4e0d\u540c\u63a8\u7406\u6e29\u5ea6\u3001\u6a21\u578b\u89c4\u6a21\uff082B-8B vs 50B-80B\uff09\u3001\u5fae\u8c03\u72b6\u6001\u7b49\u53c2\u6570\u7684\u5f71\u54cd\uff0c\u5e76\u5f00\u53d1\u4e86\u65b0\u7684\u5206\u6790\u548c\u53ef\u89c6\u5316\u5de5\u5177\u3002", "result": "\u5c0f\u578b\u6a21\u578b\u5728\u4f4e\u63a8\u7406\u6e29\u5ea6\u4e0b\uff0c\u80fd\u591f\u4e00\u81f4\u56de\u7b54\u7684\u95ee\u9898\u6bd4\u4f8b\u901a\u5e38\u572850%-80%\u4e4b\u95f4\uff0c\u4e00\u81f4\u7b54\u6848\u7684\u51c6\u786e\u6027\u4e0e\u6574\u4f53\u51c6\u786e\u6027\u6709\u5408\u7406\u76f8\u5173\u6027\u3002\u4e2d\u578b\u6a21\u578b\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u7b54\u6848\u4e00\u81f4\u6027\u6c34\u5e73\u3002", "conclusion": "\u6a21\u578b\u7684\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u5b58\u5728\u6743\u8861\u5173\u7cfb\uff0c\u4e0d\u540c\u6a21\u578b\u5728\u4e00\u81f4\u6027\u8868\u73b0\u4e0a\u5dee\u5f02\u663e\u8457\uff0c\u4e2d\u578b\u6a21\u578b\u901a\u5e38\u6bd4\u5c0f\u578b\u6a21\u578b\u5177\u6709\u66f4\u597d\u7684\u4e00\u81f4\u6027\u8868\u73b0\uff0c\u8fd9\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2509.10074", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10074", "abs": "https://arxiv.org/abs/2509.10074", "authors": ["Christos Sgouropoulos", "Christos Nikou", "Stefanos Vlachos", "Vasileios Theiou", "Christos Foukanelis", "Theodoros Giannakopoulos"], "title": "Prototypical Contrastive Learning For Improved Few-Shot Audio Classification", "comment": "Accepted and Presented at IEEE International Workshop on Machine\n  Learning for Signal Processing, Aug.\\ 31-- Sep.\\ 3, 2025, Istanbul, Turkey ,\n  6 pages, 2 figures, 1 table", "summary": "Few-shot learning has emerged as a powerful paradigm for training models with\nlimited labeled data, addressing challenges in scenarios where large-scale\nannotation is impractical. While extensive research has been conducted in the\nimage domain, few-shot learning in audio classification remains relatively\nunderexplored. In this work, we investigate the effect of integrating\nsupervised contrastive loss into prototypical few shot training for audio\nclassification. In detail, we demonstrate that angular loss further improves\nthe performance compared to the standard contrastive loss. Our method leverages\nSpecAugment followed by a self-attention mechanism to encapsulate diverse\ninformation of augmented input versions into one unified embedding. We evaluate\nour approach on MetaAudio, a benchmark including five datasets with predefined\nsplits, standardized preprocessing, and a comprehensive set of few-shot\nlearning models for comparison. The proposed approach achieves state-of-the-art\nperformance in a 5-way, 5-shot setting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\u96c6\u6210\u5230\u539f\u578b\u5c11\u6837\u672c\u97f3\u9891\u5206\u7c7b\u8bad\u7ec3\u4e2d\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u89d2\u5ea6\u635f\u5931\u548cSpecAugment\u589e\u5f3a\u6280\u672f\uff0c\u5728MetaAudio\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u867d\u7136\u5c11\u6837\u672c\u5b66\u4e60\u5728\u56fe\u50cf\u9886\u57df\u5df2\u6709\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u97f3\u9891\u5206\u7c7b\u9886\u57df\u76f8\u5bf9\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u97f3\u9891\u5206\u7c7b\u4e2d\u5c11\u6837\u672c\u5b66\u4e60\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u65b9\u6cd5\u6574\u5408\u4e86\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\u5230\u539f\u578b\u5c11\u6837\u672c\u8bad\u7ec3\u4e2d\uff0c\u4f7f\u7528\u89d2\u5ea6\u635f\u5931\u66ff\u4ee3\u6807\u51c6\u5bf9\u6bd4\u635f\u5931\uff0c\u5e76\u91c7\u7528SpecAugment\u6570\u636e\u589e\u5f3a\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c06\u589e\u5f3a\u8f93\u5165\u7248\u672c\u7684\u4e0d\u540c\u4fe1\u606f\u5c01\u88c5\u5230\u7edf\u4e00\u7684\u5d4c\u5165\u8868\u793a\u4e2d\u3002", "result": "\u5728MetaAudio\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u542b\u4e94\u4e2a\u6570\u636e\u96c6\uff09\u76845-way 5-shot\u8bbe\u7f6e\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5c06\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\u4e0e\u539f\u578b\u5c11\u6837\u672c\u8bad\u7ec3\u76f8\u7ed3\u5408\uff0c\u7279\u522b\u662f\u4f7f\u7528\u89d2\u5ea6\u635f\u5931\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u97f3\u9891\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e3a\u5c11\u6837\u672c\u97f3\u9891\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09708", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09708", "abs": "https://arxiv.org/abs/2509.09708", "authors": ["Nirmalendu Prakash", "Yeo Wei Jie", "Amir Abdullah", "Ranjan Satapathy", "Erik Cambria", "Roy Ka Wei Lee"], "title": "Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal", "comment": null, "summary": "Refusal on harmful prompts is a key safety behaviour in instruction-tuned\nlarge language models (LLMs), yet the internal causes of this behaviour remain\npoorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT\nand LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on\nresidual-stream activations. Given a harmful prompt, we search the SAE latent\nspace for feature sets whose ablation flips the model from refusal to\ncompliance, demonstrating causal influence and creating a jailbreak. Our search\nproceeds in three stages: (1) Refusal Direction: find a refusal-mediating\ndirection and collect SAE features near that direction; (2) Greedy Filtering:\nprune to a minimal set; and (3) Interaction Discovery: fit a factorization\nmachine (FM) that captures nonlinear interactions among the remaining active\nfeatures and the minimal set. This pipeline yields a broad set of\njailbreak-critical features, offering insight into the mechanistic basis of\nrefusal. Moreover, we find evidence of redundant features that remain dormant\nunless earlier features are suppressed. Our findings highlight the potential\nfor fine-grained auditing and targeted intervention in safety behaviours by\nmanipulating the interpretable latent space.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u6790LLM\u62d2\u7edd\u6709\u5bb3\u63d0\u793a\u7684\u5185\u90e8\u673a\u5236\uff0c\u53d1\u73b0\u4e86\u5bfc\u81f4\u5b89\u5168\u884c\u4e3a\u7ffb\u8f6c\u7684\u5173\u952e\u7279\u5f81\u96c6\uff0c\u5e76\u63ed\u793a\u4e86\u7279\u5f81\u5197\u4f59\u73b0\u8c61", "motivation": "\u7406\u89e3\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\u62d2\u7edd\u6709\u5bb3\u63d0\u793a\u7684\u5185\u90e8\u56e0\u679c\u673a\u5236\uff0c\u73b0\u6709\u7814\u7a76\u5bf9\u6b64\u8ba4\u8bc6\u4e0d\u8db3", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u6790\u6b8b\u5dee\u6d41\u6fc0\u6d3b\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u641c\u7d22\u6d41\u7a0b\uff08\u62d2\u7edd\u65b9\u5411\u8bc6\u522b\u3001\u8d2a\u5a6a\u8fc7\u6ee4\u3001\u4ea4\u4e92\u53d1\u73b0\uff09\u5bfb\u627e\u5bfc\u81f4\u62d2\u7edd\u884c\u4e3a\u7ffb\u8f6c\u7684\u5173\u952e\u7279\u5f81", "result": "\u6210\u529f\u8bc6\u522b\u51fa\u80fd\u591f\u5c06\u6a21\u578b\u4ece\u62d2\u7edd\u8f6c\u4e3a\u987a\u4ece\u7684\u5173\u952e\u7279\u5f81\u96c6\uff0c\u53d1\u73b0\u4e86\u7279\u5f81\u5197\u4f59\u73b0\u8c61\uff0c\u5373\u67d0\u4e9b\u7279\u5f81\u5728\u65e9\u671f\u7279\u5f81\u88ab\u6291\u5236\u65f6\u624d\u4f1a\u6fc0\u6d3b", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7ec6\u7c92\u5ea6\u5ba1\u8ba1\u548c\u9488\u5bf9\u6027\u5e72\u9884\u5b89\u5168\u884c\u4e3a\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u901a\u8fc7\u64cd\u4f5c\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u7a7a\u95f4\u6765\u5b9e\u73b0\u5b89\u5168\u63a7\u5236"}}
{"id": "2509.10234", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.10234", "abs": "https://arxiv.org/abs/2509.10234", "authors": ["Can Cui", "Paul Magron", "Mostafa Sadeghi", "Emmanuel Vincent"], "title": "Data-independent Beamforming for End-to-end Multichannel Multi-speaker ASR", "comment": "Published in the IEEE 26th International Workshop on Multimedia\n  Signal Processing (MMSP 2025)", "summary": "Automatic speech recognition (ASR) in multichannel, multi-speaker scenarios\nremains challenging due to ambient noise, reverberation and overlapping\nspeakers. In this paper, we propose a beamforming approach that processes\nspecific angular sectors based on their spherical polar coordinates before\napplying an end-to-end multichannel, multi-speaker ASR system. This method is\ndata-independent and training-free. We demonstrate that using a group of\nbeamformed signals improves ASR performance compared to using the same number\nof raw microphone signals. Moreover, increasing the number of signals used for\nbeamforming further enhances recognition accuracy, leading to a more efficient\nuse of multichannel signals while reducing the overall input load for the ASR\nsystem. We conduct experiments on the AMI meeting corpus, where the proposed\nmethod reduces word error rate by up to 11% and improves speaker counting\naccuracy by up to 27% relative compared to a multichannel ASR baseline system\nthat does not exploit beamforming.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7403\u5750\u6807\u89d2\u5ea6\u6247\u533a\u7684\u6ce2\u675f\u5f62\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u901a\u9053\u591a\u8bf4\u8bdd\u4eba\u8bed\u97f3\u8bc6\u522b\uff0c\u65e0\u9700\u8bad\u7ec3\u4e14\u6570\u636e\u72ec\u7acb\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bcd\u9519\u8bef\u7387\u548c\u63d0\u9ad8\u4e86\u8bf4\u8bdd\u4eba\u8ba1\u6570\u51c6\u786e\u7387\u3002", "motivation": "\u591a\u901a\u9053\u591a\u8bf4\u8bdd\u4eba\u573a\u666f\u4e2d\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u9762\u4e34\u73af\u5883\u566a\u58f0\u3001\u6df7\u54cd\u548c\u8bf4\u8bdd\u4eba\u91cd\u53e0\u7b49\u6311\u6218\uff0c\u9700\u8981\u6709\u6548\u5904\u7406\u591a\u901a\u9053\u4fe1\u53f7\u4ee5\u63d0\u9ad8\u8bc6\u522b\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u7403\u9762\u6781\u5750\u6807\u5904\u7406\u7279\u5b9a\u89d2\u5ea6\u6247\u533a\u7684\u6ce2\u675f\u5f62\u6210\u65b9\u6cd5\uff0c\u751f\u6210\u4e00\u7ec4\u6ce2\u675f\u5f62\u6210\u4fe1\u53f7\u540e\u8f93\u5165\u7aef\u5230\u7aef\u591a\u901a\u9053\u591a\u8bf4\u8bdd\u4ebaASR\u7cfb\u7edf\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\u4e14\u6570\u636e\u72ec\u7acb\u3002", "result": "\u5728AMI\u4f1a\u8bae\u8bed\u6599\u5e93\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u672a\u4f7f\u7528\u6ce2\u675f\u5f62\u6210\u7684\u591a\u901a\u9053ASR\u57fa\u7ebf\u7cfb\u7edf\uff0c\u8bcd\u9519\u8bef\u7387\u76f8\u5bf9\u964d\u4f4e11%\uff0c\u8bf4\u8bdd\u4eba\u8ba1\u6570\u51c6\u786e\u7387\u76f8\u5bf9\u63d0\u9ad827%\u3002", "conclusion": "\u901a\u8fc7\u6ce2\u675f\u5f62\u6210\u5904\u7406\u591a\u901a\u9053\u4fe1\u53f7\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u591a\u8bf4\u8bdd\u4ebaASR\u6027\u80fd\uff0c\u589e\u52a0\u6ce2\u675f\u5f62\u6210\u4fe1\u53f7\u6570\u91cf\u80fd\u8fdb\u4e00\u6b65\u63d0\u9ad8\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u540c\u65f6\u51cf\u5c11ASR\u7cfb\u7edf\u7684\u6574\u4f53\u8f93\u5165\u8d1f\u8f7d\u3002"}}
{"id": "2509.09709", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09709", "abs": "https://arxiv.org/abs/2509.09709", "authors": ["Jing Ren", "Weiqi Wang"], "title": "Assisting Research Proposal Writing with Large Language Models: Evaluation and Refinement", "comment": null, "summary": "Large language models (LLMs) like ChatGPT are increasingly used in academic\nwriting, yet issues such as incorrect or fabricated references raise ethical\nconcerns. Moreover, current content quality evaluations often rely on\nsubjective human judgment, which is labor-intensive and lacks objectivity,\npotentially compromising the consistency and reliability. In this study, to\nprovide a quantitative evaluation and enhance research proposal writing\ncapabilities of LLMs, we propose two key evaluation metrics--content quality\nand reference validity--and an iterative prompting method based on the scores\nderived from these two metrics. Our extensive experiments show that the\nproposed metrics provide an objective, quantitative framework for assessing\nChatGPT's writing performance. Additionally, iterative prompting significantly\nenhances content quality while reducing reference inaccuracies and\nfabrications, addressing critical ethical challenges in academic contexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5185\u5bb9\u8d28\u91cf\u548c\u53c2\u8003\u6587\u732e\u6709\u6548\u6027\u4e24\u4e2a\u91cf\u5316\u6307\u6807\uff0c\u4ee5\u53ca\u57fa\u4e8e\u8fd9\u4e9b\u6307\u6807\u7684\u8fed\u4ee3\u63d0\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u5ba2\u89c2\u8bc4\u4f30\u548c\u63d0\u5347ChatGPT\u7b49\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b66\u672f\u5199\u4f5c\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b66\u672f\u5199\u4f5c\u4e2d\u5b58\u5728\u9519\u8bef\u6216\u634f\u9020\u53c2\u8003\u6587\u732e\u7b49\u4f26\u7406\u95ee\u9898\uff0c\u4e14\u5185\u5bb9\u8d28\u91cf\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4e3b\u89c2\u4eba\u5de5\u5224\u65ad\uff0c\u7f3a\u4e4f\u5ba2\u89c2\u6027\u548c\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u5185\u5bb9\u8d28\u91cf\u548c\u53c2\u8003\u6587\u732e\u6709\u6548\u6027\u4e24\u4e2a\u5173\u952e\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u6307\u6807\u7684\u5f97\u5206\u8bbe\u8ba1\u8fed\u4ee3\u63d0\u793a\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u6307\u6807\u4e3aChatGPT\u5199\u4f5c\u6027\u80fd\u63d0\u4f9b\u4e86\u5ba2\u89c2\u91cf\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u8fed\u4ee3\u63d0\u793a\u663e\u8457\u63d0\u5347\u4e86\u5185\u5bb9\u8d28\u91cf\u5e76\u51cf\u5c11\u4e86\u53c2\u8003\u6587\u732e\u9519\u8bef\u548c\u634f\u9020\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5b66\u672f\u80cc\u666f\u4e0b\u5927\u8bed\u8a00\u6a21\u578b\u5199\u4f5c\u7684\u5173\u952e\u4f26\u7406\u6311\u6218\uff0c\u4e3a\u5ba2\u89c2\u8bc4\u4f30\u548c\u63d0\u5347\u6a21\u578b\u5b66\u672f\u5199\u4f5c\u80fd\u529b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.10391", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10391", "abs": "https://arxiv.org/abs/2509.10391", "authors": ["Shanmuka Sadhu", "Weiran Wang"], "title": "Improving Audio Event Recognition with Consistency Regularization", "comment": "Under Review", "summary": "Consistency regularization (CR), which enforces agreement between model\npredictions on augmented views, has found recent benefits in automatic speech\nrecognition [1]. In this paper, we propose the use of consistency\nregularization for audio event recognition, and demonstrate its effectiveness\non AudioSet. With extensive ablation studies for both small ($\\sim$20k) and\nlarge ($\\sim$1.8M) supervised training sets, we show that CR brings consistent\nimprovement over supervised baselines which already heavily utilize data\naugmentation, and CR using stronger augmentation and multiple augmentations\nleads to additional gain for the small training set. Furthermore, we extend the\nuse of CR into the semi-supervised setup with 20K labeled samples and 1.8M\nunlabeled samples, and obtain performance improvement over our best model\ntrained on the small set.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u4e00\u81f4\u6027\u6b63\u5219\u5316(CR)\u5e94\u7528\u4e8e\u97f3\u9891\u4e8b\u4ef6\u8bc6\u522b\uff0c\u901a\u8fc7\u5728AudioSet\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660eCR\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5c0f\u8bad\u7ec3\u96c6\u548c\u534a\u76d1\u7763\u8bbe\u7f6e\u4e2d\u6548\u679c\u663e\u8457", "motivation": "\u4e00\u81f4\u6027\u6b63\u5219\u5316\u5728\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u4e2d\u5df2\u663e\u793a\u51fa\u4f18\u52bf\uff0c\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u5176\u5728\u97f3\u9891\u4e8b\u4ef6\u8bc6\u522b\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u589e\u5f3a\u548c\u534a\u76d1\u7763\u5b66\u4e60\u573a\u666f\u4e0b\u7684\u6548\u679c", "method": "\u4f7f\u7528\u4e00\u81f4\u6027\u6b63\u5219\u5316\u6280\u672f\uff0c\u5f3a\u5236\u6a21\u578b\u5bf9\u589e\u5f3a\u89c6\u56fe\u7684\u9884\u6d4b\u4fdd\u6301\u4e00\u81f4\u3002\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\uff0c\u5305\u62ec\u5c0f\u89c4\u6a21(~20k)\u548c\u5927\u89c4\u6a21(~1.8M)\u76d1\u7763\u8bad\u7ec3\u96c6\uff0c\u4ee5\u53ca\u534a\u76d1\u7763\u8bbe\u7f6e(20K\u6807\u6ce8\u6837\u672c+1.8M\u672a\u6807\u6ce8\u6837\u672c)", "result": "CR\u5728\u5df2\u7ecf\u5927\u91cf\u4f7f\u7528\u6570\u636e\u589e\u5f3a\u7684\u76d1\u7763\u57fa\u7ebf\u4e0a\u5e26\u6765\u4e86\u6301\u7eed\u6539\u8fdb\uff1b\u4f7f\u7528\u66f4\u5f3a\u589e\u5f3a\u548c\u591a\u91cd\u589e\u5f3a\u7684CR\u4e3a\u5c0f\u8bad\u7ec3\u96c6\u5e26\u6765\u4e86\u989d\u5916\u589e\u76ca\uff1b\u5728\u534a\u76d1\u7763\u8bbe\u7f6e\u4e2d\u83b7\u5f97\u4e86\u6bd4\u5c0f\u8bad\u7ec3\u96c6\u6700\u4f73\u6a21\u578b\u66f4\u597d\u7684\u6027\u80fd", "conclusion": "\u4e00\u81f4\u6027\u6b63\u5219\u5316\u662f\u97f3\u9891\u4e8b\u4ef6\u8bc6\u522b\u4e2d\u6709\u6548\u7684\u6280\u672f\uff0c\u80fd\u591f\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u6216\u534a\u76d1\u7763\u5b66\u4e60\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa"}}
{"id": "2509.09710", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09710", "abs": "https://arxiv.org/abs/2509.09710", "authors": ["Sepehr Golrokh Amin", "Devin Rhoads", "Fatemeh Fakhrmoosavi", "Nicholas E. Lownes", "John N. Ivan"], "title": "Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data", "comment": null, "summary": "This study introduces a Large Language Model (LLM) scheme for generating\nindividual travel diaries in agent-based transportation models. While\ntraditional approaches rely on large quantities of proprietary household travel\nsurveys, the method presented in this study generates personas stochastically\nfrom open-source American Community Survey (ACS) and Smart Location Database\n(SLD) data, then synthesizes diaries through direct prompting. This study\nfeatures a novel one-to-cohort realism score: a composite of four metrics (Trip\nCount Score, Interval Score, Purpose Score, and Mode Score) validated against\nthe Connecticut Statewide Transportation Study (CSTS) diaries, matched across\ndemographic variables. The validation utilizes Jensen-Shannon Divergence to\nmeasure distributional similarities between generated and real diaries. When\ncompared to diaries generated with classical methods (Negative Binomial for\ntrip generation; Multinomial Logit for mode/purpose) calibrated on the\nvalidation set, LLM-generated diaries achieve comparable overall realism (LLM\nmean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and\ndemonstrates greater consistency (narrower realism score distribution), while\nclassical models lead in numerical estimates of trip count and activity\nduration. Aggregate validation confirms the LLM's statistical\nrepresentativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot\nviability and establishing a quantifiable metric of diary realism for future\nsynthetic diary evaluation systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u57fa\u4e8e\u4ee3\u7406\u7684\u4ea4\u901a\u6a21\u578b\u4e2d\u4e2a\u4f53\u51fa\u884c\u65e5\u8bb0\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f00\u6e90\u6570\u636e\u751f\u6210\u865a\u62df\u4eba\u7269\u5e76\u5408\u6210\u65e5\u8bb0\uff0c\u9a8c\u8bc1\u663e\u793a\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u53ef\u6bd4\u6027\u7684\u771f\u5b9e\u6027\u548c\u66f4\u597d\u7684\u7edf\u8ba1\u4ee3\u8868\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u4e13\u6709\u5bb6\u5ead\u51fa\u884c\u8c03\u67e5\u6570\u636e\uff0c\u6210\u672c\u9ad8\u4e14\u83b7\u53d6\u56f0\u96be\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u57fa\u4e8e\u5f00\u6e90\u6570\u636e\u7684\u66ff\u4ee3\u65b9\u6848\u6765\u751f\u6210\u4e2a\u4f53\u51fa\u884c\u65e5\u8bb0\u3002", "method": "\u4f7f\u7528\u5f00\u6e90\u7f8e\u56fd\u793e\u533a\u8c03\u67e5\u548c\u667a\u80fd\u4f4d\u7f6e\u6570\u636e\u5e93\u6570\u636e\u968f\u673a\u751f\u6210\u865a\u62df\u4eba\u7269\uff0c\u901a\u8fc7\u76f4\u63a5\u63d0\u793aLLM\u5408\u6210\u51fa\u884c\u65e5\u8bb0\uff0c\u5e76\u91c7\u7528\u5305\u542b\u56db\u4e2a\u6307\u6807\uff08\u51fa\u884c\u6b21\u6570\u3001\u95f4\u9694\u3001\u76ee\u7684\u3001\u65b9\u5f0f\uff09\u7684\u7efc\u5408\u771f\u5b9e\u5ea6\u8bc4\u5206\u4f53\u7cfb\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "LLM\u751f\u6210\u7684\u65e5\u8bb0\u4e0e\u4f20\u7edf\u65b9\u6cd5\uff08\u8d1f\u4e8c\u9879\u5f0f\u51fa\u884c\u751f\u6210+\u591a\u9879\u5f0f\u903b\u8f91\u56de\u5f52\u6a21\u5f0f/\u76ee\u7684\uff09\u76f8\u6bd4\u5177\u6709\u53ef\u6bd4\u7684\u6574\u4f53\u771f\u5b9e\u5ea6\uff080.485 vs 0.455\uff09\uff0c\u5728\u786e\u5b9a\u51fa\u884c\u76ee\u7684\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u4e14\u4e00\u81f4\u6027\u66f4\u597d\uff0c\u540c\u65f6\u5728\u7edf\u8ba1\u4ee3\u8868\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff080.612 vs 0.435\uff09\u3002", "conclusion": "LLM\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\u53ef\u884c\uff0c\u4e3a\u672a\u6765\u5408\u6210\u51fa\u884c\u65e5\u8bb0\u8bc4\u4f30\u7cfb\u7edf\u5efa\u7acb\u4e86\u53ef\u91cf\u5316\u7684\u771f\u5b9e\u5ea6\u6307\u6807\uff0c\u5c55\u793a\u4e86\u5728\u4ea4\u901a\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.09711", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09711", "abs": "https://arxiv.org/abs/2509.09711", "authors": ["Aya E. Fouda", "Abdelrahamn A. Hassan", "Radwa J. Hanafy", "Mohammed E. Fouda"], "title": "Psychiatry-Bench: A Multi-Task Benchmark for LLMs in Psychiatry", "comment": null, "summary": "Large language models (LLMs) hold great promise in enhancing psychiatric\npractice, from improving diagnostic accuracy to streamlining clinical\ndocumentation and therapeutic support. However, existing evaluation resources\nheavily rely on small clinical interview corpora, social media posts, or\nsynthetic dialogues, which limits their clinical validity and fails to capture\nthe full complexity of psychiatric reasoning. In this work, we introduce\nPsychiatryBench, a rigorously curated benchmark grounded exclusively in\nauthoritative, expert-validated psychiatric textbooks and casebooks.\nPsychiatryBench comprises eleven distinct question-answering tasks ranging from\ndiagnostic reasoning and treatment planning to longitudinal follow-up,\nmanagement planning, clinical approach, sequential case analysis, and\nmultiple-choice/extended matching formats totaling over 5,300 expert-annotated\nitems. We evaluate a diverse set of frontier LLMs (including Google Gemini,\nDeepSeek, LLaMA 3, and QWQ-32) alongside leading open-source medical models\n(e.g., OpenBiloLLM, MedGemma) using both conventional metrics and an\n\"LLM-as-judge\" similarity scoring framework. Our results reveal substantial\ngaps in clinical consistency and safety, particularly in multi-turn follow-up\nand management tasks, underscoring the need for specialized model tuning and\nmore robust evaluation paradigms. PsychiatryBench offers a modular, extensible\nplatform for benchmarking and improving LLM performance in high-stakes mental\nhealth applications.", "AI": {"tldr": "PsychiatryBench\u662f\u4e00\u4e2a\u57fa\u4e8e\u6743\u5a01\u7cbe\u795e\u75c5\u5b66\u6559\u79d1\u4e66\u548c\u6848\u4f8b\u96c6\u6784\u5efa\u7684\u4e34\u5e8a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b5300\u591a\u4e2a\u4e13\u5bb6\u6807\u6ce8\u9879\u76ee\uff0c\u8bc4\u4f30\u663e\u793a\u524d\u6cbfLLM\u5728\u4e34\u5e8a\u4e00\u81f4\u6027\u548c\u5b89\u5168\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u8d44\u6e90\u4e3b\u8981\u4f9d\u8d56\u5c0f\u578b\u4e34\u5e8a\u8bbf\u8c08\u8bed\u6599\u5e93\u3001\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u6216\u5408\u6210\u5bf9\u8bdd\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u6709\u6548\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u7cbe\u795e\u75c5\u5b66\u63a8\u7406\u7684\u590d\u6742\u6027\u3002", "method": "\u57fa\u4e8e\u6743\u5a01\u4e13\u5bb6\u9a8c\u8bc1\u7684\u7cbe\u795e\u75c5\u5b66\u6559\u79d1\u4e66\u548c\u6848\u4f8b\u96c6\u6784\u5efa\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b11\u4e2a\u4e0d\u540c\u7684\u95ee\u7b54\u4efb\u52a1\uff0c\u4f7f\u7528\u4f20\u7edf\u6307\u6807\u548c\"LLM-as-judge\"\u76f8\u4f3c\u6027\u8bc4\u5206\u6846\u67b6\u8bc4\u4f30\u591a\u79cd\u524d\u6cbfLLM\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5728\u4e34\u5e8a\u4e00\u81f4\u6027\u548c\u5b89\u5168\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u591a\u8f6e\u968f\u8bbf\u548c\u7ba1\u7406\u4efb\u52a1\u4e2d\u3002", "conclusion": "\u9700\u8981\u4e13\u95e8\u7684\u6a21\u578b\u8c03\u4f18\u548c\u66f4\u5f3a\u5927\u7684\u8bc4\u4f30\u8303\u5f0f\uff0cPsychiatryBench\u4e3a\u9ad8\u98ce\u9669\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u4e2d\u7684LLM\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684\u5e73\u53f0\u3002"}}
{"id": "2509.09712", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09712", "abs": "https://arxiv.org/abs/2509.09712", "authors": ["Talha Tahir"], "title": "The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization", "comment": null, "summary": "Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral\ntherapy with emerging evidence of efficacy in several psychiatric conditions.\nThis study investigates the impact of post-training methodology and explicit\nreasoning on the ability of a small open-weight large language model (LLM) to\ndeliver ACT. Using 50 sets of synthetic ACT transcripts generated by\nMistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches,\nsupervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each\nwith and without an explicit chain-of-thought (COT) reasoning step. Performance\nwas evaluated by comparing these four post-trained variants against the base\nInstruct model. These models were benchmarked in simulated therapy sessions,\nwith performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM)\nand the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned\non human evaluations. Our findings demonstrate that the ORPO-trained models\nsignificantly outperformed both their SFT and Instruct counterparts on ACT\nfidelity ($\\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\\chi^2(5) =\n140.37, p < .001$). The effect of COT was conditional as it provided a\nsignificant benefit to SFT models, improving ACT-FM scores by an average of\n2.68 points ($p < .001$), while offering no discernible advantage to the\nsuperior ORPO or instruct-tuned variants. We posit that the superiority of ORPO\nstems from its ability to learn the therapeutic `process' over imitating\n`content,' a key aspect of ACT, while COT acts as a necessary scaffold for\nmodels trained only via imitation. This study establishes that\npreference-aligned policy optimization can effectively instill ACT competencies\nin small LLMs, and that the utility of explicit reasoning is highly dependent\non the underlying training paradigm.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86SFT\u548cORPO\u4e24\u79cd\u8bad\u7ec3\u65b9\u6cd5\u5bf9\u5c0f\u578bLLM\u8fdb\u884cACT\u6cbb\u7597\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0ORPO\u65b9\u6cd5\u5728\u6cbb\u7597\u5fe0\u8bda\u5ea6\u548c\u5171\u60c5\u65b9\u9762\u663e\u8457\u4f18\u4e8eSFT\u548c\u57fa\u7840\u6a21\u578b\uff0c\u800c\u601d\u7ef4\u94fe\u63a8\u7406\u4ec5\u5bf9SFT\u6a21\u578b\u6709\u663e\u8457\u5e2e\u52a9\u3002", "motivation": "\u7814\u7a76\u7b2c\u4e09\u4ee3\u8ba4\u77e5\u884c\u4e3a\u7597\u6cd5ACT\u5728\u5c0f\u578b\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u63a2\u7d22\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u548c\u663e\u5f0f\u63a8\u7406\u5bf9\u6a21\u578b\u6cbb\u7597\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528Mistral-Large\u751f\u6210\u768450\u7ec4\u5408\u6210ACT\u8f6c\u5f55\u672c\uff0c\u5bf9Llama-3.2-3b-Instruct\u8fdb\u884c\u4e24\u79cd\u8bad\u7ec3\uff1a\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u51e0\u7387\u6bd4\u7b56\u7565\u4f18\u5316(ORPO)\uff0c\u6bcf\u79cd\u65b9\u6cd5\u90fd\u5305\u542b\u6709/\u65e0\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u53d8\u4f53\u3002\u901a\u8fc7\u6a21\u62df\u6cbb\u7597\u4f1a\u8bdd\u8bc4\u4f30\u6027\u80fd\uff0c\u4f7f\u7528ACT\u5fe0\u8bda\u5ea6\u91cf\u8868\u548c\u6cbb\u7597\u5e08\u5171\u60c5\u91cf\u8868\u8fdb\u884c\u91cf\u5316\u8bc4\u4f30\u3002", "result": "ORPO\u8bad\u7ec3\u6a21\u578b\u5728ACT\u5fe0\u8bda\u5ea6(\u03c7\u00b2=185.15, p<.001)\u548c\u6cbb\u7597\u5171\u60c5(\u03c7\u00b2=140.37, p<.001)\u65b9\u9762\u663e\u8457\u4f18\u4e8eSFT\u548c\u57fa\u7840\u6a21\u578b\u3002\u601d\u7ef4\u94fe\u63a8\u7406\u4ec5\u5bf9SFT\u6a21\u578b\u6709\u663e\u8457\u5e2e\u52a9\uff0c\u5e73\u5747\u63d0\u9ad8ACT-FM\u5f97\u52062.68\u5206(p<.001)\uff0c\u4f46\u5bf9ORPO\u6a21\u578b\u65e0\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u504f\u597d\u5bf9\u9f50\u7b56\u7565\u4f18\u5316\u80fd\u6709\u6548\u57f9\u517b\u5c0f\u578bLLM\u7684ACT\u80fd\u529b\uff0c\u663e\u5f0f\u63a8\u7406\u7684\u6548\u7528\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5e95\u5c42\u8bad\u7ec3\u8303\u5f0f\u3002ORPO\u901a\u8fc7\u5b66\u4e60\u6cbb\u7597\"\u8fc7\u7a0b\"\u800c\u975e\u6a21\u4eff\"\u5185\u5bb9\"\u8868\u73b0\u51fa\u4f18\u52bf\uff0c\u800c\u601d\u7ef4\u94fe\u63a8\u7406\u4e3a\u4ec5\u901a\u8fc7\u6a21\u4eff\u8bad\u7ec3\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u652f\u67b6\u3002"}}
{"id": "2509.09713", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09713", "abs": "https://arxiv.org/abs/2509.09713", "authors": ["Duolin Sun", "Dan Yang", "Yue Shen", "Yihan Jiao", "Zhehao Tan", "Jie Feng", "Lianzhen Zhong", "Jian Wang", "Peng Wei", "Jinjie Gu"], "title": "HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering", "comment": null, "summary": "The Retrieval-Augmented Generation (RAG) approach enhances question-answering\nsystems and dialogue generation tasks by integrating information retrieval (IR)\ntechnologies with large language models (LLMs). This strategy, which retrieves\ninformation from external knowledge bases to bolster the response capabilities\nof generative models, has achieved certain successes. However, current RAG\nmethods still face numerous challenges when dealing with multi-hop queries. For\ninstance, some approaches overly rely on iterative retrieval, wasting too many\nretrieval steps on compound queries. Additionally, using the original complex\nquery for retrieval may fail to capture content relevant to specific\nsub-queries, resulting in noisy retrieved content. If the noise is not managed,\nit can lead to the problem of noise accumulation. To address these issues, we\nintroduce HANRAG, a novel heuristic-based framework designed to efficiently\ntackle problems of varying complexity. Driven by a powerful revelator, HANRAG\nroutes queries, decomposes them into sub-queries, and filters noise from\nretrieved documents. This enhances the system's adaptability and noise\nresistance, making it highly capable of handling diverse queries. We compare\nthe proposed framework against other leading industry methods across various\nbenchmarks. The results demonstrate that our framework obtains superior\nperformance in both single-hop and multi-hop question-answering tasks.", "AI": {"tldr": "HANRAG\u662f\u4e00\u4e2a\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684RAG\u6846\u67b6\uff0c\u901a\u8fc7\u67e5\u8be2\u8def\u7531\u3001\u5b50\u67e5\u8be2\u5206\u89e3\u548c\u566a\u58f0\u8fc7\u6ee4\u6765\u9ad8\u6548\u5904\u7406\u591a\u8df3\u67e5\u8be2\u95ee\u9898\uff0c\u5728\u5355\u8df3\u548c\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u5f53\u524dRAG\u65b9\u6cd5\u5728\u5904\u7406\u591a\u8df3\u67e5\u8be2\u65f6\u9762\u4e34\u8fed\u4ee3\u68c0\u7d22\u6b65\u9aa4\u6d6a\u8d39\u3001\u590d\u6742\u67e5\u8be2\u68c0\u7d22\u6548\u679c\u5dee\u3001\u566a\u58f0\u7d2f\u79ef\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faHANRAG\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5927\u7684revelator\u8fdb\u884c\u67e5\u8be2\u8def\u7531\u3001\u5c06\u590d\u6742\u67e5\u8be2\u5206\u89e3\u4e3a\u5b50\u67e5\u8be2\u3001\u5bf9\u68c0\u7d22\u6587\u6863\u8fdb\u884c\u566a\u58f0\u8fc7\u6ee4\uff0c\u63d0\u9ad8\u7cfb\u7edf\u9002\u5e94\u6027\u548c\u6297\u566a\u80fd\u529b", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u4e1a\u754c\u9886\u5148\u65b9\u6cd5\u6bd4\u8f83\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u6846\u67b6\u5728\u5355\u8df3\u548c\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u90fd\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd", "conclusion": "HANRAG\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8df3\u67e5\u8be2\u5904\u7406\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u901a\u8fc7\u542f\u53d1\u5f0f\u65b9\u6cd5\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u7684\u6548\u7387\u548c\u51c6\u786e\u6027"}}
{"id": "2509.09714", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09714", "abs": "https://arxiv.org/abs/2509.09714", "authors": ["Serge Lionel Nikiema", "Alb\u00e9rick Euraste Djire", "Abdoul Aziz Bonkoungou", "Micheline B\u00e9n\u00e9dicte Moumoula", "Jordan Samhi", "Abdoul Kader Kabore", "Jacques Klein", "Tegawend\u00e9 F. Bissyande"], "title": "How Small Transformation Expose the Weakness of Semantic Similarity Measures", "comment": null, "summary": "This research examines how well different methods measure semantic\nsimilarity, which is important for various software engineering applications\nsuch as code search, API recommendations, automated code reviews, and\nrefactoring tools. While large language models are increasingly used for these\nsimilarity assessments, questions remain about whether they truly understand\nsemantic relationships or merely recognize surface patterns.\n  The study tested 18 different similarity measurement approaches, including\nword-based methods, embedding techniques, LLM-based systems, and\nstructure-aware algorithms. The researchers created a systematic testing\nframework that applies controlled changes to text and code to evaluate how well\neach method handles different types of semantic relationships.\n  The results revealed significant issues with commonly used metrics. Some\nembedding-based methods incorrectly identified semantic opposites as similar up\nto 99.9 percent of the time, while certain transformer-based approaches\noccasionally rated opposite meanings as more similar than synonymous ones. The\nstudy found that embedding methods' poor performance often stemmed from how\nthey calculate distances; switching from Euclidean distance to cosine\nsimilarity improved results by 24 to 66 percent. LLM-based approaches performed\nbetter at distinguishing semantic differences, producing low similarity scores\n(0.00 to 0.29) for genuinely different meanings, compared to embedding methods\nthat incorrectly assigned high scores (0.82 to 0.99) to dissimilar content.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e8618\u79cd\u8bed\u4e49\u76f8\u4f3c\u5ea6\u6d4b\u91cf\u65b9\u6cd5\uff0c\u53d1\u73b0\u5e38\u7528\u6307\u6807\u5b58\u5728\u663e\u8457\u95ee\u9898\uff0c\u67d0\u4e9b\u5d4c\u5165\u65b9\u6cd5\u5c06\u8bed\u4e49\u5bf9\u7acb\u5185\u5bb9\u8bef\u5224\u4e3a\u76f8\u4f3c\u7684\u6982\u7387\u9ad8\u8fbe99.9%\uff0c\u800c\u8f6c\u6362\u8ddd\u79bb\u8ba1\u7b97\u65b9\u5f0f\u53ef\u63d0\u5347\u6027\u80fd24-66%\u3002", "motivation": "\u8bc4\u4f30\u4e0d\u540c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u6d4b\u91cf\u65b9\u6cd5\u5728\u8f6f\u4ef6\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u8bed\u4e49\u5173\u7cfb\u800c\u975e\u4ec5\u8bc6\u522b\u8868\u9762\u6a21\u5f0f\u3002", "method": "\u521b\u5efa\u7cfb\u7edf\u6027\u6d4b\u8bd5\u6846\u67b6\uff0c\u5bf9\u6587\u672c\u548c\u4ee3\u7801\u5e94\u7528\u53d7\u63a7\u53d8\u5316\uff0c\u6d4b\u8bd518\u79cd\u65b9\u6cd5\uff08\u57fa\u4e8e\u8bcd\u3001\u5d4c\u5165\u6280\u672f\u3001LLM\u7cfb\u7edf\u548c\u7ed3\u6784\u611f\u77e5\u7b97\u6cd5\uff09\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u8bed\u4e49\u5173\u7cfb\u7684\u80fd\u529b\u3002", "result": "\u5d4c\u5165\u65b9\u6cd5\u7ecf\u5e38\u9519\u8bef\u8bc6\u522b\u8bed\u4e49\u5bf9\u7acb\u5185\u5bb9\u4e3a\u76f8\u4f3c\uff08\u6700\u9ad899.9%\uff09\uff0c\u8f6c\u6362\u5668\u65b9\u6cd5\u6709\u65f6\u5c06\u76f8\u53cd\u542b\u4e49\u8bc4\u4e3a\u6bd4\u540c\u4e49\u8bcd\u66f4\u76f8\u4f3c\u3002\u4ece\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u5207\u6362\u5230\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4f7f\u7ed3\u679c\u63d0\u534724-66%\u3002LLM\u65b9\u6cd5\u5728\u533a\u5206\u8bed\u4e49\u5dee\u5f02\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u5f53\u524d\u5e38\u7528\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002LLM\u5728\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u5d4c\u5165\u65b9\u6cd5\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2509.09715", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09715", "abs": "https://arxiv.org/abs/2509.09715", "authors": ["Naveen Lamba", "Sanju Tiwari", "Manas Gaur"], "title": "Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA", "comment": null, "summary": "Hallucination in Large Language Models (LLMs) is a well studied problem.\nHowever, the properties that make LLM intrinsically vulnerable to\nhallucinations have not been identified and studied. This research identifies\nand characterizes the key properties, allowing us to pinpoint vulnerabilities\nwithin the model's internal mechanisms. To solidify on these properties, we\nutilized two established datasets, HaluEval and TruthfulQA and convert their\nexisting format of question answering into various other formats to narrow down\nthese properties as the reason for the hallucinations. Our findings reveal that\nhallucination percentages across symbolic properties are notably high for\nGemma-2-2B, averaging 79.0% across tasks and datasets. With increased model\nscale, hallucination drops to 73.6% for Gemma-2-9B and 63.9% for Gemma-2-27B,\nreflecting a 15 percentage point reduction overall. Although the hallucination\nrate decreases as the model size increases, a substantial amount of\nhallucination caused by symbolic properties still persists. This is especially\nevident for modifiers (ranging from 84.76% to 94.98%) and named entities\n(ranging from 83.87% to 93.96%) across all Gemma models and both datasets.\nThese findings indicate that symbolic elements continue to confuse the models,\npointing to a fundamental weakness in how these LLMs process such\ninputs--regardless of their scale.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc6\u522b\u5e76\u8868\u5f81\u4e86\u5bfc\u81f4\u5927\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u5e7b\u89c9\u7684\u5173\u952e\u7b26\u53f7\u5c5e\u6027\uff0c\u53d1\u73b0\u5373\u4f7f\u6a21\u578b\u89c4\u6a21\u589e\u5927\uff0c\u7b26\u53f7\u5143\u7d20\uff08\u5982\u4fee\u9970\u8bed\u548c\u547d\u540d\u5b9e\u4f53\uff09\u4ecd\u4f1a\u6301\u7eed\u5bfc\u81f4\u9ad8\u5e7b\u89c9\u7387\u3002", "motivation": "\u867d\u7136LLM\u7684\u5e7b\u89c9\u95ee\u9898\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5bfc\u81f4\u6a21\u578b\u5185\u5728\u6613\u4ea7\u751f\u5e7b\u89c9\u7684\u5c5e\u6027\u5c1a\u672a\u88ab\u8bc6\u522b\u548c\u7814\u7a76\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528HaluEval\u548cTruthfulQA\u4e24\u4e2a\u6570\u636e\u96c6\uff0c\u5c06\u539f\u6709\u7684\u95ee\u7b54\u683c\u5f0f\u8f6c\u6362\u4e3a\u591a\u79cd\u5176\u4ed6\u683c\u5f0f\uff0c\u4ee5\u786e\u5b9a\u5bfc\u81f4\u5e7b\u89c9\u7684\u5173\u952e\u5c5e\u6027\u3002", "result": "Gemma-2-2B\u6a21\u578b\u5728\u5404\u9879\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5e73\u5747\u5e7b\u89c9\u7387\u4e3a79.0%\uff0c\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u5927\uff0cGemma-2-9B\u964d\u81f373.6%\uff0cGemma-2-27B\u964d\u81f363.9%\u3002\u4fee\u9970\u8bed\u548c\u547d\u540d\u5b9e\u4f53\u5728\u6240\u6709Gemma\u6a21\u578b\u4e2d\u90fd\u5bfc\u81f484.76%-94.98%\u7684\u9ad8\u5e7b\u89c9\u7387\u3002", "conclusion": "\u7b26\u53f7\u5143\u7d20\u6301\u7eed\u6df7\u6dc6\u6a21\u578b\uff0c\u8868\u660eLLM\u5728\u5904\u7406\u6b64\u7c7b\u8f93\u5165\u65f6\u5b58\u5728\u6839\u672c\u6027\u5f31\u70b9\uff0c\u4e14\u8fd9\u79cd\u5f31\u70b9\u4e0e\u6a21\u578b\u89c4\u6a21\u65e0\u5173\u3002"}}
{"id": "2509.09723", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ME", "I.2.6; J.4; I.5.1; H.3.3; H.2.8"], "pdf": "https://arxiv.org/pdf/2509.09723", "abs": "https://arxiv.org/abs/2509.09723", "authors": ["Kai R. Larsen", "Sen Yan", "Roland M\u00fcller", "Lan Sang", "Mikko R\u00f6nkk\u00f6", "Ravi Starzl", "Donald Edmondson"], "title": "ALIGNS: Unlocking nomological networks in psychological measurement through a large language model", "comment": null, "summary": "Psychological measurement is critical to many disciplines. Despite advances\nin measurement, building nomological networks, theoretical maps of how concepts\nand measures relate to establish validity, remains a challenge 70 years after\nCronbach and Meehl proposed them as fundamental to validation. This limitation\nhas practical consequences: clinical trials may fail to detect treatment\neffects, and public policy may target the wrong outcomes. We introduce Analysis\nof Latent Indicators to Generate Nomological Structures (ALIGNS), a large\nlanguage model-based system trained with validated questionnaire measures.\nALIGNS provides three comprehensive nomological networks containing over\n550,000 indicators across psychology, medicine, social policy, and other\nfields. This represents the first application of large language models to solve\na foundational problem in measurement validation. We report classification\naccuracy tests used to develop the model, as well as three evaluations. In the\nfirst evaluation, the widely used NIH PROMIS anxiety and depression instruments\nare shown to converge into a single dimension of emotional distress. The second\nevaluation examines child temperament measures and identifies four potential\ndimensions not captured by current frameworks, and questions one existing\ndimension. The third evaluation, an applicability check, engages expert\npsychometricians who assess the system's importance, accessibility, and\nsuitability. ALIGNS is freely available at nomologicalnetwork.org,\ncomplementing traditional validation methods with large-scale nomological\nanalysis.", "AI": {"tldr": "ALIGNS\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u751f\u6210\u5305\u542b55\u4e07+\u6307\u6807\u7684\u7efc\u5408\u7406\u8bba\u7f51\u7edc\uff0c\u89e3\u51b3\u5fc3\u7406\u5b66\u6d4b\u91cf\u4e2d\u7406\u8bba\u7f51\u7edc\u6784\u5efa\u7684\u957f\u671f\u6311\u6218\u3002", "motivation": "\u89e3\u51b370\u5e74\u6765\u7406\u8bba\u7f51\u7edc\u6784\u5efa\u7684\u96be\u9898\uff0c\u907f\u514d\u4e34\u5e8a\u8bd5\u9a8c\u68c0\u6d4b\u4e0d\u5230\u6cbb\u7597\u6548\u679c\u548c\u516c\u5171\u653f\u7b56\u76ee\u6807\u9519\u8bef\u7b49\u5b9e\u9645\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u95ee\u5377\u6d4b\u91cf\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5f00\u53d1ALIGNS\u7cfb\u7edf\uff0c\u63d0\u4f9b\u4e09\u4e2a\u7efc\u5408\u7406\u8bba\u7f51\u7edc\u3002", "result": "\u7cfb\u7edf\u5728\u4e09\u4e2a\u8bc4\u4f30\u4e2d\u8868\u73b0\u826f\u597d\uff1a\u53d1\u73b0\u7126\u8651\u6291\u90c1\u6d4b\u91cf\u6536\u655b\u4e3a\u5355\u4e00\u60c5\u7eea\u56f0\u6270\u7ef4\u5ea6\uff1b\u8bc6\u522b\u513f\u7ae5\u6c14\u8d28\u6d4b\u91cf\u7684\u56db\u4e2a\u65b0\u7ef4\u5ea6\uff1b\u4e13\u5bb6\u8bc4\u4f30\u663e\u793a\u7cfb\u7edf\u91cd\u8981\u4e14\u9002\u7528\u3002", "conclusion": "ALIGNS\u662f\u9996\u4e2a\u5e94\u7528\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u6d4b\u91cf\u9a8c\u8bc1\u57fa\u7840\u95ee\u9898\u7684\u7cfb\u7edf\uff0c\u53ef\u514d\u8d39\u4f7f\u7528\uff0c\u4e3a\u4f20\u7edf\u9a8c\u8bc1\u65b9\u6cd5\u63d0\u4f9b\u5927\u89c4\u6a21\u7406\u8bba\u5206\u6790\u8865\u5145\u3002"}}
{"id": "2509.09724", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T09"], "pdf": "https://arxiv.org/pdf/2509.09724", "abs": "https://arxiv.org/abs/2509.09724", "authors": ["Wonyoung Kim", "Sujeong Seo", "Juhyun Lee"], "title": "DiTTO-LLM: Framework for Discovering Topic-based Technology Opportunities via Large Language Model", "comment": "5 figures", "summary": "Technology opportunities are critical information that serve as a foundation\nfor advancements in technology, industry, and innovation. This paper proposes a\nframework based on the temporal relationships between technologies to identify\nemerging technology opportunities. The proposed framework begins by extracting\ntext from a patent dataset, followed by mapping text-based topics to discover\ninter-technology relationships. Technology opportunities are then identified by\ntracking changes in these topics over time. To enhance efficiency, the\nframework leverages a large language model to extract topics and employs a\nprompt for a chat-based language model to support the discovery of technology\nopportunities. The framework was evaluated using an artificial intelligence\npatent dataset provided by the United States Patent and Trademark Office. The\nexperimental results suggest that artificial intelligence technology is\nevolving into forms that facilitate everyday accessibility. This approach\ndemonstrates the potential of the proposed framework to identify future\ntechnology opportunities.", "AI": {"tldr": "\u57fa\u4e8e\u6280\u672f\u95f4\u65f6\u95f4\u5173\u7cfb\u8bc6\u522b\u65b0\u5174\u6280\u672f\u673a\u4f1a\u7684\u6846\u67b6\uff0c\u5229\u7528\u4e13\u5229\u6570\u636e\u548c\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u6280\u672f\u4e3b\u9898\u5e76\u8ffd\u8e2a\u5176\u6f14\u53d8", "motivation": "\u6280\u672f\u673a\u4f1a\u662f\u63a8\u52a8\u6280\u672f\u3001\u4ea7\u4e1a\u548c\u521b\u65b0\u8fdb\u6b65\u7684\u5173\u952e\u4fe1\u606f\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u65b9\u6cd5\u6765\u8bc6\u522b\u65b0\u5174\u6280\u672f\u673a\u4f1a", "method": "\u4ece\u4e13\u5229\u6570\u636e\u63d0\u53d6\u6587\u672c\uff0c\u6620\u5c04\u6587\u672c\u4e3b\u9898\u53d1\u73b0\u6280\u672f\u95f4\u5173\u7cfb\uff0c\u8ffd\u8e2a\u4e3b\u9898\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u4e3b\u9898\u548c\u804a\u5929\u6a21\u578b\u63d0\u793a\u6765\u652f\u6301\u6280\u672f\u673a\u4f1a\u53d1\u73b0", "result": "\u4f7f\u7528\u7f8e\u56fd\u4e13\u5229\u5546\u6807\u5c40AI\u4e13\u5229\u6570\u636e\u96c6\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eAI\u6280\u672f\u6b63\u671d\u7740\u4fbf\u4e8e\u65e5\u5e38\u8bbf\u95ee\u7684\u5f62\u5f0f\u53d1\u5c55", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u8bc6\u522b\u672a\u6765\u6280\u672f\u673a\u4f1a\u7684\u6f5c\u529b"}}
{"id": "2509.09725", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09725", "abs": "https://arxiv.org/abs/2509.09725", "authors": ["Chunyu Li", "Xindi Zheng", "Siqi Liu"], "title": "BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025", "comment": null, "summary": "Entity linking (EL) for biomedical text is typically benchmarked on\nEnglish-only corpora with flat mentions, leaving the more realistic scenario of\nnested and multilingual mentions largely unexplored. We present our system for\nthe BioNNE 2025 Multilingual Biomedical Nested Named Entity Linking shared task\n(English & Russian), closing this gap with a lightweight pipeline that keeps\nthe original EL model intact and modifies only three task-aligned components:\nTwo-stage retrieval-ranking. We leverage the same base encoder model in both\nstages: the retrieval stage uses the original pre-trained model, while the\nranking stage applies domain-specific fine-tuning. Boundary cues. In the\nranking stage, we wrap each mention with learnable [Ms] / [Me] tags, providing\nthe encoder with an explicit, language-agnostic span before robustness to\noverlap and nesting. Dataset augmentation. We also automatically expand the\nranking training corpus with three complementary data sources, enhancing\ncoverage without extra manual annotation. On the BioNNE 2025 leaderboard, our\ntwo stage system, bilingual bert (BIBERT-Pipe), ranks third in the multilingual\ntrack, demonstrating the effectiveness and competitiveness of these minimal yet\nprincipled modifications. Code are publicly available at\nhttps://github.com/Kaggle-Competitions-Code/BioNNE-L.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u53cc\u9636\u6bb5\u68c0\u7d22-\u6392\u5e8f\u7ba1\u9053\u7cfb\u7edfBIBERT-Pipe\uff0c\u7528\u4e8e\u5904\u7406\u751f\u7269\u533b\u5b66\u6587\u672c\u4e2d\u7684\u591a\u8bed\u8a00\u5d4c\u5957\u5b9e\u4f53\u94fe\u63a5\u4efb\u52a1\uff0c\u5728BioNNE 2025\u591a\u8bed\u8a00\u8d5b\u9053\u4e2d\u6392\u540d\u7b2c\u4e09", "motivation": "\u5f53\u524d\u751f\u7269\u533b\u5b66\u5b9e\u4f53\u94fe\u63a5\u4e3b\u8981\u57fa\u4e8e\u82f1\u8bed\u8bed\u6599\u5e93\u548c\u5e73\u5766\u63d0\u53ca\uff0c\u7f3a\u4e4f\u5bf9\u5d4c\u5957\u548c\u591a\u8bed\u8a00\u63d0\u53ca\u7684\u73b0\u5b9e\u573a\u666f\u7814\u7a76", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u68c0\u7d22-\u6392\u5e8f\u67b6\u6784\uff1a\u68c0\u7d22\u9636\u6bb5\u4f7f\u7528\u539f\u59cb\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6392\u5e8f\u9636\u6bb5\u8fdb\u884c\u9886\u57df\u7279\u5b9a\u5fae\u8c03\uff1b\u4f7f\u7528\u53ef\u5b66\u4e60\u7684[Ms]/[Me]\u6807\u7b7e\u5305\u88c5\u63d0\u53ca\uff1b\u901a\u8fc7\u4e09\u79cd\u6570\u636e\u6e90\u81ea\u52a8\u6269\u5c55\u6392\u5e8f\u8bad\u7ec3\u8bed\u6599", "result": "\u5728BioNNE 2025\u591a\u8bed\u8a00\u6392\u884c\u699c\u4e2d\u6392\u540d\u7b2c\u4e09\uff0c\u8bc1\u660e\u4e86\u8fd9\u4e9b\u6700\u5c0f\u4f46\u539f\u5219\u6027\u4fee\u6539\u7684\u6709\u6548\u6027\u548c\u7ade\u4e89\u529b", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4fdd\u6301\u539f\u59cb\u5b9e\u4f53\u94fe\u63a5\u6a21\u578b\u5b8c\u6574\uff0c\u4ec5\u4fee\u6539\u4e09\u4e2a\u4efb\u52a1\u5bf9\u9f50\u7ec4\u4ef6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u751f\u7269\u533b\u5b66\u591a\u8bed\u8a00\u5d4c\u5957\u5b9e\u4f53\u94fe\u63a5\u7684\u6311\u6218"}}
{"id": "2509.09726", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09726", "abs": "https://arxiv.org/abs/2509.09726", "authors": ["Seiji Hattori", "Takuya Matsuzaki", "Makoto Fujiwara"], "title": "Natural Language Translation of Formal Proofs through Informalization of Proof Steps and Recursive Summarization along Proof Structure", "comment": "Submitted to INLG 2025 (accepted)", "summary": "This paper proposes a natural language translation method for\nmachine-verifiable formal proofs that leverages the informalization\n(verbalization of formal language proof steps) and summarization capabilities\nof LLMs. For evaluation, it was applied to formal proof data created in\naccordance with natural language proofs taken from an undergraduate-level\ntextbook, and the quality of the generated natural language proofs was analyzed\nin comparison with the original natural language proofs. Furthermore, we will\ndemonstrate that this method can output highly readable and accurate natural\nlanguage proofs by applying it to existing formal proof library of the Lean\nproof assistant.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528LLM\u5c06\u673a\u5668\u53ef\u9a8c\u8bc1\u7684\u5f62\u5f0f\u5316\u8bc1\u660e\u7ffb\u8bd1\u4e3a\u81ea\u7136\u8bed\u8a00\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u5f62\u5f0f\u5316\u548c\u603b\u7ed3\u80fd\u529b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u81ea\u7136\u8bed\u8a00\u8bc1\u660e", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f62\u5f0f\u5316\u8bc1\u660e\u96be\u4ee5\u9605\u8bfb\u548c\u7406\u89e3\u7684\u95ee\u9898\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u5c06\u673a\u5668\u9a8c\u8bc1\u7684\u5f62\u5f0f\u5316\u8bc1\u660e\u8f6c\u6362\u4e3a\u66f4\u6613\u8bfb\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0", "method": "\u5229\u7528LLM\u7684\u975e\u5f62\u5f0f\u5316\uff08\u5f62\u5f0f\u8bed\u8a00\u8bc1\u660e\u6b65\u9aa4\u7684\u8a00\u8bed\u5316\uff09\u548c\u603b\u7ed3\u80fd\u529b\uff0c\u5c06\u5f62\u5f0f\u5316\u8bc1\u660e\u7ffb\u8bd1\u4e3a\u81ea\u7136\u8bed\u8a00\u3002\u65b9\u6cd5\u5e94\u7528\u4e8e\u4ece\u672c\u79d1\u6559\u79d1\u4e66\u81ea\u7136\u8bed\u8a00\u8bc1\u660e\u521b\u5efa\u7684\u5f62\u5f0f\u5316\u8bc1\u660e\u6570\u636e\uff0c\u5e76\u4e0e\u539f\u59cb\u81ea\u7136\u8bed\u8a00\u8bc1\u660e\u8fdb\u884c\u8d28\u91cf\u5bf9\u6bd4\u5206\u6790", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u9ad8\u5ea6\u53ef\u8bfb\u4e14\u51c6\u786e\u7684\u81ea\u7136\u8bed\u8a00\u8bc1\u660e\uff0c\u5728Lean\u8bc1\u660e\u52a9\u624b\u7684\u73b0\u6709\u5f62\u5f0f\u5316\u8bc1\u660e\u5e93\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eLLM\u7684\u81ea\u7136\u8bed\u8a00\u7ffb\u8bd1\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5c06\u5f62\u5f0f\u5316\u8bc1\u660e\u8f6c\u6362\u4e3a\u9ad8\u8d28\u91cf\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u63d0\u9ad8\u4e86\u8bc1\u660e\u7684\u53ef\u8bfb\u6027\u548c\u53ef\u7406\u89e3\u6027"}}
{"id": "2509.09727", "categories": ["cs.CL", "cs.CE"], "pdf": "https://arxiv.org/pdf/2509.09727", "abs": "https://arxiv.org/abs/2509.09727", "authors": ["Andy Zhu", "Yingjun Du"], "title": "A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs", "comment": "8 pages, 6 figures, Underreview", "summary": "Question answering (QA) plays a central role in financial education, yet\nexisting large language model (LLM) approaches often fail to capture the\nnuanced and specialized reasoning required for financial problem-solving. The\nfinancial domain demands multistep quantitative reasoning, familiarity with\ndomain-specific terminology, and comprehension of real-world scenarios. We\npresent a multi-agent framework that leverages role-based prompting to enhance\nperformance on domain-specific QA. Our framework comprises a Base Generator, an\nEvidence Retriever, and an Expert Reviewer agent that work in a single-pass\niteration to produce a refined answer. We evaluated our framework on a set of\n3,532 expert-designed finance education questions from Study.com, an online\nlearning platform. We leverage retrieval-augmented generation (RAG) for\ncontextual evidence from 6 finance textbooks and prompting strategies for a\ndomain-expert reviewer. Our experiments indicate that critique-based refinement\nimproves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines,\nwith the highest performance from Gemini-2.0-Flash. Furthermore, our method\nenables GPT-4o-mini to achieve performance comparable to the finance-tuned\nFinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to\nenhancing financial QA and offer insights for further research in multi-agent\nfinancial LLM systems.", "AI": {"tldr": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u6846\u67b6\u63d0\u5347\u91d1\u878d\u95ee\u7b54\u6027\u80fd\uff0c\u901a\u8fc7\u89d2\u8272\u63d0\u793a\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u5728\u91d1\u878d\u6559\u80b2\u95ee\u9898\u4e0a\u6bd4\u96f6\u6837\u672c\u601d\u7ef4\u94fe\u57fa\u7ebf\u51c6\u786e\u7387\u63d0\u53476.6-8.3%", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u9886\u57df\u95ee\u7b54\u4e2d\u96be\u4ee5\u6355\u6349\u4e13\u4e1a\u63a8\u7406\u9700\u6c42\uff0c\u91d1\u878d\u95ee\u9898\u9700\u8981\u591a\u6b65\u5b9a\u91cf\u63a8\u7406\u3001\u9886\u57df\u4e13\u4e1a\u672f\u8bed\u7406\u89e3\u548c\u73b0\u5b9e\u573a\u666f\u7406\u89e3", "method": "\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5305\u542b\u57fa\u7840\u751f\u6210\u5668\u3001\u8bc1\u636e\u68c0\u7d22\u5668\u548c\u4e13\u5bb6\u8bc4\u5ba1\u5668\uff0c\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u4ece6\u672c\u91d1\u878d\u6559\u6750\u83b7\u53d6\u4e0a\u4e0b\u6587\u8bc1\u636e\uff0c\u4f7f\u7528\u5355\u6b21\u8fed\u4ee3\u751f\u6210\u7cbe\u70bc\u7b54\u6848", "result": "\u57283,532\u4e2a\u4e13\u5bb6\u8bbe\u8ba1\u7684\u91d1\u878d\u6559\u80b2\u95ee\u9898\u4e0a\uff0c\u57fa\u4e8e\u6279\u5224\u7684\u4f18\u5316\u65b9\u6cd5\u6bd4\u96f6\u6837\u672c\u601d\u7ef4\u94fe\u57fa\u7ebf\u51c6\u786e\u7387\u63d0\u53476.6-8.3%\uff0cGemini-2.0-Flash\u8868\u73b0\u6700\u4f73\uff0cGPT-4o-mini\u8fbe\u5230\u4e0e\u91d1\u878d\u8c03\u4f18\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u91d1\u878d\u95ee\u7b54\u589e\u5f3a\u65b9\u6848\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u91d1\u878d\u5927\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u89c1\u89e3"}}
{"id": "2509.09728", "categories": ["cs.CL", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.09728", "abs": "https://arxiv.org/abs/2509.09728", "authors": ["Elena Rohde", "Jonas Klingwort", "Christian Borgs"], "title": "A meta-analysis on the performance of machine-learning based language models for sentiment analysis", "comment": null, "summary": "This paper presents a meta-analysis evaluating ML performance in sentiment\nanalysis for Twitter data. The study aims to estimate the average performance,\nassess heterogeneity between and within studies, and analyze how study\ncharacteristics influence model performance. Using PRISMA guidelines, we\nsearched academic databases and selected 195 trials from 20 studies with 12\nstudy features. Overall accuracy, the most reported performance metric, was\nanalyzed using double arcsine transformation and a three-level random effects\nmodel. The average overall accuracy of the AIC-optimized model was 0.80 [0.76,\n0.84]. This paper provides two key insights: 1) Overall accuracy is widely used\nbut often misleading due to its sensitivity to class imbalance and the number\nof sentiment classes, highlighting the need for normalization. 2) Standardized\nreporting of model performance, including reporting confusion matrices for\nindependent test sets, is essential for reliable comparisons of ML classifiers\nacross studies, which seems far from common practice.", "AI": {"tldr": "\u5bf9Twitter\u60c5\u611f\u5206\u6790\u4e2d\u673a\u5668\u5b66\u4e60\u6027\u80fd\u7684\u5143\u5206\u6790\u663e\u793a\u5e73\u5747\u51c6\u786e\u7387\u4e3a0.80\uff0c\u5f3a\u8c03\u9700\u8981\u6807\u51c6\u5316\u6027\u80fd\u62a5\u544a\u548c\u89c4\u8303\u5316\u51c6\u786e\u7387\u6307\u6807\u4ee5\u907f\u514d\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u8bef\u5bfc\u3002", "motivation": "\u8bc4\u4f30Twitter\u60c5\u611f\u5206\u6790\u4e2d\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5e73\u5747\u6027\u80fd\uff0c\u5206\u6790\u7814\u7a76\u95f4\u7684\u5f02\u8d28\u6027\uff0c\u5e76\u63a2\u8ba8\u7814\u7a76\u7279\u5f81\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u4ee5\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u6a21\u578b\u6bd4\u8f83\u57fa\u51c6\u3002", "method": "\u91c7\u7528PRISMA\u6307\u5357\u8fdb\u884c\u6587\u732e\u68c0\u7d22\uff0c\u4ece20\u9879\u7814\u7a76\u4e2d\u7b5b\u9009195\u4e2a\u8bd5\u9a8c\uff0c\u4f7f\u7528\u53cc\u53cd\u6b63\u5f26\u53d8\u6362\u548c\u4e09\u6c34\u5e73\u968f\u673a\u6548\u5e94\u6a21\u578b\u5206\u6790\u6700\u5e38\u62a5\u544a\u7684\u6574\u4f53\u51c6\u786e\u7387\u6307\u6807\u3002", "result": "AIC\u4f18\u5316\u6a21\u578b\u7684\u5e73\u5747\u6574\u4f53\u51c6\u786e\u7387\u4e3a0.80 [0.76, 0.84]\uff0c\u53d1\u73b0\u6574\u4f53\u51c6\u786e\u7387\u56e0\u5bf9\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u60c5\u611f\u7c7b\u522b\u6570\u91cf\u7684\u654f\u611f\u6027\u800c\u5bb9\u6613\u4ea7\u751f\u8bef\u5bfc\u3002", "conclusion": "\u9700\u8981\u89c4\u8303\u5316\u51c6\u786e\u7387\u6307\u6807\u4ee5\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u5efa\u7acb\u6807\u51c6\u5316\u7684\u6a21\u578b\u6027\u80fd\u62a5\u544a\u89c4\u8303\uff0c\u5305\u62ec\u4e3a\u72ec\u7acb\u6d4b\u8bd5\u96c6\u62a5\u544a\u6df7\u6dc6\u77e9\u9635\uff0c\u4ee5\u5b9e\u73b0\u8de8\u7814\u7a76\u7684\u53ef\u9760\u6bd4\u8f83\u3002"}}
{"id": "2509.09731", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09731", "abs": "https://arxiv.org/abs/2509.09731", "authors": ["Haiyang Yu", "Yuchuan Wu", "Fan Shi", "Lei Liao", "Jinghui Lu", "Xiaodong Ge", "Han Wang", "Minghan Zhuo", "Xuecheng Wu", "Xiang Fei", "Hao Feng", "Guozhi Tang", "An-Lan Wang", "Hanshen Zhu", "Yangfan He", "Quanhuan Liang", "Liyuan Meng", "Chao Feng", "Can Huang", "Jingqun Tang", "Bin Li"], "title": "Benchmarking Vision-Language Models on Chinese Ancient Documents: From OCR to Knowledge Reasoning", "comment": null, "summary": "Chinese ancient documents, invaluable carriers of millennia of Chinese\nhistory and culture, hold rich knowledge across diverse fields but face\nchallenges in digitization and understanding, i.e., traditional methods only\nscan images, while current Vision-Language Models (VLMs) struggle with their\nvisual and linguistic complexity. Existing document benchmarks focus on English\nprinted texts or simplified Chinese, leaving a gap for evaluating VLMs on\nancient Chinese documents. To address this, we present AncientDoc, the first\nbenchmark for Chinese ancient documents, designed to assess VLMs from OCR to\nknowledge reasoning. AncientDoc includes five tasks (page-level OCR, vernacular\ntranslation, reasoning-based QA, knowledge-based QA, linguistic variant QA) and\ncovers 14 document types, over 100 books, and about 3,000 pages. Based on\nAncientDoc, we evaluate mainstream VLMs using multiple metrics, supplemented by\na human-aligned large language model for scoring.", "AI": {"tldr": "AncientDoc\u662f\u9996\u4e2a\u9488\u5bf9\u4e2d\u6587\u53e4\u7c4d\u6587\u6863\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e94\u4e2a\u4efb\u52a1\uff08\u9875\u9762\u7ea7OCR\u3001\u767d\u8bdd\u7ffb\u8bd1\u3001\u63a8\u7406\u95ee\u7b54\u3001\u77e5\u8bc6\u95ee\u7b54\u3001\u8bed\u8a00\u53d8\u4f53\u95ee\u7b54\uff09\uff0c\u6db5\u76d614\u79cd\u6587\u6863\u7c7b\u578b\u3001100\u591a\u672c\u4e66\u7c4d\u548c\u7ea63000\u9875\u5185\u5bb9\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e2d\u6587\u53e4\u7c4d\u5904\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u4e2d\u6587\u53e4\u7c4d\u4f5c\u4e3a\u4e2d\u534e\u5386\u53f2\u6587\u5316\u7684\u91cd\u8981\u8f7d\u4f53\uff0c\u5728\u6570\u5b57\u5316\u548c\u7406\u89e3\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\u4ec5\u626b\u63cf\u56fe\u50cf\uff0c\u800c\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u5904\u7406\u53e4\u7c4d\u7684\u89c6\u89c9\u548c\u8bed\u8a00\u590d\u6742\u6027\u3002\u73b0\u6709\u6587\u6863\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u82f1\u6587\u5370\u5237\u6587\u672c\u6216\u7b80\u4f53\u4e2d\u6587\uff0c\u7f3a\u4e4f\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e2d\u6587\u53e4\u7c4d\u4e0a\u8868\u73b0\u7684\u57fa\u51c6\u3002", "method": "\u6784\u5efaAncientDoc\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e94\u4e2a\u6838\u5fc3\u4efb\u52a1\uff1a\u9875\u9762\u7ea7OCR\u3001\u767d\u8bdd\u7ffb\u8bd1\u3001\u63a8\u7406\u95ee\u7b54\u3001\u77e5\u8bc6\u95ee\u7b54\u3001\u8bed\u8a00\u53d8\u4f53\u95ee\u7b54\u3002\u6570\u636e\u96c6\u6db5\u76d614\u79cd\u6587\u6863\u7c7b\u578b\u3001100\u591a\u672c\u4e66\u7c4d\u548c\u7ea63000\u9875\u5185\u5bb9\u3002\u4f7f\u7528\u591a\u79cd\u6307\u6807\u8bc4\u4f30\u4e3b\u6d41\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u4eba\u7c7b\u5bf9\u9f50\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u5206\u8865\u5145\u3002", "result": "\u57fa\u4e8eAncientDoc\u57fa\u51c6\u5bf9\u4e3b\u6d41\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u4e2d\u6587\u53e4\u7c4d\u5904\u7406\u65b9\u9762\u7684\u8868\u73b0\u548c\u5c40\u9650\u6027\u3002", "conclusion": "AncientDoc\u586b\u8865\u4e86\u4e2d\u6587\u53e4\u7c4d\u6587\u6863\u8bc4\u4f30\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4e2d\u6587\u53e4\u7c4d\u5904\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u53e4\u7c4d\u6570\u5b57\u5316\u548c\u7406\u89e3\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.09734", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09734", "abs": "https://arxiv.org/abs/2509.09734", "authors": ["Zikang Guo", "Benfeng Xu", "Chiwei Zhu", "Wentao Hong", "Xiaorui Wang", "Zhendong Mao"], "title": "MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools", "comment": null, "summary": "The Model Context Protocol (MCP) is rapidly emerging as a pivotal open\nstandard, designed to enhance agent-tool integration and interoperability, and\nis positioned to unlock a new era of powerful, interconnected, and genuinely\nutilitarian agentic AI. However, despite MCP's growing adoption, existing\nbenchmarks often fail to capture real-world agent performance within this new\nparadigm, leading to a distorted perception of their true operational value and\nan inability to reliably differentiate proficiencies. To bridge this critical\nevaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark\nspecifically engineered to rigorously assess language agent capabilities in\nMCP-mediated tool interactions. Core contributions of MCP-AgentBench include:\nthe establishment of a robust MCP testbed comprising 33 operational servers\nwith 188 distinct tools; the development of a benchmark featuring 600\nsystematically designed queries distributed across 6 distinct categories of\nvarying interaction complexity; and the introduction of MCP-Eval, a novel\noutcome-oriented evaluation methodology prioritizing real-world task success.\nThrough extensive empirical evaluation of leading language agents, we provide\nfoundational insights. MCP-AgentBench aims to equip the research community with\na standardized and reliable framework to build, validate, and advance agents\ncapable of fully leveraging MCP's transformative benefits, thereby accelerating\nprogress toward truly capable and interoperable AI systems.", "AI": {"tldr": "MCP-AgentBench\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u8bc4\u4f30\u8bed\u8a00\u4ee3\u7406\u5728MCP\u534f\u8bae\u4e0b\u7684\u5de5\u5177\u4ea4\u4e92\u80fd\u529b\u800c\u8bbe\u8ba1\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b33\u4e2a\u670d\u52a1\u5668\u3001188\u4e2a\u5de5\u5177\u548c600\u4e2a\u67e5\u8be2\uff0c\u91c7\u7528\u7ed3\u679c\u5bfc\u5411\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30MCP\u534f\u8bae\u4e0bAI\u4ee3\u7406\u7684\u771f\u5b9e\u6027\u80fd\uff0c\u5bfc\u81f4\u5bf9\u5176\u5b9e\u9645\u64cd\u4f5c\u4ef7\u503c\u7684\u8ba4\u77e5\u504f\u5dee\uff0c\u65e0\u6cd5\u53ef\u9760\u533a\u5206\u4e0d\u540c\u4ee3\u7406\u7684\u80fd\u529b\u5dee\u5f02\u3002", "method": "\u5efa\u7acb\u5305\u542b33\u4e2a\u64cd\u4f5c\u670d\u52a1\u5668\u548c188\u4e2a\u4e0d\u540c\u5de5\u5177\u7684MCP\u6d4b\u8bd5\u5e8a\uff1b\u5f00\u53d1\u5305\u542b600\u4e2a\u7cfb\u7edf\u8bbe\u8ba1\u67e5\u8be2\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u5e03\u57286\u4e2a\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u4ea4\u4e92\u7c7b\u522b\uff1b\u5f15\u5165MCP-Eval\u7ed3\u679c\u5bfc\u5411\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5bf9\u9886\u5148\u8bed\u8a00\u4ee3\u7406\u7684\u5e7f\u6cdb\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4e86\u57fa\u7840\u6027\u89c1\u89e3\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u4ee3\u7406\u5728MCP\u73af\u5883\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "MCP-AgentBench\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u3001\u53ef\u9760\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u3001\u9a8c\u8bc1\u548c\u63a8\u8fdb\u80fd\u591f\u5145\u5206\u5229\u7528MCP\u53d8\u9769\u6027\u4f18\u52bf\u7684AI\u4ee3\u7406\uff0c\u52a0\u901f\u5b9e\u73b0\u771f\u6b63\u6709\u80fd\u529b\u4e14\u53ef\u4e92\u64cd\u4f5c\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2509.09735", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09735", "abs": "https://arxiv.org/abs/2509.09735", "authors": ["Willem Huijzer", "Jieying Chen"], "title": "Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation", "comment": "7 pages", "summary": "The rapid integration of Large Language Models (LLMs) into various domains\nraises concerns about societal inequalities and information bias. This study\nexamines biases in LLMs related to background, gender, and age, with a focus on\ntheir impact on decision-making and summarization tasks. Additionally, the\nresearch examines the cross-lingual propagation of these biases and evaluates\nthe effectiveness of prompt-instructed mitigation strategies. Using an adapted\nversion of the dataset by Tamkin et al. (2023) translated into Dutch, we\ncreated 151,200 unique prompts for the decision task and 176,400 for the\nsummarisation task. Various demographic variables, instructions, salience\nlevels, and languages were tested on GPT-3.5 and GPT-4o. Our analysis revealed\nthat both models were significantly biased during decision-making, favouring\nfemale gender, younger ages, and certain backgrounds such as the\nAfrican-American background. In contrast, the summarisation task showed minimal\nevidence of bias, though significant age-related differences emerged for\nGPT-3.5 in English. Cross-lingual analysis showed that bias patterns were\nbroadly similar between English and Dutch, though notable differences were\nobserved across specific demographic categories. The newly proposed mitigation\ninstructions, while unable to eliminate biases completely, demonstrated\npotential in reducing them. The most effective instruction achieved a 27\\% mean\nreduction in the gap between the most and least favorable demographics.\nNotably, contrary to GPT-3.5, GPT-4o displayed reduced biases for all prompts\nin English, indicating the specific potential for prompt-based mitigation\nwithin newer models. This research underscores the importance of cautious\nadoption of LLMs and context-specific bias testing, highlighting the need for\ncontinued development of effective mitigation strategies to ensure responsible\ndeployment of AI.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790GPT-3.5\u548cGPT-4o\u5728\u51b3\u7b56\u548c\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u504f\u89c1\u95ee\u9898\uff0c\u53d1\u73b0\u51b3\u7b56\u4efb\u52a1\u5b58\u5728\u663e\u8457\u7684\u6027\u522b\u3001\u5e74\u9f84\u548c\u80cc\u666f\u504f\u89c1\uff0c\u800c\u6458\u8981\u4efb\u52a1\u504f\u89c1\u8f83\u5c0f\u3002\u7814\u7a76\u8fd8\u6d4b\u8bd5\u4e86\u8de8\u8bed\u8a00\u504f\u89c1\u4f20\u64ad\u548c\u63d0\u793a\u6307\u4ee4\u7f13\u89e3\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5404\u9886\u57df\u7684\u5feb\u901f\u5e94\u7528\uff0c\u4eba\u4eec\u5bf9\u5176\u53ef\u80fd\u52a0\u5267\u793e\u4f1a\u4e0d\u5e73\u7b49\u548c\u4fe1\u606f\u504f\u89c1\u7684\u62c5\u5fe7\u65e5\u76ca\u589e\u52a0\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u6a21\u578b\u504f\u89c1\u53ca\u5176\u7f13\u89e3\u7b56\u7565\u3002", "method": "\u4f7f\u7528Tamkin\u7b49\u4eba(2023)\u6570\u636e\u96c6\u7684\u8377\u5170\u8bed\u7ffb\u8bd1\u7248\u672c\uff0c\u521b\u5efa\u4e86151,200\u4e2a\u51b3\u7b56\u4efb\u52a1\u63d0\u793a\u548c176,400\u4e2a\u6458\u8981\u4efb\u52a1\u63d0\u793a\uff0c\u6d4b\u8bd5\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u53d8\u91cf\u3001\u6307\u4ee4\u3001\u663e\u8457\u5ea6\u6c34\u5e73\u548c\u8bed\u8a00\u5728GPT-3.5\u548cGPT-4o\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u4e24\u4e2a\u6a21\u578b\u5728\u51b3\u7b56\u4efb\u52a1\u4e2d\u90fd\u5b58\u5728\u663e\u8457\u504f\u89c1\uff0c\u504f\u5411\u5973\u6027\u3001\u5e74\u8f7b\u5e74\u9f84\u548c\u7279\u5b9a\u80cc\u666f\uff08\u5982\u975e\u88d4\u7f8e\u56fd\u4eba\uff09\u3002\u6458\u8981\u4efb\u52a1\u504f\u89c1\u8f83\u5c0f\uff0c\u4f46GPT-3.5\u5728\u82f1\u8bed\u4e2d\u663e\u793a\u51fa\u663e\u8457\u7684\u5e74\u9f84\u76f8\u5173\u5dee\u5f02\u3002\u8de8\u8bed\u8a00\u5206\u6790\u663e\u793a\u82f1\u8bed\u548c\u8377\u5170\u8bed\u7684\u504f\u89c1\u6a21\u5f0f\u5927\u4f53\u76f8\u4f3c\u3002\u63d0\u793a\u6307\u4ee4\u7f13\u89e3\u7b56\u7565\u5e73\u5747\u80fd\u51cf\u5c1127%\u7684\u6700\u6709\u5229\u548c\u6700\u4e0d\u5229\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u8981\u8c28\u614e\u91c7\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5e76\u8fdb\u884c\u7279\u5b9a\u60c5\u5883\u7684\u504f\u89c1\u6d4b\u8bd5\uff0c\u540c\u65f6\u9700\u8981\u6301\u7eed\u5f00\u53d1\u6709\u6548\u7684\u7f13\u89e3\u7b56\u7565\u4ee5\u786e\u4fddAI\u7684\u8d1f\u8d23\u4efb\u90e8\u7f72\u3002GPT-4o\u5728\u6240\u6709\u82f1\u8bed\u63d0\u793a\u4e2d\u90fd\u663e\u793a\u51fa\u504f\u89c1\u51cf\u5c11\uff0c\u8868\u660e\u65b0\u6a21\u578b\u5728\u57fa\u4e8e\u63d0\u793a\u7684\u7f13\u89e3\u65b9\u9762\u5177\u6709\u7279\u5b9a\u6f5c\u529b\u3002"}}
{"id": "2509.09801", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07, 68T50, 68T05", "I.2.7; I.2.6; C.4"], "pdf": "https://arxiv.org/pdf/2509.09801", "abs": "https://arxiv.org/abs/2509.09801", "authors": ["Brennen Hill"], "title": "HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning", "comment": null, "summary": "The adaptation of large language models (LLMs) to specialized reasoning tasks\nis fundamentally constrained by computational resources. Parameter-Efficient\nFine-Tuning (PEFT) methods have emerged as a powerful solution, yet the\nlandscape of these techniques is diverse, with distinct methods operating in\neither the model's weight space or its representation space. This paper\ninvestigates the hypothesis that a synergistic combination of these paradigms\ncan unlock superior performance and efficiency. We introduce HEFT (Hierarchical\nEfficient Fine-Tuning), a novel hierarchical adaptation strategy that composes\ntwo distinct PEFT methods in a coarse-to-fine manner: first, a broad,\nfoundational adaptation in the weight space using Low-Rank Adaptation (LoRA),\nfollowed by a precise, surgical refinement of internal activations using\nRepresentation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a\nLlama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential\nreasoning. Our results reveal a profound synergistic effect. A model fine-tuned\nfor only three epochs with our HEFT strategy achieves an accuracy of 85.17\\%,\nexceeding the performance of models trained for 20 epochs with either LoRA-only\n(85.05\\%) or ReFT-only (83.36\\%) methodologies. This work demonstrates that the\nthoughtful composition of PEFT methods is a potent algorithmic innovation,\noffering a more efficient and effective path toward advancing the reasoning\ncapabilities of language models. By achieving superior results with a fraction\nof the computational budget, our findings present a principled approach to\novercoming the obstacles inherent in adapting large-scale models for complex\ncognitive tasks.", "AI": {"tldr": "HEFT\u662f\u4e00\u79cd\u5206\u5c42\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\uff0c\u7ed3\u5408\u4e86LoRA\u548cReFT\u4e24\u79cd\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u5728BoolQ\u63a8\u7406\u57fa\u51c6\u4e0a\u4ee5\u66f4\u5c11\u8bad\u7ec3\u8f6e\u6b21\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e13\u4e1a\u5316\u63a8\u7406\u4efb\u52a1\u9002\u914d\u53d7\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\uff0c\u4e0d\u540cPEFT\u65b9\u6cd5\u5404\u6709\u4f18\u52bf\uff0c\u7814\u7a76\u5047\u8bbe\u534f\u540c\u7ec4\u5408\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u4ee5\u89e3\u9501\u66f4\u597d\u7684\u6027\u80fd\u548c\u6548\u7387", "method": "\u63d0\u51faHEFT\u5206\u5c42\u9002\u914d\u7b56\u7565\uff1a\u5148\u5728\u6743\u91cd\u7a7a\u95f4\u4f7f\u7528LoRA\u8fdb\u884c\u5e7f\u6cdb\u57fa\u7840\u9002\u914d\uff0c\u7136\u540e\u5728\u8868\u793a\u7a7a\u95f4\u4f7f\u7528ReFT\u8fdb\u884c\u7cbe\u786e\u7ec6\u5316\u8c03\u6574", "result": "\u5728Llama-2-7B\u6a21\u578b\u4e0a\uff0c\u4ec5\u8bad\u7ec33\u4e2aepoch\u7684HEFT\u8fbe\u523085.17%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u8bad\u7ec320\u4e2aepoch\u7684LoRA-only(85.05%)\u548cReFT-only(83.36%)\u65b9\u6cd5", "conclusion": "PEFT\u65b9\u6cd5\u7684\u7cbe\u5fc3\u7ec4\u5408\u662f\u5f3a\u5927\u7684\u7b97\u6cd5\u521b\u65b0\uff0c\u4e3a\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u6709\u6548\u7684\u8def\u5f84\uff0c\u4ee5\u66f4\u5c11\u8ba1\u7b97\u9884\u7b97\u83b7\u5f97\u66f4\u597d\u7ed3\u679c"}}
{"id": "2509.09804", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09804", "abs": "https://arxiv.org/abs/2509.09804", "authors": ["Helen de Andrade Abreu", "Tiago Timponi Torrent", "Ely Edison da Silva Matos"], "title": "Pragmatic Frames Evoked by Gestures: A FrameNet Brasil Approach to Multimodality in Turn Organization", "comment": "Paper submitted to Language Sciences Journal", "summary": "This paper proposes a framework for modeling multimodal conversational turn\norganization via the proposition of correlations between language and\ninteractive gestures, based on analysis as to how pragmatic frames are\nconceptualized and evoked by communicators. As a means to provide evidence for\nthe analysis, we developed an annotation methodology to enrich a multimodal\ndataset (annotated for semantic frames) with pragmatic frames modeling\nconversational turn organization. Although conversational turn organization has\nbeen studied by researchers from diverse fields, the specific strategies,\nespecially gestures used by communicators, had not yet been encoded in a\ndataset that can be used for machine learning. To fill this gap, we enriched\nthe Frame2 dataset with annotations of gestures used for turn organization. The\nFrame2 dataset features 10 episodes from the Brazilian TV series Pedro Pelo\nMundo annotated for semantic frames evoked in both video and text. This dataset\nallowed us to closely observe how communicators use interactive gestures\noutside a laboratory, in settings, to our knowledge, not previously recorded in\nrelated literature. Our results have confirmed that communicators involved in\nface-to-face conversation make use of gestures as a tool for passing, taking\nand keeping conversational turns, and also revealed variations of some gestures\nthat had not been documented before. We propose that the use of these gestures\narises from the conceptualization of pragmatic frames, involving mental spaces,\nblending and conceptual metaphors. In addition, our data demonstrate that the\nannotation of pragmatic frames contributes to a deeper understanding of human\ncognition and language.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u8fc7\u8bed\u8a00\u548c\u4ea4\u4e92\u624b\u52bf\u5173\u8054\u6765\u5efa\u6a21\u591a\u6a21\u6001\u5bf9\u8bdd\u8f6e\u6b21\u7ec4\u7ec7\u7684\u6846\u67b6\uff0c\u57fa\u4e8e\u8bed\u7528\u6846\u67b6\u5982\u4f55\u88ab\u6982\u5ff5\u5316\u548c\u5524\u8d77\u7684\u5206\u6790\uff0c\u5e76\u5728Frame2\u6570\u636e\u96c6\u4e2d\u589e\u52a0\u4e86\u624b\u52bf\u6807\u6ce8\u6765\u9a8c\u8bc1", "motivation": "\u586b\u8865\u5bf9\u8bdd\u8f6e\u6b21\u7ec4\u7ec7\u4e2d\u7279\u5b9a\u7b56\u7565\uff08\u7279\u522b\u662f\u624b\u52bf\uff09\u5728\u673a\u5668\u5b66\u4e60\u53ef\u7528\u6570\u636e\u96c6\u4e2d\u7684\u7f16\u7801\u7a7a\u767d\uff0c\u7814\u7a76\u9762\u5bf9\u9762\u5bf9\u8bdd\u4e2d\u624b\u52bf\u5982\u4f55\u7528\u4e8e\u4f20\u9012\u3001\u83b7\u53d6\u548c\u4fdd\u6301\u5bf9\u8bdd\u8f6e\u6b21", "method": "\u5f00\u53d1\u4e86\u6807\u6ce8\u65b9\u6cd5\uff0c\u5728\u5df2\u6807\u6ce8\u8bed\u4e49\u6846\u67b6\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6Frame2\u57fa\u7840\u4e0a\u589e\u52a0\u8bed\u7528\u6846\u67b6\u6807\u6ce8\uff0c\u5206\u6790\u5df4\u897f\u7535\u89c6\u5267\u4e2d\u768410\u96c6\u5185\u5bb9\uff0c\u89c2\u5bdf\u975e\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e0b\u7684\u4ea4\u4e92\u624b\u52bf\u4f7f\u7528", "result": "\u786e\u8ba4\u4e86\u9762\u5bf9\u9762\u5bf9\u8bdd\u4e2d\u624b\u52bf\u4f5c\u4e3a\u5bf9\u8bdd\u8f6e\u6b21\u7ba1\u7406\u5de5\u5177\u7684\u4f7f\u7528\uff0c\u53d1\u73b0\u4e86\u4e4b\u524d\u672a\u8bb0\u5f55\u7684\u624b\u52bf\u53d8\u4f53\uff0c\u8bc1\u660e\u4e86\u8bed\u7528\u6846\u67b6\u6807\u6ce8\u6709\u52a9\u4e8e\u66f4\u6df1\u5165\u7406\u89e3\u4eba\u7c7b\u8ba4\u77e5\u548c\u8bed\u8a00", "conclusion": "\u624b\u52bf\u4f7f\u7528\u6e90\u4e8e\u8bed\u7528\u6846\u67b6\u7684\u6982\u5ff5\u5316\uff0c\u6d89\u53ca\u5fc3\u7406\u7a7a\u95f4\u3001\u6982\u5ff5\u6574\u5408\u548c\u6982\u5ff5\u9690\u55bb\uff0c\u8bed\u7528\u6846\u67b6\u6807\u6ce8\u4e3a\u7406\u89e3\u4eba\u7c7b\u8ba4\u77e5\u548c\u8bed\u8a00\u63d0\u4f9b\u4e86\u91cd\u8981\u6d1e\u89c1"}}
{"id": "2509.09852", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09852", "abs": "https://arxiv.org/abs/2509.09852", "authors": ["Chuyuan Li", "Austin Xu", "Shafiq Joty", "Giuseppe Carenini"], "title": "Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization", "comment": null, "summary": "A key challenge in Multi-Document Summarization (MDS) is effectively\nintegrating information from multiple sources while maintaining coherence and\ntopical relevance. While Large Language Models have shown impressive results in\nsingle-document summarization, their performance on MDS still leaves room for\nimprovement. In this paper, we propose a topic-guided reinforcement learning\napproach to improve content selection in MDS. We first show that explicitly\nprompting models with topic labels enhances the informativeness of the\ngenerated summaries. Building on this insight, we propose a novel topic reward\nwithin the Group Relative Policy Optimization (GRPO) framework to measure topic\nalignment between the generated summary and source documents. Experimental\nresults on the Multi-News and Multi-XScience datasets demonstrate that our\nmethod consistently outperforms strong baselines, highlighting the\neffectiveness of leveraging topical cues in MDS.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e3b\u9898\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3b\u9898\u5956\u52b1\u673a\u5236\u5728\u591a\u6587\u6863\u6458\u8981\u4e2d\u63d0\u5347\u5185\u5bb9\u9009\u62e9\u6548\u679c\uff0c\u5728Multi-News\u548cMulti-XScience\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u591a\u6587\u6863\u6458\u8981\u4e2d\u5982\u4f55\u6709\u6548\u6574\u5408\u591a\u4e2a\u6765\u6e90\u7684\u4fe1\u606f\u540c\u65f6\u4fdd\u6301\u8fde\u8d2f\u6027\u548c\u4e3b\u9898\u76f8\u5173\u6027\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5355\u6587\u6863\u6458\u8981\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u591a\u6587\u6863\u6458\u8981\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4", "method": "\u9996\u5148\u8bc1\u660e\u663e\u5f0f\u4f7f\u7528\u4e3b\u9898\u6807\u7b7e\u63d0\u793a\u6a21\u578b\u53ef\u4ee5\u589e\u5f3a\u751f\u6210\u6458\u8981\u7684\u4fe1\u606f\u91cf\uff0c\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u5728GRPO\u6846\u67b6\u5185\u4f7f\u7528\u65b0\u9896\u7684\u4e3b\u9898\u5956\u52b1\u6765\u8861\u91cf\u751f\u6210\u6458\u8981\u4e0e\u6e90\u6587\u6863\u4e4b\u95f4\u7684\u4e3b\u9898\u5bf9\u9f50\u5ea6", "result": "\u5728Multi-News\u548cMulti-XScience\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u5229\u7528\u4e3b\u9898\u7ebf\u7d22\u5728\u591a\u6587\u6863\u6458\u8981\u4e2d\u662f\u6709\u6548\u7684\uff0c\u4e3b\u9898\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u591a\u6587\u6863\u6458\u8981\u7684\u5185\u5bb9\u9009\u62e9\u8d28\u91cf"}}
{"id": "2509.09871", "categories": ["cs.CL", "cs.AI", "68T50 (Primary) 91F10 (Secondary)"], "pdf": "https://arxiv.org/pdf/2509.09871", "abs": "https://arxiv.org/abs/2509.09871", "authors": ["Basti\u00e1n Gonz\u00e1lez-Bustamante", "Nando Verelst", "Carla Cisternas"], "title": "Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case", "comment": "Working paper: 18 pages, 4 tables, 2 figures", "summary": "Large Language Models (LLMs) offer promising avenues for methodological and\napplied innovations in survey research by using synthetic respondents to\nemulate human answers and behaviour, potentially mitigating measurement and\nrepresentation errors. However, the extent to which LLMs recover aggregate item\ndistributions remains uncertain and downstream applications risk reproducing\nsocial stereotypes and biases inherited from training data. We evaluate the\nreliability of LLM-generated synthetic survey responses against ground-truth\nhuman responses from a Chilean public opinion probabilistic survey.\nSpecifically, we benchmark 128 prompt-model-question triplets, generating\n189,696 synthetic profiles, and pool performance metrics (i.e., accuracy,\nprecision, recall, and F1-score) in a meta-analysis across 128\nquestion-subsample pairs to test for biases along key sociodemographic\ndimensions. The evaluation spans OpenAI's GPT family and o-series reasoning\nmodels, as well as Llama and Qwen checkpoints. Three results stand out. First,\nsynthetic responses achieve excellent performance on trust items (F1-score and\naccuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform\ncomparably on this task. Third, synthetic-human alignment is highest among\nrespondents aged 45-59. Overall, LLM-based synthetic samples approximate\nresponses from a probabilistic sample, though with substantial item-level\nheterogeneity. Capturing the full nuance of public opinion remains challenging\nand requires careful calibration and additional distributional tests to ensure\nalgorithmic fidelity and reduce errors.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5408\u6210\u8c03\u67e5\u56de\u590d\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0LLM\u5728\u4fe1\u4efb\u9879\u76ee\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cGPT-4o\u7b49\u6a21\u578b\u8868\u73b0\u76f8\u5f53\uff0c\u4f46\u4e0e\u4eba\u7c7b\u56de\u7b54\u4ecd\u5b58\u5728\u9879\u76ee\u7ea7\u5f02\u8d28\u6027\u3002", "motivation": "\u8bc4\u4f30LLM\u751f\u6210\u5408\u6210\u8c03\u67e5\u56de\u590d\u7684\u53ef\u9760\u6027\uff0c\u68c0\u9a8c\u5176\u662f\u5426\u80fd\u51c6\u786e\u6a21\u62df\u4eba\u7c7b\u56de\u7b54\u884c\u4e3a\uff0c\u540c\u65f6\u907f\u514d\u590d\u5236\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u793e\u4f1a\u523b\u677f\u5370\u8c61\u548c\u504f\u89c1\u3002", "method": "\u4f7f\u7528128\u4e2a\u63d0\u793a-\u6a21\u578b-\u95ee\u9898\u4e09\u5143\u7ec4\u751f\u6210189,696\u4e2a\u5408\u6210\u914d\u7f6e\u6587\u4ef6\uff0c\u901a\u8fc7\u5143\u5206\u6790\u8bc4\u4f30\u51c6\u786e\u6027\u3001\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u7b49\u6027\u80fd\u6307\u6807\uff0c\u6d4b\u8bd5\u5173\u952e\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u7ef4\u5ea6\u4e0a\u7684\u504f\u5dee\u3002", "result": "\u5408\u6210\u56de\u590d\u5728\u4fe1\u4efb\u9879\u76ee\u4e0a\u8868\u73b0\u4f18\u5f02\uff08F1\u5206\u6570\u548c\u51c6\u786e\u7387>0.90\uff09\uff1bGPT-4o\u3001GPT-4o-mini\u548cLlama 4 Maverick\u8868\u73b0\u76f8\u5f53\uff1b45-59\u5c81\u53d7\u8bbf\u8005\u7684\u5408\u6210-\u4eba\u7c7b\u5bf9\u9f50\u5ea6\u6700\u9ad8\u3002", "conclusion": "LLM\u751f\u6210\u7684\u5408\u6210\u6837\u672c\u80fd\u591f\u8fd1\u4f3c\u6982\u7387\u6837\u672c\u7684\u56de\u7b54\uff0c\u4f46\u4ecd\u5b58\u5728\u663e\u8457\u7684\u9879\u76ee\u7ea7\u5f02\u8d28\u6027\uff0c\u9700\u8981\u4ed4\u7ec6\u6821\u51c6\u548c\u989d\u5916\u7684\u5206\u5e03\u6d4b\u8bd5\u6765\u786e\u4fdd\u7b97\u6cd5\u4fdd\u771f\u5ea6\u548c\u51cf\u5c11\u8bef\u5dee\u3002"}}
{"id": "2509.09969", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09969", "abs": "https://arxiv.org/abs/2509.09969", "authors": ["Zhitian Hou", "Zihan Ye", "Nanli Zeng", "Tianyong Hao", "Kun Zeng"], "title": "Large Language Models Meet Legal Artificial Intelligence: A Survey", "comment": null, "summary": "Large Language Models (LLMs) have significantly advanced the development of\nLegal Artificial Intelligence (Legal AI) in recent years, enhancing the\nefficiency and accuracy of legal tasks. To advance research and applications of\nLLM-based approaches in legal domain, this paper provides a comprehensive\nreview of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and\nalso gather 15 benchmarks and 29 datasets to evaluate different legal\ncapabilities. Additionally, we analyse the challenges and discuss future\ndirections for LLM-based approaches in the legal domain. We hope this paper\nprovides a systematic introduction for beginners and encourages future research\nin this field. Resources are available at\nhttps://github.com/ZhitianHou/LLMs4LegalAI.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6cd5\u5f8bAI\u9886\u57df\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u6db5\u76d6\u4e8616\u4e2a\u6cd5\u5f8bLLM\u7cfb\u5217\u300147\u4e2a\u57fa\u4e8eLLM\u7684\u6cd5\u5f8b\u4efb\u52a1\u6846\u67b6\u300115\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c29\u4e2a\u6570\u636e\u96c6\uff0c\u5e76\u5206\u6790\u4e86\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728Legal AI\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u68b3\u7406\u548c\u603b\u7ed3\u73b0\u6709\u7814\u7a76\u6210\u679c\uff0c\u4e3a\u521d\u5b66\u8005\u63d0\u4f9b\u7cfb\u7edf\u4ecb\u7ecd\u5e76\u63a8\u52a8\u8be5\u9886\u57df\u7684\u672a\u6765\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u5168\u9762\u8c03\u7814\u548c\u7efc\u8ff0\u7684\u65b9\u6cd5\uff0c\u6536\u96c6\u548c\u5206\u679016\u4e2a\u6cd5\u5f8bLLM\u7cfb\u5217\u300147\u4e2aLLM\u6846\u67b6\u300115\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c29\u4e2a\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u7cfb\u7edf\u6027\u6574\u7406\u548c\u5206\u6790\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u6cd5\u5f8bAI\u8d44\u6e90\u4f53\u7cfb\uff0c\u5305\u62ec\u6a21\u578b\u3001\u6846\u67b6\u3001\u57fa\u51c6\u548c\u6570\u636e\u96c6\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u53c2\u8003\u8d44\u6e90\u3002", "conclusion": "LLM\u5728\u6cd5\u5f8bAI\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u53d1\u5c55\u3002\u8be5\u7efc\u8ff0\u4e3a\u9886\u57df\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u548c\u672a\u6765\u65b9\u5411\u6307\u5f15\u3002"}}
{"id": "2509.09990", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09990", "abs": "https://arxiv.org/abs/2509.09990", "authors": ["Guixian Xu", "Zeli Su", "Ziyin Zhang", "Jianing Liu", "XU Han", "Ting Zhang", "Yushuang Dong"], "title": "CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China", "comment": null, "summary": "Minority languages in China, such as Tibetan, Uyghur, and Traditional\nMongolian, face significant challenges due to their unique writing systems,\nwhich differ from international standards. This discrepancy has led to a severe\nlack of relevant corpora, particularly for supervised tasks like headline\ngeneration. To address this gap, we introduce a novel dataset, Chinese Minority\nHeadline Generation (CMHG), which includes 100,000 entries for Tibetan, and\n50,000 entries each for Uyghur and Mongolian, specifically curated for headline\ngeneration tasks. Additionally, we propose a high-quality test set annotated by\nnative speakers, designed to serve as a benchmark for future research in this\ndomain. We hope this dataset will become a valuable resource for advancing\nheadline generation in Chinese minority languages and contribute to the\ndevelopment of related benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u4e2d\u56fd\u5c11\u6570\u6c11\u65cf\u8bed\u8a00\uff08\u85cf\u8bed\u3001\u7ef4\u543e\u5c14\u8bed\u3001\u8499\u53e4\u8bed\uff09\u7684\u65b0\u95fb\u6807\u9898\u751f\u6210\u6570\u636e\u96c6CMHG\uff0c\u5305\u542b20\u4e07\u6761\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u4e86\u7531\u6bcd\u8bed\u8005\u6807\u6ce8\u7684\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u96c6\u4f5c\u4e3a\u57fa\u51c6\u3002", "motivation": "\u4e2d\u56fd\u5c11\u6570\u6c11\u65cf\u8bed\u8a00\u7531\u4e8e\u4e66\u5199\u7cfb\u7edf\u4e0e\u56fd\u9645\u6807\u51c6\u4e0d\u540c\uff0c\u5bfc\u81f4\u76f8\u5173\u8bed\u6599\u5e93\u4e25\u91cd\u7f3a\u4e4f\uff0c\u7279\u522b\u662f\u5728\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\u5982\u6807\u9898\u751f\u6210\u65b9\u9762\u5b58\u5728\u660e\u663e\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86CMHG\u6570\u636e\u96c6\uff0c\u5305\u542b10\u4e07\u6761\u85cf\u8bed\u6570\u636e\u548c\u54045\u4e07\u6761\u7ef4\u543e\u5c14\u8bed\u3001\u8499\u53e4\u8bed\u6570\u636e\uff0c\u4e13\u95e8\u7528\u4e8e\u6807\u9898\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u63d0\u4f9b\u4e86\u7531\u6bcd\u8bed\u8005\u6807\u6ce8\u7684\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u96c6\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b20\u4e07\u6761\u6570\u636e\u7684\u5c11\u6570\u6c11\u65cf\u8bed\u8a00\u6807\u9898\u751f\u6210\u6570\u636e\u96c6\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u5c06\u6210\u4e3a\u63a8\u8fdb\u4e2d\u56fd\u5c11\u6570\u6c11\u65cf\u8bed\u8a00\u6807\u9898\u751f\u6210\u7814\u7a76\u7684\u91cd\u8981\u8d44\u6e90\uff0c\u5e76\u4e3a\u76f8\u5173\u57fa\u51c6\u6d4b\u8bd5\u7684\u53d1\u5c55\u505a\u51fa\u8d21\u732e\u3002"}}
{"id": "2509.10004", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10004", "abs": "https://arxiv.org/abs/2509.10004", "authors": ["Ponhvoan Srey", "Xiaobao Wu", "Anh Tuan Luu"], "title": "Unsupervised Hallucination Detection by Inspecting Reasoning Processes", "comment": "To appear in EMNLP 2025", "summary": "Unsupervised hallucination detection aims to identify hallucinated content\ngenerated by large language models (LLMs) without relying on labeled data.\nWhile unsupervised methods have gained popularity by eliminating\nlabor-intensive human annotations, they frequently rely on proxy signals\nunrelated to factual correctness. This misalignment biases detection probes\ntoward superficial or non-truth-related aspects, limiting generalizability\nacross datasets and scenarios. To overcome these limitations, we propose IRIS,\nan unsupervised hallucination detection framework, leveraging internal\nrepresentations intrinsic to factual correctness. IRIS prompts the LLM to\ncarefully verify the truthfulness of a given statement, and obtain its\ncontextualized embedding as informative features for training. Meanwhile, the\nuncertainty of each response is considered a soft pseudolabel for truthfulness.\nExperimental results demonstrate that IRIS consistently outperforms existing\nunsupervised methods. Our approach is fully unsupervised, computationally low\ncost, and works well even with few training data, making it suitable for\nreal-time detection.", "AI": {"tldr": "IRIS\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u5229\u7528LLM\u5185\u90e8\u8868\u5f81\u6765\u8bc6\u522b\u751f\u6210\u5185\u5bb9\u7684\u4e8b\u5b9e\u6b63\u786e\u6027\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\u4e14\u9002\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4b", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\u4f9d\u8d56\u4e0e\u4e8b\u5b9e\u6b63\u786e\u6027\u65e0\u5173\u7684\u4ee3\u7406\u4fe1\u53f7\uff0c\u5bfc\u81f4\u68c0\u6d4b\u504f\u5411\u8868\u9762\u7279\u5f81\uff0c\u9650\u5236\u4e86\u8de8\u6570\u636e\u96c6\u548c\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b", "method": "\u901a\u8fc7\u63d0\u793aLLM\u4ed4\u7ec6\u9a8c\u8bc1\u7ed9\u5b9a\u9648\u8ff0\u7684\u771f\u5b9e\u6027\uff0c\u83b7\u53d6\u5176\u60c5\u5883\u5316\u5d4c\u5165\u4f5c\u4e3a\u8bad\u7ec3\u7279\u5f81\uff0c\u5e76\u5c06\u6bcf\u4e2a\u54cd\u5e94\u7684\u4e0d\u786e\u5b9a\u6027\u4f5c\u4e3a\u771f\u5b9e\u6027\u7684\u8f6f\u4f2a\u6807\u7b7e", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eIRIS consistently outperforms existing unsupervised methods\uff0c\u5728\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u4e0b\u8868\u73b0\u826f\u597d", "conclusion": "IRIS\u662f\u4e00\u4e2a\u5b8c\u5168\u65e0\u76d1\u7763\u3001\u8ba1\u7b97\u6210\u672c\u4f4e\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4bLLM\u751f\u6210\u7684\u5e7b\u89c9\u5185\u5bb9\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u573a\u666f"}}
{"id": "2509.10010", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.10010", "abs": "https://arxiv.org/abs/2509.10010", "authors": ["Adnan Ahmad", "Philine Kowol", "Stefan Hillmann", "Sebastian M\u00f6ller"], "title": "Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs", "comment": null, "summary": "In this paper, we provide an extensive analysis of multi-label intent\nclassification using Large Language Models (LLMs) that are open-source,\npublicly available, and can be run in consumer hardware. We use the MultiWOZ\n2.1 dataset, a benchmark in the dialogue system domain, to investigate the\nefficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,\nMistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot\nsetup, giving 20 examples in the prompt with some instructions. Our approach\nfocuses on the differences in performance of these models across several\nperformance metrics by methodically assessing these models on multi-label\nintent classification tasks. Additionally, we compare the performance of the\ninstruction-based fine-tuning approach with supervised learning using the\nsmaller transformer model BertForSequenceClassification as a baseline. To\nevaluate the performance of the models, we use evaluation metrics like\naccuracy, precision, and recall as well as micro, macro, and weighted F1 score.\nWe also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1\noutperforms two other generative models on 11 intent classes out of 14 in terms\nof F-Score, with a weighted average of 0.50. It also has relatively lower\nHumming Loss and higher Jaccard Similarity, making it the winning model in the\nfew-shot setting. We find BERT based supervised classifier having superior\nperformance compared to the best performing few-shot generative LLM. The study\nprovides a framework for small open-source LLMs in detecting complex\nmulti-intent dialogues, enhancing the Natural Language Understanding aspect of\ntask-oriented chatbots.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6807\u7b7e\u610f\u56fe\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u6bd4\u8f83\u4e86LLama2-7B\u3001Mistral-7B\u548cYi-6B\u5728MultiWOZ 2.1\u6570\u636e\u96c6\u4e0a\u7684few-shot\u8868\u73b0\uff0c\u5e76\u4e0eBERT\u76d1\u7763\u5b66\u4e60\u57fa\u7ebf\u8fdb\u884c\u5bf9\u6bd4\u3002", "motivation": "\u7814\u7a76\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u5904\u7406\u590d\u6742\u591a\u610f\u56fe\u5bf9\u8bdd\u5206\u7c7b\u4efb\u52a1\u7684\u6709\u6548\u6027\uff0c\u4e3a\u4efb\u52a1\u5bfc\u5411\u804a\u5929\u673a\u5668\u4eba\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u63d0\u4f9b\u5b9e\u7528\u6846\u67b6\u3002", "method": "\u4f7f\u7528MultiWOZ 2.1\u6570\u636e\u96c6\uff0c\u5728few-shot\u8bbe\u7f6e\u4e0b\uff08\u63d0\u793a\u4e2d\u5305\u542b20\u4e2a\u793a\u4f8b\uff09\u6d4b\u8bd5\u4e09\u4e2a\u5f00\u6e90LLM\u6a21\u578b\uff0c\u5e76\u4e0e\u57fa\u4e8eBERT\u7684\u76d1\u7763\u5206\u7c7b\u5668\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\uff0c\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548c\u591a\u79cdF1\u5206\u6570\u3002", "result": "Mistral-7B-v0.1\u572814\u4e2a\u610f\u56fe\u7c7b\u522b\u4e2d\u768411\u4e2a\u4e0aF\u5206\u6570\u8868\u73b0\u6700\u4f73\uff0c\u52a0\u6743\u5e73\u5747F1\u4e3a0.50\uff0c\u5177\u6709\u8f83\u4f4e\u7684Hamming Loss\u548c\u8f83\u9ad8\u7684Jaccard\u76f8\u4f3c\u5ea6\u3002\u4f46BERT\u76d1\u7763\u5206\u7c7b\u5668\u7684\u6027\u80fd\u4ecd\u4f18\u4e8e\u6700\u4f73few-shot\u751f\u6210\u5f0fLLM\u3002", "conclusion": "\u867d\u7136Mistral-7B\u5728few-shot\u8bbe\u7f6e\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u76d1\u7763\u5b66\u4e60\u7684BERT\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u4ecd\u5177\u6709\u4f18\u52bf\u3002\u7814\u7a76\u4e3a\u5c0f\u578b\u5f00\u6e90LLM\u5728\u590d\u6742\u591a\u610f\u56fe\u5bf9\u8bdd\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2509.10035", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10035", "abs": "https://arxiv.org/abs/2509.10035", "authors": ["Laurin Plank", "Armin Zlomuzica"], "title": "Linguistic trajectories of bipolar disorder on social media", "comment": "Pre-print", "summary": "Language provides valuable markers of affective disorders such as bipolar\ndisorder (BD), yet clinical assessments remain limited in scale. In response,\nanalyses of social media (SM) language have gained prominence due to their high\ntemporal resolution and longitudinal scope. Here, we introduce a method to\ndetermine the timing of users' diagnoses and apply it to study language\ntrajectories from 3 years before to 21 years after BD diagnosis - contrasted\nwith uses reporting unipolar depression (UD) and non-affected users (HC). We\nshow that BD diagnosis is accompanied by pervasive linguistic alterations\nreflecting mood disturbance, psychiatric comorbidity, substance abuse,\nhospitalization, medical comorbidities, unusual thought content, and\ndisorganized thought. We further observe recurring mood-related language\nchanges across two decades after the diagnosis, with a pronounced 12-month\nperiodicity suggestive of seasonal mood episodes. Finally, trend-level evidence\nsuggests an increased periodicity in users estimated to be female. In sum, our\nfindings provide evidence for language alterations in the acute and chronic\nphase of BD. This validates and extends recent efforts leveraging SM for\nscalable monitoring of mental health.", "AI": {"tldr": "\u901a\u8fc7\u793e\u4ea4\u5a92\u4f53\u8bed\u8a00\u5206\u6790\u53cc\u76f8\u60c5\u611f\u969c\u788d\u7684\u8bca\u65ad\u524d\u540e\u8bed\u8a00\u53d8\u5316\uff0c\u53d1\u73b0\u8bca\u65ad\u540e\u51fa\u73b0\u5e7f\u6cdb\u7684\u8bed\u8a00\u6539\u53d8\uff0c\u5305\u62ec\u60c5\u7eea\u969c\u788d\u3001\u7cbe\u795e\u5171\u75c5\u7b49\u7279\u5f81\uff0c\u5e76\u89c2\u5bdf\u5230\u957f\u8fbe20\u5e74\u7684\u5468\u671f\u6027\u60c5\u7eea\u53d8\u5316", "motivation": "\u4f20\u7edf\u4e34\u5e8a\u8bc4\u4f30\u53cc\u76f8\u60c5\u611f\u969c\u788d\u89c4\u6a21\u6709\u9650\uff0c\u800c\u793e\u4ea4\u5a92\u4f53\u8bed\u8a00\u5206\u6790\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u7eb5\u5411\u8303\u56f4\u7684\u4f18\u52bf\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7814\u7a76\u8bca\u65ad\u524d\u540e\u7684\u8bed\u8a00\u8f68\u8ff9\u53d8\u5316", "method": "\u5f15\u5165\u786e\u5b9a\u7528\u6237\u8bca\u65ad\u65f6\u95f4\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u4ece\u8bca\u65ad\u524d3\u5e74\u5230\u8bca\u65ad\u540e21\u5e74\u7684\u8bed\u8a00\u8f68\u8ff9\uff0c\u4e0e\u5355\u76f8\u6291\u90c1\u75c7\u7528\u6237\u548c\u975e\u53d7\u5f71\u54cd\u7528\u6237\u8fdb\u884c\u5bf9\u6bd4", "result": "\u53cc\u76f8\u60c5\u611f\u969c\u788d\u8bca\u65ad\u4f34\u968f\u5e7f\u6cdb\u7684\u8bed\u8a00\u6539\u53d8\uff0c\u53cd\u6620\u60c5\u7eea\u969c\u788d\u3001\u7cbe\u795e\u5171\u75c5\u3001\u7269\u8d28\u6ee5\u7528\u7b49\u7279\u5f81\uff1b\u8bca\u65ad\u540e20\u5e74\u5185\u89c2\u5bdf\u5230\u53cd\u590d\u51fa\u73b0\u7684\u60c5\u7eea\u76f8\u5173\u8bed\u8a00\u53d8\u5316\uff0c\u5177\u6709\u660e\u663e\u768412\u4e2a\u6708\u5468\u671f\u6027\uff1b\u5973\u6027\u7528\u6237\u53ef\u80fd\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u5468\u671f\u6027", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u4e86\u53cc\u76f8\u60c5\u611f\u969c\u788d\u6025\u6027\u548c\u6162\u6027\u671f\u7684\u8bed\u8a00\u6539\u53d8\uff0c\u9a8c\u8bc1\u5e76\u6269\u5c55\u4e86\u5229\u7528\u793e\u4ea4\u5a92\u4f53\u8fdb\u884c\u5fc3\u7406\u5065\u5eb7\u53ef\u6269\u5c55\u76d1\u6d4b\u7684\u6700\u65b0\u52aa\u529b"}}
{"id": "2509.10040", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10040", "abs": "https://arxiv.org/abs/2509.10040", "authors": ["Mohamed Basem", "Mohamed Younes", "Seif Ahmed", "Abdelrahman Moustafa"], "title": "!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability Assessment", "comment": "10 Pages , 8 figures , ArabicNLP 2025 , Co-located with EMNLP 2025", "summary": "We present MSAs winning system for the BAREC 2025 Shared Task on fine-grained\nArabic readability assessment, achieving first place in six of six tracks. Our\napproach is a confidence-weighted ensemble of four complementary transformer\nmodels (AraBERTv2, AraELECTRA, MARBERT, and CAMeLBERT) each fine-tuned with\ndistinct loss functions to capture diverse readability signals. To tackle\nsevere class imbalance and data scarcity, we applied weighted training,\nadvanced preprocessing, SAMER corpus relabeling with our strongest model, and\nsynthetic data generation via Gemini 2.5 Flash, adding about 10,000 rare-level\nsamples. A targeted post-processing step corrected prediction distribution\nskew, delivering a 6.3 percent Quadratic Weighted Kappa (QWK) gain. Our system\nreached 87.5 percent QWK at the sentence level and 87.4 percent at the document\nlevel, demonstrating the power of model and loss diversity, confidence-informed\nfusion, and intelligent augmentation for robust Arabic readability prediction.", "AI": {"tldr": "MSA\u56e2\u961f\u5728BAREC 2025\u963f\u62c9\u4f2f\u8bed\u7ec6\u7c92\u5ea6\u53ef\u8bfb\u6027\u8bc4\u4f30\u5171\u4eab\u4efb\u52a1\u4e2d\u53d6\u5f97\u516d\u9879\u7b2c\u4e00\uff0c\u901a\u8fc7\u96c6\u6210\u56db\u79cdTransformer\u6a21\u578b\u3001\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5b9e\u73b0\u4e8687.5%\u7684QWK\u5206\u6570", "motivation": "\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u53ef\u8bfb\u6027\u8bc4\u4f30\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u7ec6\u7c92\u5ea6\u53ef\u8bfb\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027", "method": "\u4f7f\u7528\u56db\u79cdTransformer\u6a21\u578b\uff08AraBERTv2\u3001AraELECTRA\u3001MARBERT\u3001CAMeLBERT\uff09\u96c6\u6210\uff0c\u91c7\u7528\u52a0\u6743\u8bad\u7ec3\u3001SAMER\u8bed\u6599\u5e93\u91cd\u6807\u6ce8\u3001Gemini 2.5 Flash\u751f\u6210\u7ea610,000\u4e2a\u7a00\u6709\u7ea7\u522b\u6837\u672c\u7684\u5408\u6210\u6570\u636e\uff0c\u5e76\u8fdb\u884c\u9488\u5bf9\u6027\u540e\u5904\u7406", "result": "\u5728\u53e5\u5b50\u7ea7\u522b\u8fbe\u523087.5%\u7684QWK\uff0c\u6587\u6863\u7ea7\u522b\u8fbe\u523087.4%\u7684QWK\uff0c\u540e\u5904\u7406\u5e26\u67656.3%\u7684QWK\u589e\u76ca", "conclusion": "\u6a21\u578b\u591a\u6837\u6027\u3001\u635f\u5931\u51fd\u6570\u591a\u6837\u6027\u3001\u7f6e\u4fe1\u5ea6\u878d\u5408\u548c\u667a\u80fd\u6570\u636e\u589e\u5f3a\u5bf9\u963f\u62c9\u4f2f\u8bed\u53ef\u8bfb\u6027\u9884\u6d4b\u5177\u6709\u663e\u8457\u6548\u679c"}}
{"id": "2509.10078", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10078", "abs": "https://arxiv.org/abs/2509.10078", "authors": ["Dongmin Choi", "Woojung Song", "Jongwook Han", "Eun-Ju Lee", "Yohan Jo"], "title": "Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models", "comment": "17 pages, 4 figures", "summary": "Researchers have applied established psychometric questionnaires (e.g., BFI,\nPVQ) to measure the personality traits and values reflected in the responses of\nLarge Language Models (LLMs). However, concerns have been raised about applying\nthese human-designed questionnaires to LLMs. One such concern is their lack of\necological validity--the extent to which survey questions adequately reflect\nand resemble real-world contexts in which LLMs generate texts in response to\nuser queries. However, it remains unclear how established questionnaires and\necologically valid questionnaires differ in their outcomes, and what insights\nthese differences may provide. In this paper, we conduct a comprehensive\ncomparative analysis of the two types of questionnaires. Our analysis reveals\nthat established questionnaires (1) yield substantially different profiles of\nLLMs from ecologically valid ones, deviating from the psychological\ncharacteristics expressed in the context of user queries, (2) suffer from\ninsufficient items for stable measurement, (3) create misleading impressions\nthat LLMs possess stable constructs, and (4) yield exaggerated profiles for\npersona-prompted LLMs. Overall, our work cautions against the use of\nestablished psychological questionnaires for LLMs. Our code will be released\nupon publication.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4f20\u7edf\u5fc3\u7406\u6d4b\u91cf\u95ee\u5377\u4e0e\u751f\u6001\u6548\u5ea6\u95ee\u5377\u5728\u6d4b\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u4e2a\u6027\u7279\u5f81\u65f6\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u4f20\u7edf\u95ee\u5377\u5b58\u5728\u6d4b\u91cf\u4e0d\u7a33\u5b9a\u3001\u4ea7\u751f\u8bef\u5bfc\u6027\u7ed3\u679c\u7b49\u95ee\u9898\uff0c\u5efa\u8bae\u907f\u514d\u4f7f\u7528\u4f20\u7edf\u5fc3\u7406\u95ee\u5377\u8bc4\u4f30LLMs\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4f7f\u7528\u4eba\u7c7b\u8bbe\u8ba1\u7684\u5fc3\u7406\u95ee\u5377\uff08\u5982BFI\u3001PVQ\uff09\u6765\u6d4b\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e2a\u6027\u7279\u5f81\u548c\u4ef7\u503c\u89c2\uff0c\u4f46\u8fd9\u4e9b\u95ee\u5377\u7f3a\u4e4f\u751f\u6001\u6548\u5ea6\uff0c\u65e0\u6cd5\u53cd\u6620LLMs\u5728\u771f\u5b9e\u7528\u6237\u67e5\u8be2\u573a\u666f\u4e2d\u7684\u6587\u672c\u751f\u6210\u884c\u4e3a\u3002\u9700\u8981\u660e\u786e\u4e24\u79cd\u95ee\u5377\u7684\u5dee\u5f02\u53ca\u5176\u63d0\u4f9b\u7684\u6d1e\u5bdf\u3002", "method": "\u5bf9\u4e24\u79cd\u7c7b\u578b\u7684\u95ee\u5377\u8fdb\u884c\u5168\u9762\u7684\u6bd4\u8f83\u5206\u6790\uff1a\u4f20\u7edf\u5fc3\u7406\u6d4b\u91cf\u95ee\u5377\u548c\u751f\u6001\u6548\u5ea6\u95ee\u5377\uff0c\u8bc4\u4f30\u5b83\u4eec\u5728\u6d4b\u91cfLLMs\u4e2a\u6027\u7279\u5f81\u65f6\u7684\u8868\u73b0\u5dee\u5f02\u3002", "result": "\u5206\u6790\u53d1\u73b0\u4f20\u7edf\u95ee\u5377\u5b58\u5728\u56db\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4ea7\u751f\u4e0e\u751f\u6001\u6548\u5ea6\u95ee\u5377\u663e\u8457\u4e0d\u540c\u7684LLMs\u7279\u5f81\u5256\u9762\uff1b2\uff09\u9879\u76ee\u6570\u91cf\u4e0d\u8db3\u5bfc\u81f4\u6d4b\u91cf\u4e0d\u7a33\u5b9a\uff1b3\uff09\u9020\u6210LLMs\u5177\u6709\u7a33\u5b9a\u6784\u5ff5\u7684\u8bef\u5bfc\u5370\u8c61\uff1b4\uff09\u5bf9\u89d2\u8272\u63d0\u793a\u7684LLMs\u4ea7\u751f\u5938\u5927\u7684\u7279\u5f81\u5256\u9762\u3002", "conclusion": "\u7814\u7a76\u8b66\u793a\u4e0d\u8981\u4f7f\u7528\u4f20\u7edf\u5fc3\u7406\u95ee\u5377\u6765\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u95ee\u5377\u7f3a\u4e4f\u751f\u6001\u6548\u5ea6\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620LLMs\u5728\u771f\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\u7684\u5fc3\u7406\u7279\u5f81\u8868\u73b0\u3002"}}
{"id": "2509.10087", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10087", "abs": "https://arxiv.org/abs/2509.10087", "authors": ["Mustapha Adamu", "Qi Zhang", "Huitong Pan", "Longin Jan Latecki", "Eduard C. Dragut"], "title": "Querying Climate Knowledge: Semantic Retrieval for Scientific Discovery", "comment": "ACM SIGIR 2025 Workshop MANILA", "summary": "The growing complexity and volume of climate science literature make it\nincreasingly difficult for researchers to find relevant information across\nmodels, datasets, regions, and variables. This paper introduces a\ndomain-specific Knowledge Graph (KG) built from climate publications and\nbroader scientific texts, aimed at improving how climate knowledge is accessed\nand used. Unlike keyword based search, our KG supports structured, semantic\nqueries that help researchers discover precise connections such as which models\nhave been validated in specific regions or which datasets are commonly used\nwith certain teleconnection patterns. We demonstrate how the KG answers such\nquestions using Cypher queries, and outline its integration with large language\nmodels in RAG systems to improve transparency and reliability in\nclimate-related question answering. This work moves beyond KG construction to\nshow its real world value for climate researchers, model developers, and others\nwho rely on accurate, contextual scientific information.", "AI": {"tldr": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6c14\u5019\u79d1\u5b66\u9886\u57df\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u652f\u6301\u8bed\u4e49\u67e5\u8be2\u548cRAG\u7cfb\u7edf\u96c6\u6210\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u53d1\u73b0\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u533a\u57df\u4e4b\u95f4\u7684\u7cbe\u786e\u8054\u7cfb", "motivation": "\u6c14\u5019\u79d1\u5b66\u6587\u732e\u65e5\u76ca\u590d\u6742\u548c\u5e9e\u5927\uff0c\u7814\u7a76\u4eba\u5458\u96be\u4ee5\u8de8\u6a21\u578b\u3001\u6570\u636e\u96c6\u3001\u533a\u57df\u548c\u53d8\u91cf\u627e\u5230\u76f8\u5173\u4fe1\u606f\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u8bbf\u95ee\u65b9\u5f0f", "method": "\u4ece\u6c14\u5019\u51fa\u7248\u7269\u548c\u79d1\u5b66\u6587\u672c\u6784\u5efa\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u56fe\u8c31\uff0c\u652f\u6301Cypher\u67e5\u8be2\uff0c\u5e76\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u6784\u5efaRAG\u7cfb\u7edf", "result": "\u77e5\u8bc6\u56fe\u8c31\u80fd\u591f\u56de\u7b54\u5173\u4e8e\u6a21\u578b\u9a8c\u8bc1\u3001\u6570\u636e\u96c6\u4f7f\u7528\u7b49\u7cbe\u786e\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6c14\u5019\u76f8\u5173\u95ee\u7b54\u7684\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u77e5\u8bc6\u56fe\u8c31\u5728\u6c14\u5019\u7814\u7a76\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u6a21\u578b\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u51c6\u786e\u3001\u60c5\u5883\u5316\u7684\u79d1\u5b66\u4fe1\u606f\u8bbf\u95ee\u65b9\u5f0f"}}
{"id": "2509.10095", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10095", "abs": "https://arxiv.org/abs/2509.10095", "authors": ["Abdulrahman Allam", "Seif Ahmed", "Ali Hamdi", "Ammar Mohammed"], "title": "Arabic Large Language Models for Medical Text Generation", "comment": "Published in 2025 4th International Conference on Computer\n  Technologies (ICCTech)", "summary": "Efficient hospital management systems (HMS) are critical worldwide to address\nchallenges such as overcrowding, limited resources, and poor availability of\nurgent health care. Existing methods often lack the ability to provide\naccurate, real-time medical advice, particularly for irregular inputs and\nunderrepresented languages. To overcome these limitations, this study proposes\nan approach that fine-tunes large language models (LLMs) for Arabic medical\ntext generation. The system is designed to assist patients by providing\naccurate medical advice, diagnoses, drug recommendations, and treatment plans\nbased on user input. The research methodology required the collection of a\nunique dataset from social media platforms, capturing real-world medical\nconversations between patients and doctors. The dataset, which includes patient\ncomplaints together with medical advice, was properly cleaned and preprocessed\nto account for multiple Arabic dialects. Fine-tuning state-of-the-art\ngenerative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2\nMedium, optimized the system's ability to generate reliable medical text.\nResults from evaluations indicate that the fine-tuned Mistral-7B model\noutperformed the other models, achieving average BERT (Bidirectional Encoder\nRepresentations from Transformers) Score values in precision, recall, and\nF1-scores of 68.5\\%, 69.08\\%, and 68.5\\%, respectively. Comparative\nbenchmarking and qualitative assessments validate the system's ability to\nproduce coherent and relevant medical replies to informal input. This study\nhighlights the potential of generative artificial intelligence (AI) in\nadvancing HMS, offering a scalable and adaptable solution for global healthcare\nchallenges, especially in linguistically and culturally diverse environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u6587\u672c\u751f\u6210\u7684LLM\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u6536\u96c6\u793e\u4ea4\u5a92\u4f53\u533b\u7597\u5bf9\u8bdd\u6570\u636e\uff0c\u5fae\u8c03Mistral-7B\u7b49\u6a21\u578b\uff0c\u5728\u533b\u7597\u54a8\u8be2\u3001\u8bca\u65ad\u548c\u836f\u7269\u63a8\u8350\u65b9\u9762\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u533b\u9662\u7ba1\u7406\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u4e0d\u89c4\u5219\u8f93\u5165\u548c\u5c11\u6570\u8bed\u8a00\u63d0\u4f9b\u51c6\u786e\u5b9e\u65f6\u533b\u7597\u5efa\u8bae\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u963f\u62c9\u4f2f\u8bed\u73af\u5883\u4e0b\u3002\u4e3a\u89e3\u51b3\u533b\u7597\u8d44\u6e90\u7d27\u5f20\u3001\u8fc7\u5ea6\u62e5\u6324\u548c\u7d27\u6025\u533b\u7597\u670d\u52a1\u53ef\u7528\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u591a\u65b9\u8a00\u963f\u62c9\u4f2f\u8bed\u7684\u533b\u7597AI\u7cfb\u7edf\u3002", "method": "\u6536\u96c6\u793e\u4ea4\u5a92\u4f53\u4e0a\u60a3\u8005\u4e0e\u533b\u751f\u4e4b\u95f4\u7684\u771f\u5b9e\u533b\u7597\u5bf9\u8bdd\u6570\u636e\uff0c\u8fdb\u884c\u6570\u636e\u6e05\u6d17\u548c\u9884\u5904\u7406\u4ee5\u5904\u7406\u591a\u79cd\u963f\u62c9\u4f2f\u65b9\u8a00\u3002\u5fae\u8c03\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\uff08Mistral-7B-Instruct-v0.2\u3001LLaMA-2-7B\u548cGPT-2 Medium\uff09\uff0c\u4f18\u5316\u7cfb\u7edf\u751f\u6210\u53ef\u9760\u533b\u7597\u6587\u672c\u7684\u80fd\u529b\u3002", "result": "\u5fae\u8c03\u540e\u7684Mistral-7B\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5728\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u7684\u5e73\u5747BERT Score\u503c\u5206\u522b\u8fbe\u523068.5%\u300169.08%\u548c68.5%\u3002\u6bd4\u8f83\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9a\u6027\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5bf9\u975e\u6b63\u5f0f\u8f93\u5165\u4ea7\u751f\u8fde\u8d2f\u76f8\u5173\u533b\u7597\u56de\u590d\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u751f\u6210\u5f0fAI\u5728\u533b\u9662\u7ba1\u7406\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u5168\u7403\u533b\u7597\u6311\u6218\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u8bed\u8a00\u548c\u6587\u5316\u591a\u6837\u5316\u7684\u73af\u5883\u4e2d\u3002"}}
{"id": "2509.10108", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10108", "abs": "https://arxiv.org/abs/2509.10108", "authors": ["Abdulrahman Allam", "Seif Ahmed", "Ali Hamdi", "Khaled Shaban"], "title": "Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing Generative AI with Synthetic Patient Records", "comment": "Accepted in AICCSA 2025", "summary": "The development of medical chatbots in Arabic is significantly constrained by\nthe scarcity of large-scale, high-quality annotated datasets. While prior\nefforts compiled a dataset of 20,000 Arabic patient-doctor interactions from\nsocial media to fine-tune large language models (LLMs), model scalability and\ngeneralization remained limited. In this study, we propose a scalable synthetic\ndata augmentation strategy to expand the training corpus to 100,000 records.\nUsing advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated\n80,000 contextually relevant and medically coherent synthetic question-answer\npairs grounded in the structure of the original dataset. These synthetic\nsamples were semantically filtered, manually validated, and integrated into the\ntraining pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2,\nand evaluated their performance using BERTScore metrics and expert-driven\nqualitative assessments. To further analyze the effectiveness of synthetic\nsources, we conducted an ablation study comparing ChatGPT-4o and\nGemini-generated data independently. The results showed that ChatGPT-4o data\nconsistently led to higher F1-scores and fewer hallucinations across all\nmodels. Overall, our findings demonstrate the viability of synthetic\naugmentation as a practical solution for enhancing domain-specific language\nmodels in-low resource medical NLP, paving the way for more inclusive,\nscalable, and accurate Arabic healthcare chatbot systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5408\u6210\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u4f7f\u7528ChatGPT-4o\u548cGemini 2.5 Pro\u751f\u6210\u4e868\u4e07\u6761\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u95ee\u7b54\u5bf9\uff0c\u5c06\u8bad\u7ec3\u8bed\u6599\u6269\u5c55\u523010\u4e07\u6761\u8bb0\u5f55\uff0c\u663e\u8457\u63d0\u5347\u4e86\u963f\u62c9\u4f2f\u533b\u7597\u804a\u5929\u673a\u5668\u4eba\u7684\u6027\u80fd\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u804a\u5929\u673a\u5668\u4eba\u7684\u53d1\u5c55\u53d7\u5230\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u9650\u5236\uff0c\u5148\u524d\u57fa\u4e8e2\u4e07\u6761\u793e\u4ea4\u5a92\u4f53\u533b\u60a3\u4ea4\u4e92\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u4f7f\u7528ChatGPT-4o\u548cGemini 2.5 Pro\u751f\u62108\u4e07\u6761\u4e0a\u4e0b\u6587\u76f8\u5173\u4e14\u533b\u5b66\u8fde\u8d2f\u7684\u5408\u6210\u95ee\u7b54\u5bf9\uff0c\u7ecf\u8fc7\u8bed\u4e49\u8fc7\u6ee4\u548c\u4eba\u5de5\u9a8c\u8bc1\u540e\u6574\u5408\u5230\u8bad\u7ec3\u6d41\u7a0b\u4e2d\uff0c\u5bf9\u5305\u62ecMistral-7B\u548cAraGPT2\u5728\u5185\u76845\u4e2aLLM\u8fdb\u884c\u5fae\u8c03\u3002", "result": "ChatGPT-4o\u751f\u6210\u7684\u6570\u636e\u5728\u6240\u6709\u6a21\u578b\u4e2d\u59cb\u7ec8\u83b7\u5f97\u66f4\u9ad8\u7684F1\u5206\u6570\u548c\u66f4\u5c11\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86ChatGPT-4o\u751f\u6210\u6570\u636e\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u589e\u5f3a\u662f\u63d0\u5347\u4f4e\u8d44\u6e90\u533b\u7597NLP\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u6784\u5efa\u66f4\u5177\u5305\u5bb9\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\u7684\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u804a\u5929\u673a\u5668\u4eba\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.10116", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.10116", "abs": "https://arxiv.org/abs/2509.10116", "authors": ["Julian Linke", "Barbara Schuppler"], "title": "Prominence-aware automatic speech recognition for conversational speech", "comment": null, "summary": "This paper investigates prominence-aware automatic speech recognition (ASR)\nby combining prominence detection and speech recognition for conversational\nAustrian German. First, prominence detectors were developed by fine-tuning\nwav2vec2 models to classify word-level prominence. The detector was then used\nto automatically annotate prosodic prominence in a large corpus. Based on those\nannotations, we trained novel prominence-aware ASR systems that simultaneously\ntranscribe words and their prominence levels. The integration of prominence\ninformation did not change performance compared to our baseline ASR system,\nwhile reaching a prominence detection accuracy of 85.53% for utterances where\nthe recognized word sequence was correct. This paper shows that\ntransformer-based models can effectively encode prosodic information and\nrepresents a novel contribution to prosody-enhanced ASR, with potential\napplications for linguistic research and prosody-informed dialogue systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7ed3\u5408\u91cd\u97f3\u68c0\u6d4b\u548c\u8bed\u97f3\u8bc6\u522b\u7684\u663e\u8457\u6027\u611f\u77e5\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\uff0c\u9488\u5bf9\u5965\u5730\u5229\u5fb7\u8bed\u5bf9\u8bdd\u5f00\u53d1\u4e86\u57fa\u4e8ewav2vec2\u7684\u91cd\u97f3\u68c0\u6d4b\u5668\uff0c\u5e76\u8bad\u7ec3\u4e86\u540c\u65f6\u8f6c\u5f55\u5355\u8bcd\u53ca\u5176\u91cd\u97f3\u6c34\u5e73\u7684ASR\u7cfb\u7edf\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u5904\u7406\u8bed\u97f3\u8f6c\u5f55\u548c\u91cd\u97f3\u68c0\u6d4b\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\uff0c\u4e3a\u8bed\u8a00\u5b66\u7814\u7a76\u548c\u57fa\u4e8e\u97f5\u5f8b\u7684\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u5fae\u8c03wav2vec2\u6a21\u578b\u8fdb\u884c\u8bcd\u7ea7\u91cd\u97f3\u5206\u7c7b\uff1b2\uff09\u4f7f\u7528\u68c0\u6d4b\u5668\u81ea\u52a8\u6807\u6ce8\u5927\u578b\u8bed\u6599\u5e93\u4e2d\u7684\u97f5\u5f8b\u91cd\u97f3\uff1b3\uff09\u8bad\u7ec3\u65b0\u9896\u7684\u663e\u8457\u6027\u611f\u77e5ASR\u7cfb\u7edf\uff0c\u540c\u65f6\u8f6c\u5f55\u5355\u8bcd\u548c\u91cd\u97f3\u6c34\u5e73\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a\u96c6\u6210\u91cd\u97f3\u4fe1\u606f\u5e76\u672a\u6539\u53d8ASR\u7cfb\u7edf\u6027\u80fd\uff08\u4e0e\u57fa\u7ebf\u76f8\u5f53\uff09\uff0c\u4f46\u5728\u8bc6\u522b\u6b63\u786e\u7684\u8bed\u53e5\u4e2d\u91cd\u97f3\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe\u523085.53%\u3002\u57fa\u4e8etransformer\u7684\u6a21\u578b\u80fd\u6709\u6548\u7f16\u7801\u97f5\u5f8b\u4fe1\u606f\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u65b9\u6cd5\u4e3a\u97f5\u5f8b\u589e\u5f3a\u7684ASR\u63d0\u4f9b\u4e86\u65b0\u9896\u8d21\u732e\uff0c\u5c55\u793a\u4e86transformer\u6a21\u578b\u7f16\u7801\u97f5\u5f8b\u4fe1\u606f\u7684\u6709\u6548\u6027\uff0c\u5728\u8bed\u8a00\u5b66\u7814\u7a76\u548c\u97f5\u5f8b\u611f\u77e5\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.10127", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10127", "abs": "https://arxiv.org/abs/2509.10127", "authors": ["Zhengyu Hu", "Zheyuan Xiao", "Max Xiong", "Yuxuan Lei", "Tianfu Wang", "Jianxun Lian", "Kaize Ding", "Ziang Xiao", "Nicholas Jing Yuan", "Xing Xie"], "title": "Population-Aligned Persona Generation for LLM-based Social Simulation", "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled human-like\nsocial simulations at unprecedented scale and fidelity, offering new\nopportunities for computational social science. A key challenge, however, is\nthe construction of persona sets that authentically represent the diversity and\ndistribution of real-world populations. Most existing LLM-based social\nsimulation studies focus primarily on designing agentic frameworks and\nsimulation environments, often overlooking the complexities of persona\ngeneration and the potential biases introduced by unrepresentative persona\nsets. In this paper, we propose a systematic framework for synthesizing\nhigh-quality, population-aligned persona sets for LLM-driven social simulation.\nOur approach begins by leveraging LLMs to generate narrative personas from\nlong-term social media data, followed by rigorous quality assessment to filter\nout low-fidelity profiles. We then apply importance sampling to achieve global\nalignment with reference psychometric distributions, such as the Big Five\npersonality traits. To address the needs of specific simulation contexts, we\nfurther introduce a task-specific module that adapts the globally aligned\npersona set to targeted subpopulations. Extensive experiments demonstrate that\nour method significantly reduces population-level bias and enables accurate,\nflexible social simulation for a wide range of research and policy\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7cfb\u7edf\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4e0e\u4eba\u53e3\u5206\u5e03\u5bf9\u9f50\u7684AI\u89d2\u8272\u96c6\uff0c\u4ee5\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u793e\u4f1a\u6a21\u62df\u4e2d\u7684\u504f\u89c1\u95ee\u9898", "motivation": "\u73b0\u6709LLM\u793e\u4f1a\u6a21\u62df\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4ee3\u7406\u6846\u67b6\u548c\u6a21\u62df\u73af\u5883\u8bbe\u8ba1\uff0c\u5ffd\u89c6\u4e86\u89d2\u8272\u751f\u6210\u590d\u6742\u6027\u4ee5\u53ca\u975e\u4ee3\u8868\u6027\u89d2\u8272\u96c6\u5e26\u6765\u7684\u6f5c\u5728\u504f\u89c1\u95ee\u9898", "method": "\u5229\u7528LLM\u4ece\u957f\u671f\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u751f\u6210\u53d9\u4e8b\u89d2\u8272\uff0c\u901a\u8fc7\u8d28\u91cf\u8bc4\u4f30\u7b5b\u9009\uff0c\u5e94\u7528\u91cd\u8981\u6027\u91c7\u6837\u5b9e\u73b0\u4e0e\u5fc3\u7406\u6d4b\u91cf\u5206\u5e03\uff08\u5982\u5927\u4e94\u4eba\u683c\uff09\u7684\u5168\u5c40\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u4efb\u52a1\u7279\u5b9a\u6a21\u5757\u9002\u5e94\u76ee\u6807\u5b50\u7fa4\u4f53", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u7fa4\u4f53\u5c42\u9762\u7684\u504f\u89c1\uff0c\u80fd\u591f\u4e3a\u5e7f\u6cdb\u7684\u7814\u7a76\u548c\u653f\u7b56\u5e94\u7528\u63d0\u4f9b\u51c6\u786e\u7075\u6d3b\u7684\u793e\u4f1a\u6a21\u62df", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u9a71\u52a8\u7684\u793e\u4f1a\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u4eba\u53e3\u5bf9\u9f50\u7684\u89d2\u8272\u96c6\u751f\u6210\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u7684\u504f\u89c1\u95ee\u9898"}}
{"id": "2509.10129", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.10129", "abs": "https://arxiv.org/abs/2509.10129", "authors": ["Alessio Chen", "Simone Giovannini", "Andrea Gemelli", "Fabio Coppini", "Simone Marinai"], "title": "Towards Reliable and Interpretable Document Question Answering via VLMs", "comment": null, "summary": "Vision-Language Models (VLMs) have shown strong capabilities in document\nunderstanding, particularly in identifying and extracting textual information\nfrom complex documents. Despite this, accurately localizing answers within\ndocuments remains a major challenge, limiting both interpretability and\nreal-world applicability. To address this, we introduce\n\\textit{DocExplainerV0}, a plug-and-play bounding-box prediction module that\ndecouples answer generation from spatial localization. This design makes it\napplicable to existing VLMs, including proprietary systems where fine-tuning is\nnot feasible. Through systematic evaluation, we provide quantitative insights\ninto the gap between textual accuracy and spatial grounding, showing that\ncorrect answers often lack reliable localization. Our standardized framework\nhighlights these shortcomings and establishes a benchmark for future research\ntoward more interpretable and robust document information extraction VLMs.", "AI": {"tldr": "DocExplainerV0\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u8fb9\u754c\u6846\u9884\u6d4b\u6a21\u5757\uff0c\u5c06\u7b54\u6848\u751f\u6210\u4e0e\u7a7a\u95f4\u5b9a\u4f4d\u89e3\u8026\uff0c\u65e8\u5728\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6587\u6863\u7406\u89e3\u4e2d\u51c6\u786e\u4f4d\u7f6e\u5b9a\u4f4d\u7684\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6587\u6863\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u51c6\u786e\u5728\u6587\u6863\u4e2d\u5b9a\u4f4d\u7b54\u6848\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5f15\u5165DocExplainerV0\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u4e0e\u73b0\u6709VLM\u89e3\u8026\uff0c\u4e13\u95e8\u8d1f\u8d23\u8fb9\u754c\u6846\u9884\u6d4b\uff0c\u9002\u7528\u4e8e\u5305\u62ec\u65e0\u6cd5\u5fae\u8c03\u7684\u4e13\u6709\u7cfb\u7edf\u5728\u5185\u7684\u5404\u79cdVLM\u3002", "result": "\u7cfb\u7edf\u8bc4\u4f30\u663e\u793a\u6587\u672c\u51c6\u786e\u6027\u4e0e\u7a7a\u95f4\u5b9a\u4f4d\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u6b63\u786e\u7b54\u6848\u5f80\u5f80\u7f3a\u4e4f\u53ef\u9760\u7684\u4f4d\u7f6e\u5b9a\u4f4d\uff0c\u8be5\u6846\u67b6\u4e3a\u6b64\u7c7b\u95ee\u9898\u5efa\u7acb\u4e86\u57fa\u51c6\u3002", "conclusion": "DocExplainerV0\u4e3a\u89e3\u51b3\u6587\u6863\u4fe1\u606f\u63d0\u53d6\u4e2d\u7684\u7a7a\u95f4\u5b9a\u4f4d\u95ee\u9898\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6846\u67b6\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u53ef\u89e3\u91ca\u548c\u9c81\u68d2\u7684VLM\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.10179", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10179", "abs": "https://arxiv.org/abs/2509.10179", "authors": ["Ji\u0159\u00ed Mili\u010dka", "Anna Marklov\u00e1", "V\u00e1clav Cvr\u010dek"], "title": "Benchmark of stylistic variation in LLM-generated texts", "comment": null, "summary": "This study investigates the register variation in texts written by humans and\ncomparable texts produced by large language models (LLMs). Biber's\nmultidimensional analysis (MDA) is applied to a sample of human-written texts\nand AI-created texts generated to be their counterparts to find the dimensions\nof variation in which LLMs differ most significantly and most systematically\nfrom humans. As textual material, a new LLM-generated corpus AI-Brown is used,\nwhich is comparable to BE-21 (a Brown family corpus representing contemporary\nBritish English). Since all languages except English are underrepresented in\nthe training data of frontier LLMs, similar analysis is replicated on Czech\nusing AI-Koditex corpus and Czech multidimensional model. Examined were 16\nfrontier models in various settings and prompts, with emphasis placed on the\ndifference between base models and instruction-tuned models. Based on this, a\nbenchmark is created through which models can be compared with each other and\nranked in interpretable dimensions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528Biber\u591a\u7ef4\u5206\u6790\u6cd5\u6bd4\u8f83\u4eba\u7c7b\u5199\u4f5c\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u7684\u8bed\u57df\u5dee\u5f02\uff0c\u521b\u5efa\u4e86AI-Brown\u548cAI-Koditex\u8bed\u6599\u5e93\uff0c\u5206\u6790\u4e8616\u4e2a\u524d\u6cbf\u6a21\u578b\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u5efa\u7acb\u4e86\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u4e0e\u4eba\u7c7b\u5199\u4f5c\u5728\u8bed\u57df\u7279\u5f81\u4e0a\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u7279\u522b\u662f\u7531\u4e8e\u524d\u6cbfLLMs\u8bad\u7ec3\u6570\u636e\u4e2d\u975e\u82f1\u8bed\u8bed\u8a00\u7684\u4ee3\u8868\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u91c7\u7528Biber\u591a\u7ef4\u5206\u6790\u6cd5\uff0c\u4f7f\u7528AI-Brown\u8bed\u6599\u5e93\uff08\u5bf9\u5e94BE-21\u5e03\u6717\u5bb6\u65cf\u8bed\u6599\u5e93\uff09\u548cAI-Koditex\u6377\u514b\u8bed\u6599\u5e93\uff0c\u5206\u679016\u4e2a\u524d\u6cbf\u6a21\u578b\u5728\u4e0d\u540c\u63d0\u793a\u548c\u8bbe\u7f6e\u4e0b\u7684\u6587\u672c\u751f\u6210\u8868\u73b0\uff0c\u91cd\u70b9\u6bd4\u8f83\u57fa\u7840\u6a21\u578b\u548c\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u7684\u5dee\u5f02\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLMs\u5728\u591a\u4e2a\u8bed\u57df\u7ef4\u5ea6\u4e0a\u4e0e\u4eba\u7c7b\u5199\u4f5c\u5b58\u5728\u663e\u8457\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u5e76\u5efa\u7acb\u4e86\u53ef\u7528\u4e8e\u6a21\u578b\u6bd4\u8f83\u548c\u6392\u540d\u7684\u53ef\u89e3\u91ca\u7ef4\u5ea6\u57fa\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bc4\u4f30LLMs\u6587\u672c\u751f\u6210\u8d28\u91cf\u63d0\u4f9b\u4e86\u65b0\u7684\u591a\u7ef4\u5206\u6790\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u4e0e\u4eba\u7c7b\u5199\u4f5c\u7684\u8bed\u57df\u5dee\u5f02\u7279\u5f81\uff0c\u5bf9\u6a21\u578b\u6539\u8fdb\u548c\u8bc4\u4f30\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.10184", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10184", "abs": "https://arxiv.org/abs/2509.10184", "authors": ["Leen Almajed", "Abeer ALdayel"], "title": "Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive Conversations", "comment": "This paper is under review", "summary": "In emotionally supportive conversations, well-intended positivity can\nsometimes misfire, leading to responses that feel dismissive, minimizing, or\nunrealistically optimistic. We examine this phenomenon of incongruent\npositivity as miscalibrated expressions of positive support in both human and\nLLM generated responses. To this end, we collected real user-assistant\ndialogues from Reddit across a range of emotional intensities and generated\nadditional responses using large language models for the same context. We\ncategorize these conversations by intensity into two levels: Mild, which covers\nrelationship tension and general advice, and Severe, which covers grief and\nanxiety conversations. This level of categorization enables a comparative\nanalysis of how supportive responses vary across lower and higher stakes\ncontexts. Our analysis reveals that LLMs are more prone to unrealistic\npositivity through dismissive and minimizing tone, particularly in high-stakes\ncontexts. To further study the underlying dimensions of this phenomenon, we\nfinetune LLMs on datasets with strong and weak emotional reactions. Moreover,\nwe developed a weakly supervised multilabel classifier ensemble (DeBERTa and\nMentalBERT) that shows improved detection of incongruent positivity types\nacross two sorts of concerns (Mild and Severe). Our findings shed light on the\nneed to move beyond merely generating generic positive responses and instead\nstudy the congruent support measures to balance positive affect with emotional\nacknowledgment. This approach offers insights into aligning large language\nmodels with affective expectations in the online supportive dialogue, paving\nthe way toward context-aware and trust preserving online conversation systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u4e2d\u4e0d\u534f\u8c03\u7684\u79ef\u6781\u6027\u73b0\u8c61\uff0c\u53d1\u73b0\u5728\u9ad8\u98ce\u9669\u60c5\u5883\u4e0bLLM\u66f4\u5bb9\u6613\u4ea7\u751f\u4e0d\u73b0\u5b9e\u7684\u79ef\u6781\u56de\u5e94\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u68c0\u6d4b\u65b9\u6cd5\u548c\u5e73\u8861\u60c5\u611f\u652f\u6301\u7684\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u4e2d\u5584\u610f\u4f46\u53ef\u80fd\u9002\u5f97\u5176\u53cd\u7684\u79ef\u6781\u56de\u5e94\u73b0\u8c61\uff0c\u7279\u522b\u662f\u5728\u4eba\u7c7b\u548cLLM\u751f\u6210\u56de\u5e94\u4e2d\u7684\u4e0d\u534f\u8c03\u79ef\u6781\u6027\u95ee\u9898\uff0c\u65e8\u5728\u6539\u5584\u5728\u7ebf\u652f\u6301\u6027\u5bf9\u8bdd\u7684\u60c5\u611f\u5bf9\u9f50\u3002", "method": "\u6536\u96c6Reddit\u771f\u5b9e\u7528\u6237-\u52a9\u624b\u5bf9\u8bdd\uff0c\u6309\u60c5\u611f\u5f3a\u5ea6\u5206\u7c7b\u4e3a\u8f7b\u5ea6\uff08\u5173\u7cfb\u7d27\u5f20\u3001\u4e00\u822c\u5efa\u8bae\uff09\u548c\u91cd\u5ea6\uff08\u60b2\u4f24\u3001\u7126\u8651\uff09\uff0c\u4f7f\u7528LLM\u751f\u6210\u989d\u5916\u56de\u5e94\uff0c\u5fae\u8c03LLM\u5e76\u5f00\u53d1\u5f31\u76d1\u7763\u591a\u6807\u7b7e\u5206\u7c7b\u5668\u96c6\u6210\uff08DeBERTa\u548cMentalBERT\uff09\u3002", "result": "\u53d1\u73b0LLM\u5728\u9ad8\u98ce\u9669\u60c5\u5883\u4e0b\u66f4\u5bb9\u6613\u901a\u8fc7\u8f7b\u89c6\u548c\u6700\u5c0f\u5316\u8bed\u6c14\u8868\u73b0\u51fa\u4e0d\u73b0\u5b9e\u7684\u79ef\u6781\u6027\uff0c\u5f00\u53d1\u7684\u5206\u7c7b\u5668\u5728\u68c0\u6d4b\u4e0d\u534f\u8c03\u79ef\u6781\u6027\u7c7b\u578b\u65b9\u9762\u8868\u73b0\u6539\u5584\u3002", "conclusion": "\u9700\u8981\u8d85\u8d8a\u751f\u6210\u901a\u7528\u79ef\u6781\u56de\u5e94\uff0c\u7814\u7a76\u534f\u8c03\u7684\u652f\u6301\u63aa\u65bd\u6765\u5e73\u8861\u79ef\u6781\u60c5\u611f\u4e0e\u60c5\u611f\u8ba4\u540c\uff0c\u4e3a\u6784\u5efa\u60c5\u5883\u611f\u77e5\u548c\u4fe1\u4efb\u4fdd\u6301\u7684\u5728\u7ebf\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u65b9\u5411\u3002"}}
{"id": "2509.10199", "categories": ["cs.CL", "I.7; I.2; J.4"], "pdf": "https://arxiv.org/pdf/2509.10199", "abs": "https://arxiv.org/abs/2509.10199", "authors": ["Mikl\u00f3s Seb\u0151k", "Viktor Kov\u00e1cs", "Martin B\u00e1n\u00f3czy", "Daniel M\u00f8ller Eriksen", "Nathalie Neptune", "Philippe Roussille"], "title": "Beyond Token Limits: Assessing Language Model Performance on Long Text Classification", "comment": null, "summary": "The most widely used large language models in the social sciences (such as\nBERT, and its derivatives, e.g. RoBERTa) have a limitation on the input text\nlength that they can process to produce predictions. This is a particularly\npressing issue for some classification tasks, where the aim is to handle long\ninput texts. One such area deals with laws and draft laws (bills), which can\nhave a length of multiple hundred pages and, therefore, are not particularly\namenable for processing with models that can only handle e.g. 512 tokens. In\nthis paper, we show results from experiments covering 5 languages with\nXLM-RoBERTa, Longformer, GPT-3.5, GPT-4 models for the multiclass\nclassification task of the Comparative Agendas Project, which has a codebook of\n21 policy topic labels from education to health care. Results show no\nparticular advantage for the Longformer model, pre-trained specifically for the\npurposes of handling long inputs. The comparison between the GPT variants and\nthe best-performing open model yielded an edge for the latter. An analysis of\nclass-level factors points to the importance of support and substance overlaps\nbetween specific categories when it comes to performance on long text inputs.", "AI": {"tldr": "\u6bd4\u8f83\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u6587\u672c\u5206\u7c7b\u4efb\u52a1\uff08\u7279\u522b\u662f\u6cd5\u5f8b\u6587\u4ef6\uff09\u65f6\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e13\u95e8\u4e3a\u957f\u6587\u672c\u8bbe\u8ba1\u7684Longformer\u6a21\u578b\u5e76\u65e0\u4f18\u52bf\uff0c\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u4f18\u4e8eGPT\u53d8\u4f53", "motivation": "\u73b0\u6709\u4e3b\u6d41\u8bed\u8a00\u6a21\u578b\uff08\u5982BERT\u3001RoBERTa\uff09\u5b58\u5728\u8f93\u5165\u957f\u5ea6\u9650\u5236\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u50cf\u6cd5\u5f8b\u8349\u6848\u8fd9\u6837\u957f\u8fbe\u6570\u767e\u9875\u7684\u957f\u6587\u672c\u5206\u7c7b\u4efb\u52a1", "method": "\u57285\u79cd\u8bed\u8a00\u4e0a\u5b9e\u9a8c\u6bd4\u8f83XLM-RoBERTa\u3001Longformer\u3001GPT-3.5\u3001GPT-4\u6a21\u578b\uff0c\u4f7f\u7528\u6bd4\u8f83\u8bae\u7a0b\u9879\u76ee\u768421\u4e2a\u653f\u7b56\u4e3b\u9898\u6807\u7b7e\u8fdb\u884c\u591a\u5206\u7c7b\u4efb\u52a1", "result": "\u4e13\u95e8\u4e3a\u957f\u6587\u672c\u9884\u8bad\u7ec3\u7684Longformer\u6a21\u578b\u6ca1\u6709\u8868\u73b0\u51fa\u660e\u663e\u4f18\u52bf\uff1b\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u4f18\u4e8eGPT\u53d8\u4f53\uff1b\u7c7b\u522b\u95f4\u7684\u652f\u6301\u5ea6\u548c\u5185\u5bb9\u91cd\u53e0\u5ea6\u5f71\u54cd\u957f\u6587\u672c\u5904\u7406\u6027\u80fd", "conclusion": "\u5bf9\u4e8e\u957f\u6587\u672c\u5206\u7c7b\u4efb\u52a1\uff0c\u4e13\u95e8\u7684\u957f\u6587\u672c\u9884\u8bad\u7ec3\u6a21\u578b\u5e76\u975e\u5fc5\u9700\uff0c\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u66f4\u4f73\uff0c\u5206\u7c7b\u6027\u80fd\u53d7\u7c7b\u522b\u95f4\u76f8\u4f3c\u6027\u5f71\u54cd\u8f83\u5927"}}
{"id": "2509.10208", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10208", "abs": "https://arxiv.org/abs/2509.10208", "authors": ["Shengqiang Fu"], "title": "SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning", "comment": null, "summary": "Large Language Models often generate unfaithful responses in knowledge\nintensive tasks due to knowledge conflict,that is,a preference for relying on\ninternal parametric knowledge rather than the provided context.To address this\nissue,we propose a novel self improving framework,Self Improving Faithfulness\nAware Contrastive Tuning.The framework uses a self instruct mechanism that\nallows the base LLM to automatically generate high quality,structured\ncontrastive learning data,including anchor samples,semantically equivalent\npositive samples,and negative samples simulating unfaithful scenarios.This\napproach significantly reduces the cost of manual\nannotation.Subsequently,contrastive learning is applied to train the\nmodel,enabling it to pull faithful responses closer and push unfaithful\nresponses farther apart in the representation space.Experiments on knowledge\nconflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT\nmodel based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%\nover the best baseline method,while significantly reducing dependence on\ninternal memory.The results indicate that SI FACT provides strong effectiveness\nand high data efficiency in enhancing the contextual faithfulness of\nLLMs,offering a practical pathway toward building more proactive and\ntrustworthy language models.", "AI": {"tldr": "\u63d0\u51faSelf Improving Faithfulness Aware Contrastive Tuning\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u6307\u5bfc\u673a\u5236\u81ea\u52a8\u751f\u6210\u5bf9\u6bd4\u5b66\u4e60\u6570\u636e\uff0c\u63d0\u5347LLM\u5728\u77e5\u8bc6\u51b2\u7a81\u4efb\u52a1\u4e2d\u7684\u5fe0\u5b9e\u6027", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7531\u4e8e\u77e5\u8bc6\u51b2\u7a81\u800c\u503e\u5411\u4e8e\u4f9d\u8d56\u5185\u90e8\u53c2\u6570\u77e5\u8bc6\u800c\u975e\u63d0\u4f9b\u4e0a\u4e0b\u6587\u7684\u95ee\u9898", "method": "\u4f7f\u7528\u81ea\u6307\u5bfc\u673a\u5236\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u7ed3\u6784\u5316\u5bf9\u6bd4\u5b66\u4e60\u6570\u636e\uff08\u951a\u6837\u672c\u3001\u8bed\u4e49\u7b49\u4ef7\u6b63\u6837\u672c\u3001\u6a21\u62df\u4e0d\u5fe0\u5b9e\u573a\u666f\u7684\u8d1f\u6837\u672c\uff09\uff0c\u7136\u540e\u5e94\u7528\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b", "result": "\u5728ECARE KRE\u548cCOSE KRE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u57fa\u4e8eLlama3 8B Instruct\u7684SI FACT\u6a21\u578b\u5c06\u4e0a\u4e0b\u6587\u53ec\u56de\u7387\u63d0\u9ad8\u4e866.2%\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u5185\u90e8\u8bb0\u5fc6\u7684\u4f9d\u8d56", "conclusion": "SI FACT\u5728\u589e\u5f3aLLM\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u6027\u65b9\u9762\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6709\u6548\u6027\u548c\u9ad8\u6570\u636e\u6548\u7387\uff0c\u4e3a\u6784\u5efa\u66f4\u4e3b\u52a8\u548c\u53ef\u4fe1\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84"}}
{"id": "2509.10377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10377", "abs": "https://arxiv.org/abs/2509.10377", "authors": ["Yixiao Zhou", "Ziyu Zhao", "Dongzhou Cheng", "zhiliang wu", "Jie Gui", "Yi Yang", "Fei Wu", "Yu Cheng", "Hehe Fan"], "title": "Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs", "comment": "Accepted to EMNLP2025", "summary": "Sparse Mixture-of-Experts (SMoE) architectures are widely used in large\nlanguage models (LLMs) due to their computational efficiency. However, though\nonly a few experts are activated for each token, SMoE still requires loading\nall expert parameters, leading to high memory usage and challenges in\ndeployment. Previous work has tried to reduce the overhead by pruning and\nmerging experts, but primarily focused on expert-level operations, leaving\nneuron-level structure underexplored. We propose DERN (Dropping Experts,\nRecombining Neurons), a task-agnostic and retraining-free framework for expert\npruning and reconstruction. We observe that experts are often misaligned and\ncontain semantic conflicts at the neuron level, which poses challenges for\ndirect merging. To solve this, DERN works in three steps: it first prunes\nredundant experts using router statistics; then it decomposes them into\nneuron-level expert segments, assigning each segment to its most compatible\nretained expert; and finally, it merges segments within each retained expert to\nbuild a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE\nmodels show that DERN improves performance by more than 5% on commonsense\nreasoning and MMLU benchmarks under 50% expert sparsity, without extra\ntraining. It also greatly reduces the number of experts and memory usage,\nmaking SMoE LLMs easier to deploy in practice.", "AI": {"tldr": "DERN\u662f\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u4efb\u52a1\u65e0\u5173\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u526a\u679d\u548c\u795e\u7ecf\u5143\u91cd\u7ec4\u6765\u51cf\u5c11SMoE\u6a21\u578b\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e13\u5bb6\u6570\u91cf\u548c\u5185\u5b58\u5360\u7528\u3002", "motivation": "\u7a00\u758f\u6df7\u5408\u4e13\u5bb6(SMoE)\u67b6\u6784\u867d\u7136\u8ba1\u7b97\u9ad8\u6548\uff0c\u4f46\u4ecd\u9700\u8981\u52a0\u8f7d\u6240\u6709\u4e13\u5bb6\u53c2\u6570\uff0c\u5bfc\u81f4\u5185\u5b58\u4f7f\u7528\u9ad8\u4e14\u90e8\u7f72\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4e13\u5bb6\u7ea7\u64cd\u4f5c\uff0c\u5ffd\u7565\u4e86\u795e\u7ecf\u5143\u7ea7\u7ed3\u6784\u3002", "method": "DERN\u91c7\u7528\u4e09\u6b65\u6846\u67b6\uff1a1)\u4f7f\u7528\u8def\u7531\u5668\u7edf\u8ba1\u526a\u679d\u5197\u4f59\u4e13\u5bb6\uff1b2)\u5c06\u4e13\u5bb6\u5206\u89e3\u4e3a\u795e\u7ecf\u5143\u7ea7\u7247\u6bb5\uff0c\u5206\u914d\u7ed9\u6700\u517c\u5bb9\u7684\u4fdd\u7559\u4e13\u5bb6\uff1b3)\u5728\u4fdd\u7559\u4e13\u5bb6\u5185\u5408\u5e76\u7247\u6bb5\u6784\u5efa\u7d27\u51d1\u8868\u793a\u3002", "result": "\u5728Mixtral\u3001Qwen\u548cDeepSeek SMoE\u6a21\u578b\u4e0a\uff0cDERN\u572850%\u4e13\u5bb6\u7a00\u758f\u5ea6\u4e0b\uff0c\u5e38\u8bc6\u63a8\u7406\u548cMMLU\u57fa\u51c6\u6027\u80fd\u63d0\u5347\u8d85\u8fc75%\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4e13\u5bb6\u6570\u91cf\u548c\u5185\u5b58\u4f7f\u7528\u3002", "conclusion": "DERN\u901a\u8fc7\u795e\u7ecf\u5143\u7ea7\u91cd\u7ec4\u6709\u6548\u89e3\u51b3\u4e86\u4e13\u5bb6\u95f4\u8bed\u4e49\u51b2\u7a81\u95ee\u9898\uff0c\u4e3aSMoE LLMs\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u9700\u6c42\u3002"}}
{"id": "2509.10414", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10414", "abs": "https://arxiv.org/abs/2509.10414", "authors": ["Adrian de Wynter"], "title": "Is In-Context Learning Learning?", "comment": "Director's cut", "summary": "In-context learning (ICL) allows some autoregressive models to solve tasks\nvia next-token prediction and without needing further training. This has led to\nclaims about these model's ability to solve (learn) unseen tasks with only a\nfew shots (exemplars) in the prompt. However, deduction does not always imply\nlearning, as ICL does not explicitly encode a given observation. Instead, the\nmodels rely on their prior knowledge and the exemplars given, if any. We argue\nthat, mathematically, ICL does constitute learning, but its full\ncharacterisation requires empirical work. We then carry out a large-scale\nanalysis of ICL ablating out or accounting for memorisation, pretraining,\ndistributional shifts, and prompting style and phrasing. We find that ICL is an\neffective learning paradigm, but limited in its ability to learn and generalise\nto unseen tasks. We note that, in the limit where exemplars become more\nnumerous, accuracy is insensitive to exemplar distribution, model, prompt\nstyle, and the input's linguistic features. Instead, it deduces patterns from\nregularities in the prompt, which leads to distributional sensitivity,\nespecially in prompting styles such as chain-of-thought. Given the varied\naccuracies on formally similar tasks, we conclude that autoregression's ad-hoc\nencoding is not a robust mechanism, and suggests limited all-purpose\ngeneralisability.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u5206\u6790\u8bc1\u660e\uff0c\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u786e\u5b9e\u6784\u6210\u5b66\u4e60\u673a\u5236\uff0c\u4f46\u5176\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u5bf9\u63d0\u793a\u683c\u5f0f\u548c\u5206\u5e03\u53d8\u5316\u654f\u611f\uff0c\u81ea\u56de\u5f52\u7f16\u7801\u673a\u5236\u4e0d\u591f\u9c81\u68d2\u3002", "motivation": "\u63a2\u8ba8\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u662f\u5426\u771f\u6b63\u6784\u6210\u5b66\u4e60\u8fc7\u7a0b\uff0c\u800c\u975e\u4ec5\u4ec5\u662f\u57fa\u4e8e\u5148\u9a8c\u77e5\u8bc6\u7684\u63a8\u7406\uff0c\u9700\u8981\u5b9e\u8bc1\u7814\u7a76\u6765\u5168\u9762\u8868\u5f81ICL\u7684\u5b66\u4e60\u7279\u6027\u3002", "method": "\u8fdb\u884c\u5927\u89c4\u6a21ICL\u5206\u6790\uff0c\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u6392\u9664\u8bb0\u5fc6\u6548\u5e94\u3001\u9884\u8bad\u7ec3\u5f71\u54cd\uff0c\u8003\u5bdf\u5206\u5e03\u504f\u79fb\u3001\u63d0\u793a\u98ce\u683c\u548c\u63aa\u8f9e\u7b49\u56e0\u7d20\u5bf9ICL\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "ICL\u662f\u6709\u6548\u7684\u5b66\u4e60\u8303\u5f0f\uff0c\u4f46\u5b66\u4e60\u672a\u89c1\u4efb\u52a1\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff1b\u5f53\u793a\u4f8b\u589e\u591a\u65f6\uff0c\u51c6\u786e\u7387\u5bf9\u793a\u4f8b\u5206\u5e03\u3001\u6a21\u578b\u3001\u63d0\u793a\u98ce\u683c\u548c\u8bed\u8a00\u7279\u5f81\u4e0d\u654f\u611f\uff0c\u4e3b\u8981\u4ece\u63d0\u793a\u4e2d\u7684\u89c4\u5f8b\u6027\u63a8\u65ad\u6a21\u5f0f\u3002", "conclusion": "\u81ea\u56de\u5f52\u7684\u4e34\u65f6\u7f16\u7801\u673a\u5236\u4e0d\u591f\u9c81\u68d2\uff0c\u8868\u660e\u5176\u901a\u7528\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u7279\u522b\u662f\u5728\u601d\u7ef4\u94fe\u7b49\u63d0\u793a\u98ce\u683c\u4e2d\u5bf9\u5206\u5e03\u53d8\u5316\u654f\u611f\u3002"}}
{"id": "2509.10417", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10417", "abs": "https://arxiv.org/abs/2509.10417", "authors": ["Christopher Ormerod", "Gitit Kehat"], "title": "Long Context Automated Essay Scoring with Language Models", "comment": "8 pages, 2 figures, 2 tables", "summary": "Transformer-based language models are architecturally constrained to process\ntext of a fixed maximum length. Essays written by higher-grade students\nfrequently exceed the maximum allowed length for many popular open-source\nmodels. A common approach to addressing this issue when using these models for\nAutomated Essay Scoring is to truncate the input text. This raises serious\nvalidity concerns as it undermines the model's ability to fully capture and\nevaluate organizational elements of the scoring rubric, which requires long\ncontexts to assess. In this study, we evaluate several models that incorporate\narchitectural modifications of the standard transformer architecture to\novercome these length limitations using the Kaggle ASAP 2.0 dataset. The models\nconsidered in this study include fine-tuned versions of XLNet, Longformer,\nModernBERT, Mamba, and Llama models.", "AI": {"tldr": "\u8bc4\u4f30\u591a\u79cd\u6539\u8fdb\u7684Transformer\u67b6\u6784\u6a21\u578b\u7528\u4e8e\u957f\u6587\u672c\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\uff0c\u89e3\u51b3\u4f20\u7edf\u6a21\u578b\u56e0\u957f\u5ea6\u9650\u5236\u9700\u8981\u622a\u65ad\u6587\u672c\u800c\u5bfc\u81f4\u8bc4\u5206\u6709\u6548\u6027\u95ee\u9898", "motivation": "\u4f20\u7edfTransformer\u6a21\u578b\u6709\u56fa\u5b9a\u6700\u5927\u957f\u5ea6\u9650\u5236\uff0c\u9ad8\u5e74\u7ea7\u5b66\u751f\u4f5c\u6587\u7ecf\u5e38\u8d85\u8fc7\u8fd9\u4e2a\u9650\u5236\uff0c\u622a\u65ad\u5904\u7406\u4f1a\u5f71\u54cd\u6a21\u578b\u5bf9\u7ec4\u7ec7\u7ed3\u6784\u7b49\u8bc4\u5206\u6807\u51c6\u7684\u5b8c\u6574\u8bc4\u4f30\u80fd\u529b", "method": "\u4f7f\u7528Kaggle ASAP 2.0\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u5305\u62ecXLNet\u3001Longformer\u3001ModernBERT\u3001Mamba\u548cLlama\u7b49\u7ecf\u8fc7\u67b6\u6784\u6539\u8fdb\u7684\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u80fd\u591f\u5904\u7406\u66f4\u957f\u6587\u672c", "result": "\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u79cd\u80fd\u591f\u5904\u7406\u957f\u6587\u672c\u7684\u6539\u8fdb\u6a21\u578b\u5728\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u4efb\u52a1\u4e0a\u7684\u8868\u73b0", "conclusion": "\u9700\u8981\u91c7\u7528\u80fd\u591f\u5904\u7406\u957f\u6587\u672c\u7684\u6539\u8fdb\u67b6\u6784\u6a21\u578b\u6765\u89e3\u51b3\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u4e2d\u7684\u6587\u672c\u622a\u65ad\u95ee\u9898\uff0c\u786e\u4fdd\u8bc4\u5206\u6709\u6548\u6027"}}
{"id": "2509.10436", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10436", "abs": "https://arxiv.org/abs/2509.10436", "authors": ["Shadikur Rahman", "Aroosa Hameed", "Gautam Srivastava", "Syed Muhammad Danish"], "title": "RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question Solutions in Cloud and Edge Deployment", "comment": "12 pages, 5 figures, submitted to IEEE Transactions on Services\n  Computing", "summary": "To optimize the reasoning and problem-solving capabilities of Large Language\nModels (LLMs), we propose a novel cloud-edge collaborative architecture that\nenables a structured, multi-agent prompting framework. This framework comprises\nthree specialized components: GuideLLM, a lightweight model deployed at the\nedge to provide methodological guidance; SolverLLM, a more powerful model\nhosted in the cloud responsible for generating code solutions; and JudgeLLM, an\nautomated evaluator for assessing solution correctness and quality. To evaluate\nand demonstrate the effectiveness of this architecture in realistic settings,\nwe introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate\nand enhance the performance of Large Language Models (LLMs) across multi-domain\ncoding tasks. Motivated by the limitations of existing benchmarks,\nRefactorCoderQA systematically covers various technical domains, including\nSoftware Engineering, Data Science, Machine Learning, and Natural Language\nProcessing, using authentic coding challenges from Stack Overflow. Extensive\nexperiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves\nstate-of-the-art performance, significantly outperforming leading open-source\nand commercial baselines with an overall accuracy of 76.84%. Human evaluations\nfurther validate the interpretability, accuracy, and practical relevance of the\ngenerated solutions. In addition, we evaluate system-level metrics, such as\nthroughput and latency, to gain deeper insights into the performance\ncharacteristics and trade-offs of the proposed architecture.", "AI": {"tldr": "\u63d0\u51fa\u4e91\u8fb9\u534f\u4f5c\u7684\u591a\u667a\u80fd\u4f53\u63d0\u793a\u6846\u67b6\uff0c\u5305\u542bGuideLLM\u3001SolverLLM\u548cJudgeLLM\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u5e76\u521b\u5efaRefactorCoderQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728\u591a\u4e2a\u7f16\u7a0b\u9886\u57df\u8fbe\u523076.84%\u7684\u6700\u4f18\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u548c\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u591a\u9886\u57df\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528\u4e91\u8fb9\u534f\u4f5c\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u4e13\u95e8\u7ec4\u4ef6\uff1a\u8fb9\u7f18\u90e8\u7f72\u7684\u8f7b\u91cf\u7ea7GuideLLM\u63d0\u4f9b\u65b9\u6cd5\u6307\u5bfc\uff0c\u4e91\u7aef\u5f3a\u5927SolverLLM\u751f\u6210\u4ee3\u7801\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u53ca\u81ea\u52a8\u8bc4\u4f30\u5668JudgeLLM\u3002\u521b\u5efaRefactorCoderQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u8f6f\u4ef6\u5de5\u7a0b\u3001\u6570\u636e\u79d1\u5b66\u3001\u673a\u5668\u5b66\u4e60\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7b49\u591a\u4e2a\u6280\u672f\u9886\u57df\u3002", "result": "\u5fae\u8c03\u6a21\u578bRefactorCoder-MoE\u8fbe\u523076.84%\u7684\u6574\u4f53\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u9886\u5148\u7684\u5f00\u6e90\u548c\u5546\u4e1a\u57fa\u7ebf\u6a21\u578b\u3002\u4eba\u7c7b\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u751f\u6210\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u89e3\u91ca\u6027\u3001\u51c6\u786e\u6027\u548c\u5b9e\u9645\u76f8\u5173\u6027\u3002\u7cfb\u7edf\u7ea7\u6307\u6807\u5982\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u4e5f\u5f97\u5230\u8bc4\u4f30\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e91\u8fb9\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86LLMs\u5728\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0cRefactorCoderQA\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u591a\u9886\u57df\u7f16\u7801\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5168\u9762\u6807\u51c6\u3002"}}
{"id": "2509.10446", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10446", "abs": "https://arxiv.org/abs/2509.10446", "authors": ["Rui Lu", "Zhenyu Hou", "Zihan Wang", "Hanchen Zhang", "Xiao Liu", "Yujiang Li", "Shi Feng", "Jie Tang", "Yuxiao Dong"], "title": "DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL", "comment": null, "summary": "Augmenting large language models (LLMs) with browsing tools substantially\nimproves their potential as deep search agents to solve complex, real-world\ntasks. Yet, open LLMs still perform poorly in such settings due to limited\nlong-horizon reasoning capacity with browsing tools and the lack of\nsufficiently difficult supervised data. To address these challenges, we present\nDeepDive to advance deep search agents. First, we propose a strategy to\nautomatically synthesize complex, difficult, and hard-to-find questions from\nopen knowledge graphs. Second, we apply end-to-end multi-turn reinforcement\nlearning (RL) to enhance LLMs' long-horizon reasoning with deep search.\nExperiments show that DeepDive-32B achieves a new open-source competitive\nresult on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and\nSearch-o1. We demonstrate that multi-turn RL training improves deep search\nability and significantly contributes to the performance improvements across\nmultiple benchmarks. We observe that DeepDive enables test-time scaling of tool\ncalls and parallel sampling. All datasets, models, and code are publicly\navailable at https://github.com/THUDM/DeepDive.", "AI": {"tldr": "DeepDive\u662f\u4e00\u4e2a\u901a\u8fc7\u81ea\u52a8\u5408\u6210\u590d\u6742\u95ee\u9898\u548c\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u6df1\u5ea6\u641c\u7d22\u80fd\u529b\u7684\u7cfb\u7edf\uff0c\u5728BrowseComp\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u5f00\u6e90\u6a21\u578b\u7684\u6700\u4f73\u6027\u80fd", "motivation": "\u73b0\u6709\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6df1\u5ea6\u641c\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u957f\u65f6\u7a0b\u63a8\u7406\u80fd\u529b\u548c\u7f3a\u4e4f\u8db3\u591f\u96be\u5ea6\u7684\u76d1\u7763\u6570\u636e", "method": "1. \u4ece\u5f00\u653e\u77e5\u8bc6\u56fe\u8c31\u81ea\u52a8\u5408\u6210\u590d\u6742\u3001\u56f0\u96be\u4e14\u96be\u4ee5\u627e\u5230\u7684\u95ee\u9898\uff1b2. \u5e94\u7528\u7aef\u5230\u7aef\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u65f6\u7a0b\u6df1\u5ea6\u641c\u7d22\u63a8\u7406\u80fd\u529b", "result": "DeepDive-32B\u5728BrowseComp\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86WebSailor\u3001DeepSeek-R1-Browse\u548cSearch-o1\u7b49\u7ade\u4e89\u5bf9\u624b\uff0c\u53d6\u5f97\u4e86\u5f00\u6e90\u7ade\u4e89\u6027\u7ed3\u679c\u3002\u591a\u8f6eRL\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u641c\u7d22\u80fd\u529b\uff0c\u5e76\u652f\u6301\u6d4b\u8bd5\u65f6\u7684\u5de5\u5177\u8c03\u7528\u6269\u5c55\u548c\u5e76\u884c\u91c7\u6837", "conclusion": "DeepDive\u901a\u8fc7\u81ea\u52a8\u6570\u636e\u5408\u6210\u548c\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6df1\u5ea6\u641c\u7d22\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u6df1\u5ea6\u641c\u7d22\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2509.10452", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10452", "abs": "https://arxiv.org/abs/2509.10452", "authors": ["Akshat Pandey", "Karun Kumar", "Raphael Tang"], "title": "WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers", "comment": "5 pages, 2 figures", "summary": "Pretrained automatic speech recognition (ASR) models such as Whisper perform\nwell but still need domain adaptation to handle unseen vocabulary and parlance.\nIn many real-world settings, collecting speech data is impractical,\nnecessitating text-only adaptation. We propose WhisTLE, a deeply supervised,\ntext-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE\ntrains a variational autoencoder (VAE) to model encoder outputs from text and\nfine-tunes the decoder using the learned text-to-latent encoder, optionally\ncombined with text-to-speech (TTS) adaptation. At inference, the original\nencoder is restored, incurring no extra runtime cost. Across four out-of-domain\ndatasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by\n12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines\nin 27 of 32 scenarios.", "AI": {"tldr": "WhisTLE\u662f\u4e00\u79cd\u4ec5\u4f7f\u7528\u6587\u672c\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3ASR\u6a21\u578b\u9886\u57df\u9002\u5e94\u7684\u6df1\u5ea6\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5efa\u6a21\u7f16\u7801\u5668\u8f93\u51fa\u5e76\u5fae\u8c03\u89e3\u7801\u5668\uff0c\u65e0\u9700\u989d\u5916\u63a8\u7406\u6210\u672c\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u964d\u4f4e\u8bcd\u9519\u8bef\u7387\u3002", "motivation": "\u9884\u8bad\u7ec3ASR\u6a21\u578b\u5982Whisper\u5728\u672a\u89c1\u8bcd\u6c47\u548c\u65b9\u8a00\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6536\u96c6\u8bed\u97f3\u6570\u636e\u56f0\u96be\uff0c\u9700\u8981\u4ec5\u4f7f\u7528\u6587\u672c\u6570\u636e\u8fdb\u884c\u9886\u57df\u9002\u5e94\u3002", "method": "\u63d0\u51faWhisTLE\u65b9\u6cd5\uff1a1\uff09\u8bad\u7ec3\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4ece\u6587\u672c\u5efa\u6a21\u7f16\u7801\u5668\u8f93\u51fa\uff1b2\uff09\u4f7f\u7528\u5b66\u4e60\u7684\u6587\u672c\u5230\u6f5c\u5728\u7f16\u7801\u5668\u5fae\u8c03\u89e3\u7801\u5668\uff1b3\uff09\u53ef\u9009\u7ed3\u5408\u6587\u672c\u5230\u8bed\u97f3\u9002\u5e94\uff1b4\uff09\u63a8\u7406\u65f6\u6062\u590d\u539f\u59cb\u7f16\u7801\u5668\u3002", "result": "\u57284\u4e2a\u57df\u5916\u6570\u636e\u96c6\u548c4\u4e2aASR\u6a21\u578b\u4e0a\uff0cWhisTLE\u7ed3\u5408TTS\u76f8\u6bd4\u4ec5\u4f7f\u7528TTS\u9002\u5e94\u76f8\u5bf9\u964d\u4f4e12.3%\u7684\u8bcd\u9519\u8bef\u7387\uff0c\u572832\u4e2a\u573a\u666f\u4e2d\u768427\u4e2a\u8d85\u8d8a\u6240\u6709\u975eWhisTLE\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "WhisTLE\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6587\u672conly\u9886\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347ASR\u6a21\u578b\u5728\u672a\u89c1\u9886\u57df\u7684\u6027\u80fd\uff0c\u4e14\u4e0d\u589e\u52a0\u63a8\u7406\u65f6\u7684\u8ba1\u7b97\u6210\u672c\u3002"}}
