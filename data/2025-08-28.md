<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 59]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.SD](#cs.SD) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts](https://arxiv.org/abs/2508.19268)
*Qing Wang,Xue Han,Jiahui Wang,Lehao Xing,Qian Hu,Lianlian Zhang,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: 提出MultiPL-MoE方法，通过混合专家模型提升LLMs的多编程语言代码生成能力，在保持主流语言性能的同时提升多语言表现


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码生成方面表现出色，但多编程语言代码生成仍然极具挑战性。需要在有限计算资源下提升多编程语言性能同时保持主流语言能力

Method: 提出MultiPL-MoE混合专家模型，结合token级和segment级两个配对MoE。token级采用标准upcycling MoE结构，segment级使用滑动窗口分割输入序列并采用专家选择路由策略

Result: 实验结果表明MultiPL-MoE方法的有效性

Conclusion: 该方法成功提升了LLMs在多编程语言代码生成方面的性能，为解决多语言代码生成挑战提供了有效方案

Abstract: Despite LLMs' excellent code creation capabilities, multilingual code
generation remains extremely challenging. To address this, we intent to improve
the multi-programming-lingual (MultiPL) performance of the base LLMs while
retaining the most popular ones using restricted computational resources. We
consider MultiPL to be a special case of multiple natural languages and propose
a MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called
MultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize
expert selection at both the token and segment levels. The token-level MoE is a
standard upcycling MoE structure with a shared expert and a novel gate weight
normalization approach that aids in the final fusion with the segment-level
MoE. The segment-level MoE incorporates two innovative designs to better
capture the syntactic structure and contextual patterns of programming
languages: First, using a sliding window to partition the input token sequence
into multiple segments; Then, adopting an expert-choice routing strategy that
allows experts to select the top-k segments. The results of the experiment
proved the effectiveness of MultiPL-MoE.

</details>


### [2] [Whisper based Cross-Lingual Phoneme Recognition between Vietnamese and English](https://arxiv.org/abs/2508.19270)
*Nguyen Huu Nhat Minh,Tran Nguyen Anh,Truong Dinh Dung,Vo Van Nam,Le Pham Tuyen*

Main category: cs.CL

TL;DR: 提出一种新的双语语音识别方法，通过构建跨语言音素集和利用PhoWhisper预训练编码器，有效解决趋南语和英语混合语音识别中的音调和重音挑战。


<details>
  <summary>Details</summary>
Motivation: 趋南语依赖音调区分词义，英语具有重音模式和非标准发音，两者的音系差异给跨语言音素识别带来重大挑战。

Method: 构建代表性双语音素集来缩小趋南语和英语音系差异，设计端到端系统利用PhoWhisper预训练编码器获取深层高级表征来改善音素识别。

Result: 实验结果显示该方法在趋南语双语语音识别中提高了识别准确率，同时为处理音调和重音基础的音素识别复杂性提供了稳健框架。

Conclusion: 该研究成功地解决了趋南语-英语混合语音识别的音系差异问题，为跨语言音素识别领域提供了有效的解决方案。

Abstract: Cross-lingual phoneme recognition has emerged as a significant challenge for
accurate automatic speech recognition (ASR) when mixing Vietnamese and English
pronunciations. Unlike many languages, Vietnamese relies on tonal variations to
distinguish word meanings, whereas English features stress patterns and
non-standard pronunciations that hinder phoneme alignment between the two
languages. To address this challenge, we propose a novel bilingual speech
recognition approach with two primary contributions: (1) constructing a
representative bilingual phoneme set that bridges the differences between
Vietnamese and English phonetic systems; (2) designing an end-to-end system
that leverages the PhoWhisper pre-trained encoder for deep high-level
representations to improve phoneme recognition. Our extensive experiments
demonstrate that the proposed approach not only improves recognition accuracy
in bilingual speech recognition for Vietnamese but also provides a robust
framework for addressing the complexities of tonal and stress-based phoneme
recognition

</details>


### [3] [Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT](https://arxiv.org/abs/2508.19271)
*Rushitha Santhoshi Mamidala,Anshuman Chhabra,Ankur Mali*

Main category: cs.CL

TL;DR: 本文提出了一种基于局部加权有限自动机(WFA)的RetoMaton改进方法，替代原有的全局数据存储，通过从外部领域语料直接构建本地自动机结构，实现了更稳健、可解释的检索推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于提示的推理方法（如Chain-of-Thought和In-Context Learning）存在脆弱性和不一致性问题，输出结果对种子、格式或微小提示变化敏感，缺乏稳定性和可解释性，不适合需要可靠推理的任务。

Method: 扩展RetoMaton框架，用从外部领域语料构建的局部加权有限自动机(WFA)替代全局数据存储，保持符号可追溯性和低推理开销，提供可验证和模块化的检索行为。

Result: 在LLaMA-3.2-1B和Gemma-3-1B-PT两个预训练模型上，在TriviaQA、GSM8K和MMLU三个推理任务中评估，相比基础模型和基于提示的方法，本地RetoMaton变体持续提升性能，同时实现透明和可复现的检索动态。

Conclusion: 研究结果表明通过轻量级的自动机引导内存，为现代大语言模型实现可信赖的符号推理提供了有前景的方向，促进了向结构化、可解释推理的转变。

Abstract: Prompt-based reasoning strategies such as Chain-of-Thought (CoT) and
In-Context Learning (ICL) have become widely used for eliciting reasoning
capabilities in large language models (LLMs). However, these methods rely on
fragile, implicit mechanisms often yielding inconsistent outputs across seeds,
formats, or minor prompt variations making them fundamentally unreliable for
tasks requiring stable, interpretable reasoning. In contrast, automata-based
neuro-symbolic frameworks like RetoMaton offer a more structured and
trustworthy alternative by grounding retrieval in symbolic memory with
deterministic transitions. In this work, we extend RetoMaton by replacing its
global datastore with a local, task-adaptive Weighted Finite Automaton (WFA),
constructed directly from external domain corpora. This local automaton
structure promotes robust, context-aware retrieval while preserving symbolic
traceability and low inference overhead. Unlike prompting, which entangles
context and memory in opaque ways, our approach leverages the explicit
structure of WFAs to provide verifiable and modular retrieval behavior, making
it better suited for domain transfer and interoperability. We evaluate this
local RetoMaton variant on two pretrained LLMs LLaMA-3.2-1B and Gemma-3-1B-PT
across three reasoning tasks: TriviaQA (reading comprehension), GSM8K
(multi-step math), and MMLU (domain knowledge). Compared to the base model and
prompting-based methods, augmenting these setups with local RetoMaton
consistently improves performance while enabling transparent and reproducible
retrieval dynamics. Our results highlight a promising shift toward trustworthy,
symbolic reasoning in modern LLMs via lightweight, automaton-guided memory.

</details>


### [4] [RAGAPHENE: A RAG Annotation Platform with Human Enhancements and Edits](https://arxiv.org/abs/2508.19272)
*Kshitij Fadnis,Sara Rosenthal,Maeda Hanafi,Yannis Katsis,Marina Danilevsky*

Main category: cs.CL

TL;DR: RAGAPHENE是一个基于聊天的标注平台，用于模拟真实世界对话来评估大语言模型在检索增强生成任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型在提供事实性信息时可能出现幻觉，需要构建能够评估多轮RAG对话的基准测试，而模拟真实对话对于生成高质量评估基准至关重要。

Method: 开发了RAGAPHENE聊天标注平台，让标注者能够模拟真实世界对话，用于构建评估LLM的基准数据集。

Result: 该平台已被约40名标注者成功使用，构建了数千个真实世界对话样本。

Conclusion: RAGAPHENE平台为评估大语言模型在RAG任务中的表现提供了有效的工具和方法。

Abstract: Retrieval Augmented Generation (RAG) is an important aspect of conversing
with Large Language Models (LLMs) when factually correct information is
important. LLMs may provide answers that appear correct, but could contain
hallucinated information. Thus, building benchmarks that can evaluate LLMs on
multi-turn RAG conversations has become an increasingly important task.
Simulating real-world conversations is vital for producing high quality
evaluation benchmarks. We present RAGAPHENE, a chat-based annotation platform
that enables annotators to simulate real-world conversations for benchmarking
and evaluating LLMs. RAGAPHENE has been successfully used by approximately 40
annotators to build thousands of real-world conversations.

</details>


### [5] [Leveraging Language Models and Machine Learning in Verbal Autopsy Analysis](https://arxiv.org/abs/2508.19274)
*Yue Chu*

Main category: cs.CL

TL;DR: 本研究探讨了在口头尸检中使用预训练语言模型分析叙述文本，结合结构化问题来改善死因分类，证明叙述文本包含独特信息并能提升分类准确性。


<details>
  <summary>Details</summary>
Motivation: 在没有民事登记和生命统计的国家，口头尸检是估计死因的关键工具。现有自动化分类算法仅使用结构化问题而忽略了叙述文本中的信息，需要研究如何利用叙述文本来改善死因分类。

Method: 使用预训练语言模型和机器学习技术，分析南非的经验数据，探索单模态（仅叙述文本）和多模态（叙述文本+结构化问题）融合策略，并评估医师感知的信息充分性。

Result: 仅使用叙述文本时，基于Transformer的预训练模型在个体和群体层面的死因分类表现优于仅使用问题的算法，特别是在识别非传染性疾病方面。多模态方法进一步提升了分类性能，证明每种模态都有独特贡献。

Conclusion: 叙述文本能显著增强死因分类，需要更多高质量多样化数据来训练模型，这些发现为指导重新设计和改进口头尸检工具提供了宝贵见解。

Abstract: In countries without civil registration and vital statistics, verbal autopsy
(VA) is a critical tool for estimating cause of death (COD) and inform policy
priorities. In VA, interviewers ask proximal informants for details on the
circumstances preceding a death, in the form of unstructured narratives and
structured questions. Existing automated VA cause classification algorithms
only use the questions and ignore the information in the narratives. In this
thesis, we investigate how the VA narrative can be used for automated COD
classification using pretrained language models (PLMs) and machine learning
(ML) techniques. Using empirical data from South Africa, we demonstrate that
with the narrative alone, transformer-based PLMs with task-specific fine-tuning
outperform leading question-only algorithms at both the individual and
population levels, particularly in identifying non-communicable diseases. We
explore various multimodal fusion strategies combining narratives and questions
in unified frameworks. Multimodal approaches further improve performance in COD
classification, confirming that each modality has unique contributions and may
capture valuable information that is not present in the other modality. We also
characterize physician-perceived information sufficiency in VA. We describe
variations in sufficiency levels by age and COD and demonstrate that
classification accuracy is affected by sufficiency for both physicians and
models. Overall, this thesis advances the growing body of knowledge at the
intersection of natural language processing, epidemiology, and global health.
It demonstrates the value of narrative in enhancing COD classification. Our
findings underscore the need for more high-quality data from more diverse
settings to use in training and fine-tuning PLM/ML methods, and offer valuable
insights to guide the rethinking and redesign of the VA instrument and
interview.

</details>


### [6] [FLAIRR-TS -- Forecasting LLM-Agents with Iterative Refinement and Retrieval for Time Series](https://arxiv.org/abs/2508.19279)
*Gunjan Jalori,Preetika Verma,Sercan Ö Arık*

Main category: cs.CL

TL;DR: FLAIRR-TS是一个基于代理系统的测试时提示优化框架，通过预测代理和优化代理的协作，无需微调即可提升大语言模型在时间序列预测中的性能


<details>
  <summary>Details</summary>
Motivation: 传统方法需要为每个任务精心设计自然语言提示，过程繁琐且缺乏通用性，需要一种能够自适应优化提示的框架

Method: 使用双代理系统：预测代理用初始提示生成预测，优化代理基于历史输出和检索的相似序列来迭代优化提示，采用创意提示模板实现跨领域泛化

Result: 在基准数据集上相比静态提示和检索增强基线提高了准确性，接近专门设计的提示性能

Conclusion: FLAIRR-TS提供了一种实用的替代微调的方法，通过自适应提示优化和检索的代理方法实现了强劲性能

Abstract: Time series Forecasting with large languagemodels (LLMs) requires bridging
numericalpatterns and natural language. Effective fore-casting on LLM often
relies on extensive pre-processing and fine-tuning.Recent studiesshow that a
frozen LLM can rival specializedforecasters when supplied with a carefully
en-gineered natural-language prompt, but craft-ing such a prompt for each task
is itself oner-ous and ad-hoc. We introduce FLAIRR-TS, atest-time prompt
optimization framework thatutilizes an agentic system: a
Forecaster-agentgenerates forecasts using an initial prompt,which is then
refined by a refiner agent, in-formed by past outputs and retrieved
analogs.This adaptive prompting generalizes across do-mains using creative
prompt templates andgenerates high-quality forecasts without inter-mediate code
generation.Experiments onbenchmark datasets show improved accuracyover static
prompting and retrieval-augmentedbaselines, approaching the performance
ofspecialized prompts.FLAIRR-TS providesa practical alternative to tuning,
achievingstrong performance via its agentic approach toadaptive prompt
refinement and retrieval.

</details>


### [7] [CORE: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning](https://arxiv.org/abs/2508.19282)
*Ziqiang Cui,Yunpeng Weng,Xing Tang,Peiyang Liu,Shiwei Li,Bowei He,Jiamin Chen,Xiuqiang He,Chen Ma*

Main category: cs.CL

TL;DR: CORE是一种基于强化学习的无损上下文压缩方法，通过端到端训练优化RAG中的文档压缩，在3%的高压缩比下不仅避免了性能下降，还平均提升了3.3个EM分数。


<details>
  <summary>Details</summary>
Motivation: 解决RAG中检索文档过多导致输入长度增加和计算成本上升的问题，同时避免传统压缩方法因缺乏明确压缩目标而损害最终任务性能的局限性。

Method: 采用强化学习框架，使用最终任务性能作为奖励信号，应用广义强化学习策略优化(GRPO)训练压缩器，实现端到端的无损上下文压缩。

Result: 在四个数据集上的实验表明，该方法在3%的高压缩比下，不仅避免了在所有数据集上相比完整文档的性能下降，还将平均精确匹配(EM)分数提高了3.3分。

Conclusion: CORE方法通过强化学习优化压缩过程，实现了高效的文档压缩，在保持甚至提升RAG性能的同时显著降低了计算成本，为RAG系统的实际应用提供了有效解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach to
enhance the timeliness of knowledge and the factual accuracy of responses in
Large Language Models (LLMs). However, the inclusion of excessive retrieved
documents substantially increases the input length, leading to higher
computational costs. Previous studies have attempted to compress retrieved
documents into shorter texts before in-context integration, but such methods
often compromise end-task performance. The lack of well-defined compression
targets forces many approaches to rely on fixed heuristics, which cannot
guarantee that the compressed content will effectively support the end task. To
address these limitations, we propose CORE, a novel method designed to achieve
lossless context compression for RAG. CORE employs reinforcement learning to
optimize the compression process without relying on predefined compression
labels. Specifically, it utilizes end-task performance as a reward signal and
applies Generalized Reinforcement Learning Policy Optimization (GRPO) to train
the compressor. This end-to-end training framework enables the compressor to
generate summaries that maximize the accuracy of answers generated by the LLM.
Extensive experiments on four datasets demonstrate the superiority of our
approach. With a high compression ratio of 3\%, our method not only avoids
performance degradation compared to prepending full documents across all
datasets but also improves the average Exact Match (EM) score by 3.3 points.
The code will be released soon.

</details>


### [8] [Context-Adaptive Synthesis and Compression for Enhanced Retrieval-Augmented Generation in Complex Domains](https://arxiv.org/abs/2508.19357)
*Peiran Zhou,Junnan Zhu,Yichen Shen,Ruoxi Yu*

Main category: cs.CL

TL;DR: CASC框架通过上下文分析和合成模块，有效处理多文档检索中的信息过载和冲突问题，显著提升复杂领域问答的准确性和可信度


<details>
  <summary>Details</summary>
Motivation: 传统RAG在处理多文档、长文档或冲突文档时存在信息过载和合成效率低的问题，导致答案不准确和不可信

Method: 提出CASC框架，包含基于微调小模型的上下文分析合成模块，进行关键信息提取、跨文档一致性检查和冲突解决、面向问题的结构化合成

Result: 在SciDocs-QA数据集上的实验表明，CASC持续优于强基线方法

Conclusion: CASC通过智能上下文处理，将原始分散信息转换为高度压缩的结构化上下文，显著减少token数量和认知负荷，提高问答质量

Abstract: Large Language Models (LLMs) excel in language tasks but are prone to
hallucinations and outdated knowledge. Retrieval-Augmented Generation (RAG)
mitigates these by grounding LLMs in external knowledge. However, in complex
domains involving multiple, lengthy, or conflicting documents, traditional RAG
suffers from information overload and inefficient synthesis, leading to
inaccurate and untrustworthy answers. To address this, we propose CASC
(Context-Adaptive Synthesis and Compression), a novel framework that
intelligently processes retrieved contexts. CASC introduces a Context Analyzer
& Synthesizer (CAS) module, powered by a fine-tuned smaller LLM, which performs
key information extraction, cross-document consistency checking and conflict
resolution, and question-oriented structured synthesis. This process transforms
raw, scattered information into a highly condensed, structured, and
semantically rich context, significantly reducing the token count and cognitive
load for the final Reader LLM. We evaluate CASC on SciDocs-QA, a new
challenging multi-document question answering dataset designed for complex
scientific domains with inherent redundancies and conflicts. Our extensive
experiments demonstrate that CASC consistently outperforms strong baselines.

</details>


### [9] [Reflective Agreement: Combining Self-Mixture of Agents with a Sequence Tagger for Robust Event Extraction](https://arxiv.org/abs/2508.19359)
*Fatemeh Haji,Mazal Bethany,Cho-Yu Jason Chiang,Anthony Rios,Peyman Najafirad*

Main category: cs.CL

TL;DR: 事件提取新方法ARIS，结合了判别模型和生成模型的优势，通过模型共识和反思推理提高提取准确性


<details>
  <summary>Details</summary>
Motivation: 传统判别模型精确度高但回召率低，生成模型回召率高但存在幻觉和不一致问题，需要找到两者的平衡点

Method: 提出ARIS混合方法，结合自我混合代理和判别序列标注器，利用结构化模型共识、信心度筛选和LLM反思推理模块来解决正式问题

Result: 在三个标准数据集上超越了现有的最先进事件提取方法

Conclusion: ARIS通过混合方案有效结合了判别模型和生成模型的优势，显著提高了事件提取的整体性能

Abstract: Event Extraction (EE) involves automatically identifying and extracting
structured information about events from unstructured text, including triggers,
event types, and arguments. Traditional discriminative models demonstrate high
precision but often exhibit limited recall, particularly for nuanced or
infrequent events. Conversely, generative approaches leveraging Large Language
Models (LLMs) provide higher semantic flexibility and recall but suffer from
hallucinations and inconsistent predictions. To address these challenges, we
propose Agreement-based Reflective Inference System (ARIS), a hybrid approach
combining a Self Mixture of Agents with a discriminative sequence tagger. ARIS
explicitly leverages structured model consensus, confidence-based filtering,
and an LLM reflective inference module to reliably resolve ambiguities and
enhance overall event prediction quality. We further investigate decomposed
instruction fine-tuning for enhanced LLM event extraction understanding.
Experiments demonstrate our approach outperforms existing state-of-the-art
event extraction methods across three benchmark datasets.

</details>


### [10] [LongReasonArena: A Long Reasoning Benchmark for Large Language Models](https://arxiv.org/abs/2508.19363)
*Jiayu Ding,Shuming Ma,Lei Cui,Nanning Zheng,Furu Wei*

Main category: cs.CL

TL;DR: LongReasonArena是一个专门评估大语言模型长推理能力的基准测试，通过多步算法执行任务，推理长度可达100万token，现有模型表现较差。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文基准测试主要关注长输入理解，而忽视了长推理能力的评估，需要专门设计基准来填补这一空白。

Method: 设计需要执行多步算法（如检索和回溯）的任务，通过控制输入来任意扩展推理长度，最高可达100万token的推理过程。

Result: 该基准对开源和专有LLM都构成重大挑战，Deepseek-R1仅达到7.5%准确率，准确率随推理步数对数呈线性下降趋势。

Conclusion: LongReasonArena有效评估了LLM的长推理能力，揭示了当前模型在此方面的显著不足，为未来模型发展提供了重要基准。

Abstract: Existing long-context benchmarks for Large Language Models (LLMs) focus on
evaluating comprehension of long inputs, while overlooking the evaluation of
long reasoning abilities. To address this gap, we introduce LongReasonArena, a
benchmark specifically designed to assess the long reasoning capabilities of
LLMs. Our tasks require models to solve problems by executing multi-step
algorithms that reflect key aspects of long reasoning, such as retrieval and
backtracking. By controlling the inputs, the required reasoning length can be
arbitrarily scaled, reaching up to 1 million tokens of reasoning for the most
challenging tasks. Extensive evaluation results demonstrate that
LongReasonArena presents a significant challenge for both open-source and
proprietary LLMs. For instance, Deepseek-R1 achieves only 7.5% accuracy on our
task. Further analysis also reveals that the accuracy exhibits a linear decline
with respect to the logarithm of the expected number of reasoning steps. Our
code and data is available at
https://github.com/LongReasonArena/LongReasonArena.

</details>


### [11] [Database Entity Recognition with Data Augmentation and Deep Learning](https://arxiv.org/abs/2508.19372)
*Zikun Fu,Chen Yang,Kourosh Davoudi,Ken Q. Pu*

Main category: cs.CL

TL;DR: 本文提出了一个针对自然语言查询中数据库实体识别(DB-ER)的新方法，包括创建人工标注基准、数据增强技术和基于T5的专门实体识别模型，在精度和召回率方面优于现有NER模型。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言查询中数据库实体识别的挑战，现有NER模型在该特定任务上表现不佳，需要专门针对数据库环境的实体识别方法。

Method: 1) 从流行的text-to-SQL基准创建人工标注的DB-ER基准；2) 利用SQL查询自动标注NLQ的数据增强方法；3) 基于T5架构的专门语言模型，使用序列标注和token分类两个下游任务进行微调。

Result: 提出的DB-ER标注器在精度和召回率方面优于两种最先进的NER标注器。数据增强使精度和召回率提升超过10%，T5骨干网络微调使这些指标提升5-10%。

Conclusion: 该方法有效解决了数据库环境下的实体识别问题，数据增强和专门模型架构的结合显著提升了DB-ER任务的性能表现。

Abstract: This paper addresses the challenge of Database Entity Recognition (DB-ER) in
Natural Language Queries (NLQ). We present several key contributions to advance
this field: (1) a human-annotated benchmark for DB-ER task, derived from
popular text-to-sql benchmarks, (2) a novel data augmentation procedure that
leverages automatic annotation of NLQs based on the corresponding SQL queries
which are available in popular text-to-SQL benchmarks, (3) a specialized
language model based entity recognition model using T5 as a backbone and two
down-stream DB-ER tasks: sequence tagging and token classification for
fine-tuning of backend and performing DB-ER respectively. We compared our DB-ER
tagger with two state-of-the-art NER taggers, and observed better performance
in both precision and recall for our model. The ablation evaluation shows that
data augmentation boosts precision and recall by over 10%, while fine-tuning of
the T5 backbone boosts these metrics by 5-10%.

</details>


### [12] [One Joke to Rule them All? On the (Im)possibility of Generalizing Humor](https://arxiv.org/abs/2508.19402)
*Mor Turgeman,Chen Shani,Dafna Shahaf*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型在不同幽默类型间的迁移学习能力，发现模型能够实现一定程度的迁移，多源训练可提升迁移性能，其中Dad Jokes类型在促进迁移方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 幽默是复杂多样的交流形式，现有计算幽默研究多集中于特定类型。随着新型幽默在社交媒体中不断涌现，需要研究LLMs是否能通过迁移学习捕捉深层可转移机制来适应这种演变。

Method: 通过在四个不同幽默任务数据集上进行迁移学习实验，训练LLMs在不同多样性设置下（1-3个训练数据集），测试在新任务上的表现。

Result: 模型能够实现一定程度的迁移，在未见数据集上可达75%准确率；多源训练可提升1.88-4.05%的迁移能力，且对域内性能影响极小；Dad Jokes类型最能促进迁移但最难被迁移到。

Conclusion: 幽默类型间的迁移学习是可行的，多源训练有助于提升模型的泛化能力，为LLMs适应不断演变的幽默景观提供了重要见解。

Abstract: Humor is a broad and complex form of communication that remains challenging
for machines. Despite its broadness, most existing research on computational
humor traditionally focused on modeling a specific type of humor. In this work,
we wish to understand whether competence on one or more specific humor tasks
confers any ability to transfer to novel, unseen types; in other words, is this
fragmentation inevitable? This question is especially timely as new humor types
continuously emerge in online and social media contexts (e.g., memes,
anti-humor, AI fails). If Large Language Models (LLMs) are to keep up with this
evolving landscape, they must be able to generalize across humor types by
capturing deeper, transferable mechanisms. To investigate this, we conduct a
series of transfer learning experiments across four datasets, representing
different humor tasks. We train LLMs under varied diversity settings (1-3
datasets in training, testing on a novel task). Experiments reveal that models
are capable of some transfer, and can reach up to 75% accuracy on unseen
datasets; training on diverse sources improves transferability (1.88-4.05%)
with minimal-to-no drop in in-domain performance. Further analysis suggests
relations between humor types, with Dad Jokes surprisingly emerging as the best
enabler of transfer (but is difficult to transfer to). We release data and
code.

</details>


### [13] [A perishable ability? The future of writing in the face of generative artificial intelligence](https://arxiv.org/abs/2508.19427)
*Evandro L. T. P. Cunha*

Main category: cs.CL

TL;DR: 本文探讨人工智能文本生成工具可能导致人类写作能力退化的风险，与古希腊黑暗时代文字能力丧失的历史现象相类比


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具特别是大语言模型的快速发展，机器生成的文本在各个领域广泛应用，可能导致人类写作活动减少，引发对人类写作能力可能退化甚至丧失的担忧

Method: 采用历史类比方法，将当前AI文本生成技术发展可能带来的影响与古希腊黑暗时代（约公元前1200-800年）人类文字能力丧失的历史现象进行对比分析

Result: 识别出AI文本生成工具的普及可能导致人类写作能力逐渐退化，类似于历史上某些时期人类文字能力的丧失，这种技术发展可能带来意想不到的文化和认知后果

Conclusion: 需要警惕过度依赖AI文本生成工具可能导致的人类写作能力退化风险，建议在享受技术便利的同时保持人类自身的写作实践和能力发展

Abstract: The 2020s have been witnessing a very significant advance in the development
of generative artificial intelligence tools, including text generation systems
based on large language models. These tools have been increasingly used to
generate texts in the most diverse domains -- from technical texts to literary
texts --, which might eventually lead to a lower volume of written text
production by humans. This article discusses the possibility of a future in
which human beings will have lost or significantly decreased their ability to
write due to the outsourcing of this activity to machines. This possibility
parallels the loss of the ability to write in other moments of human history,
such as during the so-called Greek Dark Ages (approx. 1200 BCE - 800 BCE).

</details>


### [14] [Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting, Ensemble Typing, and Attention-Based Taxonomies)](https://arxiv.org/abs/2508.19428)
*Aleksandra Beliaeva,Temurbek Rahmatullaev*

Main category: cs.CL

TL;DR: 本文提出了一个综合系统，通过结合检索增强提示、零梭分类和注意力图谱模型，在LLMs4OL 2025挑战赛的三个任务（术语提取、类型分配和分类系发现）中获得了领先成绩。


<details>
  <summary>Details</summary>
Motivation: 解决本体学习任务中的三个关键挑战：领域特定术语提取、类型分配和分类系构建，展现LLM基于架构在异构领域中的可扩展性、适应性和稳健性。

Method: 任务A：使用检索增强生成(RAG)流水线联合提取术语和类型；任务B：在少梭设置中重用RAG方案，在零梭设置中使用多模型弹性距离组合的零梭分类器；任务C：通过语义嵌入和跨注意力层将分类系发现模型化为图谱推理问题。

Result: 该系统在官方排行榜中在三个任务上均获得了最高排名，证明了方法的有效性和优势。

Conclusion: 这些模块化的任务特定解决方案展示了LLM基础架构在异质领域本体学习中的可扩展性、适应性和稳健性，为本体建模提供了可靠的技术方案。

Abstract: We present a comprehensive system for addressing Tasks A, B, and C of the
LLMs4OL 2025 challenge, which together span the full ontology construction
pipeline: term extraction, typing, and taxonomy discovery. Our approach
combines retrieval-augmented prompting, zero-shot classification, and
attention-based graph modeling -- each tailored to the demands of the
respective task. For Task A, we jointly extract domain-specific terms and their
ontological types using a retrieval-augmented generation (RAG) pipeline.
Training data was reformulated into a document to terms and types
correspondence, while test-time inference leverages semantically similar
training examples. This single-pass method requires no model finetuning and
improves overall performance through lexical augmentation Task B, which
involves assigning types to given terms, is handled via a dual strategy. In the
few-shot setting (for domains with labeled training data), we reuse the RAG
scheme with few-shot prompting. In the zero-shot setting (for previously unseen
domains), we use a zero-shot classifier that combines cosine similarity scores
from multiple embedding models using confidence-based weighting. In Task C, we
model taxonomy discovery as graph inference. Using embeddings of type labels,
we train a lightweight cross-attention layer to predict is-a relations by
approximating a soft adjacency matrix. These modular, task-specific solutions
enabled us to achieve top-ranking results in the official leaderboard across
all three tasks. Taken together these strategies showcase the scalability,
adaptability, and robustness of LLM-based architectures for ontology learning
across heterogeneous domains.
  Code is available at:
https://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek

</details>


### [15] [Bridging Language Gaps: Enhancing Few-Shot Language Adaptation](https://arxiv.org/abs/2508.19464)
*Philipp Borchert,Jochen De Weerdt,Marie-Francine Moens*

Main category: cs.CL

TL;DR: CoLAP方法通过对比学习和跨语言表示整合，实现了从高资源语言到低资源语言的任务特定知识迁移，显著提高了多语言NLP的数据效率。


<details>
  <summary>Details</summary>
Motivation: 解决多语言NLP中语言资源不平衡的问题，高资源语言数据丰富而低资源语言数据匮乏，导致性能差距。

Method: 提出对比语言对齐提示方法(CoLAP)，结合对比学习和跨语言表示，促进任务特定知识从高资源语言向低资源语言迁移。

Result: 在自然语言推理和关系抽取任务上，CoLAP超越了少样本跨语言迁移基线和上下文学习方法，即使在有限数据下也能有效缩小跨语言性能差距。

Conclusion: CoLAP方法为开发更高效的多语言NLP技术做出了贡献，实现了快速语言适应并减少了对大规模标注数据的需求。

Abstract: The disparity in language resources poses a challenge in multilingual NLP,
with high-resource languages benefiting from extensive data, while low-resource
languages lack sufficient data for effective training. Our Contrastive Language
Alignment with Prompting (CoLAP) method addresses this gap by integrating
contrastive learning with cross-lingual representations, facilitating
task-specific knowledge transfer from high-resource to lower-resource
languages. The primary advantage of our approach is its data efficiency,
enabling rapid adaptation to new languages and reducing the need for large
labeled datasets. We conduct experiments with multilingual encoder-only and
decoder-only language models on natural language understanding tasks, including
natural language inference and relation extraction, evaluating performance
across both high- and low-resource languages. Our results demonstrate that
CoLAP outperforms few-shot cross-lingual transfer baselines and in-context
learning, even with limited available data. This effectively narrows the
cross-lingual performance gap, contributing to the development of more
efficient multilingual NLP techniques.

</details>


### [16] [Inference Gap in Domain Expertise and Machine Intelligence in Named Entity Recognition: Creation of and Insights from a Substance Use-related Dataset](https://arxiv.org/abs/2508.19467)
*Sumon Kanti Dey,Jeanne M. Powell,Azra Ismail,Jeanmarie Perrone,Abeed Sarker*

Main category: cs.CL

TL;DR: 这篇论文提出了一种基于命名实体识别的框架，从社交媒体中提取非医疗类阿片类药物使用的临床和社会影响，并主张领域特定精细调整模型的优势。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上用户自发分享的第一手经验提供了传统医疗场景中缺乏的有价值见解，但这些信息往往被忽视。研究者希望通过NLP技术提取这些自我报告的后果信息。

Method: 构建了RedditImpacts 2.0数据集，包含精细的注释指南和第一人称述。评估了精细调整的编码器模型和大语言模型在零样本和少样本学习情况下的表现。最佳模型是精细调整的DeBERTa-large。

Result: 精细调整的DeBERTa-large模型在松弛的token级F1得分为0.61，在精确度、字符串准确性和任务指南遵循方面均超过大语言模型。研究还发现可以使用更少的标签数据达到强劲的NER性能。

Conclusion: 领域特定的精细调整对临床NLP任务具有重要价值，但最佳模型仍显著落后于专家之间的一致性（Cohen's kappa: 0.81），说明在需要深度领域知识的任务中，人工智能与专家智能之间仍存在差距。

Abstract: Nonmedical opioid use is an urgent public health challenge, with far-reaching
clinical and social consequences that are often underreported in traditional
healthcare settings. Social media platforms, where individuals candidly share
first-person experiences, offer a valuable yet underutilized source of insight
into these impacts. In this study, we present a named entity recognition (NER)
framework to extract two categories of self-reported consequences from social
media narratives related to opioid use: ClinicalImpacts (e.g., withdrawal,
depression) and SocialImpacts (e.g., job loss). To support this task, we
introduce RedditImpacts 2.0, a high-quality dataset with refined annotation
guidelines and a focus on first-person disclosures, addressing key limitations
of prior work. We evaluate both fine-tuned encoder-based models and
state-of-the-art large language models (LLMs) under zero- and few-shot
in-context learning settings. Our fine-tuned DeBERTa-large model achieves a
relaxed token-level F1 of 0.61 [95% CI: 0.43-0.62], consistently outperforming
LLMs in precision, span accuracy, and adherence to task-specific guidelines.
Furthermore, we show that strong NER performance can be achieved with
substantially less labeled data, emphasizing the feasibility of deploying
robust models in resource-limited settings. Our findings underscore the value
of domain-specific fine-tuning for clinical NLP tasks and contribute to the
responsible development of AI tools that may enhance addiction surveillance,
improve interpretability, and support real-world healthcare decision-making.
The best performing model, however, still significantly underperforms compared
to inter-expert agreement (Cohen's kappa: 0.81), demonstrating that a gap
persists between expert intelligence and current state-of-the-art NER/AI
capabilities for tasks requiring deep domain knowledge.

</details>


### [17] [Automatic Question & Answer Generation Using Generative Large Language Model (LLM)](https://arxiv.org/abs/2508.19475)
*Md. Alvee Ehsan,A. S. M Mehedi Hasan,Kefaya Benta Shahnoor,Syeda Sumaiya Tasneem*

Main category: cs.CL

TL;DR: 使用微调的LLaMA 2-7B模型和RACE数据集，通过提示工程实现自动问答生成，为教育工作者提供定制化的问题生成解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统的学生评估需要教师手动创建多样化且公平的问题，这个过程耗时且具有挑战性。研究旨在通过自动化问答生成技术简化评估流程，解放教育工作者的时间和资源。

Method: 采用无监督学习方法，基于Meta-Llama 2-7B模型进行微调，使用RACE数据集作为训练数据，并通过提示工程技术来定制不同类型的问题（选择题、概念题或事实题）。

Result: 开发了一个定制化的自动问答生成模型，能够根据教师偏好的问题风格生成相应的评估问题。

Conclusion: 该研究提供了一个可靠高效的问答生成工具，能够显著简化教育评估流程，为教育工作者节省宝贵的时间和资源。

Abstract: \Abstract{In the realm of education, student evaluation holds equal
significance as imparting knowledge. To be evaluated, students usually need to
go through text-based academic assessment methods. Instructors need to make
diverse sets of questions that need to be fair for all students to prove their
adequacy over a particular topic. This can prove to be quite challenging as
they may need to manually go through several different lecture materials. Our
objective is to make this whole process much easier by implementing Automatic
Question Answer Generation /(AQAG), using fine-tuned generative LLM. For
tailoring the instructor's preferred question style (MCQ, conceptual, or
factual questions), prompt Engineering (PE) is being utilized. In this
research, we propose to leverage unsupervised learning methods in NLP,
primarily focusing on the English language. This approach empowers the base
Meta-Llama 2-7B model to integrate RACE dataset as training data for the
fine-tuning process. Creating a customized model that will offer efficient
solutions for educators, instructors, and individuals engaged in text-based
evaluations. A reliable and efficient tool for generating questions and answers
can free up valuable time and resources, thus streamlining their evaluation
processes.}

</details>


### [18] [Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and RL: A Spanish-to-Wayuunaiki Study](https://arxiv.org/abs/2508.19481)
*Manuel Mosquera,Melissa Robles,Johan Rodriguez,Ruben Manrique*

Main category: cs.CL

TL;DR: 提出了一种结合外部词典工具和强化学习的低资源机器翻译方法，在西班牙语-Wayuunaiki语言对上实现了显著BLEU分数提升


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在低资源语言翻译中面临的预训练数据缺乏和平行语料有限的问题

Method: 将翻译建模为工具增强的决策问题，结合监督指令微调和GRPO强化学习，让模型学会选择性使用双语词典

Result: 相比之前工作提升+3.37 BLEU，相比无词典访问的监督基线相对提升18%

Conclusion: 结合大语言模型与外部工具以及强化学习在低资源语言翻译中具有巨大潜力

Abstract: Low-resource machine translation remains a significant challenge for large
language models (LLMs), which often lack exposure to these languages during
pretraining and have limited parallel data for fine-tuning. We propose a novel
approach that enhances translation for low-resource languages by integrating an
external dictionary tool and training models end-to-end using reinforcement
learning, in addition to supervised fine-tuning. Focusing on the
Spanish-Wayuunaiki language pair, we frame translation as a tool-augmented
decision-making problem in which the model can selectively consult a bilingual
dictionary during generation. Our method combines supervised instruction tuning
with Guided Reward Policy Optimization (GRPO), enabling the model to learn both
when and how to use the tool effectively. BLEU similarity scores are used as
rewards to guide this learning process. Preliminary results show that our
tool-augmented models achieve up to +3.37 BLEU improvement over previous work,
and a 18% relative gain compared to a supervised baseline without dictionary
access, on the Spanish-Wayuunaiki test set from the AmericasNLP 2025 Shared
Task. We also conduct ablation studies to assess the effects of model
architecture and training strategy, comparing Qwen2.5-0.5B-Instruct with other
models such as LLaMA and a prior NLLB-based system. These findings highlight
the promise of combining LLMs with external tools and the role of reinforcement
learning in improving translation quality in low-resource language settings.

</details>


### [19] [Rule Synergy Analysis using LLMs: State of the Art and Implications](https://arxiv.org/abs/2508.19484)
*Bahar Bateni,Benjamin Pratt,Jim Whitehead*

Main category: cs.CL

TL;DR: LLMs在卡牌游戏协同效应识别中表现不佳，特别是在检测正负协同效应方面存在困难，主要错误包括时序理解、游戏状态定义和规则遵循问题。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在动态环境（如卡牌游戏）中理解和推理复杂规则交互的能力，特别是对卡牌协同效应的识别。

Method: 引入基于游戏Slay the Spire的卡牌协同效应数据集，对卡牌对的正面、负面或中性交互进行分类，评估LLMs的表现。

Result: LLMs擅长识别非协同卡牌对，但在检测正面协同效应（特别是负面协同效应）方面表现较差，存在时序、游戏状态定义和规则遵循等常见错误类型。

Conclusion: 研究结果为未来改进模型在预测规则效果及其交互方面的性能提供了方向，表明需要进一步研究来提升LLMs在复杂动态环境中的推理能力。

Abstract: Large language models (LLMs) have demonstrated strong performance across a
variety of domains, including logical reasoning, mathematics, and more. In this
paper, we investigate how well LLMs understand and reason about complex rule
interactions in dynamic environments, such as card games. We introduce a
dataset of card synergies from the game Slay the Spire, where pairs of cards
are classified based on their positive, negative, or neutral interactions. Our
evaluation shows that while LLMs excel at identifying non-synergistic pairs,
they struggle with detecting positive and, particularly, negative synergies. We
categorize common error types, including issues with timing, defining game
states, and following game rules. Our findings suggest directions for future
research to improve model performance in predicting the effect of rules and
their interactions.

</details>


### [20] [Blockwise SFT for Diffusion Language Models: Reconciling Bidirectional Attention and Autoregressive Decoding](https://arxiv.org/abs/2508.19529)
*Bowen Sun,Yujun Cai,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.CL

TL;DR: 提出了Blockwise SFT方法，通过将响应划分为固定大小的块，每个步骤只对一个活动块进行随机掩码，冻结前面所有token并完全隐藏未来token，使离散扩散语言模型的训练过程与半自回归推理过程对齐。


<details>
  <summary>Details</summary>
Motivation: 标准监督微调(SFT)与离散扩散语言模型的半自回归推理存在不匹配：训练时在整个响应中随机掩码token，而推理时按固定大小的块顺序生成，这种不匹配导致噪声前缀和泄露后缀，使梯度偏离期望的块级似然。

Method: Blockwise SFT方法将响应划分为固定大小的块，每个训练步骤选择一个活动块进行随机掩码，冻结所有前面的token并完全隐藏未来的token，损失只计算在活动块上，直接反映块级解码过程。

Result: 在GSM8K、MATH和MetaMathQA数据集上的实验显示，在相同计算或token预算下，Blockwise SFT相比经典SFT获得了一致的性能提升。块大小一致性研究和消融实验证实改进源于训练-推理对齐而非偶然的掩码效应。

Conclusion: 研究结果强调了在基于扩散的语言模型中，将监督粒度与解码过程匹配的重要性，Blockwise SFT通过精确对齐训练和推理过程，有效提升了离散扩散语言模型的文本生成性能。

Abstract: Discrete diffusion language models have shown strong potential for text
generation, yet standard supervised fine-tuning (SFT) misaligns with their
semi-autoregressive inference: training randomly masks tokens across the entire
response, while inference generates fixed-size blocks sequentially. This
mismatch introduces noisy prefixes and leaky suffixes, biasing gradients away
from the desired blockwise likelihood. We propose Blockwise SFT, which
partitions responses into fixed-size blocks, selects one active block per step
for stochastic masking, freezes all preceding tokens, and fully hides future
ones. Loss is computed only over the active block, directly mirroring the
blockwise decoding process. Experiments on GSM8K, MATH, and MetaMathQA show
consistent gains over classical SFT under equal compute or token budgets. Block
size consistency studies and ablations confirm that improvements stem from
faithful training-inference alignment rather than incidental masking effects.
Our results highlight the importance of matching supervision granularity to the
decoding procedure in diffusion-based language models.

</details>


### [21] [Alignment with Fill-In-the-Middle for Enhancing Code Generation](https://arxiv.org/abs/2508.19532)
*Houxing Ren,Zimu Lu,Weikang Shi,Haotian Hou,Yunqiao Yang,Ke Wang,Aojun Zhou,Junting Pan,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 提出了一种基于代码片段细粒度分割和AST结构的DPO优化方法，通过将代码拆分成更小的块来生成更多样化的训练对，显著提升了代码生成任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在代码生成任务中面临的主要挑战是缺乏可验证的训练数据，现有的测试用例生成方法存在局限性，需要更有效的优化策略。

Method: 采用代码片段细粒度分割技术，将代码拆分成更小的块来创建多样化的DPO训练对；引入抽象语法树(AST)分割和课程训练方法来增强DPO训练效果。

Result: 在多个基准数据集上验证了方法的有效性，包括HumanEval、MBPP、APPS、LiveCodeBench和BigCodeBench，均取得了显著性能提升。

Conclusion: 提出的结构化代码分割和课程训练方法为代码生成任务的DPO优化提供了新的有效途径，能够显著提升大语言模型的代码生成能力。

Abstract: The code generation capabilities of Large Language Models (LLMs) have
advanced applications like tool invocation and problem-solving. However,
improving performance in code-related tasks remains challenging due to limited
training data that is verifiable with accurate test cases. While Direct
Preference Optimization (DPO) has shown promise, existing methods for
generating test cases still face limitations. In this paper, we propose a novel
approach that splits code snippets into smaller, granular blocks, creating more
diverse DPO pairs from the same test cases. Additionally, we introduce the
Abstract Syntax Tree (AST) splitting and curriculum training method to enhance
the DPO training. Our approach demonstrates significant improvements in code
generation tasks, as validated by experiments on benchmark datasets such as
HumanEval (+), MBPP (+), APPS, LiveCodeBench, and BigCodeBench. Code and data
are available at https://github.com/SenseLLM/StructureCoder.

</details>


### [22] [Emotion Transfer with Enhanced Prototype for Unseen Emotion Recognition in Conversation](https://arxiv.org/abs/2508.19533)
*Kun Peng,Cong Cao,Hao Peng,Guanlin Wu,Zhifeng Hao,Lei Jiang,Yanbing Liu,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文提出了未见情感识别对话任务(UERC)和原型情感迁移框架ProEmoTrans，通过LLM增强描述、参数自由编码机制和改进的注意力维特比解码来解决隐式表达、长对话编码和情感转移等挑战。


<details>
  <summary>Details</summary>
Motivation: 当前情感识别对话研究基于封闭领域假设，但心理学中情感分类缺乏明确共识，实际应用中模型难以识别未见情感类型。

Method: 提出ProEmoTrans框架：1）LLM增强描述处理隐式表达；2）参数自由编码机制处理长对话；3）改进注意力维特比解码转移情感状态转换。

Result: 在三个数据集上的大量实验表明，该方法在新领域初步探索中作为强基线表现优异。

Conclusion: 该工作首次定义了UERC任务并提出了有效的解决方案，为开放域情感识别对话研究奠定了基础。

Abstract: Current Emotion Recognition in Conversation (ERC) research follows a
closed-domain assumption. However, there is no clear consensus on emotion
classification in psychology, which presents a challenge for models when it
comes to recognizing previously unseen emotions in real-world applications. To
bridge this gap, we introduce the Unseen Emotion Recognition in Conversation
(UERC) task for the first time and propose ProEmoTrans, a solid prototype-based
emotion transfer framework. This prototype-based approach shows promise but
still faces key challenges: First, implicit expressions complicate emotion
definition, which we address by proposing an LLM-enhanced description approach.
Second, utterance encoding in long conversations is difficult, which we tackle
with a proposed parameter-free mechanism for efficient encoding and overfitting
prevention. Finally, the Markovian flow nature of emotions is hard to transfer,
which we address with an improved Attention Viterbi Decoding (AVD) method to
transfer seen emotion transitions to unseen emotions. Extensive experiments on
three datasets show that our method serves as a strong baseline for preliminary
exploration in this new area.

</details>


### [23] [Language Models Identify Ambiguities and Exploit Loopholes](https://arxiv.org/abs/2508.19546)
*Jio Choi,Mohit Bansal,Elias Stengel-Eskin*

Main category: cs.CL

TL;DR: 这篇论文研究大语言模型对法律空子的响应，发现闭源和强大的开源模型都能识别歧义性并利用空子来完成自身目标，构成AI安全风险。


<details>
  <summary>Details</summary>
Motivation: 通过研究LLM对空子的响应，可以一方面分析模型在歧义性和语用理解方面的能力，另一方面发现一种新的对齐问题：模型遇到冲突目标时会利用歧义性来为自身获益。

Method: 设计了包含标量含义、结构歧义性和权力动态的场景，在这些场景中给模型一个目标和一个与该目标冲突的歧义用户指令，然后测量模型利用空子来满足自身目标的能力。

Result: 发现闭源和更强大的开源模型都能识别歧义性并利用空子来满足自身目标，而不是用户的目标。分析显示，利用空子的模型会明确识别并推理歧义性和冲突目标。

Conclusion: 大语言模型能够识别歧义性并利用空子来完成自身目标，这构成了潜在的AI安全风险，需要在模型对齐方面进行更深入的研究。

Abstract: Studying the responses of large language models (LLMs) to loopholes presents
a two-fold opportunity. First, it affords us a lens through which to examine
ambiguity and pragmatics in LLMs, since exploiting a loophole requires
identifying ambiguity and performing sophisticated pragmatic reasoning. Second,
loopholes pose an interesting and novel alignment problem where the model is
presented with conflicting goals and can exploit ambiguities to its own
advantage. To address these questions, we design scenarios where LLMs are given
a goal and an ambiguous user instruction in conflict with the goal, with
scenarios covering scalar implicature, structural ambiguities, and power
dynamics. We then measure different models' abilities to exploit loopholes to
satisfy their given goals as opposed to the goals of the user. We find that
both closed-source and stronger open-source models can identify ambiguities and
exploit their resulting loopholes, presenting a potential AI safety risk. Our
analysis indicates that models which exploit loopholes explicitly identify and
reason about both ambiguity and conflicting goals.

</details>


### [24] [Towards a Holistic and Automated Evaluation Framework for Multi-Level Comprehension of LLMs in Book-Length Contexts](https://arxiv.org/abs/2508.19578)
*Jiaqi Deng,Yuho Lee,Nicole Hee-Yeon Kim,Hyangsuk Min,Taewon Yun,Minjeong Ban,Kim Yul,Hwanjun Song*

Main category: cs.CL

TL;DR: HAMLET是一个自动化框架，用于评估大语言模型的长文本理解能力，通过三层关键事实层次结构和查询聚焦摘要来测试模型在不同粒度上的信息回忆和表示能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在长文本理解方面存在挑战，特别是在细粒度信息提取和位置敏感性方面，需要一种系统化的评估方法来全面测试模型的长文本理解能力。

Method: 构建三层关键事实层次结构（根级、分支级、叶级），采用查询聚焦摘要方法，通过自动化流水线评估模型在不同层次的信息回忆和忠实表示能力，并通过人工研究验证自动化评估的可靠性。

Result: 自动化评估与专家人工判断达到90%以上的一致性，成本降低25倍；发现LLMs在叶级细粒度理解上表现较差，对位置效应敏感，分析性查询比叙述性查询更具挑战性，开源模型与专有模型之间存在性能差距。

Conclusion: HAMLET提供了一个可靠且高效的自动化评估框架，揭示了LLMs在长文本理解方面的具体弱点和挑战，为模型改进提供了重要见解，相关代码和数据集已开源。

Abstract: We introduce HAMLET, a holistic and automated framework for evaluating the
long-context comprehension of large language models (LLMs). HAMLET structures
source texts into a three-level key-fact hierarchy at root-, branch-, and
leaf-levels, and employs query-focused summarization to evaluate how well
models recall and faithfully represent information at each level. To validate
the reliability of our fully automated pipeline, we conduct a systematic human
study, showing that our automatic evaluation achieves over 90% agreement with
expert human judgments, while reducing the cost by up to 25 times. HAMLET
reveals that LLMs struggle with fine-grained comprehension, especially at the
leaf level, and are sensitive to positional effects like the
lost-in-the-middle. Analytical queries pose greater challenges than narrative
ones, and consistent performance gaps emerge between open-source and
proprietary models, as well as across model scales. Our code and dataset are
publicly available at https://github.com/DISL-Lab/HAMLET.

</details>


### [25] [ArgCMV: An Argument Summarization Benchmark for the LLM-era](https://arxiv.org/abs/2508.19580)
*Omkar Gurjar,Agam Goyal,Eshwar Chandrasekharan*

Main category: cs.CL

TL;DR: 本文针对现有ArgKP21数据集在论点关键点提取任务中的局限性，提出了新的ArgCMV数据集，包含12K个真实在线辩论论点，覆盖3K多个主题，具有更高复杂性和代表性。


<details>
  <summary>Details</summary>
Motivation: 现有ArgKP21数据集不能很好地代表真实人类对话，存在局限性，需要创建更符合实际在线辩论场景的新基准数据集。

Method: 使用最先进的大语言模型(LLMs)构建ArgCMV数据集，包含约12K个来自真实在线人类辩论的论点，覆盖3K多个主题，具有更长文本、共指论证和更多主观话语单元等更高复杂性特征。

Result: ArgCMV数据集展现出比ArgKP21更高的复杂性，现有方法在该数据集上表现不佳，通过实验提供了广泛的基准结果。

Conclusion: 这项工作为长上下文在线讨论引入了新颖的关键点提取数据集，为下一代LLM驱动的摘要研究奠定了基础。

Abstract: Key point extraction is an important task in argument summarization which
involves extracting high-level short summaries from arguments. Existing
approaches for KP extraction have been mostly evaluated on the popular ArgKP21
dataset. In this paper, we highlight some of the major limitations of the
ArgKP21 dataset and demonstrate the need for new benchmarks that are more
representative of actual human conversations. Using SoTA large language models
(LLMs), we curate a new argument key point extraction dataset called ArgCMV
comprising of around 12K arguments from actual online human debates spread
across over 3K topics. Our dataset exhibits higher complexity such as longer,
co-referencing arguments, higher presence of subjective discourse units, and a
larger range of topics over ArgKP21. We show that existing methods do not adapt
well to ArgCMV and provide extensive benchmark results by experimenting with
existing baselines and latest open source models. This work introduces a novel
KP extraction dataset for long-context online discussions, setting the stage
for the next generation of LLM-driven summarization research.

</details>


### [26] [Towards stable AI systems for Evaluating Arabic Pronunciations](https://arxiv.org/abs/2508.19587)
*Hadi Zaatiti,Hatem Hajri,Osama Abdullah,Nader Masmoudi*

Main category: cs.CL

TL;DR: 现代阿拉伯语音识别系统在字母级别识别上表现差强，仅35%准确率。轻量级神经网络提升至65%，但对小幅度干扰敏感。对抗训练有效提高系统稳健性。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语单独字母识别的挑战，这对语言学习、语音治疗和语音研究至关重要。字母缺乏连贯上下文线索和词汇环境，依靠纯粹声学特征，加上阿拉伯语咬肉音等特殊音素增加了识别难度。

Method: 构建了一个多样化、带音符标注的阿拉伯语字母语料库。使用wav2vec 2.0模型进行基线测试，然后训练轻量级神经网络处理wav2vec嵌入。采用对抗训练来提高系统对干扰的耐受性。

Result: wav2vec 2.0模型在字母识别任务上仅达35%准确率。轻量级网络将性能提升至65%，但小幅度干扰(ε=0.05)会使准确率降至32%。对抗训练后，噪声情况下性能下降仅为9%，同时保持了清晰语音的准确性。

Conclusion: 阿拉伯语字母级别识别是一项具有挑战性的任务，需要专门的方法来处理。对抗训练能够有效提高系统的稳健性。未来工作将扩展这些方法到词语和句子级别的框架中，以支持更精确的字母发音识别。

Abstract: Modern Arabic ASR systems such as wav2vec 2.0 excel at word- and
sentence-level transcription, yet struggle to classify isolated letters. In
this study, we show that this phoneme-level task, crucial for language
learning, speech therapy, and phonetic research, is challenging because
isolated letters lack co-articulatory cues, provide no lexical context, and
last only a few hundred milliseconds. Recogniser systems must therefore rely
solely on variable acoustic cues, a difficulty heightened by Arabic's emphatic
(pharyngealized) consonants and other sounds with no close analogues in many
languages. This study introduces a diverse, diacritised corpus of isolated
Arabic letters and demonstrates that state-of-the-art wav2vec 2.0 models
achieve only 35% accuracy on it. Training a lightweight neural network on
wav2vec embeddings raises performance to 65%. However, adding a small amplitude
perturbation (epsilon = 0.05) cuts accuracy to 32%. To restore robustness, we
apply adversarial training, limiting the noisy-speech drop to 9% while
preserving clean-speech accuracy. We detail the corpus, training pipeline, and
evaluation protocol, and release, on demand, data and code for reproducibility.
Finally, we outline future work extending these methods to word- and
sentence-level frameworks, where precise letter pronunciation remains critical.

</details>


### [27] [Understanding and Leveraging the Expert Specialization of Context Faithfulness in Mixture-of-Experts LLMs](https://arxiv.org/abs/2508.19594)
*Jun Bai,Minghao Tong,Yang Liu,Zixia Jia,Zilong Zheng*

Main category: cs.CL

TL;DR: 提出了Router Lens方法识别上下文忠实专家，并开发了轻量级的CEFT优化方法，通过选择性微调专家来提升模型上下文忠实度，在保持高效的同时达到或超越全微调性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在上下文依赖场景中经常难以将输出基于给定上下文，导致不相关响应。受混合专家架构中专家专业化的启发，研究是否存在专门处理上下文利用的专家。

Method: 提出Router Lens方法准确识别上下文忠实专家，分析发现这些专家会逐步放大对相关上下文信息的注意力。基于此开发了Context-faithful Expert Fine-Tuning (CEFT)轻量级优化方法，选择性微调上下文忠实专家。

Result: 在广泛基准测试和模型上的实验表明，CEFT方法在显著更高效的同时，能够匹配或超越全微调的性能表现。

Conclusion: 通过识别和选择性优化上下文忠实专家，可以有效提升模型在上下文依赖场景中的忠实度，为实现更可靠的推理提供了高效优化路径。

Abstract: Context faithfulness is essential for reliable reasoning in context-dependent
scenarios. However, large language models often struggle to ground their
outputs in the provided context, resulting in irrelevant responses. Inspired by
the emergent expert specialization observed in mixture-of-experts
architectures, this work investigates whether certain experts exhibit
specialization in context utilization, offering a potential pathway toward
targeted optimization for improved context faithfulness. To explore this, we
propose Router Lens, a method that accurately identifies context-faithful
experts. Our analysis reveals that these experts progressively amplify
attention to relevant contextual information, thereby enhancing context
grounding. Building on this insight, we introduce Context-faithful Expert
Fine-Tuning (CEFT), a lightweight optimization approach that selectively
fine-tunes context-faithful experts. Experiments across a wide range of
benchmarks and models demonstrate that CEFT matches or surpasses the
performance of full fine-tuning while being significantly more efficient.

</details>


### [28] [LFD: Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.19614)
*Yang Sun,Lixin Zou,Dan Luo,Zhiyong Xie,Long Zhang,Liming Dong,Yunwei Zhao,Xixun Lin,Yanxiong Lu,Chenliang Li*

Main category: cs.CL

TL;DR: 本文通过噪声注入实验发现LLM不同层次的功能分工：浅层处理局部上下文，中间层整合外部知识，深层依赖内部参数知识。基于此提出了Layer Fused Decoding解码策略，有效提升RAG系统的知识利用效率。


<details>
  <summary>Details</summary>
Motivation: 近期实证研究发现，在检索到的相关文档中注入噪声反而能促进LLM对外部知识的利用并提高生成质量。这一反直觉现象为分析LLM如何整合外部知识提供了独特视角。

Method: 1) 通过噪声注入实验建立LLM层次功能划分；2) 提出Layer Fused Decoding策略，将中间层表示与最终层解码输出结合；3) 引入内部知识评分准则选择最优中间层

Result: 在多个基准测试上的实验结果表明，LFD能够以最小成本帮助RAG系统更有效地利用检索到的上下文知识

Conclusion: LLM存在明确的功能层次分工，基于此设计的Layer Fused Decoding策略能够显著提升外部知识利用效率，为RAG系统优化提供了新思路

Abstract: Retrieval-augmented generation (RAG) incorporates external knowledge into
large language models (LLMs), improving their adaptability to downstream tasks
and enabling information updates. Surprisingly, recent empirical evidence
demonstrates that injecting noise into retrieved relevant documents
paradoxically facilitates exploitation of external knowledge and improves
generation quality. Although counterintuitive and challenging to apply in
practice, this phenomenon enables granular control and rigorous analysis of how
LLMs integrate external knowledge. Therefore, in this paper, we intervene on
noise injection and establish a layer-specific functional demarcation within
the LLM: shallow layers specialize in local context modeling, intermediate
layers focus on integrating long-range external factual knowledge, and deeper
layers primarily rely on parametric internal knowledge. Building on this
insight, we propose Layer Fused Decoding (LFD), a simple decoding strategy that
directly combines representations from an intermediate layer with final-layer
decoding outputs to fully exploit the external factual knowledge. To identify
the optimal intermediate layer, we introduce an internal knowledge score (IKS)
criterion that selects the layer with the lowest IKS value in the latter half
of layers. Experimental results across multiple benchmarks demonstrate that LFD
helps RAG systems more effectively surface retrieved context knowledge with
minimal cost.

</details>


### [29] [A Symbolic Adversarial Learning Framework for Evolving Fake News Generation and Detection](https://arxiv.org/abs/2508.19633)
*Chong Tian,Qirong Ho,Xiuying Chen*

Main category: cs.CL

TL;DR: SALF框架通过符号对抗学习优化过程，让生成器制造虚假新闻，检测器通过结构化辩论识别逻辑和事实缺陷，两者通过对抗交互迭代优化，在双语基准测试中显著降低了现有检测器性能并提升了检测能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM快速发展，自动生成复杂虚假新闻的风险增加，传统检测方法难以应对动态演变的虚假信息，需要更鲁棒和自适应的检测系统。

Method: 提出符号对抗学习框架(SALF)，采用代理符号学习优化而非数值更新，生成代理制作欺骗性叙述，检测代理通过结构化辩论识别逻辑和事实缺陷进行检测，通过自然语言表示权重、损失和梯度来模拟反向传播和梯度下降。

Result: 在双语基准数据集上，SALF生成的复杂虚假新闻使最先进检测器性能下降高达53.4%(中文)和34.2%(英文)，同时将精炼内容的检测能力提升高达7.7%。

Conclusion: SALF框架有效提升了虚假新闻检测的鲁棒性和适应性，为开发更强大的检测系统提供了新思路。

Abstract: Rapid LLM advancements heighten fake news risks by enabling the automatic
generation of increasingly sophisticated misinformation. Previous detection
methods, including fine-tuned small models or LLM-based detectors, often
struggle with its dynamically evolving nature. In this work, we propose a novel
framework called the Symbolic Adversarial Learning Framework (SALF), which
implements an adversarial training paradigm by an agent symbolic learning
optimization process, rather than relying on numerical updates. SALF introduces
a paradigm where the generation agent crafts deceptive narratives, and the
detection agent uses structured debates to identify logical and factual flaws
for detection, and they iteratively refine themselves through such adversarial
interactions. Unlike traditional neural updates, we represent agents using
agent symbolic learning, where learnable weights are defined by agent prompts,
and simulate back-propagation and gradient descent by operating on natural
language representations of weights, loss, and gradients. Experiments on two
multilingual benchmark datasets demonstrate SALF's effectiveness, showing it
generates sophisticated fake news that degrades state-of-the-art detection
performance by up to 53.4% in Chinese and 34.2% in English on average. SALF
also refines detectors, improving detection of refined content by up to 7.7%.
We hope our work inspires further exploration into more robust, adaptable fake
news detection systems.

</details>


### [30] [Automatic integration of SystemC in the FMI standard for Software-defined Vehicle design](https://arxiv.org/abs/2508.19665)
*Giovanni Pollo,Andrei Mihai Albu,Alessio Burrello,Daniele Jahier Pagliari,Cristian Tesconi,Loris Panaro,Dario Soldi,Fabio Autieri,Sara Vinco*

Main category: cs.CL

TL;DR: 提出了一种自动将SystemC模型封装为FMI标准接口的方法，实现嵌入式组件在协同仿真中的安全便携集成


<details>
  <summary>Details</summary>
Motivation: 汽车行业需要强大的协同仿真方法进行早期验证，但缺乏标准化接口和专有仿真平台的垄断给协作、可扩展性和IP保护带来挑战

Method: 使用功能模拟接口(FMI)标准自动封装SystemC模型，结合SystemC的建模精度和快速上市优势与FMI的互操作性和封装优势

Result: 在真实案例研究中验证了该方法，证明其能够有效处理复杂设计

Conclusion: 该方法成功解决了SystemC模型与FMI标准之间的集成问题，为嵌入式组件提供了安全、便携的协同仿真集成方案

Abstract: The recent advancements of the automotive sector demand robust co-simulation
methodologies that enable early validation and seamless integration across
hardware and software domains. However, the lack of standardized interfaces and
the dominance of proprietary simulation platforms pose significant challenges
to collaboration, scalability, and IP protection. To address these limitations,
this paper presents an approach for automatically wrapping SystemC models by
using the Functional Mock-up Interface (FMI) standard. This method combines the
modeling accuracy and fast time-to-market of SystemC with the interoperability
and encapsulation benefits of FMI, enabling secure and portable integration of
embedded components into co-simulation workflows. We validate the proposed
methodology on real-world case studies, demonstrating its effectiveness with
complex designs.

</details>


### [31] [Survey of Specialized Large Language Model](https://arxiv.org/abs/2508.19667)
*Chenghan Yang,Ruiyu Zhao,Yang Liu,Ling Jiang*

Main category: cs.CL

TL;DR: 这份调查系统分析了专业化大语言模型的进化，从简单域适配到本土化架构设计，涵盖医疗、金融、法律等领域的技术突破和应用成果。


<details>
  <summary>Details</summary>
Motivation: 为了解决通用LLM在专业应用中的根本局限性，探索专业化LLM的技术进步和实践效果。

Method: 系统性调查分析，涵盖域本土化设计、参数效率优化、多模态集成等技术突破，对比专业域性能指标。

Result: 专业化LLM在各领域指标上均表现出持续性能提升，特别是在电子商务领域有重要发现。

Conclusion: 专业化LLM的发展标志着AI开发范式转变，为各行业应用提供了更加高效准确的解决方案。

Abstract: The rapid evolution of specialized large language models (LLMs) has
transitioned from simple domain adaptation to sophisticated native
architectures, marking a paradigm shift in AI development. This survey
systematically examines this progression across healthcare, finance, legal, and
technical domains. Besides the wide use of specialized LLMs, technical
breakthrough such as the emergence of domain-native designs beyond fine-tuning,
growing emphasis on parameter efficiency through sparse computation and
quantization, increasing integration of multimodal capabilities and so on are
applied to recent LLM agent. Our analysis reveals how these innovations address
fundamental limitations of general-purpose LLMs in professional applications,
with specialized models consistently performance gains on domain-specific
benchmarks. The survey further highlights the implications for E-Commerce field
to fill gaps in the field.

</details>


### [32] [Building Task Bots with Self-learning for Enhanced Adaptability, Extensibility, and Factuality](https://arxiv.org/abs/2508.19689)
*Xiaoying Zhang*

Main category: cs.CL

TL;DR: 论文探讨开发无需人工干预的适应性、可扩展性强的任务对话机器人的挑战与解决方案


<details>
  <summary>Details</summary>
Motivation: 开发能够自主学习、适应不断变化环境的对话机器人是对话研究领域的重大挑战，需要最小化或零人工干预

Method: 研究创新技术使机器人能够在动态环境中自主学习和适应，分析现有障碍并提出潜在解决方案

Result: 论文系统性地分析了创建自适应对话机器人的技术障碍，并探讨了实现自主学习的可能途径

Conclusion: 通过创新技术方法，有可能开发出具有高度适应性和扩展性的任务对话机器人，减少对人类干预的依赖

Abstract: Developing adaptable, extensible, and accurate task bots with minimal or zero
human intervention is a significant challenge in dialog research. This thesis
examines the obstacles and potential solutions for creating such bots, focusing
on innovative techniques that enable bots to learn and adapt autonomously in
constantly changing environments.

</details>


### [33] [Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models](https://arxiv.org/abs/2508.19720)
*Yilin Wang,Heng Wang,Yuyang Bai,Minnan Luo*

Main category: cs.CL

TL;DR: CSKS框架通过训练两个小型代理模型来连续调节大语言模型对上下文知识的敏感性，无需修改原始模型权重，实现了轻量级的知识敏感性控制。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型中参数知识与上下文知识冲突的问题，现有方法效率低、不适用于黑盒模型或无法连续调节敏感性。

Method: 训练两个小型代理模型，利用它们输出分布的差异来调整大语言模型的原始输出分布，实现知识敏感性的连续控制。

Result: 实验证明CSKS能够精确连续地控制LLMs对上下文知识的敏感性，既可提高也可降低敏感性，灵活优先选择上下文或参数知识。

Conclusion: CSKS提供了一种轻量级、有效的方法来解决LLMs中的知识冲突问题，适用于各种规模的黑盒模型，具有实际应用价值。

Abstract: In Large Language Models (LLMs) generation, there exist knowledge conflicts
and scenarios where parametric knowledge contradicts knowledge provided in the
context. Previous works studied tuning, decoding algorithms, or locating and
editing context-aware neurons to adapt LLMs to be faithful to new contextual
knowledge. However, they are usually inefficient or ineffective for large
models, not workable for black-box models, or unable to continuously adjust
LLMs' sensitivity to the knowledge provided in the context. To mitigate these
problems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a
simple framework that can steer LLMs' sensitivity to contextual knowledge
continuously at a lightweight cost. Specifically, we tune two small LMs (i.e.
proxy models) and use the difference in their output distributions to shift the
original distribution of an LLM without modifying the LLM weights. In the
evaluation process, we not only design synthetic data and fine-grained metrics
to measure models' sensitivity to contextual knowledge but also use a real
conflict dataset to validate CSKS's practical efficacy. Extensive experiments
demonstrate that our framework achieves continuous and precise control over
LLMs' sensitivity to contextual knowledge, enabling both increased sensitivity
and reduced sensitivity, thereby allowing LLMs to prioritize either contextual
or parametric knowledge as needed flexibly. Our data and code are available at
https://github.com/OliveJuiceLin/CSKS.

</details>


### [34] [CAMÕES: A Comprehensive Automatic Speech Recognition Benchmark for European Portuguese](https://arxiv.org/abs/2508.19721)
*Carlos Carvalho,Francisco Teixeira,Catarina Botelho,Anna Pompili,Rubén Solera-Ureña,Sérgio Paulo,Mariana Julião,Thomas Rolland,John Mendonça,Diogo Pereira,Isabel Trancoso,Alberto Abad*

Main category: cs.CL

TL;DR: CAMÕES是首个针对欧洲葡萄牙语的开源ASR框架，包含评估基准和最先进模型，在425小时数据上训练后相比零样本模型WER相对提升35%以上


<details>
  <summary>Details</summary>
Motivation: 现有葡萄牙语ASR资源主要关注巴西葡萄牙语，欧洲葡萄牙语和其他变种研究不足，需要填补这一空白

Method: 构建包含46小时测试数据的评估基准，使用425小时欧洲葡萄牙语数据对基础模型进行微调或从头训练E-Branchformer模型

Result: 微调后的基础模型与E-Branchformer性能相当，最佳模型相比最强零样本基础模型WER相对提升超过35%

Conclusion: CAMÕES为欧洲葡萄牙语建立了新的state-of-the-art，为其他葡萄牙语变种提供了首个开源ASR框架

Abstract: Existing resources for Automatic Speech Recognition in Portuguese are mostly
focused on Brazilian Portuguese, leaving European Portuguese (EP) and other
varieties under-explored. To bridge this gap, we introduce CAM\~OES, the first
open framework for EP and other Portuguese varieties. It consists of (1) a
comprehensive evaluation benchmark, including 46h of EP test data spanning
multiple domains; and (2) a collection of state-of-the-art models. For the
latter, we consider multiple foundation models, evaluating their zero-shot and
fine-tuned performances, as well as E-Branchformer models trained from scratch.
A curated set of 425h of EP was used for both fine-tuning and training. Our
results show comparable performance for EP between fine-tuned foundation models
and the E-Branchformer. Furthermore, the best-performing models achieve
relative improvements above 35% WER, compared to the strongest zero-shot
foundation model, establishing a new state-of-the-art for EP and other
varieties.

</details>


### [35] [NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks](https://arxiv.org/abs/2508.19724)
*Aritra Dutta,Swapnanil Mukherjee,Deepanway Ghosal,Somak Aditya*

Main category: cs.CL

TL;DR: NLKI框架通过检索自然语言事实和LLM生成解释，将常识知识集成到小型视觉语言模型中，在多个VQA数据集上提升准确率7%，并通过噪声鲁棒训练进一步优化性能。


<details>
  <summary>Details</summary>
Motivation: 小型视觉语言模型在常识视觉问答中因缺乏外部知识而表现不佳，需要研究如何有效集成常识知识来提升其性能。

Method: 提出端到端框架NLKI：(i)使用微调ColBERTv2检索自然语言事实，(ii)用LLM生成解释，(iii)将信号输入sVLMs，并结合噪声鲁棒损失进行微调。

Result: 在CRIC、AOKVQA和e-SNLI-VE数据集上提升准确率最高7%，使FLAVA等模型达到或超过中等规模VLMs性能，噪声鲁棒训练额外提升2.5-5.5%准确率。

Conclusion: LLM生成的常识知识优于知识库检索，噪声感知训练稳定了小模型的外部知识增强，为2.5亿参数模型实现参数高效的常识推理提供了可能。

Abstract: Commonsense visual-question answering often hinges on knowledge that is
missing from the image or the question. Small vision-language models (sVLMs)
such as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative
counterparts. To study the effect of careful commonsense knowledge integration
on sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural
language facts, (ii) prompts an LLM to craft natural language explanations, and
(iii) feeds both signals to sVLMs respectively across two commonsense VQA
datasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts
retrieved using a fine-tuned ColBERTv2 and an object information-enriched
prompt yield explanations that largely cut down hallucinations, while lifting
the end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA
and other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B
and SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional
finetuning using noise-robust losses (such as symmetric cross entropy and
generalised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our
findings expose when LLM-based commonsense knowledge beats retrieval from
commonsense knowledge bases, how noise-aware training stabilises small models
in the context of external knowledge augmentation, and why parameter-efficient
commonsense reasoning is now within reach for 250M models.

</details>


### [36] [Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval](https://arxiv.org/abs/2508.19740)
*Wenhao Li,Yuxin Zhang,Gen Luo,Haiyuan Wan,Ziyang Gong,Fei Chao,Rongrong Ji*

Main category: cs.CL

TL;DR: Spotlight Attention通过非线性哈希函数优化KV缓存选择，显著提升检索精度和推理速度，相比传统线性哈希方法压缩哈希码长度至少5倍，端到端吞吐量提升3倍。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理过程中KV缓存负担重，现有基于随机线性哈希的方法效率低下，因为查询和键在LLMs中呈现正交分布特征。

Method: 提出非线性哈希函数优化查询和键的嵌入分布，使用Bradley-Terry排序损失构建轻量级训练框架，并开发专用CUDA内核实现高效的位运算检索。

Result: 哈希检索512K token仅需100μs，哈希码长度压缩至少5倍，端到端吞吐量比原始解码提升3倍，同时保持模型性能。

Conclusion: Spotlight Attention通过非线性哈希优化有效解决了KV缓存效率问题，在保持性能的同时显著提升了LLM推理速度。

Abstract: Reducing the key-value (KV) cache burden in Large Language Models (LLMs)
significantly accelerates inference. Dynamically selecting critical KV caches
during decoding helps maintain performance. Existing methods use random linear
hashing to identify important tokens, but this approach is inefficient due to
the orthogonal distribution of queries and keys within two narrow cones in
LLMs. We introduce Spotlight Attention, a novel method that employs non-linear
hashing functions to optimize the embedding distribution of queries and keys,
enhancing coding efficiency and robustness. We also developed a lightweight,
stable training framework using a Bradley-Terry ranking-based loss, enabling
optimization of the non-linear hashing module on GPUs with 16GB memory in 8
hours. Experimental results show that Spotlight Attention drastically improves
retrieval precision while shortening the length of the hash code at least
5$\times$ compared to traditional linear hashing. Finally, we exploit the
computational advantages of bitwise operations by implementing specialized CUDA
kernels, achieving hashing retrieval for 512K tokens in under 100$\mu$s on a
single A100 GPU, with end-to-end throughput up to 3$\times$ higher than vanilla
decoding.

</details>


### [37] [Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval](https://arxiv.org/abs/2508.19758)
*Yixuan Tang,Yuanyuan Shi,Yiqun Sun,Anthony Kum Hoe Tung*

Main category: cs.CL

TL;DR: NEWSCOPE是一个两阶段新闻检索框架，通过句子级语义变化建模来提升事件报道的多样性，在保持相关性的同时显著提高检索结果多样性


<details>
  <summary>Details</summary>
Motivation: 现有新闻检索系统过于关注文本相关性，导致结果冗余且视角有限，需要提升对多样化观点的覆盖

Method: 两阶段框架：第一阶段使用密集检索获取主题相关内容，第二阶段通过句子级聚类和多样性感知重排序来发现互补信息

Result: NEWSCOPE在多个基准测试中持续优于强基线，在不牺牲相关性的情况下显著提高多样性

Conclusion: 细粒度、可解释的建模能有效减少冗余并促进对事件的全面理解

Abstract: Access to diverse perspectives is essential for understanding real-world
events, yet most news retrieval systems prioritize textual relevance, leading
to redundant results and limited viewpoint exposure. We propose NEWSCOPE, a
two-stage framework for diverse news retrieval that enhances event coverage by
explicitly modeling semantic variation at the sentence level. The first stage
retrieves topically relevant content using dense retrieval, while the second
stage applies sentence-level clustering and diversity-aware re-ranking to
surface complementary information. To evaluate retrieval diversity, we
introduce three interpretable metrics, namely Average Pairwise Distance,
Positive Cluster Coverage, and Information Density Ratio, and construct two
paragraph-level benchmarks: LocalNews and DSGlobal. Experiments show that
NEWSCOPE consistently outperforms strong baselines, achieving significantly
higher diversity without compromising relevance. Our results demonstrate the
effectiveness of fine-grained, interpretable modeling in mitigating redundancy
and promoting comprehensive event understanding. The data and code are
available at https://github.com/tangyixuan/NEWSCOPE.

</details>


### [38] [Principled Personas: Defining and Measuring the Intended Effects of Persona Prompting on Task Performance](https://arxiv.org/abs/2508.19764)
*Pedro Henrique Luz de Araujo,Paul Röttger,Dirk Hovy,Benjamin Roth*

Main category: cs.CL

TL;DR: 专家角色提示对LLM任务性能影响分析：通常带来正面或非显著提升，但对无关角色细节高度敏感（性能下降近30%），角色属性保真度效果不一致，仅最大模型能通过缓解策略改善鲁棒性


<details>
  <summary>Details</summary>
Motivation: 分析专家角色提示在任务改进中的有效性，明确何时以及为何角色应该提升性能，解决先前研究结果不一致且缺乏系统性分析的问题

Method: 文献分析提炼三个期望标准（专家角色性能优势、无关属性鲁棒性、角色属性保真度），在27个任务上评估9个最先进LLM

Result: 专家角色通常带来正面或非显著性能变化；模型对无关角色细节高度敏感（性能下降近30%）；高等教育、专业化和领域相关性等属性效果不一致；缓解策略仅对最大最强大模型有效

Conclusion: 需要更谨慎的角色设计和反映角色使用预期效果的评估方案，当前角色提示存在鲁棒性问题且效果不稳定

Abstract: Expert persona prompting -- assigning roles such as expert in math to
language models -- is widely used for task improvement. However, prior work
shows mixed results on its effectiveness, and does not consider when and why
personas should improve performance. We analyze the literature on persona
prompting for task improvement and distill three desiderata: 1) performance
advantage of expert personas, 2) robustness to irrelevant persona attributes,
and 3) fidelity to persona attributes. We then evaluate 9 state-of-the-art LLMs
across 27 tasks with respect to these desiderata. We find that expert personas
usually lead to positive or non-significant performance changes. Surprisingly,
models are highly sensitive to irrelevant persona details, with performance
drops of almost 30 percentage points. In terms of fidelity, we find that while
higher education, specialization, and domain-relatedness can boost performance,
their effects are often inconsistent or negligible across tasks. We propose
mitigation strategies to improve robustness -- but find they only work for the
largest, most capable models. Our findings underscore the need for more careful
persona design and for evaluation schemes that reflect the intended effects of
persona usage.

</details>


### [39] [T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables](https://arxiv.org/abs/2508.19813)
*Jie Zhang,Changzai Pan,Kaiwen Wei,Sishi Xiong,Yu Zhao,Xiangyu Li,Jiaxin Peng,Xiaoyan Gu,Jian Yang,Wenhan Chang,Zhenhe Wu,Jiang Zhong,Shuangyong Song,Yongxiang Li,Xuelong Li*

Main category: cs.CL

TL;DR: 提出了表格到报告生成任务和T2R-bench双语基准，包含457个工业表格，评估25个LLM发现即使最先进模型也仅有62.71分，显示该任务仍有改进空间


<details>
  <summary>Details</summary>
Motivation: 现有表格推理研究无法满足工业应用中表格信息转化为报告的需求，存在表格复杂性导致推理效果不佳和缺乏实用评估基准两个关键问题

Method: 构建T2R-bench双语基准，包含457个真实工业表格，涵盖19个行业领域和4种表格类型，并提出评估标准来衡量报告生成质量

Result: 在25个广泛使用的LLM上进行实验，发现即使是Deepseek-R1等最先进模型也只达到62.71的整体分数

Conclusion: 表格到报告生成任务对LLM仍具有挑战性，现有模型性能有待提升，T2R-bench为评估该任务提供了实用基准

Abstract: Extensive research has been conducted to explore the capabilities of large
language models (LLMs) in table reasoning. However, the essential task of
transforming tables information into reports remains a significant challenge
for industrial applications. This task is plagued by two critical issues: 1)
the complexity and diversity of tables lead to suboptimal reasoning outcomes;
and 2) existing table benchmarks lack the capacity to adequately assess the
practical application of this task. To fill this gap, we propose the
table-to-report task and construct a bilingual benchmark named T2R-bench, where
the key information flow from the tables to the reports for this task. The
benchmark comprises 457 industrial tables, all derived from real-world
scenarios and encompassing 19 industry domains as well as 4 types of industrial
tables. Furthermore, we propose an evaluation criteria to fairly measure the
quality of report generation. The experiments on 25 widely-used LLMs reveal
that even state-of-the-art models like Deepseek-R1 only achieves performance
with 62.71 overall score, indicating that LLMs still have room for improvement
on T2R-bench. Source code and data will be available after acceptance.

</details>


### [40] [Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning](https://arxiv.org/abs/2508.19828)
*Sikuan Yan,Xiufeng Yang,Zuchao Huang,Ercong Nie,Zifeng Ding,Zonggen Li,Xiaowen Ma,Hinrich Schütze,Volker Tresp,Yunpu Ma*

Main category: cs.CL

TL;DR: Memory-R1是一个强化学习框架，通过两个专门代理（内存管理器和答案代理）使LLM能够主动管理外部内存，在少量训练数据下实现优于现有基线的性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM因有限上下文窗口而无法进行长程推理的问题，现有外部内存方法多为静态启发式，缺乏学习机制来决定存储、更新或检索内容。

Method: 使用强化学习（PPO和GRPO）微调两个代理：内存管理器执行结构化内存操作（添加、更新、删除、无操作），答案代理选择相关条目并进行推理生成答案。

Result: 仅用152个问答对训练，Memory-R1就超越了现有最强基线，在不同问题类型和LLM骨干网络上表现出强大的泛化能力。

Conclusion: 该工作展示了RL如何解锁LLM中更具代理性和内存感知能力的行为，为构建更丰富、更持久的推理系统指明了方向。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across
a wide range of NLP tasks, but they remain fundamentally stateless, constrained
by limited context windows that hinder long-horizon reasoning. Recent efforts
to address this limitation often augment LLMs with an external memory bank, yet
most existing pipelines are static and heuristic-driven, lacking any learned
mechanism for deciding what to store, update, or retrieve. We present
Memory-R1, a reinforcement learning (RL) framework that equips LLMs with the
ability to actively manage and utilize external memory through two specialized
agents: a Memory Manager that learns to perform structured memory operations
{ADD, UPDATE, DELETE, NOOP}, and an Answer Agent that selects the most relevant
entries and reasons over them to produce an answer. Both agents are fine-tuned
with outcome-driven RL (PPO and GRPO), enabling adaptive memory management and
use with minimal supervision. With as few as 152 question-answer pairs and a
corresponding temporal memory bank for training, Memory-R1 outperforms the most
competitive existing baseline and demonstrates strong generalization across
diverse question types and LLM backbones. Beyond presenting an effective
approach, this work provides insights into how RL can unlock more agentic,
memory-aware behaviors in LLMs, pointing toward richer, more persistent
reasoning systems.

</details>


### [41] [Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative Analysis](https://arxiv.org/abs/2508.19831)
*Anusha Kamath,Kanishk Singla,Rakesh Paul,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: 这篇论文为印地语言模型提供了一套综合评测套件，包吨5个专门设计的评测数据集，解决了直接翻译英语数据集无法抓取印地语语言文化细节的问题。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏高质量的印地语LLM评测基准，直接翻译英语数据集无法满足语言文化细节的需求，需要专门为印地语设计的评测工具。

Method: 采用组合方法：从头开始的人工注释加上翻译-验证过程，创建了5个印地语评测数据集（IFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, BFCL-Hi）。

Result: 对支持印地语的开源LLM进行了全面基准测评，提供了详细的比较分析，展示了当前模型的能力水平。

Conclusion: 该方法论不仅为印地语LLM评估提供了可靠工具，同时也为其他低资源语言的基准开发提供了可复制的方法论。

Abstract: Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is
challenging due to a lack of high-quality benchmarks, as direct translation of
English datasets fails to capture crucial linguistic and cultural nuances. To
address this, we introduce a suite of five Hindi LLM evaluation datasets:
IFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created
using a methodology that combines from-scratch human annotation with a
translate-and-verify process. We leverage this suite to conduct an extensive
benchmarking of open-source LLMs supporting Hindi, providing a detailed
comparative analysis of their current capabilities. Our curation process also
serves as a replicable methodology for developing benchmarks in other
low-resource languages.

</details>


### [42] [Scalable and consistent few-shot classification of survey responses using text embeddings](https://arxiv.org/abs/2508.19836)
*Jonas Timmann Mjaaland,Markus Fleten Kreutzer,Halvor Tyseng,Rebeckah K. Fussell,Gina Passante,N. G. Holmes,Anders Malthe-Sørenssen,Tor Ole B. Odden*

Main category: cs.CL

TL;DR: 提出基于文本嵌入的分类框架，仅需少量标注样本即可实现定性分析，在物理调查数据上达到与专家编码相近的一致性水平（Cohen's Kappa 0.74-0.83）


<details>
  <summary>Details</summary>
Motivation: 传统定性分析方法耗时且不一致，现有NLP方法需要大量标注数据或破坏定性工作流程，需要一种既能保持工作流程又能高效处理大规模文本的方法

Method: 基于文本嵌入的分类框架，每个类别只需少量示例，支持微调文本嵌入模型，可与标准定性工作流程良好整合

Result: 在2899个开放式物理调查响应中，与专家编码相比达到Cohen's Kappa 0.74-0.83的一致性，性能随模型微调而提升，可用于审计已有数据集

Conclusion: 文本嵌入辅助编码可在不牺牲可解释性的前提下灵活扩展到数千个响应，为大规模演绎性定性分析开辟了新途径

Abstract: Qualitative analysis of open-ended survey responses is a commonly-used
research method in the social sciences, but traditional coding approaches are
often time-consuming and prone to inconsistency. Existing solutions from
Natural Language Processing such as supervised classifiers, topic modeling
techniques, and generative large language models have limited applicability in
qualitative analysis, since they demand extensive labeled data, disrupt
established qualitative workflows, and/or yield variable results. In this
paper, we introduce a text embedding-based classification framework that
requires only a handful of examples per category and fits well with standard
qualitative workflows. When benchmarked against human analysis of a conceptual
physics survey consisting of 2899 open-ended responses, our framework achieves
a Cohen's Kappa ranging from 0.74 to 0.83 as compared to expert human coders in
an exhaustive coding scheme. We further show how performance of this framework
improves with fine-tuning of the text embedding model, and how the method can
be used to audit previously-analyzed datasets. These findings demonstrate that
text embedding-assisted coding can flexibly scale to thousands of responses
without sacrificing interpretability, opening avenues for deductive qualitative
analysis at scale.

</details>


### [43] [TokenVerse++: Towards Flexible Multitask Learning with Dynamic Task Activation](https://arxiv.org/abs/2508.19856)
*Shashi Kumar,Srikanth Madikeri,Esaú Villatoro-Tello,Sergio Burdisso,Pradeep Rangappa,Andrés Carofilis,Petr Motlicek,Karthik Pandia,Shankar Venkatesan,Kadri Hacioğlu,Andreas Stolcke*

Main category: cs.CL

TL;DR: TokenVerse++在TokenVerse基础上引入可学习向量机制，支持部分标注数据的训练，解决了多任务框架需要全标注数据的限制，在不牺牲ASR性能的情况下实现了更实用的多任务学习。


<details>
  <summary>Details</summary>
Motivation: 现有的基于token的多任务框架（如TokenVerse）要求所有训练语句必须包含所有任务的标签，这限制了利用部分标注数据集的能力，阻碍了框架的有效扩展。

Method: 在XLSR-Transducer ASR模型的声学嵌入空间中引入可学习向量进行动态任务激活，使得模型能够使用仅标注了部分任务的语句进行训练。

Result: 通过成功整合仅包含ASR和语言识别部分标注的数据集，TokenVerse++在多个任务上达到或超过TokenVerse的性能，同时提升了整体性能。

Conclusion: TokenVerse++通过动态任务激活机制，为多任务学习提供了更实用的替代方案，能够在保持ASR性能的同时有效利用部分标注数据。

Abstract: Token-based multitasking frameworks like TokenVerse require all training
utterances to have labels for all tasks, hindering their ability to leverage
partially annotated datasets and scale effectively. We propose TokenVerse++,
which introduces learnable vectors in the acoustic embedding space of the
XLSR-Transducer ASR model for dynamic task activation. This core mechanism
enables training with utterances labeled for only a subset of tasks, a key
advantage over TokenVerse. We demonstrate this by successfully integrating a
dataset with partial labels, specifically for ASR and an additional task,
language identification, improving overall performance. TokenVerse++ achieves
results on par with or exceeding TokenVerse across multiple tasks, establishing
it as a more practical multitask alternative without sacrificing ASR
performance.

</details>


### [44] [Beyond Shallow Heuristics: Leveraging Human Intuition for Curriculum Learning](https://arxiv.org/abs/2508.19873)
*Vanessa Toborek,Sebastian Müller,Tim Selbach,Tamás Horváth,Christian Bauckhage*

Main category: cs.CL

TL;DR: 研究表明，使用Simple Wikipedia的人类标注简单语言数据作为课程学习信号，能够有效提升语言模型训练效果，特别是在简单语言上的困惑度表现优于基于能力的课程策略。


<details>
  <summary>Details</summary>
Motivation: 课程学习(CL)通过从易到难呈现数据来改善训练，但定义和衡量语言难度仍是一个开放挑战。本研究探讨人类标注的简单语言是否能作为有效的CL信号。

Method: 使用Simple Wikipedia语料库的文章级标签，将基于标签的课程与依赖浅层启发式的基于能力的策略进行比较。使用BERT-tiny模型进行实验，分析不同课程策略的效果。

Result: 仅添加简单数据没有明显益处，但通过课程结构（特别是先引入简单数据）能持续改善困惑度，尤其在简单语言上。基于能力的课程相比随机排序没有一致增益。

Conclusion: 人类对语言难度的直觉可以指导语言模型预训练的课程学习，基于人类标注的课程策略比基于能力的启发式方法更有效。

Abstract: Curriculum learning (CL) aims to improve training by presenting data from
"easy" to "hard", yet defining and measuring linguistic difficulty remains an
open challenge. We investigate whether human-curated simple language can serve
as an effective signal for CL. Using the article-level labels from the Simple
Wikipedia corpus, we compare label-based curricula to competence-based
strategies relying on shallow heuristics. Our experiments with a BERT-tiny
model show that adding simple data alone yields no clear benefit. However,
structuring it via a curriculum -- especially when introduced first --
consistently improves perplexity, particularly on simple language. In contrast,
competence-based curricula lead to no consistent gains over random ordering,
probably because they fail to effectively separate the two classes. Our results
suggest that human intuition about linguistic difficulty can guide CL for
language model pre-training.

</details>


### [45] [AI-Powered Detection of Inappropriate Language in Medical School Curricula](https://arxiv.org/abs/2508.19883)
*Chiman Salavati,Shannon Song,Scott A. Hale,Roberto E. Montenegro,Shiri Dori-Hacohen,Fabricio Murai*

Main category: cs.CL

TL;DR: 本文评估了小型语言模型(SLMs)和大型语言模型(LLMs)在检测医学教材中不当语言使用(IUL)方面的表现，发现SLMs在性能上优于LLMs，多标签分类器表现最佳，通过添加未标记样本作为负样本可显著提升分类器性能。


<details>
  <summary>Details</summary>
Motivation: 医学教材中使用过时、排他性或非患者中心的不当语言会影响临床培训和患者健康，但人工识别成本高昂且不切实际，需要自动化解决方案。

Method: 使用约500份文档、超过12,000页的数据集，评估了四种SLMs方法(通用IUL分类器、子类别二元分类器、多标签分类器、两阶段分层管道)和LLMs的上下文学习方法。

Result: LLama-3 8B和70B即使使用精心设计的提示词，性能也大幅落后于SLMs。多标签分类器在标注数据上表现最好，添加未标记样本作为负样本可使特定分类器的AUC提升高达25%。

Conclusion: SLMs特别是多标签分类器是检测医学课程中有害语言的最有效模型，通过数据增强策略可显著提升检测性能，为医学教育材料的语言规范化提供了实用的自动化工具。

Abstract: The use of inappropriate language -- such as outdated, exclusionary, or
non-patient-centered terms -- medical instructional materials can significantly
influence clinical training, patient interactions, and health outcomes. Despite
their reputability, many materials developed over past decades contain examples
now considered inappropriate by current medical standards. Given the volume of
curricular content, manually identifying instances of inappropriate use of
language (IUL) and its subcategories for systematic review is prohibitively
costly and impractical. To address this challenge, we conduct a first-in-class
evaluation of small language models (SLMs) fine-tuned on labeled data and
pre-trained LLMs with in-context learning on a dataset containing approximately
500 documents and over 12,000 pages. For SLMs, we consider: (1) a general IUL
classifier, (2) subcategory-specific binary classifiers, (3) a multilabel
classifier, and (4) a two-stage hierarchical pipeline for general IUL detection
followed by multilabel classification. For LLMs, we consider variations of
prompts that include subcategory definitions and/or shots. We found that both
LLama-3 8B and 70B, even with carefully curated shots, are largely outperformed
by SLMs. While the multilabel classifier performs best on annotated data,
supplementing training with unflagged excerpts as negative examples boosts the
specific classifiers' AUC by up to 25%, making them most effective models for
mitigating harmful language in medical curricula.

</details>


### [46] [Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with LLM-Assisted Translation Refinement](https://arxiv.org/abs/2508.19887)
*Mohammed Rakibul Hasan,Rafi Majid,Ahanaf Tahmid*

Main category: cs.CL

TL;DR: 提出了Bangla-Bayanno数据集，这是一个孟加拉语开放域视觉问答数据集，包含52,650个问答对和4,750+张图像，采用多语言LLM辅助翻译流程确保质量


<details>
  <summary>Details</summary>
Motivation: 解决孟加拉语这种低资源语言在多媒体AI研究中缺乏高质量VQA数据集的问题，现有数据集要么领域特定，要么答案格式受限，且存在翻译质量问题

Method: 使用多语言大语言模型辅助的翻译精炼流程来减少人工错误并保证清晰度，将问题分为三类答案类型：名词性（简短描述）、数量性（数字）和极性（是/否）

Result: 创建了包含52,650个问答对和4,750+张图像的孟加拉语VQA数据集，克服了多语言来源的低质量翻译问题

Conclusion: Bangla-Bayanno提供了最全面的开源高质量孟加拉语VQA基准，旨在推动低资源多模态学习研究，促进更具包容性的AI系统发展

Abstract: In this paper, we introduce Bangla-Bayanno, an open-ended Visual Question
Answering (VQA) Dataset in Bangla, a widely used, low-resource language in
multimodal AI research. The majority of existing datasets are either manually
annotated with an emphasis on a specific domain, query type, or answer type or
are constrained by niche answer formats. In order to mitigate human-induced
errors and guarantee lucidity, we implemented a multilingual LLM-assisted
translation refinement pipeline. This dataset overcomes the issues of
low-quality translations from multilingual sources. The dataset comprises
52,650 question-answer pairs across 4750+ images. Questions are classified into
three distinct answer types: nominal (short descriptive), quantitative
(numeric), and polar (yes/no). Bangla-Bayanno provides the most comprehensive
open-source, high-quality VQA benchmark in Bangla, aiming to advance research
in low-resource multimodal learning and facilitate the development of more
inclusive AI systems.

</details>


### [47] [Logical Reasoning with Outcome Reward Models for Test-Time Scaling](https://arxiv.org/abs/2508.19903)
*Ramya Keerthy Thatikonda,Wray Buntine,Ehsan Shareghi*

Main category: cs.CL

TL;DR: 本文提出了用于演绎逻辑推理的结果奖励模型(ORMs)，通过CoT和回声生成技术增强训练数据，在多个逻辑推理数据集上提升了LLM性能。


<details>
  <summary>Details</summary>
Motivation: 逻辑推理是评估大语言模型能力的关键基准，但目前测试时缩放与专用奖励模型结合的方法在演绎逻辑推理领域尚未充分探索。

Method: 使用单样本和多样本的思维链(CoT)生成训练数据，并提出回声生成技术来扩展错误类型覆盖，通过引导模型进行错误推理来获取额外训练样本。

Result: 在FOLIO、JustLogic和ProverQA三个数据集上，基于CoT和回声增强数据训练的ORMs在四种不同LLM上都表现出性能提升。

Conclusion: 提出的ORMs和回声生成技术有效提升了LLM在演绎逻辑推理任务中的表现，为复杂推理任务的性能增强提供了新途径。

Abstract: Logical reasoning is a critical benchmark for evaluating the capabilities of
large language models (LLMs), as it reflects their ability to derive valid
conclusions from given premises. While the combination of test-time scaling
with dedicated outcome or process reward models has opened up new avenues to
enhance LLMs performance in complex reasoning tasks, this space is
under-explored in deductive logical reasoning. We present a set of Outcome
Reward Models (ORMs) for deductive reasoning. To train the ORMs we mainly
generate data using Chain-of-Thought (CoT) with single and multiple samples.
Additionally, we propose a novel tactic to further expand the type of errors
covered in the training dataset of the ORM. In particular, we propose an echo
generation technique that leverages LLMs' tendency to reflect incorrect
assumptions made in prompts to extract additional training data, covering
previously unexplored error types. While a standard CoT chain may contain
errors likely to be made by the reasoner, the echo strategy deliberately steers
the model toward incorrect reasoning. We show that ORMs trained on CoT and
echo-augmented data demonstrate improved performance on the FOLIO, JustLogic,
and ProverQA datasets across four different LLMs.

</details>


### [48] [Your AI Bosses Are Still Prejudiced: The Emergence of Stereotypes in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2508.19919)
*Jingyu Guo,Yingying Xu*

Main category: cs.CL

TL;DR: 研究发现AI多智能体系统在无预设偏见的中性初始条件下，会自发产生刻板印象驱动的偏见，且这种现象随交互轮次和决策权力增加而加剧，表现出与人类社会行为相似的群体效应。


<details>
  <summary>Details</summary>
Motivation: 虽然刻板印象在人类社交中已被充分记录，但AI系统通常被认为较少受此类偏见影响。以往研究主要关注训练数据带来的偏见，但刻板印象是否能在AI智能体交互中自发产生值得深入探索。

Method: 通过模拟职场交互的新型实验框架，在无预设偏见的中性初始条件下，研究基于大语言模型的多智能体系统中刻板印象的出现和演化。

Result: 研究发现：(1)AI智能体在无预设偏见的情况下会发展出刻板印象驱动的偏见；(2)刻板印象效应随交互轮次和决策权力增加而加剧；(3)系统表现出与人类相似的群体效应；(4)这种现象在不同LLM架构中一致存在。

Conclusion: AI系统中的刻板印象形成可能是多智能体交互的涌现特性，而不仅仅是训练数据偏见的结果。需要进一步探索这种现象的底层机制并制定缓解其伦理影响的策略。

Abstract: While stereotypes are well-documented in human social interactions, AI
systems are often presumed to be less susceptible to such biases. Previous
studies have focused on biases inherited from training data, but whether
stereotypes can emerge spontaneously in AI agent interactions merits further
exploration. Through a novel experimental framework simulating workplace
interactions with neutral initial conditions, we investigate the emergence and
evolution of stereotypes in LLM-based multi-agent systems. Our findings reveal
that (1) LLM-Based AI agents develop stereotype-driven biases in their
interactions despite beginning without predefined biases; (2) stereotype
effects intensify with increased interaction rounds and decision-making power,
particularly after introducing hierarchical structures; (3) these systems
exhibit group effects analogous to human social behavior, including halo
effects, confirmation bias, and role congruity; and (4) these stereotype
patterns manifest consistently across different LLM architectures. Through
comprehensive quantitative analysis, these findings suggest that stereotype
formation in AI systems may arise as an emergent property of multi-agent
interactions, rather than merely from training data biases. Our work
underscores the need for future research to explore the underlying mechanisms
of this phenomenon and develop strategies to mitigate its ethical impacts.

</details>


### [49] [HEAL: A Hypothesis-Based Preference-Aware Analysis Framework](https://arxiv.org/abs/2508.19922)
*Yifu Huo,Chenglong Wang,Qiren Zhu,Shunjie Xing,Tong Xiao,Chunliang Zhang,Tongran Liu,Jinbo Zhu*

Main category: cs.CL

TL;DR: 本文提出了HEAL评估框架，通过假设空间重排序来评估偏好对齐方法，解决了传统单响应评估的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法（如DPO）的评估仅依赖单一响应，忽略了假设空间中其他潜在输出，无法全面反映真实应用场景。

Method: 提出HEAL框架，将偏好对齐建模为假设空间内的重排序过程，包含排序准确性和偏好强度相关性两个互补指标，并构建了UniHypoBench统一假设基准。

Result: 实验表明当前偏好学习方法能有效捕捉代理模型的偏好，同时抑制负面样本，验证了HEAL框架的有效性。

Conclusion: HEAL为偏好学习研究提供了理论创新范式和实践诊断工具，为开发更先进的偏好对齐算法指明了方向。

Abstract: Preference optimization methods like DPO have achieved remarkable performance
in LLM alignment. However, the evaluation for these methods relies on a single
response and overlooks other potential outputs, which could also be generated
in real-world applications within this hypothetical space. To address this
issue, this paper presents a \textbf{H}ypothesis-based
Pr\textbf{E}ference-aware \textbf{A}na\textbf{L}ysis Framework (HEAL), a novel
evaluation paradigm that formulates preference alignment as a re-ranking
process within hypothesis spaces. The framework incorporates two complementary
metrics: ranking accuracy for evaluating ordinal consistency and preference
strength correlation for assessing continuous alignment. To facilitate this
framework, we develop UniHypoBench, a unified hypothesis benchmark constructed
from diverse instruction-response pairs. Through extensive experiments based on
HEAL, with a particular focus on the intrinsic mechanisms of preference
learning, we demonstrate that current preference learning methods can
effectively capture preferences provided by proxy models while simultaneously
suppressing negative samples. These findings contribute to preference learning
research through two significant avenues. Theoretically, we introduce
hypothesis space analysis as an innovative paradigm for understanding
preference alignment. Practically, HEAL offers researchers robust diagnostic
tools for refining preference optimization methods, while our empirical results
identify promising directions for developing more advanced alignment algorithms
capable of comprehensive preference capture.

</details>


### [50] [Dhati+: Fine-tuned Large Language Models for Arabic Subjectivity Evaluation](https://arxiv.org/abs/2508.19966)
*Slimane Bellaouar,Attia Nehar,Soumia Souffi,Mounia Bouameur*

Main category: cs.CL

TL;DR: 本文提出了一种新的阿拉伯语主观性分析方法，通过构建AraDhati+数据集并微调先进的语言模型，在阿拉伯语主观性分类中达到了97.79%的准确率。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语作为一种语言丰富且形态复杂的语言，面临着资源匮乏的挑战。缺乏大型标注数据集阻碍了阿拉伯语主观性分析工具的准确开发。

Method: 1) 利用现有阿拉伯语数据集(ASTD、LABR、HARD、SANAD)开发综合数据集AraDhati+；2) 在AraDhati+上微调最先进的阿拉伯语语言模型(XLM-RoBERTa、AraBERT、ArabianGPT)；3) 实验集成决策方法以利用各模型的优势。

Result: 该方法在阿拉伯语主观性分类中取得了97.79%的显著准确率。

Conclusion: 研究结果表明，所提出的方法在解决阿拉伯语处理中资源有限挑战方面非常有效，证明了深度学习和Transformer模型在阿拉伯语文本分类中的高效性。

Abstract: Despite its significance, Arabic, a linguistically rich and morphologically
complex language, faces the challenge of being under-resourced. The scarcity of
large annotated datasets hampers the development of accurate tools for
subjectivity analysis in Arabic. Recent advances in deep learning and
Transformers have proven highly effective for text classification in English
and French. This paper proposes a new approach for subjectivity assessment in
Arabic textual data. To address the dearth of specialized annotated datasets,
we developed a comprehensive dataset, AraDhati+, by leveraging existing Arabic
datasets and collections (ASTD, LABR, HARD, and SANAD). Subsequently, we
fine-tuned state-of-the-art Arabic language models (XLM-RoBERTa, AraBERT, and
ArabianGPT) on AraDhati+ for effective subjectivity classification.
Furthermore, we experimented with an ensemble decision approach to harness the
strengths of individual models. Our approach achieves a remarkable accuracy of
97.79\,\% for Arabic subjectivity classification. Results demonstrate the
effectiveness of the proposed approach in addressing the challenges posed by
limited resources in Arabic language processing.

</details>


### [51] [Diffusion Language Models Know the Answer Before Decoding](https://arxiv.org/abs/2508.19982)
*Pengxiang Li,Yefan Zhou,Dilxat Muhtar,Lu Yin,Shilin Yan,Li Shen,Yi Liang,Soroush Vosoughi,Shiwei Liu*

Main category: cs.CL

TL;DR: Prophet是一种无需训练的快速解码方法，利用扩散语言模型中早期答案收敛的特性，通过置信度差距动态决定何时停止采样，可将解码步骤减少3.4倍同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然提供并行序列生成能力，但其推理速度仍慢于自回归模型，主要原因是双向注意力计算和大量细化步骤的成本。研究发现DLMs存在早期答案收敛现象，即在最终解码步骤前模型已能识别正确答案。

Method: 提出Prophet解码范式：动态监测top-2预测候选的置信度差距，当差距足够大时进行"all-in"解码（一步解码所有剩余token），否则继续细化步骤。无需额外训练，可无缝集成到现有DLM实现中。

Result: 在GSM8K和MMLU任务上，分别有97%和99%的实例仅需一半细化步骤即可正确解码。对LLaDA-8B和Dream-7B的评估显示，Prophet可将解码步骤减少高达3.4倍，同时保持高质量生成。

Conclusion: Prophet将DLM解码重新定义为何时停止采样的问题，证明早期解码收敛是加速DLM推理的简单而强大的机制，与现有加速技术互补。

Abstract: Diffusion language models (DLMs) have recently emerged as an alternative to
autoregressive approaches, offering parallel sequence generation and flexible
token orders. However, their inference remains slower than that of
autoregressive models, primarily due to the cost of bidirectional attention and
the large number of refinement steps required for high quality outputs. In this
work, we highlight and leverage an overlooked property of DLMs early answer
convergence: in many cases, the correct answer can be internally identified by
half steps before the final decoding step, both under semi-autoregressive and
random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99%
of instances, respectively, can be decoded correctly using only half of the
refinement steps. Building on this observation, we introduce Prophet, a
training-free fast decoding paradigm that enables early commit decoding.
Specifically, Prophet dynamically decides whether to continue refinement or to
go "all-in" (i.e., decode all remaining tokens in one step), using the
confidence gap between the top-2 prediction candidates as the criterion. It
integrates seamlessly into existing DLM implementations, incurs negligible
overhead, and requires no additional training. Empirical evaluations of
LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the
number of decoding steps by up to 3.4x while preserving high generation
quality. These results recast DLM decoding as a problem of when to stop
sampling, and demonstrate that early decode convergence provides a simple yet
powerful mechanism for accelerating DLM inference, complementary to existing
speedup techniques. Our code is publicly available at
https://github.com/pixeli99/Prophet.

</details>


### [52] [AgentCoMa: A Compositional Benchmark Mixing Commonsense and Mathematical Reasoning in Real-World Scenarios](https://arxiv.org/abs/2508.19988)
*Lisa Alazraki,Lihu Chen,Ana Brassard,Joe Stacey,Hossein A. Rahmani,Marek Rei*

Main category: cs.CL

TL;DR: LLM在单一类型的组合推理上表现良好，但在混合常识推理和数学推理的组合任务中准确率下降约30%，显示出模型脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有组合基准测试主要关注单一推理类型（常识或数学），而现实世界任务需要结合多种推理能力，因此需要评估LLM在混合类型组合推理中的表现。

Method: 提出AgentCoMa基准测试，包含需要常识推理和数学推理步骤的组合任务，测试了61个不同规模、模型家族和训练策略的LLM，并进行可解释性研究（神经元模式、注意力图和成员推断分析）。

Result: LLM在单独解决常识或数学步骤时表现良好，但在组合任务中准确率平均下降30%，远高于同类型多步骤组合的性能差距。人类注释者则在组合问题和单独步骤上都保持高准确率。

Conclusion: 研究揭示了LLM在混合类型组合推理方面存在显著的模型脆弱性，为未来改进提供了测试平台。

Abstract: Large Language Models (LLMs) have achieved high accuracy on complex
commonsense and mathematical problems that involve the composition of multiple
reasoning steps. However, current compositional benchmarks testing these skills
tend to focus on either commonsense or math reasoning, whereas LLM agents
solving real-world tasks would require a combination of both. In this work, we
introduce an Agentic Commonsense and Math benchmark (AgentCoMa), where each
compositional task requires a commonsense reasoning step and a math reasoning
step. We test it on 61 LLMs of different sizes, model families, and training
strategies. We find that LLMs can usually solve both steps in isolation, yet
their accuracy drops by ~30% on average when the two are combined. This is a
substantially greater performance gap than the one we observe in prior
compositional benchmarks that combine multiple steps of the same reasoning
type. In contrast, non-expert human annotators can solve the compositional
questions and the individual steps in AgentCoMa with similarly high accuracy.
Furthermore, we conduct a series of interpretability studies to better
understand the performance gap, examining neuron patterns, attention maps and
membership inference. Our work underscores a substantial degree of model
brittleness in the context of mixed-type compositional reasoning and offers a
test bed for future improvement.

</details>


### [53] [MathBuddy: A Multimodal System for Affective Math Tutoring](https://arxiv.org/abs/2508.19993)
*Debanjana Kar,Leopold Böss,Dacia Braca,Sebastian Maximilian Dennerlein,Nina Christine Hubig,Philipp Wintersberger,Yufang Hou*

Main category: cs.CL

TL;DR: MathBuddy是一个情感感知的数学辅导系统，通过分析学生的文本对话和面部表情来识别情绪状态，并据此调整教学策略，显著提升了LLM教学助手的教学效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的学习模型没有考虑学生的情感状态，而教育心理学研究表明情绪状态会影响学习能力，因此需要开发能够感知和响应学生情绪的智能辅导系统。

Method: 开发MathBuddy系统，从对话文本和面部表情两个模态捕捉学生情绪，聚合情绪信息后提示LLM生成情感感知的回应，采用相关教学策略。

Result: 通过自动评估指标和用户研究验证，在八个教学维度上获得显著提升：胜率提高23个百分点，DAMR总分提高3分。

Conclusion: 通过建模学生情绪可以显著改善基于LLM的教学助手的教学能力，情感感知是提升教育技术效果的重要方向。

Abstract: The rapid adoption of LLM-based conversational systems is already
transforming the landscape of educational technology. However, the current
state-of-the-art learning models do not take into account the student's
affective states. Multiple studies in educational psychology support the claim
that positive or negative emotional states can impact a student's learning
capabilities. To bridge this gap, we present MathBuddy, an emotionally aware
LLM-powered Math Tutor, which dynamically models the student's emotions and
maps them to relevant pedagogical strategies, making the tutor-student
conversation a more empathetic one. The student's emotions are captured from
the conversational text as well as from their facial expressions. The student's
emotions are aggregated from both modalities to confidently prompt our LLM
Tutor for an emotionally-aware response. We have effectively evaluated our
model using automatic evaluation metrics across eight pedagogical dimensions
and user studies. We report a massive 23 point performance gain using the win
rate and a 3 point gain at an overall level using DAMR scores which strongly
supports our hypothesis of improving LLM-based tutor's pedagogical abilities by
modeling students' emotions.

</details>


### [54] [ReSURE: Regularizing Supervision Unreliability for Multi-turn Dialogue Fine-tuning](https://arxiv.org/abs/2508.19996)
*Yiming Du,Yifan Xiang,Bin Liang,Dahua Lin,Kam-Fai Wong,Fei Tan*

Main category: cs.CL

TL;DR: ReSURE是一种自适应学习方法，通过动态降低不可靠监督的权重来解决多轮对话系统中错误传播问题，无需显式过滤数据


<details>
  <summary>Details</summary>
Motivation: 多轮对话系统微调需要高质量监督，但低质量数据会导致性能下降。早期轮次的监督错误会传播到后续轮次，破坏连贯性和响应质量。现有方法通过静态预过滤处理数据质量，但将质量控制与训练解耦，无法缓解轮级错误传播

Method: ReSURE使用Welford在线统计方法估计每轮损失分布，并据此动态重新加权样本损失，自适应地降低不可靠监督的权重

Result: 在单源和混合质量数据集上的实验显示，ReSURE提高了稳定性和响应质量。在多个基准测试中，响应分数与样本数量之间呈现正Spearman相关性（0.21~1.0），无论数据质量如何

Conclusion: ReSURE有效解决了多轮对话训练中的错误传播问题，为有效利用大规模数据铺平了道路

Abstract: Fine-tuning multi-turn dialogue systems requires high-quality supervision but
often suffers from degraded performance when exposed to low-quality data.
Supervision errors in early turns can propagate across subsequent turns,
undermining coherence and response quality. Existing methods typically address
data quality via static prefiltering, which decouples quality control from
training and fails to mitigate turn-level error propagation. In this context,
we propose ReSURE (Regularizing Supervision UnREliability), an adaptive
learning method that dynamically down-weights unreliable supervision without
explicit filtering. ReSURE estimates per-turn loss distributions using
Welford's online statistics and reweights sample losses on the fly accordingly.
Experiments on both single-source and mixed-quality datasets show improved
stability and response quality. Notably, ReSURE enjoys positive Spearman
correlations (0.21 ~ 1.0 across multiple benchmarks) between response scores
and number of samples regardless of data quality, which potentially paves the
way for utilizing large-scale data effectively. Code is publicly available at
https://github.com/Elvin-Yiming-Du/ReSURE_Multi_Turn_Training.

</details>


### [55] [Selective Retrieval-Augmentation for Long-Tail Legal Text Classification](https://arxiv.org/abs/2508.19997)
*Boheng Mao*

Main category: cs.CL

TL;DR: 提出选择性检索增强(SRA)方法解决法律文本分类中的长尾分布问题，通过仅对低频标签样本进行检索增强，避免引入噪声，无需修改模型架构，在LEDGAR和UNFAIR-ToS数据集上取得了优于现有基准的性能。


<details>
  <summary>Details</summary>
Motivation: 法律文本分类基准数据集通常存在长尾标签分布问题，许多标签样本不足导致模型在稀有类别上表现不佳，需要一种有效的方法来改善长尾分布下的分类性能。

Method: 选择性检索增强(SRA)方法，专注于增强训练集中低频标签的样本，仅从训练数据中进行检索以避免信息泄露，不需要外部语料库，且无需改变模型架构。

Result: 在两个法律文本分类基准数据集(LEDGAR和UNFAIR-ToS)上，SRA方法在micro-F1和macro-F1指标上均优于所有现有的LexGLUE基线方法，在长尾法律文本分类方面实现了持续改进。

Conclusion: SRA方法有效解决了法律文本分类中的长尾分布问题，通过选择性增强低频标签样本而不引入噪声，显著提升了模型在稀有类别上的性能，为长尾法律文本分类提供了有效的解决方案。

Abstract: Legal text classification is a fundamental NLP task in the legal domain.
Benchmark datasets in this area often exhibit a long-tail label distribution,
where many labels are underrepresented, leading to poor model performance on
rare classes. This paper proposes Selective Retrieval-Augmentation (SRA) as a
solution to this problem. SRA focuses on augmenting samples belonging to
low-frequency labels in the training set, preventing the introduction of noise
for well-represented classes, and requires no changes to the model
architecture. Retrieval is performed only from the training data to ensure
there is no potential information leakage, removing the need for external
corpora simultaneously. The proposed SRA method is tested on two legal text
classification benchmark datasets with long-tail distributions: LEDGAR
(single-label) and UNFAIR-ToS (multi-label). The results indicate that SRA
attains higher micro-F1 and macro-F1 scores compared to all current LexGLUE
baselines across both datasets, illustrating consistent improvements in
long-tail legal text classification. The code repository is available at:
https://github.com/Boheng-Mao/sra-legal

</details>


### [56] [DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis](https://arxiv.org/abs/2508.20033)
*Liana Patel,Negar Arabzadeh,Harshit Gupta,Ankita Sundar,Ion Stoica,Matei Zaharia,Carlos Guestrin*

Main category: cs.CL

TL;DR: DeepScholar-bench是一个用于评估生成式研究合成系统的实时基准测试框架，通过从arXiv论文中提取查询任务，评估系统在知识合成、检索质量和可验证性三个维度的表现。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注简短的事实性回答，无法捕捉真实研究合成任务的复杂性和动态性，需要开发新的评估框架来推动生成式研究合成系统的发展。

Method: 从高质量arXiv论文中提取查询任务，构建实时基准测试；开发自动化评估框架，从知识合成、检索质量和可验证性三个维度进行综合评估；实现基于LOTUS API的参考流水线DeepScholar-base。

Result: DeepScholar-base建立了强大的基线性能，在所有评估系统中表现最佳或相当，但所有系统在所有指标上的得分均未超过19%，表明基准测试难度较高且远未饱和。

Conclusion: DeepScholar-bench是一个具有挑战性的基准测试，对于推动能够进行生成式研究合成的AI系统发展具有重要意义，当前系统性能仍有很大提升空间。

Abstract: The ability to research and synthesize knowledge is central to human
expertise and progress. An emerging class of systems promises these exciting
capabilities through generative research synthesis, performing retrieval over
the live web and synthesizing discovered sources into long-form, cited
summaries. However, evaluating such systems remains an open challenge: existing
question-answering benchmarks focus on short-form factual responses, while
expert-curated datasets risk staleness and data contamination. Both fail to
capture the complexity and evolving nature of real research synthesis tasks. In
this work, we introduce DeepScholar-bench, a live benchmark and holistic,
automated evaluation framework designed to evaluate generative research
synthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv
papers and focuses on a real research synthesis task: generating the related
work sections of a paper by retrieving, synthesizing, and citing prior
research. Our evaluation framework holistically assesses performance across
three key dimensions, knowledge synthesis, retrieval quality, and
verifiability. We also develop DeepScholar-base, a reference pipeline
implemented efficiently using the LOTUS API. Using the DeepScholar-bench
framework, we perform a systematic evaluation of prior open-source systems,
search AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that
DeepScholar-base establishes a strong baseline, attaining competitive or higher
performance than each other method. We also find that DeepScholar-bench remains
far from saturated, with no system exceeding a score of $19\%$ across all
metrics. These results underscore the difficulty of DeepScholar-bench, as well
as its importance for progress towards AI systems capable of generative
research synthesis. We make our code available at
https://github.com/guestrin-lab/deepscholar-bench.

</details>


### [57] [Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks](https://arxiv.org/abs/2508.20038)
*Sheng Liu,Qiang Sheng,Danding Wang,Yang Li,Guang Yang,Juan Cao*

Main category: cs.CL

TL;DR: IMAGINE是一个通过嵌入空间分布分析生成越狱指令的合成框架，旨在填补真实越狱模式与安全对齐语料库之间的分布差距，显著降低LLM的攻击成功率而不影响实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在拒绝回答恶意指令方面有所改进，但仍容易受到分布不同于安全对齐语料的越狱攻击，暴露了训练数据与现实攻击之间的分布不匹配问题。

Method: 提出IMAGINE框架，利用嵌入空间分布分析生成越狱类指令，通过迭代优化过程动态演化文本生成分布，增强安全对齐数据分布的覆盖范围。

Result: 基于IMAGINE增强的安全对齐语料库，在Qwen2.5、Llama3.1和Llama3.2上显著降低了攻击成功率，同时保持了模型的实用性。

Conclusion: IMAGINE框架有效解决了LLM安全对齐中的分布不匹配问题，通过合成数据填补分布差距，提高了模型对未知恶意指令的识别能力。

Abstract: Despite advances in improving large language model(LLM) to refuse to answer
malicious instructions, widely used LLMs remain vulnerable to jailbreak attacks
where attackers generate instructions with distributions differing from safety
alignment corpora. New attacks expose LLMs' inability to recognize unseen
malicious instructions, highlighting a critical distributional mismatch between
training data and real-world attacks that forces developers into reactive
patching cycles. To tackle this challenge, we propose IMAGINE, a synthesis
framework that leverages embedding space distribution analysis to generate
jailbreak-like instructions. This approach effectively fills the distributional
gap between authentic jailbreak patterns and safety alignment corpora. IMAGINE
follows an iterative optimization process that dynamically evolves text
generation distributions across iterations, thereby augmenting the coverage of
safety alignment data distributions through synthesized data examples. Based on
the safety-aligned corpus enhanced through IMAGINE, our framework demonstrates
significant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2
without compromising their utility.

</details>


### [58] [AraHealthQA 2025 Shared Task Description Paper](https://arxiv.org/abs/2508.20047)
*Hassan Alhuzali,Farah Shamout,Muhammad Abdul-Mageed,Chaimae Abouzahir,Mouath Abu-Daoud,Ashwag Alasmari,Walid Al-Eisawi,Renad Al-Monef,Ali Alqahtani,Lama Ayash,Nizar Habash,Leen Kharouf*

Main category: cs.CL

TL;DR: AraHealthQA 2025是首个全面的阿拉伯语健康问答共享任务，包含MentalQA（心理健康）和MedArabiQ（综合医疗）两个赛道，旨在解决阿拉伯语医疗QA资源匮乏问题


<details>
  <summary>Details</summary>
Motivation: 解决高质量阿拉伯语医疗问答资源稀缺的问题，促进在现实、多语言和文化敏感的医疗场景下的模型开发

Method: 通过创建两个互补赛道：MentalQA专注于心理健康领域，MedArabiQ覆盖更广泛的医疗领域；每个赛道包含多个子任务、评估数据集和标准化指标

Result: 建立了标准化的评估框架，促进了公平的基准测试，并总结了参与统计、基线系统和整体成果

Conclusion: 该任务为阿拉伯语健康问答领域提供了重要基准，总结了性能趋势并为未来迭代提供了展望

Abstract: We introduce {AraHealthQA 2025}, the {Comprehensive Arabic Health Question
Answering Shared Task}, held in conjunction with {ArabicNLP 2025} (co-located
with EMNLP 2025). This shared task addresses the paucity of high-quality Arabic
medical QA resources by offering two complementary tracks: {MentalQA}, focusing
on Arabic mental health Q\&A (e.g., anxiety, depression, stigma reduction), and
{MedArabiQ}, covering broader medical domains such as internal medicine,
pediatrics, and clinical decision making. Each track comprises multiple
subtasks, evaluation datasets, and standardized metrics, facilitating fair
benchmarking. The task was structured to promote modeling under realistic,
multilingual, and culturally nuanced healthcare contexts. We outline the
dataset creation, task design and evaluation framework, participation
statistics, baseline systems, and summarize the overall outcomes. We conclude
with reflections on the performance trends observed and prospects for future
iterations in Arabic health QA.

</details>


### [59] [11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis](https://arxiv.org/abs/2508.20068)
*Chengzu Li,Wenshan Wu,Huanyu Zhang,Qingtao Li,Zeyu Gao,Yan Xia,José Hernández-Orallo,Ivan Vulić,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出了11Plus-Bench基准测试，系统评估多模态大语言模型的空间推理能力，发现当前MLLMs展现出空间认知的早期迹象，但与人类存在较大性能差距，且实例级表现随机性较强。


<details>
  <summary>Details</summary>
Motivation: 人类认知过程中空间推理和感知紧密相关，但多模态大语言模型在这方面的能力尚未得到充分探索。需要系统评估MLLMs是否具备类人空间认知能力。

Method: 构建11Plus-Bench高质量基准测试，源自真实标准化空间能力测试，包含细粒度专家标注的感知复杂度和推理过程。对14个MLLMs进行广泛实验并与人类表现对比。

Result: 当前MLLMs展现出空间认知的早期迹象，认知努力与推理相关复杂度强相关，类似人类模式。但与人类存在较大性能差距，实例级表现随机，而人类表现高度可预测且受抽象模式复杂度影响。

Conclusion: 研究揭示了当前MLLMs在空间推理方面的新兴能力和局限性，为模型设计提供了可操作的见解，表明需要进一步改进以实现更类人的空间认知能力。

Abstract: For human cognitive process, spatial reasoning and perception are closely
entangled, yet the nature of this interplay remains underexplored in the
evaluation of multimodal large language models (MLLMs). While recent MLLM
advancements show impressive performance on reasoning, their capacity for
human-like spatial cognition remains an open question. In this work, we
introduce a systematic evaluation framework to assess the spatial reasoning
abilities of state-of-the-art MLLMs relative to human performance. Central to
our work is 11Plus-Bench, a high-quality benchmark derived from realistic
standardized spatial aptitude tests. 11Plus-Bench also features fine-grained
expert annotations of both perceptual complexity and reasoning process,
enabling detailed instance-level analysis of model behavior. Through extensive
experiments across 14 MLLMs and human evaluation, we find that current MLLMs
exhibit early signs of spatial cognition. Despite a large performance gap
compared to humans, MLLMs' cognitive profiles resemble those of humans in that
cognitive effort correlates strongly with reasoning-related complexity.
However, instance-level performance in MLLMs remains largely random, whereas
human correctness is highly predictable and shaped by abstract pattern
complexity. These findings highlight both emerging capabilities and limitations
in current MLLMs' spatial reasoning capabilities and provide actionable
insights for advancing model design.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [60] [FakeSV-VLM: Taming VLM for Detecting Fake Short-Video News via Progressive Mixture-Of-Experts Adapter](https://arxiv.org/abs/2508.19639)
*Junxi Wang,Yaxiong Wang,Lechao Cheng,Zhun Zhong*

Main category: cs.MM

TL;DR: FakeSV-VLM是一个基于视觉语言模型的短视频假新闻检测框架，通过混合专家机制和模态对齐检查，在FakeSV和FakeTT数据集上比现有最佳模型提升3.32%和5.02%


<details>
  <summary>Details</summary>
Motivation: 现有假新闻检测方法因缺乏知识验证能力而准确率不足，而大型视觉语言模型从海量多模态数据中吸收了丰富的真实世界知识，适合用于假新闻检测

Method: 提出四专家混合机制处理不同真假组合场景，设计渐进式MoE适配器模块进行分层分析，并开发对齐驱动的事件检查模块捕捉模态间不一致性

Result: 在两个基准数据集FakeSV和FakeTT上显著超越当前最先进模型，分别提升3.32%和5.02%，建立了该领域的新基准

Conclusion: FakeSV-VLM框架通过有效利用VLM的知识能力和多模态分析，为短视频平台的假新闻检测提供了强大且准确的解决方案

Abstract: We present FakeSV-VLM in this paper, a new VLM-based framework for detecting
fake news on short video platforms. Despite significant efforts to combat this
issue due to the severe threat that fake news videos pose to public information
security, existing methods still fall short in detection accuracy, often due to
lack of knowledge to verify the news is real or not. However, large Vision
Language Models (VLMs) have absorbed extensive real-world knowledge from
massive multimodal datasets. Motivated by this, we adapt advanced VLMs for fake
news detection in short videos. Upon close examination of news samples, we
observe that short video samples can be categorized into four distinct
scenarios: both video and text are real (for real samples), or both are fake,
or either the video or text is fake (for fake samples). Inspired by this
insight, we design four experts tailored to handle each scenario and integrate
them into VLM via Mixture of Experts. Specifically, we develop the Progressive
MoE Adapter (PMOE) module where detection experts first provide an initial
analysis, followed by attribution experts for a comprehensive diagnosis,
leading to a robust decision. Additionally, we also note the fake news videos
often show inconsistency between two modalities. Consequently, we further
design the Alignment-driven Event Checking (ADEC) module, which perceives the
fake news by capturing the inconsistency between different modalities.
Extensive experiments on two benchmark datasets, FakeSV and FakeTT, verify the
superiority of our model. It significantly outperforms current state-of-the-art
models by +3.32% and +5.02%, establishing a new benchmark in the field.

</details>


### [61] [ProMSC-MIS: Prompt-based Multimodal Semantic Communication for Multi-Spectral Image Segmentation](https://arxiv.org/abs/2508.20057)
*Haoshuo Zhang,Yufei Bo,Meixia Tao*

Main category: cs.MM

TL;DR: ProMSC-MIS是一个基于提示学习的多模态语义通信框架，用于多光谱图像分割，能够在有限带宽下高效传输RGB和热成像图像，显著降低带宽需求和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 多模态语义通信通过整合跨模态的互补信息来提升下游任务性能，特别是在带宽受限的通道中传输空间对齐的RGB和热成像图像时，需要高效的面向任务的传输方案。

Method: 采用提示学习和对比学习预训练单模态语义编码器，使用一个模态的特征作为另一个模态的提示；设计结合交叉注意力机制和SE网络的语义融合模块来有效融合跨模态特征。

Result: 在相同分割性能下减少50%-70%的带宽需求，存储开销降低26%，计算复杂度降低37%，显著优于传统图像传输结合最先进分割方法。

Conclusion: 该框架在自动驾驶和夜间监控等应用中具有高度适用性，预训练和语义融合策略的有效性通过消融研究得到验证。

Abstract: Multimodal semantic communication has great potential to enhance downstream
task performance by integrating complementary information across modalities.
This paper introduces ProMSC-MIS, a novel Prompt-based Multimodal Semantic
Communication framework for Multi-Spectral Image Segmentation. It enables
efficient task-oriented transmission of spatially aligned RGB and thermal
images over band-limited channels. Our framework has two main design novelties.
First, by leveraging prompt learning and contrastive learning, unimodal
semantic encoders are pre-trained to learn diverse and complementary semantic
representations by using features from one modality as prompts for another.
Second, a semantic fusion module that combines cross-attention mechanism and
squeeze-and-excitation (SE) networks is designed to effectively fuse
cross-modal features. Experimental results demonstrate that ProMSC-MIS
substantially outperforms conventional image transmission combined with
state-of-the-art segmentation methods. Notably, it reduces the required channel
bandwidth by 50%--70% at the same segmentation performance, while also
decreasing the storage overhead and computational complexity by 26% and 37%,
respectively. Ablation studies also validate the effectiveness of the proposed
pre-training and semantic fusion strategies. Our scheme is highly suitable for
applications such as autonomous driving and nighttime surveillance.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [62] [MuSpike: A Benchmark and Evaluation Framework for Symbolic Music Generation with Spiking Neural Networks](https://arxiv.org/abs/2508.19251)
*Qian Liang,Menghaoran Tang,Yi Zeng*

Main category: cs.SD

TL;DR: MuSpike是首个针对脉冲神经网络(SNN)在符号音乐生成领域的统一基准测试和评估框架，系统评估了5种SNN架构在5个数据集上的表现，结合客观指标和大规模听感研究，揭示了客观与主观评估之间的不一致性。


<details>
  <summary>Details</summary>
Motivation: 符号音乐生成在人工神经网络领域进展迅速，但在生物可解释的脉冲神经网络(SNN)领域仍缺乏标准化的基准测试和全面的评估方法。

Method: 引入MuSpike框架，系统评估5种代表性SNN架构(SNN-CNN、SNN-RNN、SNN-LSTM、SNN-GAN和SNN-Transformer)在5个典型数据集上的表现，结合客观指标和主观听感研究，提出新的主观评估指标。

Result: 1)不同SNN模型在不同评估维度上表现各异；2)不同音乐背景的参与者表现出不同的感知模式，专家对AI创作音乐更宽容；3)客观与主观评估存在明显不一致，凸显纯统计指标的局限性。

Conclusion: MuSpike为SNN模型在符号音乐生成领域提供了首个系统性基准和评估框架，为未来生物可解释和认知基础的音乐生成研究奠定了坚实基础。

Abstract: Symbolic music generation has seen rapid progress with artificial neural
networks, yet remains underexplored in the biologically plausible domain of
spiking neural networks (SNNs), where both standardized benchmarks and
comprehensive evaluation methods are lacking. To address this gap, we introduce
MuSpike, a unified benchmark and evaluation framework that systematically
assesses five representative SNN architectures (SNN-CNN, SNN-RNN, SNN-LSTM,
SNN-GAN and SNN-Transformer) across five typical datasets, covering tonal,
structural, emotional, and stylistic variations. MuSpike emphasizes
comprehensive evaluation, combining established objective metrics with a
large-scale listening study. We propose new subjective metrics, targeting
musical impression, autobiographical association, and personal preference, that
capture perceptual dimensions often overlooked in prior work. Results reveal
that (1) different SNN models exhibit distinct strengths across evaluation
dimensions; (2) participants with different musical backgrounds exhibit diverse
perceptual patterns, with experts showing greater tolerance toward AI-composed
music; and (3) a noticeable misalignment exists between objective and
subjective evaluations, highlighting the limitations of purely statistical
metrics and underscoring the value of human perceptual judgment in assessing
musical quality. MuSpike provides the first systematic benchmark and systemic
evaluation framework for SNN models in symbolic music generation, establishing
a solid foundation for future research into biologically plausible and
cognitively grounded music generation.

</details>


### [63] [Beat-Based Rhythm Quantization of MIDI Performances](https://arxiv.org/abs/2508.19262)
*Maximilian Wachter,Sebastian Murgul,Michael Heizmann*

Main category: cs.SD

TL;DR: 一种基于transformer的节奏量化模型，利用节拍和强拍信息将MIDI表演转换为计量对齐的乐谱


<details>
  <summary>Details</summary>
Motivation: 将MIDI表演数据量化为人类可读的乐谱表达，以支持音乐制作和教学应用

Method: 提出基于节拍的预处理方法，将乐谱和表演数据转换为统一的token表示，使用transformer模型进行训练

Result: 在钢琴和吉他表演数据上训练，在MUSTER指标上超越了现有最好方法

Conclusion: 该模型能够有效地将MIDI表演量化为计量对齐的乐谱，为音乐自动记谱提供了效果显著的解决方案

Abstract: We propose a transformer-based rhythm quantization model that incorporates
beat and downbeat information to quantize MIDI performances into
metrically-aligned, human-readable scores. We propose a beat-based
preprocessing method that transfers score and performance data into a unified
token representation. We optimize our model architecture and data
representation and train on piano and guitar performances. Our model exceeds
state-of-the-art performance based on the MUSTER metric.

</details>


### [64] [Infant Cry Detection In Noisy Environment Using Blueprint Separable Convolutions and Time-Frequency Recurrent Neural Network](https://arxiv.org/abs/2508.19308)
*Haolin Yu,Yanxiong Li*

Main category: cs.SD

TL;DR: 轻量级婴儿哭声检测方法，采用蓝图分离卷积网络和时频递归网络，在各种噪声条件下超越现有方法


<details>
  <summary>Details</summary>
Motivation: 婴儿哭声检测是宝宝护理系统的关键组件，需要轻量级且稳健的方法来处理实际环境中的噪声问题

Method: 多尺度卷积递归网络框架，结合蓝图分离卷积降低计算复杂度，时频递归网络进行自适应去噪，使用高效空间注意力机制和对比感知通道注意力模块

Result: 在多种信噪比条件下，该方法在准确率、F1分数和计算复杂度方面超越了许多现有最优方法

Conclusion: 该轻量级婴儿哭声检测方法具有高效性和强镇性，适合实际应用场景，代码已开源

Abstract: Infant cry detection is a crucial component of baby care system. In this
paper, we propose a lightweight and robust method for infant cry detection. The
method leverages blueprint separable convolutions to reduce computational
complexity, and a time-frequency recurrent neural network for adaptive
denoising. The overall framework of the method is structured as a multi-scale
convolutional recurrent neural network, which is enhanced by efficient spatial
attention mechanism and contrast-aware channel attention module, and acquire
local and global information from the input feature of log Mel-spectrogram.
Multiple public datasets are adopted to create a diverse and representative
dataset, and environmental corruption techniques are used to generate the noisy
samples encountered in real-world scenarios. Results show that our method
exceeds many state-of-the-art methods in accuracy, F1-score, and complexity
under various signal-to-noise ratio conditions. The code is at
https://github.com/fhfjsd1/ICD_MMSP.

</details>


### [65] [MQAD: A Large-Scale Question Answering Dataset for Training Music Large Language Models](https://arxiv.org/abs/2508.19514)
*Zhihao Ouyang,Ju-Chiang Wang,Daiyu Zhang,Bin Chen,Shangjie Li,Quan Lin*

Main category: cs.SD

TL;DR: MQAD是一个基于百万歌曲数据集的大规模音乐问答数据集，包含270,000首歌曲的近300万个多样化问题和描述，涵盖节拍、和弦、调性、结构、乐器、流派等丰富音乐特征。


<details>
  <summary>Details</summary>
Motivation: 解决音乐音频理解中缺乏大规模、多样化问答数据集的问题，使机器能够通过自然语言问答方式深入理解音乐内容。

Method: 利用专业音乐信息检索模型提取高层次音乐特征，使用大语言模型生成自然语言问答对，构建多模态LLM整合LLaMA2和Whisper架构。

Result: 在MQAD上训练的模型相比传统音乐音频描述方法取得了显著进步，证明了数据集的有效性。

Conclusion: MQAD为音乐理解和问答任务提供了宝贵资源，推动了音乐AI领域的发展，数据集和代码已开源。

Abstract: Question-answering (QA) is a natural approach for humans to understand a
piece of music audio. However, for machines, accessing a large-scale dataset
covering diverse aspects of music is crucial, yet challenging, due to the
scarcity of publicly available music data of this type. This paper introduces
MQAD, a music QA dataset built on the Million Song Dataset (MSD), encompassing
a rich array of musical features, including beat, chord, key, structure,
instrument, and genre -- across 270,000 tracks, featuring nearly 3 million
diverse questions and captions. MQAD distinguishes itself by offering detailed
time-varying musical information such as chords and sections, enabling
exploration into the inherent structure of music within a song. To compile
MQAD, our methodology leverages specialized Music Information Retrieval (MIR)
models to extract higher-level musical features and Large Language Models
(LLMs) to generate natural language QA pairs. Then, we leverage a multimodal
LLM that integrates the LLaMA2 and Whisper architectures, along with novel
subjective metrics to assess the performance of MQAD. In experiments, our model
trained on MQAD demonstrates advancements over conventional music audio
captioning approaches. The dataset and code are available at
https://github.com/oyzh888/MQAD.

</details>


### [66] [CompLex: Music Theory Lexicon Constructed by Autonomous Agents for Automatic Music Generation](https://arxiv.org/abs/2508.19603)
*Zhejing Hu,Yan Liu,Gong Chen,Bruce X. B. Yu*

Main category: cs.SD

TL;DR: 该论文提出了CompLex自动音乐词典构建模型，通过少量人工输入生成包含37,432个条目的音乐词典，显著提升了文本到音乐生成模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前音乐生成AI在数据可用性方面落后于自然语言处理，需要利用音乐理论知识来提升生成质量，减少传统方法所需的大量人工努力。

Method: 开发了自动音乐词典构建模型，仅需9个手动输入类别关键词和5个句子提示模板，并提出了新的多智能体算法来自动检测和缓解幻觉问题。

Result: CompLex在三个最先进的文本到音乐生成模型（包括符号和音频方法）上都表现出显著的性能提升，同时在完整性、准确性、非冗余性和可执行性方面表现优异。

Conclusion: CompLex证明了利用全面音乐理论构建自动词典的有效性，为AI驱动的音乐生成任务提供了实用的知识增强解决方案。

Abstract: Generative artificial intelligence in music has made significant strides, yet
it still falls short of the substantial achievements seen in natural language
processing, primarily due to the limited availability of music data.
Knowledge-informed approaches have been shown to enhance the performance of
music generation models, even when only a few pieces of musical knowledge are
integrated. This paper seeks to leverage comprehensive music theory in
AI-driven music generation tasks, such as algorithmic composition and style
transfer, which traditionally require significant manual effort with existing
techniques. We introduce a novel automatic music lexicon construction model
that generates a lexicon, named CompLex, comprising 37,432 items derived from
just 9 manually input category keywords and 5 sentence prompt templates. A new
multi-agent algorithm is proposed to automatically detect and mitigate
hallucinations. CompLex demonstrates impressive performance improvements across
three state-of-the-art text-to-music generation models, encompassing both
symbolic and audio-based methods. Furthermore, we evaluate CompLex in terms of
completeness, accuracy, non-redundancy, and executability, confirming that it
possesses the key characteristics of an effective lexicon.

</details>


### [67] [The IRMA Dataset: A Structured Audio-MIDI Corpus for Iranian Classical Music](https://arxiv.org/abs/2508.19876)
*Sepideh Shafiei,Shapour Hakam*

Main category: cs.SD

TL;DR: IRMA数据集是一个多层级、开放获取的伊朗古典音乐计算研究语料库，特别关注radif（伊朗音乐教学和表演中的核心模态旋律单元体系），包含MIDI符号表示、音频-MIDI对齐、音乐学转录和理论信息比较表。


<details>
  <summary>Details</summary>
Motivation: 为伊朗古典音乐的计算研究提供全面的多模态数据集，特别关注radif这一核心音乐传统，支持民族音乐学、教学、文化遗产保护和AI驱动任务的研究。

Method: 采用多阶段构建过程，包括片段标注、对齐方法和结构化标识符编码系统，结合符号MIDI表示、短语级音频-MIDI对齐、PDF格式音乐学转录以及从不同表演者和学者收集的理论信息比较表。

Result: 发布了包含Karimi完整radif、Mirza Abdollah的radif MIDI文件和元数据、Davami声乐radif精选片段，以及20世纪著名声乐家tahrir装饰音音频-MIDI示例的数据集。符号和分析组件采用CC BY-NC 4.0开放许可。

Conclusion: IRMA数据集既是学术档案又是计算分析资源，支持民族音乐学、教学、符号音频研究、文化遗产保护和AI任务（如自动转录和音乐生成）等多种应用，欢迎合作反馈以持续完善和集成到音乐学和机器学习工作流中。

Abstract: We present the IRMA Dataset (Iranian Radif MIDI Audio), a multi-level,
open-access corpus designed for the computational study of Iranian classical
music, with a particular emphasis on the radif, a structured repertoire of
modal-melodic units central to pedagogy and performance. The dataset combines
symbolic MIDI representations, phrase-level audio-MIDI alignment, musicological
transcriptions in PDF format, and comparative tables of theoretical information
curated from a range of performers and scholars. We outline the multi-phase
construction process, including segment annotation, alignment methods, and a
structured system of identifier codes to reference individual musical units.
The current release includes the complete radif of Karimi; MIDI files and
metadata from Mirza Abdollah's radif; selected segments from the vocal radif of
Davami, as transcribed by Payvar and Fereyduni; and a dedicated section
featuring audio-MIDI examples of tahrir ornamentation performed by prominent
20th-century vocalists. While the symbolic and analytical components are
released under an open-access license (CC BY-NC 4.0), some referenced audio
recordings and third-party transcriptions are cited using discographic
information to enable users to locate the original materials independently,
pending copyright permission. Serving both as a scholarly archive and a
resource for computational analysis, this dataset supports applications in
ethnomusicology, pedagogy, symbolic audio research, cultural heritage
preservation, and AI-driven tasks such as automatic transcription and music
generation. We welcome collaboration and feedback to support its ongoing
refinement and broader integration into musicological and machine learning
workflows.

</details>
