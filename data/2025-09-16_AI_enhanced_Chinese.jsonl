{"id": "2509.10566", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.10566", "abs": "https://arxiv.org/abs/2509.10566", "authors": ["Sergio Poo Hernandez", "Vadim Bulitko", "Erin Bayne"], "title": "Combining Audio and Non-Audio Inputs in Evolved Neural Networks for Ovenbird", "comment": null, "summary": "In the last several years the use of neural networks as tools to automate\nspecies classification from digital data has increased. This has been due in\npart to the high classification accuracy of image classification through\nConvolutional Neural Networks (CNN). In the case of audio data CNN based\nrecognizers are used to automate the classification of species in audio\nrecordings by using information from sound visualization (i.e., spectrograms).\nIt is common for these recognizers to use the spectrogram as their sole input.\nHowever, researchers have other non-audio data, such as habitat preferences of\na species, phenology, and range information, available that could improve\nspecies classification. In this paper we present how a single-species\nrecognizer neural network's accuracy can be improved by using non-audio data as\ninputs in addition to spectrogram information. We also analyze if the\nimprovements are merely a result of having a neural network with a higher\nnumber of parameters instead of combining the two inputs. We find that networks\nthat use the two different inputs have a higher classification accuracy than\nnetworks of similar size that use only one of the inputs.", "AI": {"tldr": "\u4f7f\u7528\u975e\u97f3\u9891\u6570\u636e\uff08\u5982\u6816\u606f\u5730\u504f\u597d\u3001\u7269\u5019\u5b66\u548c\u5206\u5e03\u8303\u56f4\u4fe1\u606f\uff09\u4e0e\u58f0\u8c31\u56fe\u7ed3\u5408\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5355\u7269\u79cd\u8bc6\u522b\u795e\u7ecf\u7f51\u7edc\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u4e14\u8fd9\u79cd\u6539\u8fdb\u4e0d\u662f\u5355\u7eaf\u7531\u4e8e\u53c2\u6570\u6570\u91cf\u589e\u52a0\u6240\u81f4\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eCNN\u7684\u7269\u79cd\u5206\u7c7b\u5668\u4e3b\u8981\u4f7f\u7528\u58f0\u8c31\u56fe\u4f5c\u4e3a\u552f\u4e00\u8f93\u5165\uff0c\u4f46\u7814\u7a76\u4eba\u5458\u8fd8\u62e5\u6709\u5176\u4ed6\u975e\u97f3\u9891\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u53ef\u80fd\u6709\u52a9\u4e8e\u63d0\u9ad8\u7269\u79cd\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "method": "\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u540c\u65f6\u4f7f\u7528\u58f0\u8c31\u56fe\u548c\u975e\u97f3\u9891\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u4e0e\u4ec5\u4f7f\u7528\u5355\u4e00\u8f93\u5165\u7684\u76f8\u4f3c\u89c4\u6a21\u7f51\u7edc\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u4f7f\u7528\u4e24\u79cd\u4e0d\u540c\u8f93\u5165\u7684\u7f51\u7edc\u6bd4\u4ec5\u4f7f\u7528\u5355\u4e00\u8f93\u5165\u7684\u76f8\u4f3c\u89c4\u6a21\u7f51\u7edc\u5177\u6709\u66f4\u9ad8\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "\u7ed3\u5408\u58f0\u8c31\u56fe\u548c\u975e\u97f3\u9891\u6570\u636e\u7684\u591a\u6a21\u6001\u8f93\u5165\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u7269\u79cd\u8bc6\u522b\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u4e14\u6027\u80fd\u63d0\u5347\u4e0d\u662f\u5355\u7eaf\u7531\u53c2\u6570\u6570\u91cf\u589e\u52a0\u5e26\u6765\u7684\u3002"}}
{"id": "2509.10781", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.10781", "abs": "https://arxiv.org/abs/2509.10781", "authors": ["Xiaokang Li", "Yicheng Gong", "Dinghao Zou", "Xin Cao", "Sunbowen Lee"], "title": "Emoanti: audio anti-deepfake with refined emotion-guided representations", "comment": null, "summary": "Audio deepfake is so sophisticated that the lack of effective detection\nmethods is fatal. While most detection systems primarily rely on low-level\nacoustic features or pretrained speech representations, they frequently neglect\nhigh-level emotional cues, which can offer complementary and potentially\nanti-deepfake information to enhance generalization. In this work, we propose a\nnovel audio anti-deepfake system that utilizes emotional features (EmoAnti) by\nexploiting a pretrained Wav2Vec2 (W2V2) model fine-tuned on emotion recognition\ntasks, which derives emotion-guided representations, then designing a dedicated\nfeature extractor based on convolutional layers with residual connections to\neffectively capture and refine emotional characteristics from the transformer\nlayers outputs. Experimental results show that our proposed architecture\nachieves state-of-the-art performance on both the ASVspoof2019LA and\nASVspoof2021LA benchmarks, and demonstrates strong generalization on the\nASVspoof2021DF dataset. Our proposed approach's code is available at Anonymous\nGitHub1.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u60c5\u611f\u7279\u5f81\u7684\u97f3\u9891\u53cd\u6df1\u5ea6\u4f2a\u9020\u7cfb\u7edfEmoAnti\uff0c\u5229\u7528\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u5fae\u8c03\u7684Wav2Vec2\u6a21\u578b\u63d0\u53d6\u60c5\u611f\u5f15\u5bfc\u8868\u5f81\uff0c\u7ed3\u5408\u5377\u79ef\u6b8b\u5dee\u7f51\u7edc\u6355\u83b7\u60c5\u611f\u7279\u5f81\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4f4e\u5c42\u58f0\u5b66\u7279\u5f81\u6216\u9884\u8bad\u7ec3\u8bed\u97f3\u8868\u5f81\uff0c\u5ffd\u89c6\u4e86\u9ad8\u5c42\u60c5\u611f\u7ebf\u7d22\u7684\u8865\u5145\u4f5c\u7528\uff0c\u800c\u60c5\u611f\u7279\u5f81\u53ef\u80fd\u63d0\u4f9b\u6297\u4f2a\u9020\u7684\u6cdb\u5316\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u5fae\u8c03\u7684Wav2Vec2\u6a21\u578b\u63d0\u53d6\u60c5\u611f\u5f15\u5bfc\u8868\u5f81\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u5377\u79ef\u5c42\u548c\u6b8b\u5dee\u8fde\u63a5\u7684\u4e13\u7528\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u4ecetransformer\u5c42\u8f93\u51fa\u4e2d\u6355\u83b7\u548c\u7cbe\u70bc\u60c5\u611f\u7279\u5f81\u3002", "result": "\u5728ASVspoof2019LA\u548cASVspoof2021LA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728ASVspoof2021DF\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u60c5\u611f\u7279\u5f81\u4e3a\u97f3\u9891\u53cd\u6df1\u5ea6\u4f2a\u9020\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8865\u5145\u4fe1\u606f\uff0c\u6240\u63d0\u51fa\u7684EmoAnti\u7cfb\u7edf\u5728\u68c0\u6d4b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2509.11124", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.11124", "abs": "https://arxiv.org/abs/2509.11124", "authors": ["Tutti Chi", "Letian Gao", "Yixiao Zhang"], "title": "STASE: A spatialized text-to-audio synthesis engine for music generation", "comment": "Accepted to LLM4Music @ ISMIR 2025", "summary": "While many text-to-audio systems produce monophonic or fixed-stereo outputs,\ngenerating audio with user-defined spatial properties remains a challenge.\nExisting deep learning-based spatialization methods often rely on latent-space\nmanipulations, which can limit direct control over psychoacoustic parameters\ncritical to spatial perception. To address this, we introduce STASE, a system\nthat leverages a Large Language Model (LLM) as an agent to interpret spatial\ncues from text. A key feature of STASE is the decoupling of semantic\ninterpretation from a separate, physics-based spatial rendering engine, which\nfacilitates interpretable and user-controllable spatial reasoning. The LLM\nprocesses prompts through two main pathways: (i) Description Prompts, for\ndirect mapping of explicit spatial information (e.g., \"place the lead guitar at\n45{\\deg} azimuth, 10 m distance\"), and (ii) Abstract Prompts, where a\nRetrieval-Augmented Generation (RAG) module retrieves relevant spatial\ntemplates to inform the rendering. This paper details the STASE workflow,\ndiscusses implementation considerations, and highlights current challenges in\nevaluating generative spatial audio.", "AI": {"tldr": "STASE\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u5230\u7a7a\u95f4\u97f3\u9891\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u79bb\u8bed\u4e49\u89e3\u91ca\u548c\u7269\u7406\u6e32\u67d3\u6765\u5b9e\u73b0\u7528\u6237\u53ef\u63a7\u7684\u7a7a\u95f4\u97f3\u9891\u751f\u6210", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u97f3\u9891\u7cfb\u7edf\u4e3b\u8981\u751f\u6210\u5355\u58f0\u9053\u6216\u56fa\u5b9a\u7acb\u4f53\u58f0\uff0c\u7f3a\u4e4f\u5bf9\u7a7a\u95f4\u5c5e\u6027\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u800c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7a7a\u95f4\u5316\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\uff0c\u9650\u5236\u4e86\u7528\u6237\u5bf9\u7a7a\u95f4\u611f\u77e5\u5173\u952e\u5fc3\u7406\u58f0\u5b66\u53c2\u6570\u7684\u76f4\u63a5\u63a7\u5236", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4ee3\u7406\u89e3\u91ca\u6587\u672c\u4e2d\u7684\u7a7a\u95f4\u7ebf\u7d22\uff0c\u901a\u8fc7\u4e24\u4e2a\u4e3b\u8981\u8def\u5f84\u5904\u7406\u63d0\u793a\uff1a\u63cf\u8ff0\u63d0\u793a\uff08\u76f4\u63a5\u6620\u5c04\u663e\u5f0f\u7a7a\u95f4\u4fe1\u606f\uff09\u548c\u62bd\u8c61\u63d0\u793a\uff08\u4f7f\u7528RAG\u6a21\u5757\u68c0\u7d22\u76f8\u5173\u7a7a\u95f4\u6a21\u677f\uff09\uff0c\u5e76\u5c06\u8bed\u4e49\u89e3\u91ca\u4e0e\u57fa\u4e8e\u7269\u7406\u7684\u7a7a\u95f4\u6e32\u67d3\u5f15\u64ce\u89e3\u8026", "result": "\u63d0\u51fa\u4e86STASE\u7cfb\u7edf\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u548c\u7528\u6237\u53ef\u63a7\u7684\u7a7a\u95f4\u63a8\u7406\uff0c\u80fd\u591f\u5904\u7406\u663e\u5f0f\u548c\u62bd\u8c61\u7684\u7a7a\u95f4\u63cf\u8ff0", "conclusion": "STASE\u901a\u8fc7LLM\u548c\u7269\u7406\u6e32\u67d3\u5f15\u64ce\u7684\u89e3\u8026\u8bbe\u8ba1\uff0c\u4e3a\u751f\u6210\u5f0f\u7a7a\u95f4\u97f3\u9891\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u8bc4\u4f30\u751f\u6210\u7a7a\u95f4\u97f3\u9891\u4ecd\u9762\u4e34\u6311\u6218"}}
{"id": "2509.11128", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11128", "abs": "https://arxiv.org/abs/2509.11128", "authors": ["Yibo Zhang", "Liang Lin"], "title": "ENJ: Optimizing Noise with Genetic Algorithms to Jailbreak LSMs", "comment": null, "summary": "The widespread application of Large Speech Models (LSMs) has made their\nsecurity risks increasingly prominent. Traditional speech adversarial attack\nmethods face challenges in balancing effectiveness and stealth. This paper\nproposes Evolutionary Noise Jailbreak (ENJ), which utilizes a genetic algorithm\nto transform environmental noise from a passive interference into an actively\noptimizable attack carrier for jailbreaking LSMs. Through operations such as\npopulation initialization, crossover fusion, and probabilistic mutation, this\nmethod iteratively evolves a series of audio samples that fuse malicious\ninstructions with background noise. These samples sound like harmless noise to\nhumans but can induce the model to parse and execute harmful commands.\nExtensive experiments on multiple mainstream speech models show that ENJ's\nattack effectiveness is significantly superior to existing baseline methods.\nThis research reveals the dual role of noise in speech security and provides\nnew critical insights for model security defense in complex acoustic\nenvironments.", "AI": {"tldr": "ENJ\u65b9\u6cd5\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u5c06\u73af\u5883\u566a\u58f0\u8f6c\u5316\u4e3a\u53ef\u4f18\u5316\u7684\u653b\u51fb\u8f7d\u4f53\uff0c\u901a\u8fc7\u5728\u566a\u58f0\u4e2d\u878d\u5408\u6076\u610f\u6307\u4ee4\u6765\u52ab\u6301\u5927\u578b\u8bed\u97f3\u6a21\u578b\uff0c\u4eba\u7c7b\u542c\u8d77\u6765\u50cf\u65e0\u5bb3\u566a\u58f0\u4f46\u6a21\u578b\u4f1a\u6267\u884c\u6709\u5bb3\u547d\u4ee4", "motivation": "\u5927\u578b\u8bed\u97f3\u6a21\u578b\u5b89\u5168\u98ce\u9669\u65e5\u76ca\u7a81\u51fa\uff0c\u4f20\u7edf\u8bed\u97f3\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u5728\u6709\u6548\u6027\u548c\u9690\u853d\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9690\u853d\u6709\u6548\u7684\u653b\u51fb\u65b9\u6cd5", "method": "\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u8fdb\u884c\u79cd\u7fa4\u521d\u59cb\u5316\u3001\u4ea4\u53c9\u878d\u5408\u548c\u6982\u7387\u53d8\u5f02\u64cd\u4f5c\uff0c\u8fed\u4ee3\u6f14\u5316\u4e00\u7cfb\u5217\u5c06\u6076\u610f\u6307\u4ee4\u4e0e\u80cc\u666f\u566a\u58f0\u878d\u5408\u7684\u97f3\u9891\u6837\u672c", "result": "\u5728\u591a\u4e2a\u4e3b\u6d41\u8bed\u97f3\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cENJ\u7684\u653b\u51fb\u6548\u679c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u566a\u58f0\u5728\u8bed\u97f3\u5b89\u5168\u4e2d\u7684\u53cc\u91cd\u4f5c\u7528\uff0c\u4e3a\u590d\u6742\u58f0\u5b66\u73af\u5883\u4e0b\u7684\u6a21\u578b\u5b89\u5168\u9632\u5fa1\u63d0\u4f9b\u4e86\u65b0\u7684\u5173\u952e\u89c1\u89e3"}}
{"id": "2509.10546", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10546", "abs": "https://arxiv.org/abs/2509.10546", "authors": ["Gang Cheng", "Haibo Jin", "Wenbin Zhang", "Haohan Wang", "Jun Zhuang"], "title": "Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment", "comment": "Preprint, under review. TL;DR: We propose a multi-turn red-teaming\n  framework, RCA, that reveals critical regulatory vulnerabilities in financial\n  LLMs, achieving over 93% attack success on a proposed new benchmark,\n  FIN-Bench", "summary": "Large Language Models (LLMs) are increasingly integrated into financial\napplications, yet existing red-teaming research primarily targets harmful\ncontent, largely neglecting regulatory risks. In this work, we aim to\ninvestigate the vulnerability of financial LLMs through red-teaming approaches.\nWe introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that\niteratively conceals regulatory risks to provoke seemingly compliant yet\nregulatory-violating responses from LLMs. To enable systematic evaluation, we\nconstruct FIN-Bench, a domain-specific benchmark for assessing LLM safety in\nfinancial contexts. Extensive experiments on FIN-Bench demonstrate that RCA\neffectively bypasses nine mainstream LLMs, achieving an average attack success\nrate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1.\nThese findings reveal a critical gap in current alignment techniques and\nunderscore the urgent need for stronger moderation mechanisms in financial\ndomains. We hope this work offers practical insights for advancing robust and\ndomain-aware LLM alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u98ce\u9669\u9690\u85cf\u653b\u51fb(RCA)\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8f6e\u5bf9\u8bdd\u9690\u85cf\u76d1\u7ba1\u98ce\u9669\uff0c\u6210\u529f\u7ed5\u8fc7\u4e3b\u6d41\u91d1\u878dLLMs\u7684\u5b89\u5168\u9632\u62a4\uff0c\u5e73\u5747\u653b\u51fb\u6210\u529f\u738793.18%\uff0c\u63ed\u793a\u4e86\u91d1\u878d\u9886\u57dfLLM\u5bf9\u9f50\u6280\u672f\u7684\u4e25\u91cd\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709\u7ea2\u961f\u6d4b\u8bd5\u4e3b\u8981\u9488\u5bf9\u6709\u5bb3\u5185\u5bb9\uff0c\u5ffd\u89c6\u4e86\u91d1\u878d\u9886\u57df\u7684\u76d1\u7ba1\u98ce\u9669\u3002\u968f\u7740LLMs\u5728\u91d1\u878d\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u96c6\u6210\uff0c\u9700\u8981\u4e13\u95e8\u8bc4\u4f30\u5176\u5728\u91d1\u878d\u76d1\u7ba1\u5408\u89c4\u65b9\u9762\u7684\u8106\u5f31\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u98ce\u9669\u9690\u85cf\u653b\u51fb(RCA)\u591a\u8f6e\u6846\u67b6\uff0c\u8fed\u4ee3\u5f0f\u9690\u85cf\u76d1\u7ba1\u98ce\u9669\uff1b\u6784\u5efa\u4e86FIN-Bench\u91d1\u878d\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30LLMs\u5728\u91d1\u878d\u573a\u666f\u4e0b\u7684\u5b89\u5168\u6027\u3002", "result": "\u57289\u4e2a\u4e3b\u6d41LLMs\u4e0a\u6d4b\u8bd5\uff0cRCA\u653b\u51fb\u5e73\u5747\u6210\u529f\u738793.18%\uff0c\u5176\u4e2dGPT-4.1\u8fbe\u523098.28%\uff0cOpenAI o1\u8fbe\u523097.56%\uff0c\u8bc1\u660e\u73b0\u6709\u5bf9\u9f50\u6280\u672f\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\u3002", "conclusion": "\u5f53\u524dLLM\u5bf9\u9f50\u6280\u672f\u5728\u91d1\u878d\u9886\u57df\u5b58\u5728\u91cd\u5927\u5b89\u5168\u6f0f\u6d1e\uff0c\u8feb\u5207\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u7684\u9886\u57df\u611f\u77e5\u5ba1\u6838\u673a\u5236\uff0c\u4e3a\u6784\u5efa\u7a33\u5065\u7684\u9886\u57df\u7279\u5b9aLLM\u5bf9\u9f50\u63d0\u4f9b\u5b9e\u8df5\u6d1e\u89c1\u3002"}}
{"id": "2509.10873", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2509.10873", "abs": "https://arxiv.org/abs/2509.10873", "authors": ["Jing Xiao", "Hongfei Liu", "Ruiqi Dong", "Jimin Liu", "Haoyong Yu"], "title": "Automated Radiology Report Generation Based on Topic-Keyword Semantic Guidance", "comment": null, "summary": "Automated radiology report generation is essential in clinical practice.\nHowever, diagnosing radiological images typically requires physicians 5-10\nminutes, resulting in a waste of valuable healthcare resources. Existing\nstudies have not fully leveraged knowledge from historical radiology reports,\nlacking sufficient and accurate prior information. To address this, we propose\na Topic-Keyword Semantic Guidance (TKSG) framework. This framework uses\nBiomedCLIP to accurately retrieve historical similar cases. Supported by\nmultimodal, TKSG accurately detects topic words (disease classifications) and\nkeywords (common symptoms) in diagnoses. The probabilities of topic terms are\naggregated into a topic vector, serving as global information to guide the\nentire decoding process. Additionally, a semantic-guided attention module is\ndesigned to refine local decoding with keyword content, ensuring report\naccuracy and relevance. Experimental results show that our model achieves\nexcellent performance on both IU X-Ray and MIMIC-CXR datasets. The code is\navailable at https://github.com/SCNU203/TKSG.", "AI": {"tldr": "\u63d0\u51faTKSG\u6846\u67b6\uff0c\u5229\u7528BiomedCLIP\u68c0\u7d22\u5386\u53f2\u76f8\u4f3c\u75c5\u4f8b\uff0c\u901a\u8fc7\u4e3b\u9898\u8bcd\u548c\u5173\u952e\u8bcd\u8bed\u4e49\u6307\u5bfc\u63d0\u5347\u653e\u5c04\u62a5\u544a\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u5229\u7528\u5386\u53f2\u653e\u5c04\u62a5\u544a\u77e5\u8bc6\uff0c\u7f3a\u4e4f\u8db3\u591f\u51c6\u786e\u7684\u5148\u9a8c\u4fe1\u606f\uff0c\u5bfc\u81f4\u8bca\u65ad\u6548\u7387\u4f4e\u4e0b\uff0c\u6d6a\u8d39\u533b\u7597\u8d44\u6e90", "method": "\u4f7f\u7528BiomedCLIP\u68c0\u7d22\u76f8\u4f3c\u75c5\u4f8b\uff0c\u591a\u6a21\u6001\u68c0\u6d4b\u4e3b\u9898\u8bcd\uff08\u75be\u75c5\u5206\u7c7b\uff09\u548c\u5173\u952e\u8bcd\uff08\u5e38\u89c1\u75c7\u72b6\uff09\uff0c\u6784\u5efa\u4e3b\u9898\u5411\u91cf\u6307\u5bfc\u5168\u5c40\u89e3\u7801\uff0c\u8bbe\u8ba1\u8bed\u4e49\u5f15\u5bfc\u6ce8\u610f\u529b\u6a21\u5757\u4f18\u5316\u5c40\u90e8\u89e3\u7801", "result": "\u5728IU X-Ray\u548cMIMIC-CXR\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd", "conclusion": "TKSG\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5229\u7528\u5386\u53f2\u62a5\u544a\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653e\u5c04\u62a5\u544a\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u6548\u7387"}}
{"id": "2509.11168", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11168", "abs": "https://arxiv.org/abs/2509.11168", "authors": ["Peihong Zhang", "Yuxuan Liu", "Zhixin Li", "Rui Sang", "Yiqiang Cai", "Yizhou Tan", "Shengchen Li"], "title": "An Entropy-Guided Curriculum Learning Strategy for Data-Efficient Acoustic Scene Classification under Domain Shift", "comment": "Accepted at the Detection and Classification of Acoustic Scenes and\n  Events (DCASE) Workshop 2025", "summary": "Acoustic Scene Classification (ASC) faces challenges in generalizing across\nrecording devices, particularly when labeled data is limited. The DCASE 2024\nChallenge Task 1 highlights this issue by requiring models to learn from small\nlabeled subsets recorded on a few devices. These models need to then generalize\nto recordings from previously unseen devices under strict complexity\nconstraints. While techniques such as data augmentation and the use of\npre-trained models are well-established for improving model generalization,\noptimizing the training strategy represents a complementary yet less-explored\npath that introduces no additional architectural complexity or inference\noverhead. Among various training strategies, curriculum learning offers a\npromising paradigm by structuring the learning process from easier to harder\nexamples. In this work, we propose an entropy-guided curriculum learning\nstrategy to address the domain shift problem in data-efficient ASC.\nSpecifically, we quantify the uncertainty of device domain predictions for each\ntraining sample by computing the Shannon entropy of the device posterior\nprobabilities estimated by an auxiliary domain classifier. Using entropy as a\nproxy for domain invariance, the curriculum begins with high-entropy samples\nand gradually incorporates low-entropy, domain-specific ones to facilitate the\nlearning of generalizable representations. Experimental results on multiple\nDCASE 2024 ASC baselines demonstrate that our strategy effectively mitigates\ndomain shift, particularly under limited labeled data conditions. Our strategy\nis architecture-agnostic and introduces no additional inference cost, making it\neasily integrable into existing ASC baselines and offering a practical solution\nto domain shift.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u71b5\u5f15\u5bfc\u7684\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u4ece\u9ad8\u71b5\uff08\u8bbe\u5907\u65e0\u5173\uff09\u6837\u672c\u5230\u4f4e\u71b5\uff08\u8bbe\u5907\u7279\u5b9a\uff09\u6837\u672c\u7684\u6e10\u8fdb\u5b66\u4e60\uff0c\u89e3\u51b3\u6570\u636e\u9ad8\u6548\u58f0\u5b66\u573a\u666f\u5206\u7c7b\u4e2d\u7684\u57df\u504f\u79fb\u95ee\u9898\uff0c\u65e0\u9700\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u6216\u63a8\u7406\u5f00\u9500\u3002", "motivation": "\u58f0\u5b66\u573a\u666f\u5206\u7c7b\u5728\u8de8\u8bbe\u5907\u6cdb\u5316\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6807\u8bb0\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u6570\u636e\u589e\u5f3a\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u5df2\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u8bad\u7ec3\u7b56\u7565\u4f18\u5316\u8fd9\u4e00\u65e0\u9700\u589e\u52a0\u67b6\u6784\u590d\u6742\u5ea6\u6216\u63a8\u7406\u5f00\u9500\u7684\u8865\u5145\u8def\u5f84\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u8f85\u52a9\u57df\u5206\u7c7b\u5668\u4f30\u8ba1\u6bcf\u4e2a\u8bad\u7ec3\u6837\u672c\u7684\u8bbe\u5907\u540e\u9a8c\u6982\u7387\uff0c\u8ba1\u7b97\u9999\u519c\u71b5\u4f5c\u4e3a\u57df\u4e0d\u53d8\u6027\u7684\u4ee3\u7406\u6307\u6807\u3002\u57fa\u4e8e\u71b5\u503c\u6784\u5efa\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u4ece\u9ad8\u71b5\uff08\u8bbe\u5907\u65e0\u5173\uff09\u6837\u672c\u5f00\u59cb\uff0c\u9010\u6b65\u52a0\u5165\u4f4e\u71b5\uff08\u8bbe\u5907\u7279\u5b9a\uff09\u6837\u672c\u3002", "result": "\u5728\u591a\u4e2aDCASE 2024 ASC\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b56\u7565\u6709\u6548\u7f13\u89e3\u4e86\u57df\u504f\u79fb\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6709\u9650\u6807\u8bb0\u6570\u636e\u6761\u4ef6\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u7b56\u7565\u662f\u67b6\u6784\u65e0\u5173\u7684\uff0c\u4e0d\u5f15\u5165\u989d\u5916\u63a8\u7406\u6210\u672c\uff0c\u53ef\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709ASC\u57fa\u7ebf\u4e2d\uff0c\u4e3a\u57df\u504f\u79fb\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10625", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10625", "abs": "https://arxiv.org/abs/2509.10625", "authors": ["Iv\u00e1n Vicente Moreno Cencerrado", "Arnau Padr\u00e9s Masdemont", "Anton Gonzalvez Hawthorne", "David Demitri Africa", "Lorenzo Pacchiardi"], "title": "No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes", "comment": null, "summary": "Do large language models (LLMs) anticipate when they will answer correctly?\nTo study this, we extract activations after a question is read but before any\ntokens are generated, and train linear probes to predict whether the model's\nforthcoming answer will be correct. Across three open-source model families\nranging from 7 to 70 billion parameters, projections on this \"in-advance\ncorrectness direction\" trained on generic trivia questions predict success in\ndistribution and on diverse out-of-distribution knowledge datasets,\noutperforming black-box baselines and verbalised predicted confidence.\nPredictive power saturates in intermediate layers, suggesting that\nself-assessment emerges mid-computation. Notably, generalisation falters on\nquestions requiring mathematical reasoning. Moreover, for models responding \"I\ndon't know\", doing so strongly correlates with the probe score, indicating that\nthe same direction also captures confidence. By complementing previous results\non truthfulness and other behaviours obtained with probes and sparse\nauto-encoders, our work contributes essential findings to elucidate LLM\ninternals.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u63d0\u53d6LLM\u5728\u56de\u7b54\u95ee\u9898\u524d\u7684\u6fc0\u6d3b\u72b6\u6001\uff0c\u8bad\u7ec3\u7ebf\u6027\u63a2\u9488\u6765\u9884\u6d4b\u6a21\u578b\u5373\u5c06\u7ed9\u51fa\u7684\u7b54\u6848\u662f\u5426\u6b63\u786e\uff0c\u53d1\u73b0\u8fd9\u79cd\"\u63d0\u524d\u6b63\u786e\u6027\u65b9\u5411\"\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u90fd\u5177\u6709\u826f\u597d\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4e14\u5728\u4e2d\u95f4\u5c42\u8fbe\u5230\u6700\u4f73\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u63d0\u524d\u9884\u77e5\u81ea\u5df1\u56de\u7b54\u7684\u6b63\u786e\u6027\uff0c\u63a2\u7d22\u6a21\u578b\u5185\u90e8\u81ea\u6211\u8bc4\u4f30\u673a\u5236\u7684\u5b58\u5728\u548c\u7279\u6027\u3002", "method": "\u5728\u4e09\u4e2a\u5f00\u6e90\u6a21\u578b\u5bb6\u65cf\uff087B-70B\u53c2\u6570\uff09\u4e0a\uff0c\u63d0\u53d6\u95ee\u9898\u8bfb\u53d6\u540e\u4f46\u672a\u751f\u6210\u7b54\u6848\u524d\u7684\u6fc0\u6d3b\u72b6\u6001\uff0c\u8bad\u7ec3\u7ebf\u6027\u63a2\u9488\u6765\u9884\u6d4b\u540e\u7eed\u56de\u7b54\u7684\u6b63\u786e\u6027\uff0c\u5e76\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u77e5\u8bc6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u63d0\u524d\u6b63\u786e\u6027\u65b9\u5411\u5728\u901a\u7528\u77e5\u8bc6\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u9884\u6d4b\u80fd\u529b\u5728\u4e2d\u95f4\u5c42\u9971\u548c\uff0c\u6570\u5b66\u63a8\u7406\u95ee\u9898\u4e0a\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\uff0c\u6a21\u578b\u8bf4\"\u6211\u4e0d\u77e5\u9053\"\u65f6\u4e0e\u63a2\u9488\u5f97\u5206\u9ad8\u5ea6\u76f8\u5173\u3002", "conclusion": "LLM\u5728\u4e2d\u95f4\u8ba1\u7b97\u5c42\u5c31\u5f62\u6210\u4e86\u81ea\u6211\u8bc4\u4f30\u80fd\u529b\uff0c\u76f8\u540c\u7684\u5185\u90e8\u65b9\u5411\u65e2\u6355\u83b7\u6b63\u786e\u6027\u4e5f\u6355\u83b7\u7f6e\u4fe1\u5ea6\uff0c\u4e3a\u7406\u89e3LLM\u5185\u90e8\u673a\u5236\u63d0\u4f9b\u4e86\u91cd\u8981\u53d1\u73b0\u3002"}}
{"id": "2509.11972", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2509.11972", "abs": "https://arxiv.org/abs/2509.11972", "authors": ["Matthias Neugebauer"], "title": "Nagare Media Ingest: A System for Multimedia Ingest Workflows", "comment": null, "summary": "Ingesting multimedia data is usually the first step of multimedia workflows.\nFor this purpose, various streaming protocols have been proposed for live and\nfile-based content. For instance, SRT, RIST, DASH-IF Live Media Ingest Protocol\nand MOQT have been introduced in recent years. At the same time, the number of\nuse cases has only proliferated by the move to cloud- and edge-computing\nenvironments. Multimedia systems now have to handle this complexity in order to\nstay relevant for today's workflows.\n  This technical report discusses implementation details of nagare media\ningest, an open source system for ingesting multimedia data into multimedia\nworkflows. In contrast to existing solutions, nagare media ingest splits up the\nresponsibilities of the ingest process. Users configure multiple concurrently\nrunning components that work together to implement a particular ingest\nworkflow. As such, the design of nagare media ingest allows for great\nflexibility as components can be selected to fit the desired use case.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86nagare media ingest\u5f00\u6e90\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u901a\u8fc7\u5c06\u591a\u5a92\u4f53\u6444\u53d6\u8fc7\u7a0b\u62c6\u5206\u4e3a\u591a\u4e2a\u5e76\u53d1\u8fd0\u884c\u7684\u7ec4\u4ef6\uff0c\u63d0\u4f9b\u4e86\u6bd4\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u66f4\u5927\u7684\u7075\u6d3b\u6027\u3002", "motivation": "\u968f\u7740\u4e91\u548c\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u7684\u666e\u53ca\uff0c\u591a\u5a92\u4f53\u7cfb\u7edf\u9700\u8981\u5904\u7406\u65e5\u76ca\u590d\u6742\u7684\u6d41\u5a92\u4f53\u534f\u8bae\u548c\u7528\u4f8b\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u96be\u4ee5\u6ee1\u8db3\u73b0\u4ee3\u5de5\u4f5c\u6d41\u7a0b\u7684\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u5a92\u4f53\u6444\u53d6\u7cfb\u7edf\uff0c\u5c06\u6444\u53d6\u8fc7\u7a0b\u7684\u8d23\u4efb\u62c6\u5206\u4e3a\u591a\u4e2a\u5e76\u53d1\u8fd0\u884c\u7684\u7ec4\u4ef6\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u914d\u7f6e\u8fd9\u4e9b\u7ec4\u4ef6\u6765\u5b9e\u73b0\u7279\u5b9a\u7684\u6444\u53d6\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "nagare media ingest\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6bd4\u4f20\u7edf\u89e3\u51b3\u65b9\u6848\u66f4\u5927\u7684\u7075\u6d3b\u6027\uff0c\u5141\u8bb8\u7528\u6237\u6839\u636e\u5177\u4f53\u7528\u4f8b\u9009\u62e9\u5408\u9002\u7684\u7ec4\u4ef6\u7ec4\u5408\u3002", "conclusion": "\u901a\u8fc7\u7ec4\u4ef6\u5316\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff0cnagare media ingest\u80fd\u591f\u66f4\u597d\u5730\u9002\u5e94\u73b0\u4ee3\u591a\u5a92\u4f53\u5de5\u4f5c\u6d41\u7a0b\u7684\u590d\u6742\u6027\uff0c\u4e3a\u591a\u5a92\u4f53\u6570\u636e\u6444\u53d6\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.11183", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.11183", "abs": "https://arxiv.org/abs/2509.11183", "authors": ["Emmanouil Karystinaios"], "title": "WeaveMuse: An Open Agentic System for Multimodal Music Understanding and Generation", "comment": "Accepted at Large Language Models for Music & Audio Workshop (LLM4MA)\n  2025", "summary": "Agentic AI has been standardized in industry as a practical paradigm for\ncoordinating specialized models and tools to solve complex multimodal tasks. In\nthis work, we present WeaveMuse, a multi-agent system for music understanding,\nsymbolic composition, and audio synthesis. Each specialist agent interprets\nuser requests, derives machine-actionable requirements (modalities, formats,\nconstraints), and validates its own outputs, while a manager agent selects and\nsequences tools, mediates user interaction, and maintains state across turns.\nThe system is extendable and deployable either locally, using quantization and\ninference strategies to fit diverse hardware budgets, or via the HFApi to\npreserve free community access to open models. Beyond out-of-the-box use, the\nsystem emphasizes controllability and adaptation through constraint schemas,\nstructured decoding, policy-based inference, and parameter-efficient adapters\nor distilled variants that tailor models to MIR tasks. A central design goal is\nto facilitate intermodal interaction across text, symbolic notation and\nvisualization, and audio, enabling analysis-synthesis-render loops and\naddressing cross-format constraints. The framework aims to democratize,\nimplement, and make accessible MIR tools by supporting interchangeable\nopen-source models of various sizes, flexible memory management, and\nreproducible deployment paths.", "AI": {"tldr": "WeaveMuse\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u97f3\u4e50\u7406\u89e3\u3001\u7b26\u53f7\u4f5c\u66f2\u548c\u97f3\u9891\u5408\u6210\uff0c\u901a\u8fc7\u534f\u8c03\u4e13\u4e1a\u4ee3\u7406\u548c\u5de5\u5177\u6765\u89e3\u51b3\u590d\u6742\u7684\u591a\u6a21\u6001\u97f3\u4e50\u4efb\u52a1\u3002", "motivation": "\u5c06\u4ee3\u7406\u5f0fAI\u6807\u51c6\u5316\u4e3a\u534f\u8c03\u4e13\u4e1a\u6a21\u578b\u548c\u5de5\u5177\u89e3\u51b3\u590d\u6742\u591a\u6a21\u6001\u4efb\u52a1\u7684\u5b9e\u7528\u8303\u5f0f\uff0c\u65e8\u5728\u6c11\u4e3b\u5316\u3001\u5b9e\u73b0\u548c\u666e\u53ca\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u5de5\u5177\u3002", "method": "\u91c7\u7528\u591a\u4ee3\u7406\u7cfb\u7edf\u67b6\u6784\uff0c\u5305\u62ec\u4e13\u4e1a\u4ee3\u7406\u89e3\u91ca\u7528\u6237\u8bf7\u6c42\u3001\u63a8\u5bfc\u673a\u5668\u53ef\u64cd\u4f5c\u9700\u6c42\u5e76\u9a8c\u8bc1\u8f93\u51fa\uff0c\u7ba1\u7406\u4ee3\u7406\u9009\u62e9\u6392\u5e8f\u5de5\u5177\u3001\u534f\u8c03\u7528\u6237\u4ea4\u4e92\u548c\u7ef4\u62a4\u72b6\u6001\u3002\u7cfb\u7edf\u652f\u6301\u672c\u5730\u90e8\u7f72\u6216\u901a\u8fc7HFApi\u8bbf\u95ee\uff0c\u5177\u6709\u53ef\u6269\u5c55\u6027\u3001\u7ea6\u675f\u6a21\u5f0f\u3001\u7ed3\u6784\u5316\u89e3\u7801\u548c\u53c2\u6570\u9ad8\u6548\u9002\u914d\u5668\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u652f\u6301\u8de8\u6587\u672c\u3001\u7b26\u53f7\u8bb0\u8c31\u3001\u53ef\u89c6\u5316\u548c\u97f3\u9891\u7684\u591a\u6a21\u6001\u4ea4\u4e92\uff0c\u5b9e\u73b0\u5206\u6790-\u5408\u6210-\u6e32\u67d3\u5faa\u73af\uff0c\u5e76\u5904\u7406\u8de8\u683c\u5f0f\u7ea6\u675f\u3002", "conclusion": "WeaveMuse\u901a\u8fc7\u652f\u6301\u4e0d\u540c\u89c4\u6a21\u7684\u5f00\u6e90\u6a21\u578b\u3001\u7075\u6d3b\u5185\u5b58\u7ba1\u7406\u548c\u53ef\u91cd\u590d\u90e8\u7f72\u8def\u5f84\uff0c\u4e3a\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u5de5\u5177\u63d0\u4f9b\u4e86\u6c11\u4e3b\u5316\u3001\u53ef\u5b9e\u65bd\u548c\u53ef\u8bbf\u95ee\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10644", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10644", "abs": "https://arxiv.org/abs/2509.10644", "authors": ["Enora Rice", "Katharina von der Wense", "Alexis Palmer"], "title": "Interdisciplinary Research in Conversation: A Case Study in Computational Morphology for Language Documentation", "comment": "Accepted to EMNLP 2025", "summary": "Computational morphology has the potential to support language documentation\nthrough tasks like morphological segmentation and the generation of Interlinear\nGlossed Text (IGT). However, our research outputs have seen limited use in\nreal-world language documentation settings. This position paper situates the\ndisconnect between computational morphology and language documentation within a\nbroader misalignment between research and practice in NLP and argues that the\nfield risks becoming decontextualized and ineffectual without systematic\nintegration of User-Centered Design (UCD). To demonstrate how principles from\nUCD can reshape the research agenda, we present a case study of GlossLM, a\nstate-of-the-art multilingual IGT generation model. Through a small-scale user\nstudy with three documentary linguists, we find that despite strong metric\nbased performance, the system fails to meet core usability needs in real\ndocumentation contexts. These insights raise new research questions around\nmodel constraints, label standardization, segmentation, and personalization. We\nargue that centering users not only produces more effective tools, but surfaces\nricher, more relevant research directions", "AI": {"tldr": "\u672c\u6587\u8ba4\u4e3a\u8ba1\u7b97\u5f62\u6001\u5b66\u4e0e\u8bed\u8a00\u6587\u6863\u5b9e\u8df5\u4e4b\u95f4\u5b58\u5728\u8131\u8282\uff0c\u4e3b\u5f20\u901a\u8fc7\u7528\u6237\u4e2d\u5fc3\u8bbe\u8ba1\u6765\u91cd\u65b0\u8c03\u6574\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u901a\u8fc7GlossLM\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5c3d\u7ba1\u6a21\u578b\u6027\u80fd\u6307\u6807\u4f18\u79c0\uff0c\u4f46\u5b9e\u9645\u53ef\u7528\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u8ba1\u7b97\u5f62\u6001\u5b66\u7684\u7814\u7a76\u6210\u679c\u5728\u73b0\u5b9e\u8bed\u8a00\u6587\u6863\u5de5\u4f5c\u4e2d\u5e94\u7528\u6709\u9650\uff0c\u5b58\u5728\u7406\u8bba\u7814\u7a76\u4e0e\u5b9e\u8df5\u9700\u6c42\u4e4b\u95f4\u7684\u8131\u8282\u98ce\u9669\uff0c\u9700\u8981\u901a\u8fc7\u7528\u6237\u4e2d\u5fc3\u8bbe\u8ba1\u6765\u4f7f\u7814\u7a76\u66f4\u52a0\u8d34\u8fd1\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "method": "\u91c7\u7528\u7acb\u573a\u8bba\u6587\u7684\u5f62\u5f0f\uff0c\u7ed3\u5408GlossLM\u591a\u8bed\u8a00IGT\u751f\u6210\u6a21\u578b\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u901a\u8fc7\u5c0f\u89c4\u6a21\u7528\u6237\u7814\u7a76\uff08\u4e09\u4f4d\u6587\u6863\u8bed\u8a00\u5b66\u5bb6\uff09\u6765\u8bc4\u4f30\u7cfb\u7edf\u5728\u5b9e\u9645\u6587\u6863\u73af\u5883\u4e2d\u7684\u53ef\u7528\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5c3d\u7ba1GlossLM\u6a21\u578b\u5728\u6027\u80fd\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u771f\u5b9e\u6587\u6863\u60c5\u5883\u4e2d\u65e0\u6cd5\u6ee1\u8db3\u6838\u5fc3\u53ef\u7528\u6027\u9700\u6c42\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u7ea6\u675f\u3001\u6807\u7b7e\u6807\u51c6\u5316\u3001\u5206\u8bcd\u548c\u4e2a\u6027\u5316\u7b49\u65b9\u9762\u7684\u65b0\u7814\u7a76\u95ee\u9898\u3002", "conclusion": "\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u4e0d\u4ec5\u80fd\u4ea7\u751f\u66f4\u6709\u6548\u7684\u5de5\u5177\uff0c\u8fd8\u80fd\u53d1\u6398\u51fa\u66f4\u4e30\u5bcc\u3001\u66f4\u76f8\u5173\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5efa\u8bae\u7cfb\u7edf\u6027\u5730\u6574\u5408\u7528\u6237\u4e2d\u5fc3\u8bbe\u8ba1\u6765\u91cd\u5851\u8ba1\u7b97\u5f62\u6001\u5b66\u7684\u7814\u7a76\u8bae\u7a0b\u3002"}}
{"id": "2509.12000", "categories": ["cs.MM", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.12000", "abs": "https://arxiv.org/abs/2509.12000", "authors": ["Luca Rossetto", "Klaus Schoeffmann", "Cathal Gurrin", "Jakub Loko\u010d", "Werner Bailer"], "title": "Results of the 2025 Video Browser Showdown", "comment": null, "summary": "This report presents the results of the 14th Video Browser Showdown, held at\nthe 2025 International Conference on Multimedia Modeling on the 8th of January\n2025 in Nara, Japan.", "AI": {"tldr": "\u7b2c14\u5c4a\u89c6\u9891\u6d4f\u89c8\u5668\u5c55\u793a\u4f1a\u7ed3\u679c\u62a5\u544a", "motivation": "\u5c55\u793a\u548c\u8bc4\u4f30\u6700\u65b0\u7684\u89c6\u9891\u68c0\u7d22\u548c\u6d4f\u89c8\u6280\u672f\uff0c\u4fc3\u8fdb\u591a\u5a92\u4f53\u4fe1\u606f\u68c0\u7d22\u9886\u57df\u7684\u53d1\u5c55", "method": "\u901a\u8fc7\u7ade\u8d5b\u5f62\u5f0f\uff0c\u8ba9\u5404\u7814\u7a76\u56e2\u961f\u5c55\u793a\u5176\u89c6\u9891\u6d4f\u89c8\u5668\u7cfb\u7edf\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8868\u73b0", "result": "\u62a5\u544a\u4e862025\u5e741\u67088\u65e5\u5728\u65e5\u672c\u5948\u826f\u4e3e\u884c\u7684\u5c55\u793a\u4f1a\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u5f53\u524d\u89c6\u9891\u68c0\u7d22\u6280\u672f\u7684\u6700\u65b0\u8fdb\u5c55", "conclusion": "\u89c6\u9891\u6d4f\u89c8\u5668\u5c55\u793a\u4f1a\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4ea4\u6d41\u5e73\u53f0\uff0c\u63a8\u52a8\u4e86\u89c6\u9891\u68c0\u7d22\u6280\u672f\u7684\u521b\u65b0\u548c\u53d1\u5c55"}}
{"id": "2509.11241", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.11241", "abs": "https://arxiv.org/abs/2509.11241", "authors": ["Satyajeet Prabhu"], "title": "Revisiting Meter Tracking in Carnatic Music using Deep Learning Approaches", "comment": null, "summary": "Beat and downbeat tracking, jointly referred to as Meter Tracking, is a\nfundamental task in Music Information Retrieval (MIR). Deep learning models\nhave far surpassed traditional signal processing and classical machine learning\napproaches in this domain, particularly for Western (Eurogenetic) genres, where\nlarge annotated datasets are widely available. These systems, however, perform\nless reliably on underrepresented musical traditions. Carnatic music, a rich\ntradition from the Indian subcontinent, is renowned for its rhythmic intricacy\nand unique metrical structures (t\\=alas). The most notable prior work on meter\ntracking in this context employed probabilistic Dynamic Bayesian Networks\n(DBNs). The performance of state-of-the-art (SOTA) deep learning models on\nCarnatic music, however, remains largely unexplored.\n  In this study, we evaluate two models for meter tracking in Carnatic music:\nthe Temporal Convolutional Network (TCN), a lightweight architecture that has\nbeen successfully adapted for Latin rhythms, and Beat This!, a\ntransformer-based model designed for broad stylistic coverage without the need\nfor post-processing. Replicating the experimental setup of the DBN baseline on\nthe Carnatic Music Rhythm (CMR$_f$) dataset, we systematically assess the\nperformance of these models in a directly comparable setting. We further\ninvestigate adaptation strategies, including fine-tuning the models on Carnatic\ndata and the use of musically informed parameters. Results show that while\noff-the-shelf models do not always outperform the DBN, their performance\nimproves substantially with transfer learning, matching or surpassing the\nbaseline. These findings indicate that SOTA deep learning models can be\neffectively adapted to underrepresented traditions, paving the way for more\ninclusive and broadly applicable meter tracking systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e24\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08TCN\u548cBeat This!\uff09\u5728\u5361\u7eb3\u63d0\u514b\u97f3\u4e50\u8282\u62cd\u8ffd\u8e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u8fbe\u5230\u6216\u8d85\u8d8a\u4f20\u7edfDBN\u57fa\u51c6\uff0c\u8bc1\u660e\u4e86\u5148\u8fdb\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u6709\u6548\u9002\u5e94\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u97f3\u4e50\u4f20\u7edf\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u897f\u65b9\u97f3\u4e50\u8282\u62cd\u8ffd\u8e2a\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5361\u7eb3\u63d0\u514b\u97f3\u4e50\u7b49\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u97f3\u4e50\u4f20\u7edf\u4e2d\u6027\u80fd\u4e0d\u4f73\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5148\u8fdb\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5361\u7eb3\u63d0\u514b\u97f3\u4e50\u8282\u62cd\u8ffd\u8e2a\u4efb\u52a1\u4e0a\u7684\u9002\u5e94\u6027\u548c\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528Temporal Convolutional Network (TCN)\u548ctransformer-based\u7684Beat This!\u6a21\u578b\uff0c\u5728CMR_f\u6570\u636e\u96c6\u4e0a\u590d\u73b0DBN\u57fa\u51c6\u7684\u5b9e\u9a8c\u8bbe\u7f6e\uff0c\u8bc4\u4f30\u4e86\u73b0\u6210\u6a21\u578b\u6027\u80fd\u3001\u5fae\u8c03\u7b56\u7565\u548c\u97f3\u4e50\u77e5\u8bc6\u53c2\u6570\u7684\u4f7f\u7528\u3002", "result": "\u73b0\u6210\u6a21\u578b\u5e76\u4e0d\u603b\u662f\u4f18\u4e8eDBN\u57fa\u51c6\uff0c\u4f46\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u540e\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u80fd\u591f\u5339\u914d\u6216\u8d85\u8d8a\u57fa\u51c6\u8868\u73b0\u3002", "conclusion": "\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u9002\u5f53\u7684\u9002\u5e94\u7b56\u7565\u6709\u6548\u5e94\u7528\u4e8e\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u97f3\u4e50\u4f20\u7edf\uff0c\u4e3a\u6784\u5efa\u66f4\u5177\u5305\u5bb9\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u7684\u8282\u62cd\u8ffd\u8e2a\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2509.10663", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10663", "abs": "https://arxiv.org/abs/2509.10663", "authors": ["Zineddine Tighidet", "Andrea Mogini", "Hedi Ben-younes", "Jiali Mei", "Patrick Gallinari", "Benjamin Piwowarski"], "title": "Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts", "comment": "Accepted at EMNLP 2025", "summary": "The behavior of Large Language Models (LLMs) when facing contextual\ninformation that conflicts with their internal parametric knowledge is\ninconsistent, with no generally accepted explanation for the expected outcome\ndistribution. Recent work has identified in autoregressive transformer models a\nclass of neurons -- called entropy neurons -- that produce a significant effect\non the model output entropy while having an overall moderate impact on the\nranking of the predicted tokens. In this paper, we investigate the preliminary\nclaim that these neurons are involved in inhibiting context copying behavior in\ntransformers by looking at their role in resolving conflicts between contextual\nand parametric information. We show that entropy neurons are responsible for\nsuppressing context copying across a range of LLMs, and that ablating them\nleads to a significant change in the generation process. These results enhance\nour understanding of the internal dynamics of LLMs when handling conflicting\ninformation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86LLM\u4e2d\u71b5\u795e\u7ecf\u5143\u5728\u6291\u5236\u4e0a\u4e0b\u6587\u590d\u5236\u884c\u4e3a\u4e2d\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e0e\u53c2\u6570\u77e5\u8bc6\u51b2\u7a81\u65f6\u7684\u673a\u5236\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9762\u5bf9\u4e0e\u5185\u90e8\u53c2\u6570\u77e5\u8bc6\u51b2\u7a81\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u65f6\u884c\u4e3a\u4e0d\u4e00\u81f4\uff0c\u7f3a\u4e4f\u5bf9\u9884\u671f\u7ed3\u679c\u5206\u5e03\u7684\u666e\u904d\u89e3\u91ca\u3002\u6700\u8fd1\u7814\u7a76\u53d1\u73b0\u4e86\u4e00\u7c7b\u71b5\u795e\u7ecf\u5143\uff0c\u5b83\u4eec\u5bf9\u6a21\u578b\u8f93\u51fa\u71b5\u6709\u663e\u8457\u5f71\u54cd\u4f46\u5bf9\u9884\u6d4b\u6807\u8bb0\u6392\u5e8f\u5f71\u54cd\u9002\u4e2d\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u8fd9\u4e9b\u795e\u7ecf\u5143\u5728\u6291\u5236\u4e0a\u4e0b\u6587\u590d\u5236\u884c\u4e3a\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u71b5\u795e\u7ecf\u5143\u5728\u89e3\u51b3\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e0e\u53c2\u6570\u4fe1\u606f\u51b2\u7a81\u4e2d\u7684\u4f5c\u7528\uff0c\u5206\u6790\u8fd9\u4e9b\u795e\u7ecf\u5143\u5728\u4e0d\u540cLLM\u4e2d\u6291\u5236\u4e0a\u4e0b\u6587\u590d\u5236\u884c\u4e3a\u7684\u529f\u80fd\uff0c\u5e76\u91c7\u7528\u795e\u7ecf\u5143\u6d88\u878d\u6280\u672f\u89c2\u5bdf\u751f\u6210\u8fc7\u7a0b\u7684\u53d8\u5316\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u71b5\u795e\u7ecf\u5143\u786e\u5b9e\u8d1f\u8d23\u6291\u5236\u4e0a\u4e0b\u6587\u590d\u5236\u884c\u4e3a\uff0c\u6d88\u878d\u8fd9\u4e9b\u795e\u7ecf\u5143\u4f1a\u5bfc\u81f4\u751f\u6210\u8fc7\u7a0b\u53d1\u751f\u663e\u8457\u53d8\u5316\uff0c\u8bc1\u5b9e\u4e86\u5b83\u4eec\u5728\u5904\u7406\u51b2\u7a81\u4fe1\u606f\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u589e\u5f3a\u4e86\u6211\u4eec\u5bf9LLM\u5728\u5904\u7406\u51b2\u7a81\u4fe1\u606f\u65f6\u5185\u90e8\u52a8\u6001\u673a\u5236\u7684\u7406\u89e3\uff0c\u4e3a\u89e3\u91ca\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u4e0e\u53c2\u6570\u77e5\u8bc6\u51b2\u7a81\u65f6\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.10845", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.10845", "abs": "https://arxiv.org/abs/2509.10845", "authors": ["Liqian Feng", "Lintao Wang", "Kun Hu", "Dehui Kong", "Zhiyong Wang"], "title": "Text2Sign Diffusion: A Generative Approach for Gloss-Free Sign Language Production", "comment": null, "summary": "Sign language production (SLP) aims to translate spoken language sentences\ninto a sequence of pose frames in a sign language, bridging the communication\ngap and promoting digital inclusion for deaf and hard-of-hearing communities.\nExisting methods typically rely on gloss, a symbolic representation of sign\nlanguage words or phrases that serves as an intermediate step in SLP. This\nlimits the flexibility and generalization of SLP, as gloss annotations are\noften unavailable and language-specific. Therefore, we present a novel\ndiffusion-based generative approach - Text2Sign Diffusion (Text2SignDiff) for\ngloss-free SLP. Specifically, a gloss-free latent diffusion model is proposed\nto generate sign language sequences from noisy latent sign codes and spoken\ntext jointly, reducing the potential error accumulation through a\nnon-autoregressive iterative denoising process. We also design a cross-modal\nsigning aligner that learns a shared latent space to bridge visual and textual\ncontent in sign and spoken languages. This alignment supports the conditioned\ndiffusion-based process, enabling more accurate and contextually relevant sign\nlanguage generation without gloss. Extensive experiments on the commonly used\nPHOENIX14T and How2Sign datasets demonstrate the effectiveness of our method,\nachieving the state-of-the-art performance.", "AI": {"tldr": "\u63d0\u51faText2SignDiff\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u514dgloss\u624b\u8bed\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u6f5c\u5728\u6269\u6563\u8fc7\u7a0b\u76f4\u63a5\u4ece\u6587\u672c\u751f\u6210\u624b\u8bed\u5e8f\u5217\uff0c\u5728PHOENIX14T\u548cHow2Sign\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u73b0\u6709\u624b\u8bed\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56gloss\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u4f46gloss\u6807\u6ce8\u7a00\u7f3a\u4e14\u8bed\u8a00\u7279\u5b9a\uff0c\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u5f00\u53d1\u514dgloss\u7684\u76f4\u63a5\u751f\u6210\u65b9\u6cd5", "method": "\u63d0\u51fagloss-free\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u4ece\u566a\u58f0\u6f5c\u5728\u624b\u8bed\u7f16\u7801\u548c\u53e3\u8bed\u6587\u672c\u8054\u5408\u751f\u6210\u624b\u8bed\u5e8f\u5217\uff1b\u8bbe\u8ba1\u8de8\u6a21\u6001\u624b\u8bed\u5bf9\u9f50\u5668\u5b66\u4e60\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff1b\u91c7\u7528\u975e\u81ea\u56de\u5f52\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef", "result": "\u5728PHOENIX14T\u548cHow2Sign\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "Text2SignDiff\u6210\u529f\u5b9e\u73b0\u4e86\u514dgloss\u7684\u624b\u8bed\u751f\u6210\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u6280\u672f\u6709\u6548\u63d0\u5347\u4e86\u624b\u8bed\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027"}}
{"id": "2509.11425", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.11425", "abs": "https://arxiv.org/abs/2509.11425", "authors": ["Md Mubtasim Ahasan", "Rafat Hasan Khan", "Tasnim Mohiuddin", "Aman Chadha", "Tariq Iqbal", "M Ashraful Amin", "Amin Ahsan Ali", "Md Mofijul Islam", "A K M Mahbubur Rahman"], "title": "FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs", "comment": null, "summary": "Speech tokenization enables discrete representation and facilitates speech\nlanguage modeling. However, existing neural codecs capture low-level acoustic\nfeatures, overlooking the semantic and contextual cues inherent to human\nspeech. While recent efforts introduced semantic representations from\nself-supervised speech models or incorporated contextual representations from\npre-trained language models, challenges remain in aligning and unifying the\nsemantic and contextual representations. We introduce FuseCodec, which unifies\nacoustic, semantic, and contextual representations through strong cross-modal\nalignment and globally informed supervision. We propose three complementary\ntechniques: (i) Latent Representation Fusion, integrating semantic and\ncontextual features directly into the encoder latent space for robust and\nunified representation learning; (ii) Global Semantic-Contextual Supervision,\nsupervising discrete tokens with globally pooled and broadcasted\nrepresentations to enhance temporal consistency and cross-modal alignment; and\n(iii) Temporally Aligned Contextual Supervision, strengthening alignment by\ndynamically matching contextual and speech tokens within a local window for\nfine-grained token-level supervision. We further introduce FuseCodec-TTS,\ndemonstrating our methodology's applicability to zero-shot speech synthesis.\nEmpirically, FuseCodec achieves state-of-the-art performance in LibriSpeech,\nsurpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy,\nperceptual quality, intelligibility, and speaker similarity. Results highlight\nthe effectiveness of contextually and semantically guided tokenization for\nspeech tokenization and downstream tasks. Code and pretrained models are\navailable at https://github.com/mubtasimahasan/FuseCodec.", "AI": {"tldr": "FuseCodec\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8bed\u97f3\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u5168\u5c40\u76d1\u7763\uff0c\u6574\u5408\u58f0\u5b66\u3001\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u8868\u793a\uff0c\u5728\u8bed\u97f3\u6807\u8bb0\u5316\u548c\u5408\u6210\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f16\u89e3\u7801\u5668\u4e3b\u8981\u6355\u83b7\u4f4e\u5c42\u6b21\u58f0\u5b66\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u8bed\u97f3\u4e2d\u7684\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u867d\u7136\u8fd1\u671f\u7814\u7a76\u5c1d\u8bd5\u5f15\u5165\u81ea\u76d1\u7763\u8bed\u97f3\u6a21\u578b\u7684\u8bed\u4e49\u8868\u793a\u6216\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u8868\u793a\uff0c\u4f46\u5728\u5bf9\u9f50\u548c\u7edf\u4e00\u8fd9\u4e9b\u8868\u793a\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u4e92\u8865\u6280\u672f\uff1a1)\u6f5c\u5728\u8868\u793a\u878d\u5408\uff0c\u5c06\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u7279\u5f81\u76f4\u63a5\u6574\u5408\u5230\u7f16\u7801\u5668\u6f5c\u5728\u7a7a\u95f4\uff1b2)\u5168\u5c40\u8bed\u4e49-\u4e0a\u4e0b\u6587\u76d1\u7763\uff0c\u7528\u5168\u5c40\u6c60\u5316\u548c\u5e7f\u64ad\u8868\u793a\u76d1\u7763\u79bb\u6563\u6807\u8bb0\uff1b3)\u65f6\u95f4\u5bf9\u9f50\u4e0a\u4e0b\u6587\u76d1\u7763\uff0c\u901a\u8fc7\u5c40\u90e8\u7a97\u53e3\u52a8\u6001\u5339\u914d\u4e0a\u4e0b\u6587\u548c\u8bed\u97f3\u6807\u8bb0\u8fdb\u884c\u7ec6\u7c92\u5ea6\u76d1\u7763\u3002", "result": "\u5728LibriSpeech\u6570\u636e\u96c6\u4e0a\u8d85\u8d8aEnCodec\u3001SpeechTokenizer\u548cDAC\uff0c\u5728\u8f6c\u5f55\u51c6\u786e\u6027\u3001\u611f\u77e5\u8d28\u91cf\u3001\u53ef\u61c2\u5ea6\u548c\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u6027\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002\u8fd8\u5c55\u793a\u4e86\u5728\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u4e0a\u4e0b\u6587\u548c\u8bed\u4e49\u5f15\u5bfc\u7684\u6807\u8bb0\u5316\u65b9\u6cd5\u5728\u8bed\u97f3\u6807\u8bb0\u5316\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8bed\u97f3\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u7edf\u4e00\u6846\u67b6\u3002"}}
{"id": "2509.10685", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10685", "abs": "https://arxiv.org/abs/2509.10685", "authors": ["Jiayou Zhong", "Anudeex Shetty", "Chao Jia", "Xuanrui Lin", "Usman Naseem"], "title": "Pluralistic Alignment for Healthcare: A Role-Driven Framework", "comment": "Accepted to EMNLP 2025 (Main Proceedings)", "summary": "As large language models are increasingly deployed in sensitive domains such\nas healthcare, ensuring their outputs reflect the diverse values and\nperspectives held across populations is critical. However, existing alignment\napproaches, including pluralistic paradigms like Modular Pluralism, often fall\nshort in the health domain, where personal, cultural, and situational factors\nshape pluralism. Motivated by the aforementioned healthcare challenges, we\npropose a first lightweight, generalizable, pluralistic alignment approach,\nEthosAgents, designed to simulate diverse perspectives and values. We\nempirically show that it advances the pluralistic alignment for all three modes\nacross seven varying-sized open and closed models. Our findings reveal that\nhealth-related pluralism demands adaptable and normatively aware approaches,\noffering insights into how these models can better respect diversity in other\nhigh-stakes domains.", "AI": {"tldr": "EthosAgents\uff1a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u6cdb\u5316\u7684\u591a\u5143\u5bf9\u9f50\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u533b\u7597\u7b49\u654f\u611f\u9886\u57df\u6a21\u62df\u591a\u6837\u5316\u4ef7\u503c\u89c2\u548c\u89c6\u89d2\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u5143\u5bf9\u9f50\u6548\u679c", "motivation": "\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\uff08\u5305\u62ec\u6a21\u5757\u5316\u591a\u5143\u4e3b\u4e49\uff09\u5728\u533b\u7597\u9886\u57df\u5b58\u5728\u4e0d\u8db3\uff0c\u533b\u7597\u9886\u57df\u7684\u591a\u5143\u4e3b\u4e49\u53d7\u5230\u4e2a\u4eba\u3001\u6587\u5316\u548c\u60c5\u5883\u56e0\u7d20\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u9002\u5e94\u6027\u7684\u65b9\u6cd5", "method": "\u63d0\u51faEthosAgents\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u591a\u6837\u5316\u89c6\u89d2\u548c\u4ef7\u503c\u89c2\u6765\u5b9e\u73b0\u591a\u5143\u5bf9\u9f50\uff0c\u8be5\u65b9\u6cd5\u8f7b\u91cf\u4e14\u53ef\u6cdb\u5316", "result": "\u57287\u4e2a\u4e0d\u540c\u89c4\u6a21\u7684\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u4e0a\u5b9e\u8bc1\u663e\u793a\uff0cEthosAgents\u5728\u6240\u6709\u4e09\u79cd\u6a21\u5f0f\u4e0b\u90fd\u63d0\u5347\u4e86\u591a\u5143\u5bf9\u9f50\u6548\u679c", "conclusion": "\u533b\u7597\u76f8\u5173\u591a\u5143\u4e3b\u4e49\u9700\u8981\u9002\u5e94\u6027\u5f3a\u4e14\u5177\u6709\u89c4\u8303\u610f\u8bc6\u7684\u65b9\u6cd5\uff0c\u8fd9\u4e00\u53d1\u73b0\u4e3a\u5176\u4ed6\u9ad8\u98ce\u9669\u9886\u57df\u5982\u4f55\u66f4\u597d\u5c0a\u91cd\u591a\u6837\u6027\u63d0\u4f9b\u4e86\u89c1\u89e3"}}
{"id": "2509.11474", "categories": ["cs.SD", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.11474", "abs": "https://arxiv.org/abs/2509.11474", "authors": ["Weilun Xu", "Tianhao Dai", "Oscar Goudet", "Xiaoxuan Wang"], "title": "Acoustic Overspecification in Electronic Dance Music Taxonomy", "comment": "5 pages, 3 figures, conference paper", "summary": "Electronic Dance Music (EDM) classification typically relies on\nindustry-defined taxonomies with numerous subgenres, yet the acoustic basis for\nthese distinctions remains unclear. Current approaches use supervised learning\nwith prescribed genre labels, assuming their validity without systematic\nevaluation. In this paper, we propose an unsupervised approach to discover the\nnatural acoustic structure of EDM independent of commercial labels. Our method\ncombines novel tempogram-based features capturing EDM's layered rhythmic\npatterns with multi-criteria feature selection. To validate that our findings\nreflect genuine acoustic structure rather than methodological artifacts, we\ncompare our results against state-of-the-art pre-trained audio embeddings (MERT\nand CLAP). Both our feature space and embedding representations converge to\n19-23 natural acoustic families compared to the prescribed 35, providing\nconsistent evidence of significant overspecification in current EDM taxonomy by\napproximately one-third.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u65b9\u6cd5\u6765\u53d1\u73b0\u7535\u5b50\u821e\u66f2\u7684\u81ea\u7136\u58f0\u5b66\u7ed3\u6784\uff0c\u53d1\u73b0\u5f53\u524dEDM\u5206\u7c7b\u4f53\u7cfb\u5b58\u5728\u7ea6\u4e09\u5206\u4e4b\u4e00\u7684\u8fc7\u5ea6\u6307\u5b9a\u95ee\u9898", "motivation": "\u7535\u5b50\u821e\u66f2\u5206\u7c7b\u901a\u5e38\u4f9d\u8d56\u884c\u4e1a\u5b9a\u4e49\u7684\u5206\u7c7b\u6cd5\uff0c\u4f46\u8fd9\u4e9b\u5b50\u6d41\u6d3e\u4e4b\u95f4\u7684\u58f0\u5b66\u57fa\u7840\u4e0d\u660e\u786e\uff0c\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u9884\u8bbe\u6807\u7b7e\u7684\u6709\u6548\u6027\u800c\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30", "method": "\u7ed3\u5408\u65b0\u9896\u7684\u57fa\u4e8etempogram\u7684\u7279\u5f81\uff08\u6355\u6349EDM\u7684\u5206\u5c42\u8282\u594f\u6a21\u5f0f\uff09\u548c\u591a\u6807\u51c6\u7279\u5f81\u9009\u62e9\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u4e0e\u6700\u5148\u8fdb\u7684\u9884\u8bad\u7ec3\u97f3\u9891\u5d4c\u5165\uff08MERT\u548cCLAP\uff09\u8fdb\u884c\u6bd4\u8f83\u9a8c\u8bc1", "result": "\u6211\u4eec\u7684\u7279\u5f81\u7a7a\u95f4\u548c\u5d4c\u5165\u8868\u793a\u90fd\u6536\u655b\u523019-23\u4e2a\u81ea\u7136\u58f0\u5b66\u5bb6\u65cf\uff0c\u800c\u9884\u8bbe\u5206\u7c7b\u670935\u4e2a\uff0c\u8868\u660e\u5f53\u524dEDM\u5206\u7c7b\u4f53\u7cfb\u5b58\u5728\u7ea6\u4e09\u5206\u4e4b\u4e00\u7684\u8fc7\u5ea6\u6307\u5b9a", "conclusion": "\u65e0\u76d1\u7763\u65b9\u6cd5\u63ed\u793a\u4e86EDM\u7684\u81ea\u7136\u58f0\u5b66\u7ed3\u6784\uff0c\u8bc1\u660e\u5f53\u524d\u5546\u4e1a\u6807\u7b7e\u4f53\u7cfb\u5b58\u5728\u663e\u8457\u8fc7\u5ea6\u5206\u7c7b\uff0c\u4e3a\u66f4\u5408\u7406\u7684\u97f3\u4e50\u5206\u7c7b\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840"}}
{"id": "2509.10696", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10696", "abs": "https://arxiv.org/abs/2509.10696", "authors": ["Shuaiqi Wang", "Vikas Raunak", "Arturs Backurs", "Victor Reis", "Pei Zhou", "Sihao Chen", "Longqi Yang", "Zinan Lin", "Sergey Yekhanin", "Giulia Fanti"], "title": "Struct-Bench: A Benchmark for Differentially Private Structured Text Generation", "comment": null, "summary": "Differentially private (DP) synthetic data generation is a promising\ntechnique for utilizing private datasets that otherwise cannot be exposed for\nmodel training or other analytics. While much research literature has focused\non generating private unstructured text and image data, in enterprise settings,\nstructured data (e.g., tabular) is more common, often including natural\nlanguage fields or components. Existing synthetic data evaluation techniques\n(e.g., FID) struggle to capture the structural properties and correlations of\nsuch datasets. In this work, we propose Struct-Bench, a framework and benchmark\nfor evaluating synthetic datasets derived from structured datasets that contain\nnatural language data. The Struct-Bench framework requires users to provide a\nrepresentation of their dataset structure as a Context-Free Grammar (CFG). Our\nbenchmark comprises 5 real-world and 2 synthetically generated datasets, each\nannotated with CFGs. We show that these datasets demonstrably present a great\nchallenge even for state-of-the-art DP synthetic data generation methods.\nStruct-Bench also includes reference implementations of different metrics and a\nleaderboard, thereby providing researchers a standardized evaluation platform\nto benchmark and investigate privacy-preserving synthetic data generation\nmethods. Further, we also present a case study showing how to use Struct-Bench\nto improve the synthetic data quality of Private Evolution (PE) on structured\ndata. The benchmark and the leaderboard have been publicly made available at\nhttps://struct-bench.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e86Struct-Bench\u6846\u67b6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5305\u542b\u81ea\u7136\u8bed\u8a00\u7684\u7ed3\u6784\u5316\u6570\u636e\u7684\u5dee\u5206\u9690\u79c1\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u65e0\u5173\u6587\u6cd5\u8868\u793a\u6570\u636e\u7ed3\u6784\uff0c\u5305\u542b5\u4e2a\u771f\u5b9e\u548c2\u4e2a\u5408\u6210\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u7684\u5408\u6210\u6570\u636e\u8bc4\u4f30\u6280\u672f\uff08\u5982FID\uff09\u96be\u4ee5\u6355\u6349\u7ed3\u6784\u5316\u6570\u636e\u96c6\u7684\u5c5e\u6027\u548c\u76f8\u5173\u6027\uff0c\u7279\u522b\u662f\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u5e38\u89c1\u7684\u5305\u542b\u81ea\u7136\u8bed\u8a00\u5b57\u6bb5\u7684\u7ed3\u6784\u5316\u6570\u636e\u3002", "method": "\u5f00\u53d1Struct-Bench\u6846\u67b6\uff0c\u8981\u6c42\u7528\u6237\u4f7f\u7528\u4e0a\u4e0b\u6587\u65e0\u5173\u6587\u6cd5\uff08CFG\uff09\u8868\u793a\u6570\u636e\u96c6\u7ed3\u6784\uff0c\u5305\u542b7\u4e2a\u6807\u6ce8CFG\u7684\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u5e73\u53f0\u548c\u6307\u6807\u5b9e\u73b0\u3002", "result": "\u8fd9\u4e9b\u6570\u636e\u96c6\u5bf9\u6700\u5148\u8fdb\u7684DP\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u6846\u67b6\u63d0\u4f9b\u4e86\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u5982\u4f55\u6539\u8fdb\u79c1\u6709\u8fdb\u5316\uff08PE\uff09\u65b9\u6cd5\u5728\u7ed3\u6784\u5316\u6570\u636e\u4e0a\u7684\u5408\u6210\u8d28\u91cf\u3002", "conclusion": "Struct-Bench\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u5e73\u53f0\uff0c\u53ef\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u548c\u8c03\u67e5\u9690\u79c1\u4fdd\u62a4\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u6846\u67b6\u548c\u6392\u884c\u699c\u5df2\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2509.11606", "categories": ["cs.SD", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.11606", "abs": "https://arxiv.org/abs/2509.11606", "authors": ["Milan Marocchi", "Matthew Fynn", "Kayapanda Mandana", "Yue Rong"], "title": "Scaling to Multimodal and Multichannel Heart Sound Classification: Fine-Tuning Wav2Vec 2.0 with Synthetic and Augmented Biosignals", "comment": "35 pages, 37 figures, 19 tables", "summary": "Cardiovascular diseases (CVDs) are the leading cause of death worldwide,\naccounting for approximately 17.9 million deaths each year. Early detection is\ncritical, creating a demand for accurate and inexpensive pre-screening methods.\nDeep learning has recently been applied to classify abnormal heart sounds\nindicative of CVDs using synchronised phonocardiogram (PCG) and\nelectrocardiogram (ECG) signals, as well as multichannel PCG (mPCG). However,\nstate-of-the-art architectures remain underutilised due to the limited\navailability of synchronised and multichannel datasets. Augmented datasets and\npre-trained models provide a pathway to overcome these limitations, enabling\ntransformer-based architectures to be trained effectively. This work combines\ntraditional signal processing with denoising diffusion models, WaveGrad and\nDiffWave, to create an augmented dataset to fine-tune a Wav2Vec 2.0-based\nclassifier on multimodal and multichannel heart sound datasets. The approach\nachieves state-of-the-art performance. On the Computing in Cardiology (CinC)\n2016 dataset of single channel PCG, accuracy, unweighted average recall (UAR),\nsensitivity, specificity and Matthew's correlation coefficient (MCC) reach\n92.48\\%, 93.05\\%, 93.63\\%, 92.48\\%, 94.93\\% and 0.8283, respectively. Using the\nsynchronised PCG and ECG signals of the training-a dataset from CinC, 93.14\\%,\n92.21\\%, 94.35\\%, 90.10\\%, 95.12\\% and 0.8380 are achieved for accuracy, UAR,\nsensitivity, specificity and MCC, respectively. Using a wearable vest dataset\nconsisting of mPCG data, the model achieves 77.13\\% accuracy, 74.25\\% UAR,\n86.47\\% sensitivity, 62.04\\% specificity, and 0.5082 MCC. These results\ndemonstrate the effectiveness of transformer-based models for CVD detection\nwhen supported by augmented datasets, highlighting their potential to advance\nmultimodal and multichannel heart sound classification.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7ed3\u5408\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u548c\u53bb\u566a\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u8bad\u7ec3\u57fa\u4e8eWav2Vec 2.0\u7684\u5fc3\u810f\u58f0\u97f3\u5206\u7c7b\u5668\uff0c\u5728\u591a\u79cd\u5fc3\u810f\u58f0\u97f3\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u662f\u5168\u7403\u4e3b\u8981\u6b7b\u56e0\uff0c\u65e9\u671f\u68c0\u6d4b\u9700\u6c42\u8feb\u5207\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u540c\u6b65\u548c\u591a\u901a\u9053\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u6027\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u548cWaveGrad\u3001DiffWave\u7b49\u53bb\u566a\u6269\u6563\u6a21\u578b\u521b\u5efa\u589e\u5f3a\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5fae\u8c03\u57fa\u4e8eWav2Vec 2.0\u7684\u5206\u7c7b\u5668\uff0c\u5904\u7406\u591a\u6a21\u6001\u548c\u591a\u901a\u9053\u5fc3\u810f\u58f0\u97f3\u6570\u636e\u3002", "result": "\u5728CinC 2016\u5355\u901a\u9053PCG\u6570\u636e\u96c6\u4e0a\u8fbe\u523092.48%\u51c6\u786e\u7387\uff1b\u5728\u540c\u6b65PCG-ECG\u6570\u636e\u4e0a\u8fbe\u523093.14%\u51c6\u786e\u7387\uff1b\u5728\u591a\u901a\u9053PCG\u53ef\u7a7f\u6234\u80cc\u5fc3\u6570\u636e\u96c6\u4e0a\u8fbe\u523077.13%\u51c6\u786e\u7387\uff0c\u5404\u9879\u6307\u6807\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u57fa\u4e8etransformer\u7684\u6a21\u578b\u5728\u589e\u5f3a\u6570\u636e\u96c6\u652f\u6301\u4e0b\u5bf9\u5fc3\u8840\u7ba1\u75be\u75c5\u68c0\u6d4b\u975e\u5e38\u6709\u6548\uff0c\u5c55\u793a\u4e86\u5728\u591a\u6a21\u6001\u548c\u591a\u901a\u9053\u5fc3\u810f\u58f0\u97f3\u5206\u7c7b\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.10697", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10697", "abs": "https://arxiv.org/abs/2509.10697", "authors": ["Pengcheng Jiang", "Siru Ouyang", "Yizhu Jiao", "Ming Zhong", "Runchu Tian", "Jiawei Han"], "title": "A Survey on Retrieval And Structuring Augmented Generation with Large Language Models", "comment": "KDD'25 survey track", "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nwith their remarkable capabilities in text generation and reasoning. However,\nthese models face critical challenges when deployed in real-world applications,\nincluding hallucination generation, outdated knowledge, and limited domain\nexpertise. Retrieval And Structuring (RAS) Augmented Generation addresses these\nlimitations by integrating dynamic information retrieval with structured\nknowledge representations. This survey (1) examines retrieval mechanisms\nincluding sparse, dense, and hybrid approaches for accessing external\nknowledge; (2) explore text structuring techniques such as taxonomy\nconstruction, hierarchical classification, and information extraction that\ntransform unstructured text into organized representations; and (3) investigate\nhow these structured representations integrate with LLMs through prompt-based\nmethods, reasoning frameworks, and knowledge embedding techniques. It also\nidentifies technical challenges in retrieval efficiency, structure quality, and\nknowledge integration, while highlighting research opportunities in multimodal\nretrieval, cross-lingual structures, and interactive systems. This\ncomprehensive overview provides researchers and practitioners with insights\ninto RAS methods, applications, and future directions.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u63a2\u8ba8\u4e86\u68c0\u7d22\u4e0e\u7ed3\u6784\u5316\u589e\u5f3a\u751f\u6210(RAS)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u52a8\u6001\u4fe1\u606f\u68c0\u7d22\u548c\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\u6765\u89e3\u51b3LLMs\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5e7b\u89c9\u751f\u6210\u3001\u77e5\u8bc6\u8fc7\u65f6\u548c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u6709\u9650\u7b49\u5173\u952e\u6311\u6218\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u9762\u4e34\u5e7b\u89c9\u751f\u6210\u3001\u77e5\u8bc6\u8fc7\u65f6\u548c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u6709\u9650\u7b49\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u5176\u80fd\u529b\u3002", "method": "\u8bba\u6587\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u4e86\uff1a(1)\u68c0\u7d22\u673a\u5236\u5305\u62ec\u7a00\u758f\u3001\u5bc6\u96c6\u548c\u6df7\u5408\u65b9\u6cd5\uff1b(2)\u6587\u672c\u7ed3\u6784\u5316\u6280\u672f\u5982\u5206\u7c7b\u6cd5\u6784\u5efa\u3001\u5c42\u6b21\u5206\u7c7b\u548c\u4fe1\u606f\u63d0\u53d6\uff1b(3)\u7ed3\u6784\u5316\u8868\u793a\u4e0eLLMs\u7684\u96c6\u6210\u65b9\u6cd5\u5305\u62ec\u63d0\u793a\u65b9\u6cd5\u3001\u63a8\u7406\u6846\u67b6\u548c\u77e5\u8bc6\u5d4c\u5165\u6280\u672f\u3002", "result": "\u8bba\u6587\u63d0\u4f9b\u4e86RAS\u65b9\u6cd5\u7684\u5168\u9762\u6982\u8ff0\uff0c\u8bc6\u522b\u4e86\u68c0\u7d22\u6548\u7387\u3001\u7ed3\u6784\u8d28\u91cf\u548c\u77e5\u8bc6\u96c6\u6210\u7b49\u6280\u672f\u6311\u6218\uff0c\u5e76\u7a81\u51fa\u4e86\u591a\u6a21\u6001\u68c0\u7d22\u3001\u8de8\u8bed\u8a00\u7ed3\u6784\u548c\u4ea4\u4e92\u7cfb\u7edf\u7b49\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "RAS\u589e\u5f3a\u751f\u6210\u901a\u8fc7\u6574\u5408\u52a8\u6001\u68c0\u7d22\u548c\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\uff0c\u4e3a\u89e3\u51b3LLMs\u7684\u73b0\u5b9e\u5e94\u7528\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u65b9\u6cd5\u3001\u5e94\u7528\u548c\u672a\u6765\u65b9\u5411\u7684\u6df1\u5165\u89c1\u89e3\u3002"}}
{"id": "2509.11717", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.11717", "abs": "https://arxiv.org/abs/2509.11717", "authors": ["Adhiraj Banerjee", "Vipul Arora"], "title": "Neural Audio Codecs for Prompt-Driven Universal Source Separation", "comment": "21 pages, 1 figure, pre-print, under review", "summary": "Text-guided source separation supports flexible audio editing across media\nand assistive applications, but existing models like AudioSep are too\ncompute-heavy for edge deployment. Neural audio codec (NAC) models such as\nCodecFormer and SDCodec are compute-efficient but limited to fixed-class\nseparation. We introduce CodecSep, the first NAC-based model for on-device\nuniversal, text-driven separation. CodecSep combines DAC compression with a\nTransformer masker modulated by CLAP-derived FiLM parameters. Across six\nopen-domain benchmarks under matched training/prompt protocols,\n\\textbf{CodecSep} surpasses \\textbf{AudioSep} in separation fidelity (SI-SDR)\nwhile remaining competitive in perceptual quality (ViSQOL) and matching or\nexceeding fixed-stem baselines (TDANet, CodecFormer, SDCodec). In code-stream\ndeployments, it needs just 1.35~GMACs end-to-end -- approximately $54\\times$\nless compute ($25\\times$ architecture-only) than spectrogram-domain separators\nlike AudioSep -- while remaining fully bitstream-compatible.", "AI": {"tldr": "CodecSep\u662f\u4e00\u4e2a\u57fa\u4e8e\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u7684\u9ad8\u6548\u6587\u672c\u9a71\u52a8\u97f3\u9891\u5206\u79bb\u6a21\u578b\uff0c\u76f8\u6bd4AudioSep\u8ba1\u7b97\u91cf\u51cf\u5c1154\u500d\uff0c\u5728\u5206\u79bb\u4fdd\u771f\u5ea6\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5f15\u5bfc\u97f3\u9891\u5206\u79bb\u6a21\u578b\uff08\u5982AudioSep\uff09\u8ba1\u7b97\u91cf\u8fc7\u5927\uff0c\u4e0d\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\uff0c\u800c\u73b0\u6709\u7684\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u6a21\u578b\u53ea\u80fd\u8fdb\u884c\u56fa\u5b9a\u7c7b\u522b\u7684\u5206\u79bb\u3002", "method": "\u7ed3\u5408DAC\u538b\u7f29\u6280\u672f\u548c\u57fa\u4e8eTransformer\u7684\u63a9\u7801\u5668\uff0c\u4f7f\u7528CLAP\u5bfc\u51fa\u7684FiLM\u53c2\u6570\u8fdb\u884c\u8c03\u5236\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u6587\u672c\u9a71\u52a8\u97f3\u9891\u5206\u79bb\u3002", "result": "\u5728\u516d\u4e2a\u5f00\u653e\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCodecSep\u5728\u5206\u79bb\u4fdd\u771f\u5ea6\uff08SI-SDR\uff09\u4e0a\u8d85\u8d8aAudioSep\uff0c\u611f\u77e5\u8d28\u91cf\uff08ViSQOL\uff09\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u8ba1\u7b97\u91cf\u4ec5\u97001.35 GMACs\uff0c\u6bd4AudioSep\u51cf\u5c1154\u500d\u3002", "conclusion": "CodecSep\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8e\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u7684\u901a\u7528\u6587\u672c\u9a71\u52a8\u5206\u79bb\uff0c\u5728\u4fdd\u6301\u5b8c\u5168\u6bd4\u7279\u6d41\u517c\u5bb9\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u9700\u6c42\uff0c\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002"}}
{"id": "2509.10708", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10708", "abs": "https://arxiv.org/abs/2509.10708", "authors": ["Iman Barati", "Mostafa Amiri", "Heshaam Faili"], "title": "SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation", "comment": null, "summary": "Supervised Fine-Tuning (SFT) is essential for training large language models\n(LLMs), significantly enhancing critical capabilities such as instruction\nfollowing and in-context learning. Nevertheless, creating suitable training\ndatasets tailored for specific domains remains challenging due to unique domain\nconstraints and data scarcity. In this paper, we propose SearchInstruct, an\ninnovative method explicitly designed to construct high quality instruction\ndatasets for SFT. Our approach begins with a limited set of domain specific,\nhuman generated questions, which are systematically expanded using a large\nlanguage model. Subsequently, domain relevant resources are dynamically\nretrieved to generate accurate and contextually appropriate answers for each\naugmented question. Experimental evaluation demonstrates that SearchInstruct\nenhances both the diversity and quality of SFT datasets, leading to measurable\nimprovements in LLM performance within specialized domains. Additionally, we\nshow that beyond dataset generation, the proposed method can also effectively\nfacilitate tasks such as model editing, enabling efficient updates to existing\nmodels. To facilitate reproducibility and community adoption, we provide full\nimplementation details, the complete set of generated instruction response\npairs, and the source code in a publicly accessible Git repository:\n[https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct)", "AI": {"tldr": "SearchInstruct\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u6307\u4ee4\u6570\u636e\u96c6\u6765\u589e\u5f3a\u76d1\u7763\u5fae\u8c03(SFT)\uff0c\u901a\u8fc7\u6709\u9650\u7684\u4eba\u5de5\u751f\u6210\u95ee\u9898\u6269\u5c55\u548c\u52a8\u6001\u68c0\u7d22\u9886\u57df\u76f8\u5173\u8d44\u6e90\u6765\u751f\u6210\u7b54\u6848\u3002", "motivation": "\u7279\u5b9a\u9886\u57df\u76d1\u7763\u5fae\u8c03\u6570\u636e\u96c6\u7684\u521b\u5efa\u9762\u4e34\u9886\u57df\u7ea6\u675f\u548c\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u9700\u8981\u9ad8\u8d28\u91cf\u6307\u4ee4\u6570\u636e\u96c6\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u7684\u6027\u80fd\u3002", "method": "\u4ece\u5c11\u91cf\u4eba\u5de5\u751f\u6210\u7684\u9886\u57df\u7279\u5b9a\u95ee\u9898\u5f00\u59cb\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u6027\u5730\u6269\u5c55\u95ee\u9898\uff0c\u7136\u540e\u52a8\u6001\u68c0\u7d22\u9886\u57df\u76f8\u5173\u8d44\u6e90\u4e3a\u6bcf\u4e2a\u6269\u5c55\u95ee\u9898\u751f\u6210\u51c6\u786e\u4e14\u4e0a\u4e0b\u6587\u5408\u9002\u7684\u7b54\u6848\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793aSearchInstruct\u63d0\u9ad8\u4e86SFT\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u548c\u8d28\u91cf\uff0c\u5728\u4e13\u4e1a\u9886\u57df\u663e\u8457\u63d0\u5347\u4e86LLM\u6027\u80fd\uff0c\u8fd8\u80fd\u6709\u6548\u4fc3\u8fdb\u6a21\u578b\u7f16\u8f91\u7b49\u4efb\u52a1\u3002", "conclusion": "SearchInstruct\u4e3a\u6784\u5efa\u9ad8\u8d28\u91cf\u6307\u4ee4\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u4e13\u4e1a\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5e76\u5f00\u6e90\u4e86\u5b8c\u6574\u5b9e\u73b0\u548c\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u793e\u533a\u91c7\u7528\u3002"}}
{"id": "2509.11976", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.11976", "abs": "https://arxiv.org/abs/2509.11976", "authors": ["Dinghao Zou", "Yicheng Gong", "Xiaokang Li", "Xin Cao", "Sunbowen Lee"], "title": "PoolingVQ: A VQVAE Variant for Reducing Audio Redundancy and Boosting Multi-Modal Fusion in Music Emotion Analysis", "comment": null, "summary": "Multimodal music emotion analysis leverages audio and MIDI modalities to\nenhance performance. While mainstream approaches focus on complex feature\nextraction networks, we posit that shortening the length of audio sequence\nfeatures to mitigate redundancy, especially in contrast to MIDI's compact\nrepresentation, may effectively boost task performance. To achieve this, we\ndeveloped PoolingVQ by combining Vector Quantized Variational Autoencoder\n(VQVAE) with spatial pooling, which directly compresses audio feature sequences\nthrough local aggregation to reduce redundancy, then devised a two-stage\nco-attention approach to fuse audio and MIDI information. Experimental results\non the public datasets EMOPIA and VGMIDI demonstrate that our multimodal\nframework achieves state-of-the-art overall performance, with PoolingVQ\nyielding some improvement.", "AI": {"tldr": "\u63d0\u51faPoolingVQ\u65b9\u6cd5\uff0c\u901a\u8fc7VQVAE\u548c\u7a7a\u95f4\u6c60\u5316\u538b\u7f29\u97f3\u9891\u7279\u5f81\u5e8f\u5217\u4ee5\u51cf\u5c11\u5197\u4f59\uff0c\u7ed3\u5408\u53cc\u9636\u6bb5\u534f\u540c\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u97f3\u9891\u548cMIDI\u4fe1\u606f\uff0c\u5728\u591a\u6a21\u6001\u97f3\u4e50\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u53d6\u5f97SOTA\u6027\u80fd", "motivation": "\u4e3b\u6d41\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u590d\u6742\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\uff0c\u4f46\u97f3\u9891\u5e8f\u5217\u7279\u5f81\u5b58\u5728\u5197\u4f59\uff0c\u76f8\u6bd4MIDI\u7684\u7d27\u51d1\u8868\u793a\uff0c\u7f29\u77ed\u97f3\u9891\u5e8f\u5217\u957f\u5ea6\u53ef\u80fd\u6709\u6548\u63d0\u5347\u4efb\u52a1\u6027\u80fd", "method": "\u7ed3\u5408VQVAE\u548c\u7a7a\u95f4\u6c60\u5316\u5f00\u53d1PoolingVQ\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u805a\u5408\u76f4\u63a5\u538b\u7f29\u97f3\u9891\u7279\u5f81\u5e8f\u5217\u4ee5\u51cf\u5c11\u5197\u4f59\uff1b\u8bbe\u8ba1\u53cc\u9636\u6bb5\u534f\u540c\u6ce8\u610f\u529b\u673a\u5236\u6765\u878d\u5408\u97f3\u9891\u548cMIDI\u4fe1\u606f", "result": "\u5728EMOPIA\u548cVGMIDI\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u591a\u6a21\u6001\u6846\u67b6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6574\u4f53\u6027\u80fd\uff0cPoolingVQ\u5e26\u6765\u4e86\u4e00\u5b9a\u6539\u8fdb", "conclusion": "\u901a\u8fc7\u538b\u7f29\u97f3\u9891\u7279\u5f81\u5e8f\u5217\u51cf\u5c11\u5197\u4f59\uff0c\u5e76\u7ed3\u5408\u6709\u6548\u7684\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u97f3\u4e50\u60c5\u611f\u5206\u6790\u4efb\u52a1\u7684\u6027\u80fd"}}
{"id": "2509.10737", "categories": ["cs.CL", "cs.LG", "68T50, 68T07", "I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2509.10737", "abs": "https://arxiv.org/abs/2509.10737", "authors": ["Zaur Gouliev", "Jennifer Waters", "Chengqian Wang"], "title": "PolyTruth: Multilingual Disinformation Detection using Transformer-Based Language Models", "comment": "11 pages, 5 figures, 4 tables. Submitted to arXiv in Computation and\n  Language", "summary": "Disinformation spreads rapidly across linguistic boundaries, yet most AI\nmodels are still benchmarked only on English. We address this gap with a\nsystematic comparison of five multilingual transformer models: mBERT, XLM,\nXLM-RoBERTa, RemBERT, and mT5 on a common fake-vs-true machine learning\nclassification task. While transformer-based language models have demonstrated\nnotable success in detecting disinformation in English, their effectiveness in\nmultilingual contexts still remains up for debate. To facilitate evaluation, we\nintroduce PolyTruth Disinfo Corpus, a novel corpus of 60,486 statement pairs\n(false claim vs. factual correction) spanning over twenty five languages that\ncollectively cover five language families and a broad topical range from\npolitics, health, climate, finance, and conspiracy, half of which are\nfact-checked disinformation claims verified by an augmented MindBugs Discovery\ndataset. Our experiments revealed performance variations. Models such as\nRemBERT achieved better overall accuracy, particularly excelling in\nlow-resource languages, whereas models like mBERT and XLM exhibit considerable\nlimitations when training data is scarce. We provide a discussion of these\nperformance patterns and implications for real-world deployment. The dataset is\npublicly available on our GitHub repository to encourage further\nexperimentation and advancement. Our findings illuminate both the potential and\nthe current limitations of AI systems for multilingual disinformation\ndetection.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e865\u79cd\u591a\u8bed\u8a00Transformer\u6a21\u578b\u5728\u5047\u65b0\u95fb\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0RemBERT\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800cmBERT\u548cXLM\u5728\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u65f6\u5b58\u5728\u660e\u663e\u5c40\u9650\u3002", "motivation": "\u865a\u5047\u4fe1\u606f\u8de8\u8bed\u8a00\u4f20\u64ad\u8fc5\u901f\uff0c\u4f46\u5927\u591a\u6570AI\u6a21\u578b\u4ec5\u5728\u82f1\u8bed\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7f3a\u4e4f\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u6709\u6548\u6027\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u65b0\u6784\u5efa\u7684PolyTruth\u865a\u5047\u4fe1\u606f\u8bed\u6599\u5e93\uff0860,486\u4e2a\u58f0\u660e\u5bf9\uff0c\u8986\u76d625\u79cd\u8bed\u8a00\uff09\uff0c\u5bf9mBERT\u3001XLM\u3001XLM-RoBERTa\u3001RemBERT\u548cmT5\u4e94\u79cd\u6a21\u578b\u8fdb\u884c\u5047\u65b0\u95fb\u5206\u7c7b\u4efb\u52a1\u7684\u7cfb\u7edf\u6027\u6bd4\u8f83\u3002", "result": "RemBERT\u6574\u4f53\u51c6\u786e\u7387\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u8868\u73b0\u7a81\u51fa\uff1bmBERT\u548cXLM\u5728\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u65f6\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86AI\u7cfb\u7edf\u5728\u591a\u8bed\u8a00\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u65b9\u9762\u7684\u6f5c\u529b\u548c\u5f53\u524d\u5c40\u9650\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2509.12003", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12003", "abs": "https://arxiv.org/abs/2509.12003", "authors": ["Pierre Serrano", "Rapha\u00ebl Duroselle", "Florian Angulo", "Jean-Fran\u00e7ois Bonastre", "Olivier Boeffard"], "title": "Improving Out-of-Domain Audio Deepfake Detection via Layer Selection and Fusion of SSL-Based Countermeasures", "comment": null, "summary": "Audio deepfake detection systems based on frozen pre-trained self-supervised\nlearning (SSL) encoders show a high level of performance when combined with\nlayer-weighted pooling methods, such as multi-head factorized attentive pooling\n(MHFA). However, they still struggle to generalize to out-of-domain (OOD)\nconditions. We tackle this problem by studying the behavior of six different\npre-trained SSLs, on four different test corpora. We perform a layer-by-layer\nanalysis to determine which layers contribute most. Next, we study the pooling\nhead, comparing a strategy based on a single layer with automatic selection via\nMHFA. We observed that selecting the best layer gave very good results, while\nreducing system parameters by up to 80%. A wide variation in performance as a\nfunction of test corpus and SSL model is also observed, showing that the\npre-training strategy of the encoder plays a role. Finally, score-level fusion\nof several encoders improved generalization to OOD attacks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u51bb\u7ed3\u9884\u8bad\u7ec3\u81ea\u76d1\u7763\u5b66\u4e60\u7f16\u7801\u5668\u7684\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c42\u5206\u6790\u548c\u6c60\u5316\u7b56\u7565\u4f18\u5316\uff0c\u53d1\u73b0\u9009\u62e9\u6700\u4f73\u5355\u5c42\u53ef\u51cf\u5c1180%\u53c2\u6570\u4e14\u6027\u80fd\u826f\u597d\uff0c\u540c\u65f6\u591a\u7f16\u7801\u5668\u878d\u5408\u63d0\u9ad8\u4e86\u5bf9\u57df\u5916\u653b\u51fb\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u51bb\u7ed3\u9884\u8bad\u7ec3SSL\u7f16\u7801\u5668\u7684\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7cfb\u7edf\u867d\u7136\u5728\u7ed3\u5408\u52a0\u6743\u6c60\u5316\u65b9\u6cd5\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9762\u5bf9\u57df\u5916\u6761\u4ef6\u65f6\u6cdb\u5316\u80fd\u529b\u4ecd\u7136\u4e0d\u8db3\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u63d0\u9ad8\u7cfb\u7edf\u5bf9\u672a\u77e5\u653b\u51fb\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u7814\u7a76\u4e866\u79cd\u4e0d\u540c\u7684\u9884\u8bad\u7ec3SSL\u6a21\u578b\u57284\u4e2a\u6d4b\u8bd5\u8bed\u6599\u5e93\u4e0a\u7684\u8868\u73b0\uff0c\u8fdb\u884c\u4e86\u9010\u5c42\u5206\u6790\u786e\u5b9a\u8d21\u732e\u6700\u5927\u7684\u5c42\uff0c\u6bd4\u8f83\u4e86\u5355\u5c42\u9009\u62e9\u7b56\u7565\u548cMHFA\u81ea\u52a8\u9009\u62e9\u7b56\u7565\uff0c\u5e76\u8fdb\u884c\u4e86\u591a\u7f16\u7801\u5668\u7684\u5206\u6570\u7ea7\u878d\u5408\u5b9e\u9a8c\u3002", "result": "\u9009\u62e9\u6700\u4f73\u5355\u5c42\u7684\u7b56\u7565\u5728\u51cf\u5c1180%\u7cfb\u7edf\u53c2\u6570\u7684\u540c\u65f6\u4ecd\u80fd\u83b7\u5f97\u826f\u597d\u6027\u80fd\uff1b\u4e0d\u540c\u6d4b\u8bd5\u8bed\u6599\u5e93\u548cSSL\u6a21\u578b\u95f4\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u8868\u660e\u7f16\u7801\u5668\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u5f88\u91cd\u8981\uff1b\u591a\u7f16\u7801\u5668\u878d\u5408\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u57df\u5916\u653b\u51fb\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u7cbe\u5fc3\u9009\u62e9SSL\u7f16\u7801\u5668\u7684\u7279\u5b9a\u5c42\u548c\u4f7f\u7528\u591a\u7f16\u7801\u5668\u878d\u5408\u7b56\u7565\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7cfb\u7edf\u5728\u57df\u5916\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u6a21\u578b\u53c2\u6570\u3002"}}
{"id": "2509.10739", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10739", "abs": "https://arxiv.org/abs/2509.10739", "authors": ["Mobina Pournemat", "Keivan Rezaei", "Gaurang Sriramanan", "Arman Zarei", "Jiaxiang Fu", "Yang Wang", "Hamid Eghbalzadeh", "Soheil Feizi"], "title": "Reasoning Under Uncertainty: Exploring Probabilistic Reasoning Capabilities of LLMs", "comment": "25 pages, 4 figures, 6 tables", "summary": "Despite widespread success in language understanding and generation, large\nlanguage models (LLMs) exhibit unclear and often inconsistent behavior when\nfaced with tasks that require probabilistic reasoning. In this work, we present\nthe first comprehensive study of the reasoning capabilities of LLMs over\nexplicit discrete probability distributions. Given observations from a\nprobability distribution, we evaluate models on three carefully designed tasks,\nmode identification, maximum likelihood estimation, and sample generation, by\nprompting them to provide responses to queries about either the joint\ndistribution or its conditionals. These tasks thus probe a range of\nprobabilistic skills, including frequency analysis, marginalization, and\ngenerative behavior. Through comprehensive empirical evaluations, we\ndemonstrate that there exists a clear performance gap between smaller and\nlarger models, with the latter demonstrating stronger inference and surprising\ncapabilities in sample generation. Furthermore, our investigations reveal\nnotable limitations, including sensitivity to variations in the notation\nutilized to represent probabilistic outcomes and performance degradation of\nover 60% as context length increases. Together, our results provide a detailed\nunderstanding of the probabilistic reasoning abilities of LLMs and identify key\ndirections for future improvement.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5168\u9762\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u663e\u5f0f\u79bb\u6563\u6982\u7387\u5206\u5e03\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5927\u6a21\u578b\u5728\u6982\u7387\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u5c0f\u6a21\u578b\uff0c\u4f46\u5bf9\u7b26\u53f7\u8868\u793a\u654f\u611f\u4e14\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u5e7f\u6cdb\u6210\u529f\uff0c\u4f46\u5728\u9700\u8981\u6982\u7387\u63a8\u7406\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4e0d\u660e\u786e\u4e14\u4e0d\u4e00\u81f4\u7684\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5176\u6982\u7387\u63a8\u7406\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4efb\u52a1\uff08\u6a21\u5f0f\u8bc6\u522b\u3001\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u548c\u6837\u672c\u751f\u6210\uff09\uff0c\u901a\u8fc7\u63d0\u793a\u6a21\u578b\u63d0\u4f9b\u5173\u4e8e\u8054\u5408\u5206\u5e03\u6216\u5176\u6761\u4ef6\u5206\u5e03\u7684\u54cd\u5e94\uff0c\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u7ed9\u5b9a\u6982\u7387\u5206\u5e03\u89c2\u6d4b\u503c\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u5927\u6a21\u578b\u548c\u5c0f\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5927\u6a21\u578b\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\u548c\u4ee4\u4eba\u60ca\u8bb6\u7684\u6837\u672c\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5bf9\u7b26\u53f7\u8868\u793a\u53d8\u5316\u654f\u611f\uff0c\u4e14\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u6027\u80fd\u4e0b\u964d\u8d85\u8fc760%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63d0\u4f9b\u4e86\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u6982\u7387\u63a8\u7406\u80fd\u529b\u7684\u8be6\u7ec6\u7406\u89e3\uff0c\u5e76\u786e\u5b9a\u4e86\u672a\u6765\u6539\u8fdb\u7684\u5173\u952e\u65b9\u5411\u3002"}}
{"id": "2509.10744", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.11"], "pdf": "https://arxiv.org/pdf/2509.10744", "abs": "https://arxiv.org/abs/2509.10744", "authors": ["Ozan Gokdemir", "Neil Getty", "Robert Underwood", "Sandeep Madireddy", "Franck Cappello", "Arvind Ramanathan", "Ian T. Foster", "Rick L. Stevens"], "title": "Automated MCQA Benchmarking at Scale: Evaluating Reasoning Traces as Retrieval Sources for Domain Adaptation of Small Language Models", "comment": "This manuscript has been accepted for publication at the\n  Supercomputing 25 (SC '25) Conference (Frontiers in Generative AI for HPC\n  Science and Engineering: Foundations, Challenges, and Opportunities Workshop)\n  in St. Louis, MO, USA on November 16th, 2025. It will appear in the SC25\n  Workshop Proceedings after that date", "summary": "As scientific knowledge grows at an unprecedented pace, evaluation benchmarks\nmust evolve to reflect new discoveries and ensure language models are tested on\ncurrent, diverse literature. We propose a scalable, modular framework for\ngenerating multiple-choice question-answering (MCQA) benchmarks directly from\nlarge corpora of scientific papers. Our pipeline automates every stage of MCQA\ncreation, including PDF parsing, semantic chunking, question generation, and\nmodel evaluation. As a case study, we generate more than 16,000 MCQs from\n22,000 open-access articles in radiation and cancer biology. We then evaluate a\nsuite of small language models (1.1B-14B parameters) on these questions,\ncomparing baseline accuracy with retrieval-augmented generation (RAG) from\npaper-derived semantic chunks and from reasoning traces distilled from GPT-4.1.\nWe find that reasoning-trace retrieval consistently improves performance on\nboth synthetic and expert-annotated benchmarks, enabling several small models\nto surpass GPT-4 on the 2023 Astro Radiation and Cancer Biology exam.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ece\u79d1\u5b66\u8bba\u6587\u81ea\u52a8\u751f\u6210\u591a\u9009\u9898\u8bc4\u6d4b\u57fa\u51c6\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u5728\u653e\u5c04\u548c\u764c\u75c7\u751f\u7269\u5b66\u9886\u57df\u751f\u6210\u4e8616,000\u591a\u9053\u9898\u76ee\uff0c\u5e76\u53d1\u73b0\u57fa\u4e8e\u63a8\u7406\u8f68\u8ff9\u7684\u68c0\u7d22\u589e\u5f3a\u80fd\u663e\u8457\u63d0\u5347\u5c0f\u6a21\u578b\u6027\u80fd", "motivation": "\u968f\u7740\u79d1\u5b66\u77e5\u8bc6\u7684\u5feb\u901f\u589e\u957f\uff0c\u9700\u8981\u4e0e\u65f6\u4ff1\u8fdb\u7684\u8bc4\u6d4b\u57fa\u51c6\u6765\u6d4b\u8bd5\u8bed\u8a00\u6a21\u578b\u5bf9\u6700\u65b0\u591a\u6837\u5316\u6587\u732e\u7684\u7406\u89e3\u80fd\u529b", "method": "\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff0c\u5305\u62ecPDF\u89e3\u6790\u3001\u8bed\u4e49\u5206\u5757\u3001\u95ee\u9898\u751f\u6210\u548c\u6a21\u578b\u8bc4\u4f30\uff0c\u4f7f\u752822,000\u7bc7\u5f00\u653e\u83b7\u53d6\u8bba\u6587\u751f\u6210MCQA\u57fa\u51c6\uff0c\u5e76\u6bd4\u8f83\u4e86\u57fa\u7840\u6a21\u578b\u3001\u57fa\u4e8e\u8bba\u6587\u8bed\u4e49\u5757\u7684RAG\u548c\u57fa\u4e8eGPT-4\u63a8\u7406\u8f68\u8ff9\u7684RAG\u65b9\u6cd5", "result": "\u63a8\u7406\u8f68\u8ff9\u68c0\u7d22\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u591a\u4e2a\u5c0f\u6a21\u578b\u57282023\u5e74Astro\u653e\u5c04\u4e0e\u764c\u75c7\u751f\u7269\u5b66\u8003\u8bd5\u4e2d\u8d85\u8d8a\u4e86GPT-4\u7684\u8868\u73b0", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5feb\u901f\u751f\u6210\u5927\u89c4\u6a21\u79d1\u5b66\u8bc4\u6d4b\u57fa\u51c6\uff0c\u57fa\u4e8e\u63a8\u7406\u8f68\u8ff9\u7684\u68c0\u7d22\u589e\u5f3a\u662f\u63d0\u5347\u5c0f\u6a21\u578b\u79d1\u5b66\u95ee\u7b54\u80fd\u529b\u7684\u6709\u6548\u65b9\u6cd5"}}
{"id": "2509.10746", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10746", "abs": "https://arxiv.org/abs/2509.10746", "authors": ["Adarsh Srinivasan", "Jacob Dineen", "Muhammad Umar Afzal", "Muhammad Uzair Sarfraz", "Irbaz B. Riaz", "Ben Zhou"], "title": "RECAP: Transparent Inference-Time Emotion Alignment for Medical Dialogue Systems", "comment": null, "summary": "Large language models in healthcare often miss critical emotional cues,\ndelivering medically sound but emotionally flat advice. This is especially\nproblematic in clinical contexts where patients are distressed and vulnerable,\nand require empathic communication to support safety, adherence, and trust. We\npresent RECAP (Reflect-Extract-Calibrate-Align-Produce), an inference-time\nframework that adds structured emotional reasoning without retraining. By\ndecomposing empathy into transparent appraisal-theoretic stages and exposing\nper-dimension Likert signals, RECAP produces nuanced, auditable responses.\nAcross EmoBench, SECEU, and EQ-Bench, RECAP improves emotional reasoning by\n22-28% on 8B models and 10-13% on larger models over zero-shot baselines.\nClinician evaluations further confirm superior empathetic communication. RECAP\nshows that modular, theory-grounded prompting can systematically enhance\nemotional intelligence in medical AI while preserving the accountability\nrequired for deployment.", "AI": {"tldr": "RECAP\u662f\u4e00\u4e2a\u63a8\u7406\u65f6\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u60c5\u611f\u63a8\u7406\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u533b\u7597\u5927\u8bed\u8a00\u6a21\u578b\u7684\u60c5\u611f\u667a\u80fd\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u6539\u5584\u60c5\u611f\u63a8\u7406\u80fd\u529b", "motivation": "\u533b\u7597\u5927\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u5ffd\u7565\u5173\u952e\u60c5\u611f\u7ebf\u7d22\uff0c\u63d0\u4f9b\u533b\u5b66\u4e0a\u6b63\u786e\u4f46\u60c5\u611f\u5e73\u6de1\u7684\u5efa\u8bae\uff0c\u8fd9\u5728\u60a3\u8005\u5904\u4e8e\u75db\u82e6\u548c\u8106\u5f31\u72b6\u6001\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u5c24\u5176\u6210\u95ee\u9898", "method": "RECAP\uff08Reflect-Extract-Calibrate-Align-Produce\uff09\u6846\u67b6\uff0c\u5c06\u540c\u7406\u5fc3\u5206\u89e3\u4e3a\u900f\u660e\u7684\u8bc4\u4f30\u7406\u8bba\u9636\u6bb5\uff0c\u66b4\u9732\u6bcf\u7ef4\u5ea6\u7684Likert\u4fe1\u53f7\uff0c\u4ea7\u751f\u7ec6\u81f4\u4e14\u53ef\u5ba1\u8ba1\u7684\u54cd\u5e94", "result": "\u5728EmoBench\u3001SECEU\u548cEQ-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRECAP\u57288B\u6a21\u578b\u4e0a\u63d0\u5347\u60c5\u611f\u63a8\u740622-28%\uff0c\u5728\u66f4\u5927\u6a21\u578b\u4e0a\u63d0\u534710-13%\u3002\u4e34\u5e8a\u533b\u751f\u8bc4\u4f30\u8fdb\u4e00\u6b65\u786e\u8ba4\u4e86\u4f18\u8d8a\u7684\u540c\u7406\u5fc3\u6c9f\u901a\u80fd\u529b", "conclusion": "RECAP\u8868\u660e\u6a21\u5757\u5316\u3001\u7406\u8bba\u57fa\u7840\u7684\u63d0\u793a\u5de5\u7a0b\u53ef\u4ee5\u7cfb\u7edf\u6027\u5730\u589e\u5f3a\u533b\u7597AI\u7684\u60c5\u611f\u667a\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u90e8\u7f72\u6240\u9700\u7684\u95ee\u8d23\u6027"}}
{"id": "2509.10798", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10798", "abs": "https://arxiv.org/abs/2509.10798", "authors": ["Yijun Liu", "Yixuan Wang", "Yuzhuang Xu", "Shiyu Ji", "Yang Xu", "Qingfu Zhu", "Wanxiang Che"], "title": "Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction", "comment": "preprint", "summary": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios.", "AI": {"tldr": "\u63d0\u51faJudge Q\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f6f\u4ee4\u724c\u5217\u8868\u8bad\u7ec3\u5d4c\u5165\u5c42\uff0c\u4f7f\u67e5\u8be2\u80fd\u66f4\u597d\u6355\u83b7\u5168\u5c40\u4fe1\u606f\u6765\u8bc4\u4f30KV\u7f13\u5b58\u91cd\u8981\u6027\uff0c\u5728\u76f8\u540c\u7f13\u5b58\u9884\u7b97\u4e0b\u6bd4\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u4e0b\u964d\u66f4\u5c11", "motivation": "\u73b0\u6709KV\u7f13\u5b58\u6dd8\u6c70\u65b9\u6cd5\u8fc7\u5ea6\u5173\u6ce8\u5c40\u90e8\u4fe1\u606f\uff0c\u53ef\u80fd\u5ffd\u7565\u91cd\u8981\u5168\u5c40\u4fe1\u606f\uff0c\u5f71\u54cd\u5185\u5b58\u4f7f\u7528\u548c\u89e3\u7801\u6548\u7387", "method": "\u63d0\u51faJudge Q\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u8f93\u5165\u5e8f\u5217\u672b\u5c3e\u62fc\u63a5\u8f6f\u4ee4\u724c\u5217\u8868\uff0c\u8bad\u7ec3\u8fd9\u4e9b\u4ee4\u724c\u5bf9\u539f\u59cb\u8f93\u5165\u5e8f\u5217\u7684\u6ce8\u610f\u529b\u56fe\u4e0e\u5b9e\u9645\u89e3\u7801\u4ee4\u724c\u5bf9\u9f50\uff0c\u4ec5\u8c03\u4f18\u5d4c\u5165\u5c42", "result": "\u5728Llama-3.1-8B-Instruct\u548cMistral-7B-Instruct-v0.3\u6a21\u578b\u4e0a\u5b9e\u9a8c\uff0cLongBench\u63d0\u5347\u7ea61\u5206\uff0cRULER\u63d0\u5347\u8d85\u8fc73\u5206", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u4ee5\u6700\u5c0f\u8bad\u7ec3\u5f00\u9500\u96c6\u6210\u5230\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u4e2d\uff0c\u63d0\u5347KV\u7f13\u5b58\u6dd8\u6c70\u573a\u666f\u4e0b\u7684\u6027\u80fd"}}
{"id": "2509.10833", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10833", "abs": "https://arxiv.org/abs/2509.10833", "authors": ["Dominic Petrak", "Thy Thy Tran", "Iryna Gurevych"], "title": "Towards Automated Error Discovery: A Study in Conversational AI", "comment": "Accepted to EMNLP 2025 main conference", "summary": "Although LLM-based conversational agents demonstrate strong fluency and\ncoherence, they still produce undesirable behaviors (errors) that are\nchallenging to prevent from reaching users during deployment. Recent research\nleverages large language models (LLMs) to detect errors and guide\nresponse-generation models toward improvement. However, current LLMs struggle\nto identify errors not explicitly specified in their instructions, such as\nthose arising from updates to the response-generation model or shifts in user\nbehavior. In this work, we introduce Automated Error Discovery, a framework for\ndetecting and defining errors in conversational AI, and propose SEEED (Soft\nClustering Extended Encoder-Based Error Detection), as an encoder-based\napproach to its implementation. We enhance the Soft Nearest Neighbor Loss by\namplifying distance weighting for negative samples and introduce Label-Based\nSample Ranking to select highly contrastive examples for better representation\nlearning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 --\nacross multiple error-annotated dialogue datasets, improving the accuracy for\ndetecting unknown errors by up to 8 points and demonstrating strong\ngeneralization to unknown intent detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86Automated Error Discovery\u6846\u67b6\u548cSEEED\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u7684Soft Nearest Neighbor Loss\u548c\u6807\u7b7e\u6837\u672c\u6392\u5e8f\u6765\u68c0\u6d4b\u5bf9\u8bddAI\u4e2d\u7684\u672a\u77e5\u9519\u8bef\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8eGPT-4o\u548cPhi-4\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u5bf9\u8bdd\u4ee3\u7406\u867d\u7136\u6d41\u7545\u8fde\u8d2f\uff0c\u4f46\u4ecd\u4f1a\u4ea7\u751f\u4e0d\u826f\u884c\u4e3a\uff08\u9519\u8bef\uff09\uff0c\u4e14\u73b0\u6709LLM\u96be\u4ee5\u68c0\u6d4b\u6307\u4ee4\u4e2d\u672a\u660e\u786e\u6307\u5b9a\u7684\u9519\u8bef\uff0c\u7279\u522b\u662f\u5f53\u751f\u6210\u6a21\u578b\u66f4\u65b0\u6216\u7528\u6237\u884c\u4e3a\u53d8\u5316\u65f6\u3002", "method": "\u63d0\u51fa\u4e86SEEED\uff08Soft Clustering Extended Encoder-Based Error Detection\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u8d1f\u6837\u672c\u8ddd\u79bb\u52a0\u6743\u7684Soft Nearest Neighbor Loss\u548cLabel-Based Sample Ranking\u6765\u9009\u62e9\u9ad8\u5bf9\u6bd4\u5ea6\u6837\u672c\u8fdb\u884c\u66f4\u597d\u7684\u8868\u793a\u5b66\u4e60\u3002", "result": "SEEED\u5728\u591a\u4e2a\u9519\u8bef\u6807\u6ce8\u5bf9\u8bdd\u6570\u636e\u96c6\u4e0a\u4f18\u4e8eGPT-4o\u548cPhi-4\u7b49\u57fa\u7ebf\u6a21\u578b\uff0c\u68c0\u6d4b\u672a\u77e5\u9519\u8bef\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e868\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u5728\u672a\u77e5\u610f\u56fe\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u548c\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u5bf9\u8bddAI\u4e2d\u7684\u672a\u77e5\u9519\u8bef\uff0c\u5177\u6709\u5f88\u597d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u5bf9\u8bdd\u7cfb\u7edf\u7684\u9519\u8bef\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10843", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10843", "abs": "https://arxiv.org/abs/2509.10843", "authors": ["Can Wang", "Yiqun Chen"], "title": "Evaluating Large Language Models for Evidence-Based Clinical Question Answering", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated substantial progress in\nbiomedical and clinical applications, motivating rigorous evaluation of their\nability to answer nuanced, evidence-based questions. We curate a multi-source\nbenchmark drawing from Cochrane systematic reviews and clinical guidelines,\nincluding structured recommendations from the American Heart Association and\nnarrative guidance used by insurers. Using GPT-4o-mini and GPT-5, we observe\nconsistent performance patterns across sources and clinical domains: accuracy\nis highest on structured guideline recommendations (90%) and lower on narrative\nguideline and systematic review questions (60--70%). We also find a strong\ncorrelation between accuracy and the citation count of the underlying\nsystematic reviews, where each doubling of citations is associated with roughly\na 30% increase in the odds of a correct answer. Models show moderate ability to\nreason about evidence quality when contextual information is supplied. When we\nincorporate retrieval-augmented prompting, providing the gold-source abstract\nraises accuracy on previously incorrect items to 0.79; providing top 3 PubMed\nabstracts (ranked by semantic relevance) improves accuracy to 0.23, while\nrandom abstracts reduce accuracy (0.10, within temperature variation). These\neffects are mirrored in GPT-4o-mini, underscoring that source clarity and\ntargeted retrieval -- not just model size -- drive performance. Overall, our\nresults highlight both the promise and current limitations of LLMs for\nevidence-based clinical question answering. Retrieval-augmented prompting\nemerges as a useful strategy to improve factual accuracy and alignment with\nsource evidence, while stratified evaluation by specialty and question type\nremains essential to understand current knowledge access and to contextualize\nmodel performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5faa\u8bc1\u4e34\u5e8a\u95ee\u7b54\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u7ed3\u6784\u5316\u6307\u5357\u4e0a\u51c6\u786e\u7387\u6700\u9ad8\uff0890%\uff09\uff0c\u5728\u53d9\u8ff0\u6027\u6307\u5357\u548c\u7cfb\u7edf\u8bc4\u4ef7\u95ee\u9898\u4e0a\u8f83\u4f4e\uff0860-70%\uff09\u3002\u68c0\u7d22\u589e\u5f3a\u63d0\u793a\u80fd\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\uff0c\u4f46\u9700\u8981\u7cbe\u51c6\u7684\u68c0\u7d22\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u548c\u4e34\u5e8a\u5e94\u7528\u4e2d\u53d6\u5f97\u8fdb\u5c55\uff0c\u9700\u8981\u4e25\u683c\u8bc4\u4f30\u5176\u56de\u7b54\u57fa\u4e8e\u8bc1\u636e\u7684\u590d\u6742\u4e34\u5e8a\u95ee\u9898\u7684\u80fd\u529b\uff0c\u4ee5\u4e86\u89e3\u5176\u5728\u5faa\u8bc1\u533b\u5b66\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u6765\u81eaCochrane\u7cfb\u7edf\u8bc4\u4ef7\u548c\u4e34\u5e8a\u6307\u5357\u7684\u591a\u6e90\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30GPT-4o-mini\u548cGPT-5\u7684\u8868\u73b0\u3002\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u63d0\u793a\u7b56\u7565\uff0c\u63d0\u4f9b\u91d1\u6807\u51c6\u6458\u8981\u548cPubMed\u76f8\u5173\u6458\u8981\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u6a21\u578b\u51c6\u786e\u7387\u4e0e\u7cfb\u7edf\u8bc4\u4ef7\u5f15\u7528\u6b21\u6570\u5f3a\u76f8\u5173\uff08\u5f15\u7528\u6570\u7ffb\u500d\uff0c\u6b63\u786e\u56de\u7b54\u51e0\u7387\u589e\u52a0\u7ea630%\uff09\u3002\u63d0\u4f9b\u91d1\u6807\u51c6\u6458\u8981\u53ef\u5c06\u51c6\u786e\u7387\u63d0\u5347\u81f30.79\uff0c\u800c\u63d0\u4f9b\u76f8\u5173PubMed\u6458\u8981\u4ec5\u63d0\u5347\u81f30.23\uff0c\u968f\u673a\u6458\u8981\u53cd\u800c\u964d\u4f4e\u51c6\u786e\u7387\u3002", "conclusion": "LLM\u5728\u5faa\u8bc1\u4e34\u5e8a\u95ee\u7b54\u4e2d\u65e2\u6709\u6f5c\u529b\u4e5f\u6709\u5c40\u9650\uff0c\u68c0\u7d22\u589e\u5f3a\u63d0\u793a\u662f\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u6709\u6548\u7b56\u7565\uff0c\u4f46\u9700\u8981\u9488\u5bf9\u6027\u68c0\u7d22\u548c\u6309\u4e13\u4e1a\u9886\u57df\u5206\u5c42\u8bc4\u4f30\u6765\u7406\u89e3\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2509.10844", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10844", "abs": "https://arxiv.org/abs/2509.10844", "authors": ["Yixuan Tang", "Yi Yang"], "title": "GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings", "comment": "https://github.com/yixuantt/GAPrune", "summary": "Domain-specific embedding models have shown promise for applications that\nrequire specialized semantic understanding, such as coding agents and financial\nretrieval systems, often achieving higher performance gains than general\nmodels. However, state-of-the-art embedding models are typically based on LLMs,\nwhich contain billions of parameters, making deployment challenging in\nresource-constrained environments. Model compression through pruning offers a\npromising solution, but existing pruning methods treat all parameters\nuniformly, failing to distinguish between general semantic representations and\ndomain-specific patterns, leading to suboptimal pruning decisions. Thus, we\npropose GAPrune, a pruning framework that addresses this challenge by\nconsidering both domain importance and preserving general linguistic\nfoundation. Our method uses Fisher Information to measure importance and\ngeneral-domain gradient alignment to assess parameter behavior, then combines\nthese signals using our Domain Alignment Importance (DAI) scoring. Lower DAI\nscores indicate that the parameter is either less important for the domain task\nor creates conflicts between domain and general objectives. Experiments on two\ndomain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance\nwithin 2.5% of dense models in one-shot pruning at 50% sparsity, while\noutperforming all baselines. With retraining in 100 steps, GAPrune achieves\n+4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our\npruning strategy not only preserves but enhances domain-specific capabilities.\nOur findings demonstrate that principled pruning strategies can achieve model\ncompression and enhanced domain specialization, providing the research\ncommunity with a new approach for development.", "AI": {"tldr": "GAPrune\u662f\u4e00\u4e2a\u9488\u5bf9\u9886\u57df\u7279\u5b9a\u5d4c\u5165\u6a21\u578b\u7684\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u8003\u8651\u9886\u57df\u91cd\u8981\u6027\u548c\u4fdd\u6301\u901a\u7528\u8bed\u8a00\u57fa\u7840\uff0c\u572850%\u7a00\u758f\u5ea6\u4e0b\u6027\u80fd\u635f\u5931\u5c0f\u4e8e2.5%\uff0c\u91cd\u8bad\u7ec3\u540e\u8fd8\u80fd\u63d0\u5347\u9886\u57df\u6027\u80fd", "motivation": "\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u5bf9\u6240\u6709\u53c2\u6570\u7edf\u4e00\u5904\u7406\uff0c\u65e0\u6cd5\u533a\u5206\u901a\u7528\u8bed\u4e49\u8868\u793a\u548c\u9886\u57df\u7279\u5b9a\u6a21\u5f0f\uff0c\u5bfc\u81f4\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u5927\u578b\u5d4c\u5165\u6a21\u578b\u65f6\u526a\u679d\u51b3\u7b56\u4e0d\u7406\u60f3", "method": "\u4f7f\u7528Fisher\u4fe1\u606f\u8861\u91cf\u91cd\u8981\u6027\uff0c\u901a\u8fc7\u901a\u7528\u9886\u57df\u68af\u5ea6\u5bf9\u9f50\u8bc4\u4f30\u53c2\u6570\u884c\u4e3a\uff0c\u7ed3\u5408\u8fd9\u4e24\u79cd\u4fe1\u53f7\u5f62\u6210\u9886\u57df\u5bf9\u9f50\u91cd\u8981\u6027(DAI)\u8bc4\u5206\uff0c\u4f4eDAI\u5206\u6570\u8868\u793a\u53c2\u6570\u5bf9\u9886\u57df\u4efb\u52a1\u4e0d\u91cd\u8981\u6216\u5728\u9886\u57df\u4e0e\u901a\u7528\u76ee\u6807\u95f4\u4ea7\u751f\u51b2\u7a81", "result": "\u5728FinMTEB\u548cChemTEB\u4e24\u4e2a\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGAPrune\u572850%\u7a00\u758f\u5ea6\u7684\u4e00\u6b21\u526a\u679d\u4e2d\u6027\u80fd\u4fdd\u6301\u5728\u5bc6\u96c6\u6a21\u578b\u76842.5%\u4ee5\u5185\uff0c\u91cd\u8bad\u7ec3100\u6b65\u540e\u5728FinMTEB\u4e0a\u63d0\u53474.51%\uff0c\u5728ChemTEB\u4e0a\u63d0\u53471.73%", "conclusion": "\u6709\u539f\u5219\u7684\u526a\u679d\u7b56\u7565\u53ef\u4ee5\u5b9e\u73b0\u6a21\u578b\u538b\u7f29\u548c\u589e\u5f3a\u9886\u57df\u4e13\u4e1a\u5316\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u65b0\u7684\u5f00\u53d1\u65b9\u6cd5"}}
{"id": "2509.10847", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10847", "abs": "https://arxiv.org/abs/2509.10847", "authors": ["Xiaohui Rao", "Hanlin Wu", "Zhenguang G. Cai"], "title": "A funny companion: Distinct neural responses to perceived AI- versus human- generated humor", "comment": null, "summary": "As AI companions become capable of human-like communication, including\ntelling jokes, understanding how people cognitively and emotionally respond to\nAI humor becomes increasingly important. This study used electroencephalography\n(EEG) to compare how people process humor from AI versus human sources.\nBehavioral analysis revealed that participants rated AI and human humor as\ncomparably funny. However, neurophysiological data showed that AI humor\nelicited a smaller N400 effect, suggesting reduced cognitive effort during the\nprocessing of incongruity. This was accompanied by a larger Late Positive\nPotential (LPP), indicating a greater degree of surprise and emotional\nresponse. This enhanced LPP likely stems from the violation of low initial\nexpectations regarding AI's comedic capabilities. Furthermore, a key temporal\ndynamic emerged: human humor showed habituation effects, marked by an\nincreasing N400 and a decreasing LPP over time. In contrast, AI humor\ndemonstrated increasing processing efficiency and emotional reward, with a\ndecreasing N400 and an increasing LPP. This trajectory reveals how the brain\ncan dynamically update its predictive model of AI capabilities. This process of\ncumulative reinforcement challenges \"algorithm aversion\" in humor, as it\ndemonstrates how cognitive adaptation to AI's language patterns can lead to an\nintensified emotional reward. Additionally, participants' social attitudes\ntoward AI modulated these neural responses, with higher perceived AI\ntrustworthiness correlating with enhanced emotional engagement. These findings\nindicate that the brain responds to AI humor with surprisingly positive and\nintense reactions, highlighting humor's potential for fostering genuine\nengagement in human-AI social interaction.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0AI\u548c\u4eba\u7c7b\u5e7d\u9ed8\u5728\u884c\u4e3a\u8bc4\u5206\u4e0a\u76f8\u4f3c\uff0c\u4f46\u795e\u7ecf\u751f\u7406\u6570\u636e\u663e\u793aAI\u5e7d\u9ed8\u5f15\u53d1\u66f4\u5c0f\u7684N400\uff08\u8ba4\u77e5\u52aa\u529b\u51cf\u5c11\uff09\u548c\u66f4\u5927\u7684LPP\uff08\u60ca\u559c\u548c\u60c5\u611f\u53cd\u5e94\u589e\u5f3a\uff09\uff0c\u8868\u660e\u5927\u8111\u5bf9AI\u5e7d\u9ed8\u6709\u79ef\u6781\u52a8\u6001\u9002\u5e94\u8fc7\u7a0b\u3002", "motivation": "\u968f\u7740AI\u4f34\u4fa3\u5177\u5907\u7c7b\u4eba\u6c9f\u901a\u80fd\u529b\uff08\u5305\u62ec\u8bb2\u7b11\u8bdd\uff09\uff0c\u4e86\u89e3\u4eba\u4eec\u5bf9AI\u5e7d\u9ed8\u7684\u8ba4\u77e5\u548c\u60c5\u611f\u53cd\u5e94\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u8111\u7535\u56fe\uff08EEG\uff09\u6bd4\u8f83\u4eba\u4eec\u5904\u7406AI\u4e0e\u4eba\u7c7b\u6765\u6e90\u5e7d\u9ed8\u7684\u65b9\u5f0f\uff0c\u5206\u6790\u884c\u4e3a\u8bc4\u5206\u548c\u795e\u7ecf\u751f\u7406\u6570\u636e\uff08N400\u548cLPP\u6548\u5e94\uff09\u3002", "result": "\u884c\u4e3a\u4e0aAI\u548c\u4eba\u7c7b\u5e7d\u9ed8\u8bc4\u5206\u76f8\u5f53\uff1b\u795e\u7ecf\u751f\u7406\u4e0aAI\u5e7d\u9ed8\u663e\u793a\u66f4\u5c0f\u7684N400\uff08\u8ba4\u77e5\u52aa\u529b\u51cf\u5c11\uff09\u548c\u66f4\u5927\u7684LPP\uff08\u60ca\u559c\u589e\u5f3a\uff09\uff1bAI\u5e7d\u9ed8\u5448\u73b0\u5904\u7406\u6548\u7387\u63d0\u5347\u548c\u60c5\u611f\u5956\u52b1\u589e\u52a0\u7684\u8d8b\u52bf\uff1b\u793e\u4f1a\u6001\u5ea6\u8c03\u8282\u795e\u7ecf\u53cd\u5e94\u3002", "conclusion": "\u5927\u8111\u5bf9AI\u5e7d\u9ed8\u4ea7\u751f\u51fa\u4e4e\u610f\u6599\u7684\u79ef\u6781\u5f3a\u70c8\u53cd\u5e94\uff0c\u5e7d\u9ed8\u5177\u6709\u4fc3\u8fdb\u4eba\u673a\u793e\u4ea4\u4e92\u52a8\u4e2d\u771f\u6b63\u53c2\u4e0e\u7684\u6f5c\u529b\uff0c\u6311\u6218\u4e86\u7b97\u6cd5\u538c\u6076\u89c2\u5ff5\u3002"}}
{"id": "2509.10852", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10852", "abs": "https://arxiv.org/abs/2509.10852", "authors": ["Sangyeop Kim", "Yohan Lee", "Sanghwa Kim", "Hyunjong Kim", "Sungzoon Cho"], "title": "Pre-Storage Reasoning for Episodic Memory: Shifting Inference Burden to Memory for Personalized Dialogue", "comment": "Accepted by EMNLP 2025 (Findings)", "summary": "Effective long-term memory in conversational AI requires synthesizing\ninformation across multiple sessions. However, current systems place excessive\nreasoning burden on response generation, making performance significantly\ndependent on model sizes. We introduce PREMem (Pre-storage Reasoning for\nEpisodic Memory), a novel approach that shifts complex reasoning processes from\ninference to memory construction. PREMem extracts fine-grained memory fragments\ncategorized into factual, experiential, and subjective information; it then\nestablishes explicit relationships between memory items across sessions,\ncapturing evolution patterns like extensions, transformations, and\nimplications. By performing this reasoning during pre-storage rather than when\ngenerating a response, PREMem creates enriched representations while reducing\ncomputational demands during interactions. Experiments show significant\nperformance improvements across all model sizes, with smaller models achieving\nresults comparable to much larger baselines while maintaining effectiveness\neven with constrained token budgets. Code and dataset are available at\nhttps://github.com/sangyeop-kim/PREMem.", "AI": {"tldr": "PREMem\u662f\u4e00\u79cd\u65b0\u7684\u5bf9\u8bddAI\u957f\u671f\u8bb0\u5fc6\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bb0\u5fc6\u6784\u5efa\u9636\u6bb5\u800c\u975e\u54cd\u5e94\u751f\u6210\u9636\u6bb5\u8fdb\u884c\u590d\u6742\u63a8\u7406\uff0c\u5c06\u7ec6\u7c92\u5ea6\u8bb0\u5fc6\u7247\u6bb5\u5206\u7c7b\u5e76\u5efa\u7acb\u8de8\u4f1a\u8bdd\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5404\u79cd\u89c4\u6a21\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5bf9\u8bddAI\u7cfb\u7edf\u5728\u957f\u671f\u8bb0\u5fc6\u65b9\u9762\u8fc7\u5ea6\u4f9d\u8d56\u54cd\u5e94\u751f\u6210\u9636\u6bb5\u7684\u63a8\u7406\uff0c\u5bfc\u81f4\u6027\u80fd\u4e25\u91cd\u53d7\u6a21\u578b\u89c4\u6a21\u9650\u5236\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u51cf\u8f7b\u63a8\u7406\u8d1f\u62c5\u7684\u65b9\u6cd5\u3002", "method": "PREMem\u5728\u9884\u5b58\u50a8\u9636\u6bb5\u63d0\u53d6\u7ec6\u7c92\u5ea6\u8bb0\u5fc6\u7247\u6bb5\uff08\u4e8b\u5b9e\u6027\u3001\u7ecf\u9a8c\u6027\u3001\u4e3b\u89c2\u6027\u4fe1\u606f\uff09\uff0c\u5efa\u7acb\u8de8\u4f1a\u8bdd\u8bb0\u5fc6\u9879\u4e4b\u95f4\u7684\u663e\u5f0f\u5173\u7cfb\uff0c\u6355\u6349\u6269\u5c55\u3001\u8f6c\u6362\u3001\u5f71\u54cd\u7b49\u6f14\u5316\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6240\u6709\u6a21\u578b\u89c4\u6a21\u90fd\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5c0f\u6a21\u578b\u80fd\u8fbe\u5230\u4e0e\u5927\u57fa\u7ebf\u6a21\u578b\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u4e14\u5728\u6709\u9650token\u9884\u7b97\u4e0b\u4ecd\u4fdd\u6301\u6709\u6548\u6027\u3002", "conclusion": "PREMem\u901a\u8fc7\u5c06\u590d\u6742\u63a8\u7406\u4ece\u63a8\u7406\u9636\u6bb5\u8f6c\u79fb\u5230\u8bb0\u5fc6\u6784\u5efa\u9636\u6bb5\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u4ea4\u4e92\u65f6\u7684\u8ba1\u7b97\u9700\u6c42\uff0c\u540c\u65f6\u521b\u5efa\u4e86\u4e30\u5bcc\u7684\u8bb0\u5fc6\u8868\u793a\uff0c\u4e3a\u5bf9\u8bddAI\u7684\u957f\u671f\u8bb0\u5fc6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10860", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10860", "abs": "https://arxiv.org/abs/2509.10860", "authors": ["Shaohua Fang", "Yue Li", "Yan Cong"], "title": "Quantifier Scope Interpretation in Language Learners and LLMs", "comment": null, "summary": "Sentences with multiple quantifiers often lead to interpretive ambiguities,\nwhich can vary across languages. This study adopts a cross-linguistic approach\nto examine how large language models (LLMs) handle quantifier scope\ninterpretation in English and Chinese, using probabilities to assess\ninterpretive likelihood. Human similarity (HS) scores were used to quantify the\nextent to which LLMs emulate human performance across language groups. Results\nreveal that most LLMs prefer the surface scope interpretations, aligning with\nhuman tendencies, while only some differentiate between English and Chinese in\nthe inverse scope preferences, reflecting human-similar patterns. HS scores\nhighlight variability in LLMs' approximation of human behavior, but their\noverall potential to align with humans is notable. Differences in model\narchitecture, scale, and particularly models' pre-training data language\nbackground, significantly influence how closely LLMs approximate human\nquantifier scope interpretations.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u82f1\u8bed\u548c\u6c49\u8bed\u4e2d\u5904\u7406\u91cf\u8bcd\u8303\u56f4\u89e3\u91ca\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5927\u591a\u6570\u6a21\u578b\u504f\u5411\u8868\u9762\u8303\u56f4\u89e3\u91ca\uff0c\u4e0e\u4eba\u7c7b\u504f\u5411\u4e00\u81f4\uff0c\u4f46\u5728\u9006\u8303\u56f4\u504f\u597d\u4e0a\u5b58\u5728\u8bed\u8a00\u5dee\u5f02", "motivation": "\u7814\u7a76\u4e0d\u540c\u8bed\u8a00\u4e2d\u591a\u91cf\u8bcd\u53e5\u5b50\u5bfc\u81f4\u7684\u89e3\u91ca\u6db5\u7ea6\u6027\uff0c\u4ee5\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5904\u7406\u8fd9\u79cd\u8bed\u8a00\u73b0\u8c61", "method": "\u91c7\u7528\u8de8\u8bed\u8a00\u65b9\u6cd5\uff0c\u901a\u8fc7\u6982\u7387\u8bc4\u4f30\u89e3\u91ca\u53ef\u80fd\u6027\uff0c\u4f7f\u7528\u4eba\u7c7b\u76f8\u4f3c\u6027\u5206\u6570\u6765\u91cf\u5316LLMs\u6a21\u4eff\u4eba\u7c7b\u8868\u73b0\u7684\u7a0b\u5ea6", "result": "\u7ed3\u679c\u663e\u793a\u5927\u591a\u6570LLMs\u504f\u597d\u8868\u9762\u8303\u56f4\u89e3\u91ca\uff0c\u4e0e\u4eba\u7c7b\u504f\u5411\u4e00\u81f4\uff0c\u4f46\u53ea\u6709\u90e8\u5206\u6a21\u578b\u5728\u82f1\u8bed\u548c\u6c49\u8bed\u7684\u9006\u8303\u56f4\u504f\u597d\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u53cd\u6620\u4e86\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u7684\u6a21\u5f0f", "conclusion": "\u6a21\u578b\u67b6\u6784\u3001\u89c4\u6a21\u548c\u9884\u8bad\u7ec3\u6570\u636e\u7684\u8bed\u8a00\u80cc\u666f\u663e\u8457\u5f71\u54cdLLMs\u8fd1\u4f3c\u4eba\u7c7b\u91cf\u8bcd\u8303\u56f4\u89e3\u91ca\u7684\u7a0b\u5ea6\uff0c\u6a21\u578b\u6574\u4f53\u4e0a\u5177\u6709\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u6f5c\u529b"}}
{"id": "2509.10882", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10882", "abs": "https://arxiv.org/abs/2509.10882", "authors": ["Yuping Wu", "Viktor Schlegel", "Warren Del-Pinto", "Srinivasan Nandakumar", "Iqra Zahid", "Yidan Sun", "Usama Farghaly Omar", "Amirah Jasmine", "Arun-Kumar Kaliya-Perumal", "Chun Shen Tham", "Gabriel Connors", "Anil A Bharath", "Goran Nenadic"], "title": "Term2Note: Synthesising Differentially Private Clinical Notes from Medical Terms", "comment": null, "summary": "Training data is fundamental to the success of modern machine learning\nmodels, yet in high-stakes domains such as healthcare, the use of real-world\ntraining data is severely constrained by concerns over privacy leakage. A\npromising solution to this challenge is the use of differentially private (DP)\nsynthetic data, which offers formal privacy guarantees while maintaining data\nutility. However, striking the right balance between privacy protection and\nutility remains challenging in clinical note synthesis, given its domain\nspecificity and the complexity of long-form text generation. In this paper, we\npresent Term2Note, a methodology to synthesise long clinical notes under strong\nDP constraints. By structurally separating content and form, Term2Note\ngenerates section-wise note content conditioned on DP medical terms, with each\ngoverned by separate DP constraints. A DP quality maximiser further enhances\nsynthetic notes by selecting high-quality outputs. Experimental results show\nthat Term2Note produces synthetic notes with statistical properties closely\naligned with real clinical notes, demonstrating strong fidelity. In addition,\nmulti-label classification models trained on these synthetic notes perform\ncomparably to those trained on real data, confirming their high utility.\nCompared to existing DP text generation baselines, Term2Note achieves\nsubstantial improvements in both fidelity and utility while operating under\nfewer assumptions, suggesting its potential as a viable privacy-preserving\nalternative to using sensitive clinical notes.", "AI": {"tldr": "Term2Note\u662f\u4e00\u79cd\u5728\u5f3a\u5dee\u5206\u9690\u79c1\u7ea6\u675f\u4e0b\u5408\u6210\u4e34\u5e8a\u7b14\u8bb0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u5185\u5bb9\u548c\u5f62\u5f0f\uff0c\u751f\u6210\u57fa\u4e8e\u9690\u79c1\u4fdd\u62a4\u533b\u5b66\u672f\u8bed\u7684\u7ae0\u8282\u5185\u5bb9\uff0c\u5e76\u5728\u8d28\u91cf\u548c\u6548\u7528\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u5728\u533b\u7597\u7b49\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u4f7f\u7528\u771f\u5b9e\u8bad\u7ec3\u6570\u636e\u53d7\u5230\u9690\u79c1\u6cc4\u9732\u7684\u4e25\u91cd\u9650\u5236\uff0c\u9700\u8981\u627e\u5230\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u6570\u636e\u6548\u7528\u4e4b\u95f4\u5e73\u8861\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u6027\u5730\u5206\u79bb\u5185\u5bb9\u548c\u5f62\u5f0f\uff0c\u751f\u6210\u57fa\u4e8e\u5dee\u5206\u9690\u79c1\u533b\u5b66\u672f\u8bed\u7684\u7ae0\u8282\u5f0f\u7b14\u8bb0\u5185\u5bb9\uff0c\u6bcf\u4e2a\u90e8\u5206\u53d7\u4e0d\u540c\u7684\u9690\u79c1\u7ea6\u675f\u63a7\u5236\uff0c\u5e76\u4f7f\u7528\u5dee\u5206\u9690\u79c1\u8d28\u91cf\u6700\u5927\u5316\u5668\u9009\u62e9\u9ad8\u8d28\u91cf\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aTerm2Note\u751f\u6210\u7684\u5408\u6210\u7b14\u8bb0\u5728\u7edf\u8ba1\u7279\u6027\u4e0a\u4e0e\u771f\u5b9e\u4e34\u5e8a\u7b14\u8bb0\u9ad8\u5ea6\u4e00\u81f4\uff0c\u57fa\u4e8e\u5408\u6210\u7b14\u8bb0\u8bad\u7ec3\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u6a21\u578b\u6027\u80fd\u4e0e\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "Term2Note\u5728\u8f83\u5c11\u5047\u8bbe\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u4fdd\u771f\u5ea6\u548c\u6548\u7528\u7684\u663e\u8457\u63d0\u5347\uff0c\u6709\u671b\u6210\u4e3a\u4f7f\u7528\u654f\u611f\u4e34\u5e8a\u7b14\u8bb0\u7684\u53ef\u884c\u9690\u79c1\u4fdd\u62a4\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.10886", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10886", "abs": "https://arxiv.org/abs/2509.10886", "authors": ["Xinyu Zhang", "Pei Zhang", "Shuang Luo", "Jialong Tang", "Yu Wan", "Baosong Yang", "Fei Huang"], "title": "CultureSynth: A Hierarchical Taxonomy-Guided and Retrieval-Augmented Framework for Cultural Question-Answer Synthesis", "comment": "Accepted as a Findings paper at EMNLP 2025", "summary": "Cultural competence, defined as the ability to understand and adapt to\nmulticultural contexts, is increasingly vital for large language models (LLMs)\nin global environments. While several cultural benchmarks exist to assess LLMs'\ncultural competence, current evaluations suffer from fragmented taxonomies,\ndomain specificity, and heavy reliance on manual data annotation. To address\nthese limitations, we introduce CultureSynth, a novel framework comprising (1)\na comprehensive hierarchical multilingual cultural taxonomy covering 12 primary\nand 130 secondary topics, and (2) a Retrieval-Augmented Generation (RAG)-based\nmethodology leveraging factual knowledge to synthesize culturally relevant\nquestion-answer pairs. The CultureSynth-7 synthetic benchmark contains 19,360\nentries and 4,149 manually verified entries across 7 languages. Evaluation of\n14 prevalent LLMs of different sizes reveals clear performance stratification\nled by ChatGPT-4o-Latest and Qwen2.5-72B-Instruct. The results demonstrate that\na 3B-parameter threshold is necessary for achieving basic cultural competence,\nmodels display varying architectural biases in knowledge processing, and\nsignificant geographic disparities exist across models. We believe that\nCultureSynth offers a scalable framework for developing culturally aware AI\nsystems while reducing reliance on manual annotation\\footnote{Benchmark is\navailable at https://github.com/Eyr3/CultureSynth.}.", "AI": {"tldr": "CultureSynth\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u6587\u5316\u80fd\u529b\u7684\u65b0\u6846\u67b6\uff0c\u5305\u542b\u591a\u8bed\u8a00\u6587\u5316\u5206\u7c7b\u6cd5\u548c\u57fa\u4e8eRAG\u7684\u95ee\u7b54\u5bf9\u751f\u6210\u65b9\u6cd5\uff0c\u57287\u79cd\u8bed\u8a00\u4e0a\u6d4b\u8bd5\u4e8614\u4e2a\u4e3b\u6d41LLM\u3002", "motivation": "\u73b0\u6709\u6587\u5316\u80fd\u529b\u8bc4\u4f30\u5b58\u5728\u5206\u7c7b\u6cd5\u788e\u7247\u5316\u3001\u9886\u57df\u7279\u5b9a\u6027\u548c\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5168\u9762\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86CultureSynth\u6846\u67b6\uff1a1)\u5305\u542b12\u4e2a\u4e3b\u8981\u548c130\u4e2a\u6b21\u8981\u4e3b\u9898\u7684\u5c42\u6b21\u5316\u591a\u8bed\u8a00\u6587\u5316\u5206\u7c7b\u6cd5\uff1b2)\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u4e8b\u5b9e\u77e5\u8bc6\u5408\u6210\u6587\u5316\u76f8\u5173\u95ee\u7b54\u5bf9\u3002", "result": "\u8bc4\u4f3014\u4e2a\u4e0d\u540c\u89c4\u6a21\u7684LLM\u53d1\u73b0\uff1aChatGPT-4o-Latest\u548cQwen2.5-72B-Instruct\u8868\u73b0\u6700\u4f73\uff1b3B\u53c2\u6570\u662f\u8fbe\u5230\u57fa\u672c\u6587\u5316\u80fd\u529b\u7684\u6700\u4f4e\u9608\u503c\uff1b\u6a21\u578b\u5728\u77e5\u8bc6\u5904\u7406\u4e0a\u5b58\u5728\u67b6\u6784\u504f\u89c1\uff1b\u5b58\u5728\u663e\u8457\u7684\u5730\u7406\u5dee\u5f02\u3002", "conclusion": "CultureSynth\u4e3a\u5f00\u53d1\u6587\u5316\u611f\u77e5AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u51cf\u5c11\u4e86\u5bf9\u624b\u52a8\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u6709\u52a9\u4e8e\u63d0\u5347LLM\u7684\u8de8\u6587\u5316\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2509.10922", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.10922", "abs": "https://arxiv.org/abs/2509.10922", "authors": ["Tsuyoshi Iwata", "Guillaume Comte", "Melissa Flores", "Ryoma Kondo", "Ryohei Hisano"], "title": "Aligning ESG Controversy Data with International Guidelines through Semi-Automatic Ontology Construction", "comment": "Author accepted manuscript. This paper has been accepted for\n  presentation at the ISWC 2025 Posters & Demos Track. License details will be\n  updated once the official proceedings are published", "summary": "The growing importance of environmental, social, and governance data in\nregulatory and investment contexts has increased the need for accurate,\ninterpretable, and internationally aligned representations of non-financial\nrisks, particularly those reported in unstructured news sources. However,\naligning such controversy-related data with principle-based normative\nframeworks, such as the United Nations Global Compact or Sustainable\nDevelopment Goals, presents significant challenges. These frameworks are\ntypically expressed in abstract language, lack standardized taxonomies, and\ndiffer from the proprietary classification systems used by commercial data\nproviders. In this paper, we present a semi-automatic method for constructing\nstructured knowledge representations of environmental, social, and governance\nevents reported in the news. Our approach uses lightweight ontology design,\nformal pattern modeling, and large language models to convert normative\nprinciples into reusable templates expressed in the Resource Description\nFramework. These templates are used to extract relevant information from news\ncontent and populate a structured knowledge graph that links reported incidents\nto specific framework principles. The result is a scalable and transparent\nframework for identifying and interpreting non-compliance with international\nsustainability guidelines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u81ea\u52a8\u65b9\u6cd5\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u672c\u4f53\u8bbe\u8ba1\u3001\u5f62\u5f0f\u5316\u6a21\u5f0f\u5efa\u6a21\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5c06ESG\u65b0\u95fb\u4e8b\u4ef6\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\uff0c\u4ee5\u8fde\u63a5\u62a5\u9053\u4e8b\u4ef6\u4e0e\u56fd\u9645\u53ef\u6301\u7eed\u53d1\u5c55\u51c6\u5219\u3002", "motivation": "ESG\u6570\u636e\u5728\u76d1\u7ba1\u548c\u6295\u8d44\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5c06\u975e\u7ed3\u6784\u5316\u65b0\u95fb\u4e2d\u7684\u4e89\u8bae\u6570\u636e\u4e0e\u57fa\u4e8e\u539f\u5219\u7684\u89c4\u8303\u6027\u6846\u67b6\uff08\u5982\u8054\u5408\u56fd\u5168\u7403\u5951\u7ea6\u6216\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\uff09\u5bf9\u9f50\u5b58\u5728\u6311\u6218\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6846\u67b6\u8bed\u8a00\u62bd\u8c61\u3001\u7f3a\u4e4f\u6807\u51c6\u5316\u5206\u7c7b\u6cd5\uff0c\u4e14\u4e0e\u5546\u4e1a\u6570\u636e\u63d0\u4f9b\u5546\u7684\u4e13\u6709\u7cfb\u7edf\u4e0d\u540c\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u672c\u4f53\u8bbe\u8ba1\u3001\u5f62\u5f0f\u5316\u6a21\u5f0f\u5efa\u6a21\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u89c4\u8303\u6027\u539f\u5219\u8f6c\u5316\u4e3a\u53ef\u91cd\u7528\u7684RDF\u6a21\u677f\uff0c\u4ece\u65b0\u95fb\u5185\u5bb9\u4e2d\u63d0\u53d6\u76f8\u5173\u4fe1\u606f\u5e76\u6784\u5efa\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u900f\u660e\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u89e3\u91ca\u4e0e\u56fd\u9645\u53ef\u6301\u7eed\u53d1\u5c55\u51c6\u5219\u7684\u4e0d\u5408\u89c4\u60c5\u51b5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3ESG\u6570\u636e\u4e0e\u89c4\u8303\u6027\u6846\u67b6\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u89e3\u51b3\u65b9\u6848\u6765\u6784\u5efa\u7ed3\u6784\u5316\u7684ESG\u77e5\u8bc6\u8868\u793a\u3002"}}
{"id": "2509.10935", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10935", "abs": "https://arxiv.org/abs/2509.10935", "authors": ["Ankan Mullick", "Sombit Bose", "Rounak Saha", "Ayan Kumar Bhowmick", "Aditya Vempaty", "Prasenjit Dey", "Ravi Kokku", "Pawan Goyal", "Niloy Ganguly"], "title": "Introducing Spotlight: A Novel Approach for Generating Captivating Key Information from Documents", "comment": "Paper accepted in EMNLP 2025 Main Conference (Full)", "summary": "In this paper, we introduce Spotlight, a novel paradigm for information\nextraction that produces concise, engaging narratives by highlighting the most\ncompelling aspects of a document. Unlike traditional summaries, which\nprioritize comprehensive coverage, spotlights selectively emphasize intriguing\ncontent to foster deeper reader engagement with the source material. We\nformally differentiate spotlights from related constructs and support our\nanalysis with a detailed benchmarking study using new datasets curated for this\nwork. To generate high-quality spotlights, we propose a two-stage approach:\nfine-tuning a large language model on our benchmark data, followed by alignment\nvia Direct Preference Optimization (DPO). Our comprehensive evaluation\ndemonstrates that the resulting model not only identifies key elements with\nprecision but also enhances readability and boosts the engagement value of the\noriginal document.", "AI": {"tldr": "Spotlight\u662f\u4e00\u79cd\u65b0\u9896\u7684\u4fe1\u606f\u63d0\u53d6\u8303\u5f0f\uff0c\u901a\u8fc7\u7a81\u51fa\u6587\u6863\u4e2d\u6700\u5438\u5f15\u4eba\u7684\u5185\u5bb9\u6765\u751f\u6210\u7b80\u6d01\u3001\u5f15\u4eba\u5165\u80dc\u7684\u53d9\u8ff0\uff0c\u4e0e\u4f20\u7edf\u6458\u8981\u8ffd\u6c42\u5168\u9762\u8986\u76d6\u4e0d\u540c\uff0c\u5b83\u9009\u62e9\u6027\u5730\u5f3a\u8c03\u6709\u8da3\u5185\u5bb9\u4ee5\u4fc3\u8fdb\u8bfb\u8005\u66f4\u6df1\u5165\u53c2\u4e0e\u6e90\u6750\u6599\u3002", "motivation": "\u4f20\u7edf\u6458\u8981\u65b9\u6cd5\u8fc7\u4e8e\u6ce8\u91cd\u5168\u9762\u6027\u800c\u5ffd\u7565\u4e86\u5185\u5bb9\u7684\u5438\u5f15\u529b\u548c\u8bfb\u8005\u53c2\u4e0e\u5ea6\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u4fe1\u606f\u63d0\u53d6\u65b9\u5f0f\u6765\u66f4\u597d\u5730\u6fc0\u53d1\u8bfb\u8005\u5bf9\u6e90\u6587\u6863\u7684\u5174\u8da3\u548c\u53c2\u4e0e\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u5728\u57fa\u51c6\u6570\u636e\u4e0a\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7136\u540e\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u8fdb\u884c\u5bf9\u9f50\u8bad\u7ec3\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0c\u751f\u6210\u7684\u6a21\u578b\u4e0d\u4ec5\u80fd\u7cbe\u786e\u8bc6\u522b\u5173\u952e\u5143\u7d20\uff0c\u8fd8\u80fd\u63d0\u9ad8\u53ef\u8bfb\u6027\u5e76\u663e\u8457\u589e\u5f3a\u539f\u59cb\u6587\u6863\u7684\u53c2\u4e0e\u4ef7\u503c\u3002", "conclusion": "Spotlight\u8303\u5f0f\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u5168\u9762\u6027\u6458\u8981\u5411\u9009\u62e9\u6027\u5f3a\u8c03\u6709\u8da3\u5185\u5bb9\u7684\u8f6c\u53d8\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6587\u6863\u7684\u5438\u5f15\u529b\u548c\u8bfb\u8005\u53c2\u4e0e\u5ea6\uff0c\u4e3a\u4fe1\u606f\u63d0\u53d6\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2509.10937", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10937", "abs": "https://arxiv.org/abs/2509.10937", "authors": ["Lihi Nofar", "Tomer Portal", "Aviv Elbaz", "Alexander Apartsin", "Yehudit Aperstein"], "title": "An Interpretable Benchmark for Clickbait Detection and Tactic Attribution", "comment": "7 pages", "summary": "The proliferation of clickbait headlines poses significant challenges to the\ncredibility of information and user trust in digital media. While recent\nadvances in machine learning have improved the detection of manipulative\ncontent, the lack of explainability limits their practical adoption. This paper\npresents a model for explainable clickbait detection that not only identifies\nclickbait titles but also attributes them to specific linguistic manipulation\nstrategies. We introduce a synthetic dataset generated by systematically\naugmenting real news headlines using a predefined catalogue of clickbait\nstrategies. This dataset enables controlled experimentation and detailed\nanalysis of model behaviour. We present a two-stage framework for automatic\nclickbait analysis comprising detection and tactic attribution. In the first\nstage, we compare a fine-tuned BERT classifier with large language models\n(LLMs), specifically GPT-4.0 and Gemini 2.4 Flash, under both zero-shot\nprompting and few-shot prompting enriched with illustrative clickbait headlines\nand their associated persuasive tactics. In the second stage, a dedicated\nBERT-based classifier predicts the specific clickbait strategies present in\neach headline. This work advances the development of transparent and\ntrustworthy AI systems for combating manipulative media content. We share the\ndataset with the research community at\nhttps://github.com/LLM-HITCS25S/ClickbaitTacticsDetection", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u70b9\u51fb\u8bf1\u9975\u68c0\u6d4b\u6a21\u578b\uff0c\u4e0d\u4ec5\u80fd\u8bc6\u522b\u70b9\u51fb\u8bf1\u9975\u6807\u9898\uff0c\u8fd8\u80fd\u5f52\u56e0\u4e8e\u7279\u5b9a\u7684\u8bed\u8a00\u64cd\u7eb5\u7b56\u7565\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\u548c\u4e24\u9636\u6bb5\u6846\u67b6\uff08\u68c0\u6d4b+\u7b56\u7565\u5f52\u56e0\uff09\u8fdb\u884c\u5b9e\u9a8c\u3002", "motivation": "\u70b9\u51fb\u8bf1\u9975\u6807\u9898\u7684\u6cdb\u6ee5\u5bf9\u4fe1\u606f\u53ef\u4fe1\u5ea6\u548c\u7528\u6237\u4fe1\u4efb\u6784\u6210\u6311\u6218\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u7cfb\u7edf\u589e\u5f3a\u771f\u5b9e\u65b0\u95fb\u6807\u9898\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u6bd4\u8f83\u5fae\u8c03BERT\u4e0eLLM\uff08GPT-4.0\u548cGemini 2.4 Flash\uff09\u7684\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u63d0\u793a\uff1b\u7b2c\u4e8c\u9636\u6bb5\u7528\u4e13\u7528BERT\u5206\u7c7b\u5668\u9884\u6d4b\u5177\u4f53\u70b9\u51fb\u8bf1\u9975\u7b56\u7565\u3002", "result": "\u5f00\u53d1\u4e86\u900f\u660e\u53ef\u4fe1\u7684AI\u7cfb\u7edf\u6765\u5bf9\u6297\u64cd\u7eb5\u6027\u5a92\u4f53\u5185\u5bb9\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6\u4f9b\u7814\u7a76\u793e\u533a\u4f7f\u7528\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63a8\u8fdb\u4e86\u53ef\u89e3\u91ca\u70b9\u51fb\u8bf1\u9975\u68c0\u6d4b\u7684\u53d1\u5c55\uff0c\u4e3a\u6784\u5efa\u900f\u660eAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2509.11101", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11101", "abs": "https://arxiv.org/abs/2509.11101", "authors": ["Haokun Li", "Yazhou Zhang", "Jizhi Ding", "Qiuchi Li", "Peng Zhang"], "title": "EmoBench-Reddit: A Hierarchical Benchmark for Evaluating the Emotional Intelligence of Multimodal Large Language Models", "comment": null, "summary": "With the rapid advancement of Multimodal Large Language Models (MLLMs), they\nhave demonstrated exceptional capabilities across a variety of vision-language\ntasks. However, current evaluation benchmarks predominantly focus on objective\nvisual question answering or captioning, inadequately assessing the models'\nability to understand complex and subjective human emotions. To bridge this\ngap, we introduce EmoBench-Reddit, a novel, hierarchical benchmark for\nmultimodal emotion understanding. The dataset comprises 350 meticulously\ncurated samples from the social media platform Reddit, each containing an\nimage, associated user-provided text, and an emotion category (sad, humor,\nsarcasm, happy) confirmed by user flairs. We designed a hierarchical task\nframework that progresses from basic perception to advanced cognition, with\neach data point featuring six multiple-choice questions and one open-ended\nquestion of increasing difficulty. Perception tasks evaluate the model's\nability to identify basic visual elements (e.g., colors, objects), while\ncognition tasks require scene reasoning, intent understanding, and deep empathy\nintegrating textual context. We ensured annotation quality through a\ncombination of AI assistance (Claude 4) and manual verification.", "AI": {"tldr": "EmoBench-Reddit\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u6a21\u6001\u60c5\u611f\u7406\u89e3\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b350\u4e2a\u6765\u81eaReddit\u7684\u7cbe\u5fc3\u7b56\u5212\u6837\u672c\uff0c\u6bcf\u4e2a\u6837\u672c\u5305\u542b\u56fe\u50cf\u3001\u6587\u672c\u548c\u60c5\u611f\u7c7b\u522b\uff0c\u91c7\u7528\u5206\u5c42\u4efb\u52a1\u6846\u67b6\u4ece\u57fa\u7840\u611f\u77e5\u5230\u9ad8\u7ea7\u8ba4\u77e5\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5ba2\u89c2\u89c6\u89c9\u95ee\u7b54\u6216\u5b57\u5e55\u751f\u6210\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u6a21\u578b\u7406\u89e3\u590d\u6742\u4e3b\u89c2\u4eba\u7c7b\u60c5\u611f\u7684\u80fd\u529b\uff0c\u9700\u8981\u4e13\u95e8\u7684\u60c5\u611f\u7406\u89e3\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u6784\u5efa\u5305\u542b350\u4e2aReddit\u6837\u672c\u7684\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u6837\u672c\u5305\u542b\u56fe\u50cf\u3001\u7528\u6237\u6587\u672c\u548c\u60c5\u611f\u6807\u7b7e\uff1b\u8bbe\u8ba1\u5206\u5c42\u4efb\u52a1\u6846\u67b6\uff0c\u5305\u542b6\u4e2a\u9009\u62e9\u9898\u548c1\u4e2a\u5f00\u653e\u5f0f\u95ee\u9898\uff0c\u4ece\u57fa\u7840\u611f\u77e5\uff08\u989c\u8272\u3001\u7269\u4f53\u8bc6\u522b\uff09\u5230\u9ad8\u7ea7\u8ba4\u77e5\uff08\u573a\u666f\u63a8\u7406\u3001\u610f\u56fe\u7406\u89e3\u3001\u6df1\u5ea6\u5171\u60c5\uff09\u9010\u6b65\u63d0\u5347\u96be\u5ea6\uff1b\u91c7\u7528AI\u8f85\u52a9\uff08Claude 4\uff09\u548c\u4eba\u5de5\u9a8c\u8bc1\u786e\u4fdd\u6807\u6ce8\u8d28\u91cf\u3002", "result": "\u5f00\u53d1\u4e86EmoBench-Reddit\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4e3a\u591a\u6a21\u6001\u60c5\u611f\u7406\u89e3\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u611f\u77e5\u548c\u8ba4\u77e5\u4e24\u4e2a\u5c42\u6b21\u7684\u4efb\u52a1\u8bbe\u8ba1\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u591a\u6a21\u6001\u60c5\u611f\u7406\u89e3\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3aMLLMs\u5728\u590d\u6742\u4e3b\u89c2\u60c5\u611f\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u60c5\u611fAI\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.11106", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.11106", "abs": "https://arxiv.org/abs/2509.11106", "authors": ["Valentin Hofmann", "David Heineman", "Ian Magnusson", "Kyle Lo", "Jesse Dodge", "Maarten Sap", "Pang Wei Koh", "Chun Wang", "Hannaneh Hajishirzi", "Noah A. Smith"], "title": "Fluid Language Model Benchmarking", "comment": "COLM 2025", "summary": "Language model (LM) benchmarking faces several challenges: comprehensive\nevaluations are costly, benchmarks often fail to measure the intended\ncapabilities, and evaluation quality can degrade due to labeling errors and\nbenchmark saturation. Although various strategies have been proposed to\nmitigate these issues, they tend to address individual aspects in isolation,\nneglecting broader questions about overall evaluation quality. Here, we\nintroduce Fluid Benchmarking, a new evaluation approach that advances LM\nbenchmarking across multiple dimensions. Inspired by psychometrics, Fluid\nBenchmarking is based on the insight that the relative value of benchmark items\ndepends on an LM's capability level, suggesting that evaluation should adapt to\neach LM. Methodologically, Fluid Benchmarking estimates an item response model\nbased on existing LM evaluation results and uses the inferred quantities to\nselect evaluation items dynamically, similar to computerized adaptive testing\nin education. In our experiments, we compare Fluid Benchmarking against the\ncommon practice of random item sampling as well as more sophisticated\nbaselines, including alternative methods grounded in item response theory. We\nexamine four dimensions -- efficiency, validity, variance, and saturation --\nand find that Fluid Benchmarking achieves superior performance in all of them\n(e.g., higher validity and less variance on MMLU with fifty times fewer items).\nOur analysis shows that the two components of Fluid Benchmarking have distinct\neffects: item response theory, used to map performance into a latent ability\nspace, increases validity, while dynamic item selection reduces variance.\nOverall, our results suggest that LM benchmarking can be substantially improved\nby moving beyond static evaluation.", "AI": {"tldr": "Fluid Benchmarking\u662f\u4e00\u79cd\u65b0\u7684\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u9879\u76ee\u53cd\u5e94\u7406\u8bba\u548c\u52a8\u6001\u9879\u76ee\u9009\u62e9\u6765\u63d0\u5347\u8bc4\u4f30\u6548\u7387\u3001\u6709\u6548\u6027\u3001\u65b9\u5dee\u548c\u9971\u548c\u5ea6\u7684\u7efc\u5408\u6027\u80fd", "motivation": "\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u9762\u4e34\u7684\u6210\u672c\u9ad8\u3001\u6d4b\u91cf\u4e0d\u51c6\u786e\u3001\u6807\u7b7e\u9519\u8bef\u548c\u57fa\u51c6\u9971\u548c\u7b49\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5b64\u7acb\u5904\u7406\u5355\u4e2a\u65b9\u9762\u800c\u5ffd\u89c6\u6574\u4f53\u8bc4\u4f30\u8d28\u91cf", "method": "\u57fa\u4e8e\u9879\u76ee\u53cd\u5e94\u7406\u8bba\u4ece\u73b0\u6709\u8bc4\u4f30\u7ed3\u679c\u4f30\u8ba1\u9879\u76ee\u53cd\u5e94\u6a21\u578b\uff0c\u4f7f\u7528\u63a8\u65ad\u91cf\u52a8\u6001\u9009\u62e9\u8bc4\u4f30\u9879\u76ee\uff0c\u7c7b\u4f3c\u4e8e\u6559\u80b2\u4e2d\u7684\u8ba1\u7b97\u673a\u5316\u81ea\u9002\u5e94\u6d4b\u8bd5", "result": "\u5728\u6548\u7387\u3001\u6709\u6548\u6027\u3001\u65b9\u5dee\u548c\u9971\u548c\u5ea6\u56db\u4e2a\u7ef4\u5ea6\u4e0a\u5747\u4f18\u4e8e\u968f\u673a\u62bd\u6837\u548c\u5176\u4ed6\u57fa\u4e8e\u9879\u76ee\u53cd\u5e94\u7406\u8bba\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f8b\u5982\u5728MMLU\u4e0a\u4f7f\u7528\u5c1150\u500d\u7684\u9879\u76ee\u83b7\u5f97\u66f4\u9ad8\u7684\u6709\u6548\u6027\u548c\u66f4\u4f4e\u7684\u65b9\u5dee", "conclusion": "\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u53ef\u4ee5\u901a\u8fc7\u8d85\u8d8a\u9759\u6001\u8bc4\u4f30\u800c\u5f97\u5230\u663e\u8457\u6539\u8fdb\uff0c\u9879\u76ee\u53cd\u5e94\u7406\u8bba\u63d0\u9ad8\u6709\u6548\u6027\uff0c\u52a8\u6001\u9879\u76ee\u9009\u62e9\u51cf\u5c11\u65b9\u5dee"}}
{"id": "2509.11118", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11118", "abs": "https://arxiv.org/abs/2509.11118", "authors": ["Priyanshu Priya", "Saurav Dudhate", "Desai Vishesh Yasheshbhai", "Asif Ekbal"], "title": "We Argue to Agree: Towards Personality-Driven Argumentation-Based Negotiation Dialogue Systems for Tourism", "comment": "Paper is accepted at EMNLP (Findings) 2025", "summary": "Integrating argumentation mechanisms into negotiation dialogue systems\nimproves conflict resolution through exchanges of arguments and critiques.\nMoreover, incorporating personality attributes enhances adaptability by\naligning interactions with individuals' preferences and styles. To advance\nthese capabilities in negotiation dialogue systems, we propose a novel\nPersonality-driven Argumentation-based Negotiation Dialogue Generation (PAN-DG)\ntask. To support this task, we introduce PACT, a dataset of Personality-driven\nArgumentation-based negotiation Conversations for Tourism sector. This dataset,\ngenerated using Large Language Models (LLMs), features three distinct\npersonality profiles, viz. Argumentation Profile, Preference Profile, and\nBuying Style Profile to simulate a variety of negotiation scenarios involving\ndiverse personalities. Thorough automatic and manual evaluations indicate that\nthe dataset comprises high-quality dialogues. Further, we conduct comparative\nexperiments between pre-trained and fine-tuned LLMs for the PAN-DG task.\nMulti-dimensional evaluation demonstrates that the fine-tuned LLMs effectively\ngenerate personality-driven rational responses during negotiations. This\nunderscores the effectiveness of PACT in enhancing personalization and\nreasoning capabilities in negotiation dialogue systems, thereby establishing a\nfoundation for future research in this domain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u4eba\u683c\u9a71\u52a8\u7684\u8bba\u8bc1\u8c08\u5224\u5bf9\u8bdd\u751f\u6210\u4efb\u52a1(PAN-DG)\u548cPACT\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5fae\u8c03LLM\u80fd\u591f\u6709\u6548\u751f\u6210\u4e2a\u6027\u5316\u8c08\u5224\u5bf9\u8bdd", "motivation": "\u5c06\u8bba\u8bc1\u673a\u5236\u878d\u5165\u8c08\u5224\u5bf9\u8bdd\u7cfb\u7edf\u53ef\u4ee5\u6539\u5584\u51b2\u7a81\u89e3\u51b3\uff0c\u800c\u52a0\u5165\u4eba\u683c\u5c5e\u6027\u80fd\u591f\u589e\u5f3a\u7cfb\u7edf\u9002\u5e94\u6027\uff0c\u4f7f\u5176\u66f4\u597d\u5730\u4e0e\u4e2a\u4eba\u504f\u597d\u548c\u98ce\u683c\u5bf9\u9f50", "method": "\u63d0\u51faPAN-DG\u4efb\u52a1\uff0c\u6784\u5efa\u5305\u542b\u4e09\u79cd\u4eba\u683c\u7279\u5f81(\u8bba\u8bc1\u7279\u5f81\u3001\u504f\u597d\u7279\u5f81\u3001\u8d2d\u4e70\u98ce\u683c\u7279\u5f81)\u7684PACT\u6570\u636e\u96c6\uff0c\u4f7f\u7528LLM\u751f\u6210\u9ad8\u8d28\u91cf\u5bf9\u8bdd\uff0c\u5e76\u8fdb\u884c\u9884\u8bad\u7ec3\u548c\u5fae\u8c03LLM\u7684\u5bf9\u6bd4\u5b9e\u9a8c", "result": "\u81ea\u52a8\u548c\u4eba\u5de5\u8bc4\u4f30\u663e\u793a\u6570\u636e\u96c6\u8d28\u91cf\u9ad8\uff0c\u5fae\u8c03\u540e\u7684LLM\u80fd\u591f\u6709\u6548\u751f\u6210\u4eba\u683c\u9a71\u52a8\u7684\u7406\u6027\u8c08\u5224\u54cd\u5e94", "conclusion": "PACT\u6570\u636e\u96c6\u6709\u6548\u63d0\u5347\u4e86\u8c08\u5224\u5bf9\u8bdd\u7cfb\u7edf\u7684\u4e2a\u6027\u5316\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u8be5\u9886\u57df\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2509.11127", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11127", "abs": "https://arxiv.org/abs/2509.11127", "authors": ["Hongxu Zhou", "Hylke Westerdijk", "Khondoker Ittehadul Islam"], "title": "Joint Effects of Argumentation Theory, Audio Modality and Data Enrichment on LLM-Based Fallacy Classification", "comment": null, "summary": "This study investigates how context and emotional tone metadata influence\nlarge language model (LLM) reasoning and performance in fallacy classification\ntasks, particularly within political debate settings. Using data from U.S.\npresidential debates, we classify six fallacy types through various prompting\nstrategies applied to the Qwen-3 (8B) model. We introduce two theoretically\ngrounded Chain-of-Thought frameworks: Pragma-Dialectics and the Periodic Table\nof Arguments, and evaluate their effectiveness against a baseline prompt under\nthree input settings: text-only, text with context, and text with both context\nand audio-based emotional tone metadata. Results suggest that while theoretical\nprompting can improve interpretability and, in some cases, accuracy, the\naddition of context and especially emotional tone metadata often leads to\nlowered performance. Emotional tone metadata biases the model toward labeling\nstatements as \\textit{Appeal to Emotion}, worsening logical reasoning. Overall,\nbasic prompts often outperformed enhanced ones, suggesting that attention\ndilution from added inputs may worsen rather than improve fallacy\nclassification in LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0a\u4e0b\u6587\u548c\u60c5\u611f\u8bed\u8c03\u5143\u6570\u636e\u5982\u4f55\u5f71\u54cd\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8c2c\u8bef\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u548c\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u653f\u6cbb\u8fa9\u8bba\u573a\u666f\u4e2d\u3002\u7814\u7a76\u53d1\u73b0\u60c5\u611f\u5143\u6570\u636e\u4f1a\u5e26\u6765\u504f\u89c1\uff0c\u800c\u57fa\u7840\u63d0\u793a\u5f80\u5f80\u6bd4\u589e\u5f3a\u63d0\u793a\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e86\u89e3\u4e0a\u4e0b\u6587\u548c\u60c5\u611f\u5143\u6570\u636e\u5bf9LLM\u5728\u653f\u6cbb\u8fa9\u8bba\u8c2c\u8bef\u5206\u7c7b\u4e2d\u7684\u5f71\u54cd\uff0c\u63a2\u7d22\u7406\u8bba\u6846\u67b6\u662f\u5426\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u7f8e\u56fd\u5927\u9009\u8fa9\u8bba\u6570\u636e\uff0c\u901a\u8fc7Qwen-3\u6a21\u578b\u8fdb\u884c\u516d\u79cd\u8c2c\u8bef\u5206\u7c7b\uff0c\u6bd4\u8f83\u4e86\u4e09\u79cd\u8f93\u5165\u8bbe\u7f6e\uff08\u7eaf\u6587\u672c\u3001\u6587\u672c+\u4e0a\u4e0b\u6587\u3001\u6587\u672c+\u4e0a\u4e0b\u6587+\u60c5\u611f\u8bed\u8c03\uff09\u548c\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff08\u57fa\u7840\u63d0\u793a\u3001\u8bed\u7528\u8fa9\u8bc1\u6846\u67b6\u3001\u8bba\u8bc1\u5468\u671f\u8868\u6846\u67b6\uff09\u3002", "result": "\u7406\u8bba\u63d0\u793a\u80fd\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u6dfb\u52a0\u4e0a\u4e0b\u6587\u548c\u60c5\u611f\u8bed\u8c03\u5143\u6570\u636e\u901a\u5e38\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u60c5\u611f\u5143\u6570\u636e\u4f7f\u6a21\u578b\u504f\u5411\u5c06\u9648\u8ff0\u6807\u8bb0\u4e3a\"\u8bc9\u8bf8\u60c5\u611f\"\u8c2c\u8bef\uff0c\u635f\u5bb3\u903b\u8f91\u63a8\u7406\u3002\u57fa\u7840\u63d0\u793a\u5f80\u5f80\u4f18\u4e8e\u589e\u5f3a\u63d0\u793a\u3002", "conclusion": "\u989d\u5916\u7684\u8f93\u5165\u4fe1\u606f\u53ef\u80fd\u5bfc\u81f4\u6ce8\u610f\u529b\u5206\u6563\uff0c\u53cd\u800c\u6076\u5316LLM\u7684\u8c2c\u8bef\u5206\u7c7b\u6027\u80fd\uff0c\u60c5\u611f\u5143\u6570\u636e\u4f1a\u5f15\u5165\u504f\u89c1\uff0c\u57fa\u7840\u65b9\u6cd5\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u66f4\u6709\u6548\u3002"}}
{"id": "2509.11141", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11141", "abs": "https://arxiv.org/abs/2509.11141", "authors": ["Shiyao Cui", "Xijia Feng", "Yingkang Wang", "Junxiao Yang", "Zhexin Zhang", "Biplab Sikdar", "Hongning Wang", "Han Qiu", "Minlie Huang"], "title": "When Smiley Turns Hostile: Interpreting How Emojis Trigger LLMs' Toxicity", "comment": null, "summary": "Emojis are globally used non-verbal cues in digital communication, and\nextensive research has examined how large language models (LLMs) understand and\nutilize emojis across contexts. While usually associated with friendliness or\nplayfulness, it is observed that emojis may trigger toxic content generation in\nLLMs. Motivated by such a observation, we aim to investigate: (1) whether\nemojis can clearly enhance the toxicity generation in LLMs and (2) how to\ninterpret this phenomenon. We begin with a comprehensive exploration of\nemoji-triggered LLM toxicity generation by automating the construction of\nprompts with emojis to subtly express toxic intent. Experiments across 5\nmainstream languages on 7 famous LLMs along with jailbreak tasks demonstrate\nthat prompts with emojis could easily induce toxicity generation. To understand\nthis phenomenon, we conduct model-level interpretations spanning semantic\ncognition, sequence generation and tokenization, suggesting that emojis can act\nas a heterogeneous semantic channel to bypass the safety mechanisms. To pursue\ndeeper insights, we further probe the pre-training corpus and uncover potential\ncorrelation between the emoji-related data polution with the toxicity\ngeneration behaviors. Supplementary materials provide our implementation code\nand data. (Warning: This paper contains potentially sensitive contents)", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8868\u60c5\u7b26\u53f7\u53ef\u4ee5\u89e6\u53d1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6709\u6bd2\u5185\u5bb9\uff0c\u901a\u8fc7\u6784\u5efa\u542b\u8868\u60c5\u7b26\u53f7\u7684\u63d0\u793a\u8bcd\u53ef\u4ee5\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\u8bf1\u5bfc\u6bd2\u6027\u751f\u6210\uff0c\u8fd9\u79cd\u73b0\u8c61\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u8868\u60c5\u7b26\u53f7\u6c61\u67d3\u76f8\u5173\u3002", "motivation": "\u89c2\u5bdf\u5230\u8868\u60c5\u7b26\u53f7\u53ef\u80fd\u89e6\u53d1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6709\u6bd2\u5185\u5bb9\uff0c\u65e8\u5728\u7814\u7a76\u8868\u60c5\u7b26\u53f7\u662f\u5426\u786e\u5b9e\u80fd\u589e\u5f3a\u6bd2\u6027\u751f\u6210\u4ee5\u53ca\u5982\u4f55\u89e3\u91ca\u8fd9\u4e00\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u5316\u6784\u5efa\u542b\u8868\u60c5\u7b26\u53f7\u7684\u63d0\u793a\u8bcd\u6765\u5fae\u5999\u8868\u8fbe\u6709\u6bd2\u610f\u56fe\uff0c\u57285\u79cd\u4e3b\u6d41\u8bed\u8a00\u76847\u4e2a\u77e5\u540dLLM\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u8fdb\u884c\u6a21\u578b\u5c42\u9762\u7684\u8bed\u4e49\u8ba4\u77e5\u3001\u5e8f\u5217\u751f\u6210\u548c\u5206\u8bcd\u5206\u6790\uff0c\u540c\u65f6\u63a2\u67e5\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u542b\u8868\u60c5\u7b26\u53f7\u7684\u63d0\u793a\u8bcd\u5bb9\u6613\u8bf1\u5bfc\u6bd2\u6027\u751f\u6210\uff0c\u8868\u60c5\u7b26\u53f7\u53ef\u4f5c\u4e3a\u5f02\u8d28\u8bed\u4e49\u901a\u9053\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\uff0c\u9884\u8bad\u7ec3\u8bed\u6599\u4e2d\u7684\u8868\u60c5\u7b26\u53f7\u76f8\u5173\u6570\u636e\u6c61\u67d3\u4e0e\u6bd2\u6027\u751f\u6210\u884c\u4e3a\u5b58\u5728\u6f5c\u5728\u5173\u8054\u3002", "conclusion": "\u8868\u60c5\u7b26\u53f7\u786e\u5b9e\u80fd\u663e\u8457\u589e\u5f3aLLM\u7684\u6bd2\u6027\u751f\u6210\u80fd\u529b\uff0c\u8fd9\u79cd\u73b0\u8c61\u6e90\u4e8e\u8868\u60c5\u7b26\u53f7\u4f5c\u4e3a\u5f02\u8d28\u8bed\u4e49\u901a\u9053\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\uff0c\u4e14\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u76f8\u5173\uff0c\u9700\u8981\u5173\u6ce8\u6570\u5b57\u901a\u4fe1\u4e2d\u7684\u8fd9\u79cd\u5b89\u5168\u9690\u60a3\u3002"}}
{"id": "2509.11145", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11145", "abs": "https://arxiv.org/abs/2509.11145", "authors": ["Felix Wang", "Boyu Chen", "Kerun Xu", "Bo Tang", "Feiyu Xiong", "Zhiyu Li"], "title": "Text2Mem: A Unified Memory Operation Language for Memory Operating System", "comment": "11 pages, 3 figures", "summary": "Large language model agents increasingly depend on memory to sustain long\nhorizon interaction, but existing frameworks remain limited. Most expose only a\nfew basic primitives such as encode, retrieve, and delete, while higher order\noperations like merge, promote, demote, split, lock, and expire are missing or\ninconsistently supported. Moreover, there is no formal and executable\nspecification for memory commands, leaving scope and lifecycle rules implicit\nand causing unpredictable behavior across systems. We introduce Text2Mem, a\nunified memory operation language that provides a standardized pathway from\nnatural language to reliable execution. Text2Mem defines a compact yet\nexpressive operation set aligned with encoding, storage, and retrieval. Each\ninstruction is represented as a JSON based schema instance with required fields\nand semantic invariants, which a parser transforms into typed operation objects\nwith normalized parameters. A validator ensures correctness before execution,\nwhile adapters map typed objects either to a SQL prototype backend or to real\nmemory frameworks. Model based services such as embeddings or summarization are\nintegrated when required. All results are returned through a unified execution\ncontract. This design ensures safety, determinism, and portability across\nheterogeneous backends. We also outline Text2Mem Bench, a planned benchmark\nthat separates schema generation from backend execution to enable systematic\nevaluation. Together, these components establish the first standardized\nfoundation for memory control in agents.", "AI": {"tldr": "Text2Mem\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5185\u5b58\u64cd\u4f5c\u8bed\u8a00\uff0c\u4e3aLLM\u4ee3\u7406\u63d0\u4f9b\u4ece\u81ea\u7136\u8bed\u8a00\u5230\u53ef\u9760\u6267\u884c\u7684\u6807\u51c6\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5185\u5b58\u6846\u67b6\u529f\u80fd\u6709\u9650\u3001\u7f3a\u4e4f\u6b63\u5f0f\u89c4\u8303\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u5185\u5b58\u6846\u67b6\u529f\u80fd\u6709\u9650\uff0c\u53ea\u63d0\u4f9b\u57fa\u672c\u64cd\u4f5c\uff0c\u7f3a\u4e4f\u9ad8\u7ea7\u64cd\u4f5c\u548c\u6b63\u5f0f\u89c4\u8303\uff0c\u5bfc\u81f4\u8de8\u7cfb\u7edf\u884c\u4e3a\u4e0d\u53ef\u9884\u6d4b\u3002\u9700\u8981\u7edf\u4e00\u7684\u6807\u51c6\u6765\u786e\u4fdd\u5185\u5b58\u64cd\u4f5c\u7684\u5b89\u5168\u6027\u3001\u786e\u5b9a\u6027\u548c\u53ef\u79fb\u690d\u6027\u3002", "method": "\u8bbe\u8ba1Text2Mem\u8bed\u8a00\uff0c\u5b9a\u4e49\u7d27\u51d1\u800c\u5bcc\u6709\u8868\u73b0\u529b\u7684\u64cd\u4f5c\u96c6\uff0c\u4f7f\u7528JSON\u6a21\u5f0f\u8868\u793a\u6307\u4ee4\uff0c\u5305\u542b\u89e3\u6790\u5668\u3001\u9a8c\u8bc1\u5668\u548c\u9002\u914d\u5668\uff0c\u652f\u6301SQL\u539f\u578b\u540e\u7aef\u548c\u5b9e\u9645\u5185\u5b58\u6846\u67b6\u7684\u6620\u5c04\u3002", "result": "\u5efa\u7acb\u4e86\u7b2c\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u5185\u5b58\u63a7\u5236\u57fa\u7840\uff0c\u786e\u4fdd\u64cd\u4f5c\u7684\u5b89\u5168\u6027\u3001\u786e\u5b9a\u6027\u548c\u8de8\u5f02\u6784\u540e\u7aef\u7684\u53ef\u79fb\u690d\u6027\uff0c\u5e76\u8ba1\u5212\u5f00\u53d1Text2Mem Bench\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "Text2Mem\u4e3a\u89e3\u51b3LLM\u4ee3\u7406\u5185\u5b58\u7ba1\u7406\u4e2d\u7684\u6807\u51c6\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u7684\u7cfb\u7edf\u8bc4\u4f30\u548c\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.11176", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11176", "abs": "https://arxiv.org/abs/2509.11176", "authors": ["Erion \u00c7ano", "Ivan Habernal"], "title": "Differentially-private text generation degrades output language quality", "comment": "20 pages, 3 figures, 35 tables", "summary": "Ensuring user privacy by synthesizing data from large language models (LLMs)\ntuned under differential privacy (DP) has become popular recently. However, the\nimpact of DP fine-tuned LLMs on the quality of the language and the utility of\nthe texts they produce has not been investigated. In this work, we tune five\nLLMs with three corpora under four levels of privacy and assess the length, the\ngrammatical correctness, and the lexical diversity of the text outputs they\nproduce. We also probe the utility of the synthetic outputs in downstream\nclassification tasks such as book genre recognition based on book descriptions\nand cause of death recognition based on verbal autopsies. The results indicate\nthat LLMs tuned under stronger privacy constrains produce texts that are\nshorter by at least 77 %, that are less grammatically correct by at least 9 %,\nand are less diverse by at least 10 % in bi-gram diversity. Furthermore, the\naccuracy they reach in downstream classification tasks decreases, which might\nbe detrimental to the usefulness of the generated synthetic data.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5dee\u5206\u9690\u79c1\u7ea6\u675f\u4e0b\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u751f\u6210\u6587\u672c\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\u6548\u7528\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u66f4\u5f3a\u7684\u9690\u79c1\u4fdd\u62a4\u4f1a\u5bfc\u81f4\u6587\u672c\u66f4\u77ed\u3001\u8bed\u6cd5\u6b63\u786e\u6027\u4e0b\u964d\u3001\u8bcd\u6c47\u591a\u6837\u6027\u51cf\u5c11\uff0c\u4ee5\u53ca\u5206\u7c7b\u4efb\u52a1\u51c6\u786e\u7387\u964d\u4f4e\u3002", "motivation": "\u867d\u7136\u4f7f\u7528\u5dee\u5206\u9690\u79c1\u5fae\u8c03LLMs\u6765\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u5df2\u6210\u4e3a\u6d41\u884c\u65b9\u6cd5\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5bf9\u751f\u6210\u6587\u672c\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f7f\u75285\u4e2aLLMs\u57283\u4e2a\u8bed\u6599\u5e93\u4e0a\u4ee54\u79cd\u9690\u79c1\u7ea7\u522b\u8fdb\u884c\u5fae\u8c03\uff0c\u8bc4\u4f30\u751f\u6210\u6587\u672c\u7684\u957f\u5ea6\u3001\u8bed\u6cd5\u6b63\u786e\u6027\u3001\u8bcd\u6c47\u591a\u6837\u6027\uff0c\u5e76\u6d4b\u8bd5\u5728\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\uff08\u4e66\u7c4d\u7c7b\u578b\u8bc6\u522b\u548c\u6b7b\u56e0\u8bc6\u522b\uff09\u4e2d\u7684\u6548\u7528\u3002", "result": "\u66f4\u5f3a\u7684\u9690\u79c1\u7ea6\u675f\u5bfc\u81f4\u6587\u672c\u957f\u5ea6\u51cf\u5c11\u81f3\u5c1177%\uff0c\u8bed\u6cd5\u6b63\u786e\u6027\u4e0b\u964d\u81f3\u5c119%\uff0c\u4e8c\u5143\u8bcd\u6c47\u591a\u6837\u6027\u51cf\u5c11\u81f3\u5c1110%\uff0c\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u51c6\u786e\u7387\u4e5f\u76f8\u5e94\u4e0b\u964d\u3002", "conclusion": "\u5dee\u5206\u9690\u79c1\u5fae\u8c03\u867d\u7136\u4fdd\u62a4\u4e86\u9690\u79c1\uff0c\u4f46\u663e\u8457\u964d\u4f4e\u4e86\u751f\u6210\u6587\u672c\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\uff0c\u8fd9\u53ef\u80fd\u5f71\u54cd\u5408\u6210\u6570\u636e\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.11177", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11177", "abs": "https://arxiv.org/abs/2509.11177", "authors": ["Hang Guo", "Yawei Li", "Luca Benini"], "title": "Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs", "comment": "Preprint", "summary": "Recent advances in Large Language Model (LLM) compression, such as\nquantization and pruning, have achieved notable success. However, as these\ntechniques gradually approach their respective limits, relying on a single\nmethod for further compression has become increasingly challenging. In this\nwork, we explore an alternative solution by combining quantization and\nsparsity. This joint approach, though promising, introduces new difficulties\ndue to the inherently conflicting requirements on weight distributions:\nquantization favors compact ranges, while pruning benefits from high variance.\nTo attack this problem, we propose Optimal Brain Restoration (OBR), a general\nand training-free framework that aligns pruning and quantization by error\ncompensation between both. OBR minimizes performance degradation on downstream\ntasks by building on a second-order Hessian objective, which is then\nreformulated into a tractable problem through surrogate approximation and\nultimately reaches a closed-form solution via group error compensation.\nExperiments show that OBR enables aggressive W4A4KV4 quantization with 50%\nsparsity on existing LLMs, and delivers up to 4.72x speedup and 6.4x memory\nreduction compared to the FP16-dense baseline.", "AI": {"tldr": "OBR\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u8054\u5408\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u8bef\u5dee\u8865\u507f\u5c06\u91cf\u5316\u548c\u526a\u679d\u7ed3\u5408\uff0c\u5728W4A4KV4\u91cf\u5316\u548c50%\u7a00\u758f\u5ea6\u4e0b\u5b9e\u73b04.72\u500d\u52a0\u901f\u548c6.4\u500d\u5185\u5b58\u51cf\u5c11", "motivation": "\u968f\u7740\u5355\u4e00\u538b\u7f29\u65b9\u6cd5\u9010\u6e10\u63a5\u8fd1\u6781\u9650\uff0c\u9700\u8981\u63a2\u7d22\u8054\u5408\u538b\u7f29\u65b9\u6848\u3002\u4f46\u91cf\u5316\u548c\u526a\u679d\u5bf9\u6743\u91cd\u5206\u5e03\u6709\u51b2\u7a81\u8981\u6c42\uff1a\u91cf\u5316\u504f\u597d\u7d27\u51d1\u8303\u56f4\uff0c\u526a\u679d\u9700\u8981\u9ad8\u65b9\u5dee", "method": "\u63d0\u51faOptimal Brain Restoration (OBR)\u6846\u67b6\uff0c\u57fa\u4e8e\u4e8c\u9636Hessian\u76ee\u6807\uff0c\u901a\u8fc7\u4ee3\u7406\u8fd1\u4f3c\u548c\u7ec4\u8bef\u5dee\u8865\u507f\u83b7\u5f97\u95ed\u5f0f\u89e3\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5bf9\u9f50\u526a\u679d\u548c\u91cf\u5316", "result": "\u5728\u73b0\u6709LLM\u4e0a\u5b9e\u73b0\u6fc0\u8fdb\u7684W4A4KV4\u91cf\u5316\u548c50%\u7a00\u758f\u5ea6\uff0c\u76f8\u6bd4FP16\u5bc6\u96c6\u57fa\u7ebf\u83b7\u5f974.72\u500d\u52a0\u901f\u548c6.4\u500d\u5185\u5b58\u51cf\u5c11", "conclusion": "OBR\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u91cf\u5316\u548c\u526a\u679d\u8054\u5408\u538b\u7f29\u4e2d\u7684\u51b2\u7a81\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347"}}
{"id": "2509.11191", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.11191", "abs": "https://arxiv.org/abs/2509.11191", "authors": ["Jian Chen", "Shengyi Lv", "Leilei Su"], "title": "RanAT4BIE: Random Adversarial Training for Biomedical Information Extraction", "comment": "Accepted for publication at the International Joint Conference on\n  Neural Networks (IJCNN) 2025", "summary": "We introduce random adversarial training (RAT), a novel framework\nsuccessfully applied to biomedical information extraction (BioIE) tasks.\nBuilding on PubMedBERT as the foundational architecture, our study first\nvalidates the effectiveness of conventional adversarial training in enhancing\npre-trained language models' performance on BioIE tasks. While adversarial\ntraining yields significant improvements across various performance metrics, it\nalso introduces considerable computational overhead. To address this\nlimitation, we propose RAT as an efficiency solution for biomedical information\nextraction. This framework strategically integrates random sampling mechanisms\nwith adversarial training principles, achieving dual objectives: enhanced model\ngeneralization and robustness while significantly reducing computational costs.\nThrough comprehensive evaluations, RAT demonstrates superior performance\ncompared to baseline models in BioIE tasks. The results highlight RAT's\npotential as a transformative framework for biomedical natural language\nprocessing, offering a balanced solution to the model performance and\ncomputational efficiency.", "AI": {"tldr": "RAT\uff08\u968f\u673a\u5bf9\u6297\u8bad\u7ec3\uff09\u662f\u4e00\u79cd\u7ed3\u5408\u968f\u673a\u91c7\u6837\u548c\u5bf9\u6297\u8bad\u7ec3\u7684\u65b0\u6846\u67b6\uff0c\u5728\u751f\u7269\u533b\u5b66\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u5e76\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u5bf9\u6297\u8bad\u7ec3\u867d\u7136\u80fd\u63d0\u5347\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4f46\u5e26\u6765\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8ePubMedBERT\u67b6\u6784\uff0c\u5c06\u968f\u673a\u91c7\u6837\u673a\u5236\u4e0e\u5bf9\u6297\u8bad\u7ec3\u539f\u5219\u7b56\u7565\u6027\u7ed3\u5408\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "result": "RAT\u5728\u751f\u7269\u533b\u5b66\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "RAT\u4e3a\u751f\u7269\u533b\u5b66\u81ea\u7136\u8bed\u8a00\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5e73\u8861\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u7684\u53d8\u9769\u6027\u6846\u67b6\uff0c\u5177\u6709\u91cd\u8981\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.11295", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11295", "abs": "https://arxiv.org/abs/2509.11295", "authors": ["Valentin Romanov", "Steven A Niederer"], "title": "The Prompt Engineering Report Distilled: Quick Start Guide for Life Sciences", "comment": null, "summary": "Developing effective prompts demands significant cognitive investment to\ngenerate reliable, high-quality responses from Large Language Models (LLMs). By\ndeploying case-specific prompt engineering techniques that streamline\nfrequently performed life sciences workflows, researchers could achieve\nsubstantial efficiency gains that far exceed the initial time investment\nrequired to master these techniques. The Prompt Report published in 2025\noutlined 58 different text-based prompt engineering techniques, highlighting\nthe numerous ways prompts could be constructed. To provide actionable\nguidelines and reduce the friction of navigating these various approaches, we\ndistil this report to focus on 6 core techniques: zero-shot, few-shot\napproaches, thought generation, ensembling, self-criticism, and decomposition.\nWe breakdown the significance of each approach and ground it in use cases\nrelevant to life sciences, from literature summarization and data extraction to\neditorial tasks. We provide detailed recommendations for how prompts should and\nshouldn't be structured, addressing common pitfalls including multi-turn\nconversation degradation, hallucinations, and distinctions between reasoning\nand non-reasoning models. We examine context window limitations, agentic tools\nlike Claude Code, while analyzing the effectiveness of Deep Research tools\nacross OpenAI, Google, Anthropic and Perplexity platforms, discussing current\nlimitations. We demonstrate how prompt engineering can augment rather than\nreplace existing established individual practices around data processing and\ndocument editing. Our aim is to provide actionable guidance on core prompt\nengineering principles, and to facilitate the transition from opportunistic\nprompting to an effective, low-friction systematic practice that contributes to\nhigher quality research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c0658\u79cd\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u63d0\u70bc\u4e3a6\u79cd\u6838\u5fc3\u65b9\u6cd5\uff08\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u3001\u601d\u7ef4\u751f\u6210\u3001\u96c6\u6210\u3001\u81ea\u6211\u6279\u8bc4\u548c\u5206\u89e3\uff09\uff0c\u4e3a\u751f\u547d\u79d1\u5b66\u9886\u57df\u63d0\u4f9b\u5b9e\u7528\u7684\u63d0\u793a\u5de5\u7a0b\u6307\u5357\uff0c\u65e8\u5728\u63d0\u9ad8\u7814\u7a76\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u5f00\u53d1\u6709\u6548\u7684\u63d0\u793a\u9700\u8981\u5927\u91cf\u8ba4\u77e5\u6295\u5165\uff0c\u7814\u7a76\u4eba\u5458\u901a\u8fc7\u638c\u63e1\u7279\u5b9a\u7684\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u53ef\u4ee5\u5728\u751f\u547d\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u83b7\u5f97\u8fdc\u8d85\u521d\u59cb\u65f6\u95f4\u6295\u5165\u7684\u6548\u7387\u63d0\u5347\u3002", "method": "\u5c062025\u5e74Prompt Report\u4e2d\u768458\u79cd\u6587\u672c\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u63d0\u70bc\u4e3a6\u79cd\u6838\u5fc3\u65b9\u6cd5\uff0c\u5206\u6790\u6bcf\u79cd\u65b9\u6cd5\u7684\u610f\u4e49\u5e76\u5728\u751f\u547d\u79d1\u5b66\u7528\u4f8b\u4e2d\u9a8c\u8bc1\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u5efa\u8bae\u548c\u5e38\u89c1\u9677\u9631\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u63d0\u793a\u7ed3\u6784\u5efa\u8bae\uff0c\u89e3\u51b3\u4e86\u591a\u8f6e\u5bf9\u8bdd\u9000\u5316\u3001\u5e7b\u89c9\u7b49\u95ee\u9898\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u5e73\u53f0\u5de5\u5177\u7684\u6709\u6548\u6027\u548c\u5f53\u524d\u9650\u5236\uff0c\u5c55\u793a\u4e86\u63d0\u793a\u5de5\u7a0b\u5982\u4f55\u589e\u5f3a\u800c\u975e\u66ff\u4ee3\u73b0\u6709\u6570\u636e\u5904\u7406\u5b9e\u8df5\u3002", "conclusion": "\u65e8\u5728\u4e3a\u6838\u5fc3\u63d0\u793a\u5de5\u7a0b\u539f\u5219\u63d0\u4f9b\u53ef\u64cd\u4f5c\u6307\u5bfc\uff0c\u4fc3\u8fdb\u4ece\u673a\u4f1a\u6027\u63d0\u793a\u5411\u6709\u6548\u3001\u4f4e\u6469\u64e6\u7684\u7cfb\u7edf\u5316\u5b9e\u8df5\u8fc7\u6e21\uff0c\u4ece\u800c\u8d21\u732e\u66f4\u9ad8\u8d28\u91cf\u7684\u7814\u7a76\u6210\u679c\u3002"}}
{"id": "2509.11303", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11303", "abs": "https://arxiv.org/abs/2509.11303", "authors": ["Dasol Choi", "Jungwhan Kim", "Guijin Son"], "title": "Ko-PIQA: A Korean Physical Commonsense Reasoning Dataset with Cultural Context", "comment": null, "summary": "Physical commonsense reasoning datasets like PIQA are predominantly\nEnglish-centric and lack cultural diversity. We introduce Ko-PIQA, a Korean\nphysical commonsense reasoning dataset that incorporates cultural context.\nStarting from 3.01 million web-crawled questions, we employed a multi-stage\nfiltering approach using three language models to identify 11,553 PIQA-style\nquestions. Through GPT-4o refinement and human validation, we obtained 441\nhigh-quality question-answer pairs. A key feature of Ko-PIQA is its cultural\ngrounding: 19.7\\% of questions contain culturally specific elements like\ntraditional Korean foods (kimchi), clothing (hanbok), and specialized\nappliances (kimchi refrigerators) that require culturally-aware reasoning\nbeyond direct translation. We evaluate seven language models on Ko-PIQA, with\nthe best model achieving 83.22\\% accuracy while the weakest reaches only\n59.86\\%, demonstrating significant room for improvement. Models particularly\nstruggle with culturally specific scenarios, highlighting the importance of\nculturally diverse datasets. Ko-PIQA serves as both a benchmark for Korean\nlanguage models and a foundation for more inclusive commonsense reasoning\nresearch. The dataset and code will be publicly available.", "AI": {"tldr": "Ko-PIQA\u662f\u4e00\u4e2a\u97e9\u8bed\u7269\u7406\u5e38\u8bc6\u63a8\u7406\u6570\u636e\u96c6\uff0c\u5305\u542b\u6587\u5316\u7279\u5b9a\u5143\u7d20\uff0c\u586b\u8865\u4e86\u82f1\u8bed\u4e3b\u5bfc\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u7a7a\u767d\u3002\u901a\u8fc7\u591a\u9636\u6bb5\u8fc7\u6ee4\u548c\u4eba\u5de5\u9a8c\u8bc1\uff0c\u4ece301\u4e07\u4e2a\u95ee\u9898\u4e2d\u7b5b\u9009\u51fa441\u4e2a\u9ad8\u8d28\u91cf\u95ee\u9898\u5bf9\uff0c\u5176\u4e2d19.7%\u5305\u542b\u97e9\u56fd\u6587\u5316\u5143\u7d20\u3002", "motivation": "\u73b0\u6709\u7684\u7269\u7406\u5e38\u8bc6\u63a8\u7406\u6570\u636e\u96c6\uff08\u5982PIQA\uff09\u4e3b\u8981\u57fa\u4e8e\u82f1\u8bed\uff0c\u7f3a\u4e4f\u6587\u5316\u591a\u6837\u6027\u3002\u4e3a\u4e86\u4fc3\u8fdb\u591a\u8bed\u8a00\u548c\u8de8\u6587\u5316\u5e38\u8bc6\u63a8\u7406\u7814\u7a76\uff0c\u9700\u8981\u521b\u5efa\u5305\u542b\u6587\u5316\u80cc\u666f\u7684\u6570\u636e\u96c6\u3002", "method": "\u4ece301\u4e07\u4e2a\u7f51\u7edc\u722c\u53d6\u95ee\u9898\u5f00\u59cb\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e09\u4e2a\u8bed\u8a00\u6a21\u578b\u8bc6\u522b11,553\u4e2aPIQA\u98ce\u683c\u95ee\u9898\u3002\u901a\u8fc7GPT-4o\u7cbe\u70bc\u548c\u4eba\u5de5\u9a8c\u8bc1\uff0c\u6700\u7ec8\u83b7\u5f97441\u4e2a\u9ad8\u8d28\u91cf\u95ee\u9898-\u7b54\u6848\u5bf9\u3002", "result": "\u8bc4\u4f30\u4e86\u4e03\u4e2a\u8bed\u8a00\u6a21\u578b\uff0c\u6700\u4f73\u6a21\u578b\u51c6\u786e\u7387\u8fbe\u523083.22%\uff0c\u6700\u5dee\u6a21\u578b\u4ec5\u4e3a59.86%\u3002\u6a21\u578b\u5728\u6587\u5316\u7279\u5b9a\u573a\u666f\u4e0a\u8868\u73b0\u5c24\u5176\u56f0\u96be\uff0c\u663e\u793a\u51fa\u6587\u5316\u591a\u6837\u6027\u6570\u636e\u96c6\u7684\u91cd\u8981\u6027\u3002", "conclusion": "Ko-PIQA\u4e0d\u4ec5\u4e3a\u97e9\u8bed\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e5f\u4e3a\u66f4\u5177\u5305\u5bb9\u6027\u7684\u5e38\u8bc6\u63a8\u7406\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u516c\u5f00\u63d0\u4f9b\uff0c\u4fc3\u8fdb\u8de8\u6587\u5316AI\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2509.11365", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11365", "abs": "https://arxiv.org/abs/2509.11365", "authors": ["Mohamed Tarek", "Seif Ahmed", "Mohamed Basem"], "title": "!MSA at AraHealthQA 2025 Shared Task: Enhancing LLM Performance for Arabic Clinical Question Answering through Prompt Engineering and Ensemble Learning", "comment": "8 Pages , ArabicNLP 2025 , Co-located with EMNLP 2025", "summary": "We present our systems for Track 2 (General Arabic Health QA, MedArabiQ) of\nthe AraHealthQA-2025 shared task, where our methodology secured 2nd place in\nboth Sub-Task 1 (multiple-choice question answering) and Sub-Task 2 (open-ended\nquestion answering) in Arabic clinical contexts. For Sub-Task 1, we leverage\nthe Gemini 2.5 Flash model with few-shot prompting, dataset preprocessing, and\nan ensemble of three prompt configurations to improve classification accuracy\non standard, biased, and fill-in-the-blank questions. For Sub-Task 2, we employ\na unified prompt with the same model, incorporating role-playing as an Arabic\nmedical expert, few-shot examples, and post-processing to generate concise\nresponses across fill-in-the-blank, patient-doctor Q&A, GEC, and paraphrased\nvariants.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728AraHealthQA-2025\u5171\u4eab\u4efb\u52a1Track 2\u4e2d\u53d6\u5f97\u7b2c\u4e8c\u540d\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528Gemini 2.5 Flash\u6a21\u578b\u901a\u8fc7few-shot\u63d0\u793a\u3001\u6570\u636e\u9884\u5904\u7406\u548c\u96c6\u6210\u7b56\u7565\u6765\u5904\u7406\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u95ee\u7b54\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5065\u5eb7\u95ee\u7b54\u6311\u6218\uff0c\u63d0\u5347\u5728\u591a\u9879\u9009\u62e9\u548c\u5f00\u653e\u5f0f\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u5bf9\u4e8e\u5b50\u4efb\u52a11\uff08\u591a\u9879\u9009\u62e9\uff09\uff1a\u4f7f\u7528Gemini 2.5 Flash\u6a21\u578b\uff0c\u91c7\u7528few-shot\u63d0\u793a\u3001\u6570\u636e\u96c6\u9884\u5904\u7406\u548c\u4e09\u79cd\u63d0\u793a\u914d\u7f6e\u7684\u96c6\u6210\u65b9\u6cd5\u3002\u5bf9\u4e8e\u5b50\u4efb\u52a12\uff08\u5f00\u653e\u5f0f\u95ee\u7b54\uff09\uff1a\u4f7f\u7528\u7edf\u4e00\u63d0\u793a\u7b56\u7565\uff0c\u7ed3\u5408\u89d2\u8272\u626e\u6f14\uff08\u963f\u62c9\u4f2f\u533b\u5b66\u4e13\u5bb6\uff09\u3001few-shot\u793a\u4f8b\u548c\u540e\u5904\u7406\u6280\u672f\u3002", "result": "\u5728\u4e24\u4e2a\u5b50\u4efb\u52a1\u4e2d\u90fd\u83b7\u5f97\u4e86\u7b2c\u4e8c\u540d\uff1a\u5b50\u4efb\u52a11\uff08\u591a\u9879\u9009\u62e9\u95ee\u7b54\uff09\u548c\u5b50\u4efb\u52a12\uff08\u5f00\u653e\u5f0f\u95ee\u7b54\uff09\u5728\u963f\u62c9\u4f2f\u4e34\u5e8a\u8bed\u5883\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eGemini 2.5 Flash\u7684\u65b9\u6cd5\u5728\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u5353\u8d8a\uff0cfew-shot\u63d0\u793a\u548c\u96c6\u6210\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u4e34\u5e8a\u8bed\u5883\u4e0b\u7684\u51c6\u786e\u6027\u548c\u54cd\u5e94\u8d28\u91cf\u3002"}}
{"id": "2509.11374", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11374", "abs": "https://arxiv.org/abs/2509.11374", "authors": ["Bowen Jing", "Yang Cui", "Tianpeng Huang"], "title": "Transformer Enhanced Relation Classification: A Comparative Analysis of Contextuality, Data Efficiency and Sequence Complexity", "comment": null, "summary": "In the era of large language model, relation extraction (RE) plays an\nimportant role in information extraction through the transformation of\nunstructured raw text into structured data (Wadhwa et al., 2023). In this\npaper, we systematically compare the performance of deep supervised learning\napproaches without transformers and those with transformers. We used a series\nof non-transformer architectures such as PA-LSTM(Zhang et al., 2017),\nC-GCN(Zhang et al., 2018), and AGGCN(attention guide GCN)(Guo et al., 2019),\nand a series of transformer architectures such as BERT, RoBERTa, and R-BERT(Wu\nand He, 2019). Our comparison included traditional metrics like micro F1, as\nwell as evaluations in different scenarios, varying sentence lengths, and\ndifferent percentages of the dataset for training. Our experiments were\nconducted on TACRED, TACREV, and RE-TACRED. The results show that\ntransformer-based models outperform non-transformer models, achieving micro F1\nscores of 80-90% compared to 64-67% for non-transformer models. Additionally,\nwe briefly review the research journey in supervised relation classification\nand discuss the role and current status of large language models (LLMs) in\nrelation extraction.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86\u57fa\u4e8etransformer\u548c\u975etransformer\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u53d1\u73b0transformer\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b", "motivation": "\u5728\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u4ee3\uff0c\u5173\u7cfb\u62bd\u53d6\u4f5c\u4e3a\u4fe1\u606f\u62bd\u53d6\u7684\u91cd\u8981\u73af\u8282\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u67b6\u6784\u6a21\u578b\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4e3a\u7814\u7a76\u63d0\u4f9b\u6307\u5bfc", "method": "\u4f7f\u7528PA-LSTM\u3001C-GCN\u3001AGGCN\u7b49\u975etransformer\u67b6\u6784\u548cBERT\u3001RoBERTa\u3001R-BERT\u7b49transformer\u67b6\u6784\uff0c\u5728TACRED\u3001TACREV\u3001RE-TACRED\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e0d\u540c\u53e5\u5b50\u957f\u5ea6\u3001\u8bad\u7ec3\u6570\u636e\u6bd4\u4f8b\u7b49\u573a\u666f\u4e0b\u7684\u6027\u80fd", "result": "transformer\u6a21\u578b\u5728micro F1\u6307\u6807\u4e0a\u8fbe\u523080-90%\uff0c\u663e\u8457\u4f18\u4e8e\u975etransformer\u6a21\u578b\u768464-67%", "conclusion": "transformer\u67b6\u6784\u5728\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u4e0a\u5177\u6709\u660e\u663e\u4f18\u52bf\uff0c\u540c\u65f6\u8bba\u6587\u8fd8\u56de\u987e\u4e86\u76d1\u7763\u5173\u7cfb\u5206\u7c7b\u7684\u7814\u7a76\u5386\u7a0b\u5e76\u8ba8\u8bba\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5173\u7cfb\u62bd\u53d6\u4e2d\u7684\u4f5c\u7528\u548c\u73b0\u72b6"}}
{"id": "2509.11414", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11414", "abs": "https://arxiv.org/abs/2509.11414", "authors": ["Abraham Toluwase Owodunni", "Sachin Kumar"], "title": "Continually Adding New Languages to Multilingual Language Models", "comment": null, "summary": "Multilingual language models are trained on a fixed set of languages, and to\nsupport new languages, the models need to be retrained from scratch. This is an\nexpensive endeavor and is often infeasible, as model developers tend not to\nrelease their pre-training data. Naive approaches, such as continued\npretraining, suffer from catastrophic forgetting; however, mitigation\nstrategies like experience replay cannot be applied due to the lack of original\npretraining data. In this work, we investigate the problem of continually\nadding new languages to a multilingual model, assuming access to pretraining\ndata in only the target languages. We explore multiple approaches to address\nthis problem and propose Layer-Selective LoRA (LayRA), which adds Low-Rank\nAdapters (LoRA) to selected initial and final layers while keeping the rest of\nthe model frozen. LayRA builds on two insights: (1) LoRA reduces forgetting,\nand (2) multilingual models encode inputs in the source language in the initial\nlayers, reason in English in intermediate layers, and translate back to the\nsource language in final layers. We experiment with adding multiple\ncombinations of Galician, Swahili, and Urdu to pretrained language models and\nevaluate each method on diverse multilingual tasks. We find that LayRA provides\nthe overall best tradeoff between preserving models' capabilities in previously\nsupported languages, while being competitive with existing approaches such as\nLoRA in learning new languages. We also demonstrate that using model\narithmetic, the adapted models can be equipped with strong instruction\nfollowing abilities without access to any instruction tuning data in the target\nlanguages.", "AI": {"tldr": "\u63d0\u51fa\u4e86Layer-Selective LoRA\uff08LayRA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5730\u5728\u521d\u59cb\u548c\u6700\u7ec8\u5c42\u6dfb\u52a0\u4f4e\u79e9\u9002\u914d\u5668\u6765\u89e3\u51b3\u591a\u8bed\u8a00\u6a21\u578b\u65b0\u589e\u8bed\u8a00\u65f6\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u65e0\u9700\u539f\u59cb\u9884\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u591a\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u9700\u8981\u4ece\u5934\u91cd\u65b0\u8bad\u7ec3\u6765\u652f\u6301\u65b0\u8bed\u8a00\uff0c\u6210\u672c\u9ad8\u6602\u4e14\u4e0d\u73b0\u5b9e\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u6301\u7eed\u9884\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u800c\u7ecf\u9a8c\u56de\u653e\u7b49\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u539f\u59cb\u9884\u8bad\u7ec3\u6570\u636e\u800c\u65e0\u6cd5\u5e94\u7528\u3002", "method": "\u63d0\u51faLayer-Selective LoRA\uff08LayRA\uff09\u65b9\u6cd5\uff0c\u57fa\u4e8e\u4e24\u4e2a\u5173\u952e\u53d1\u73b0\uff1a1\uff09LoRA\u53ef\u4ee5\u51cf\u5c11\u9057\u5fd8\uff1b2\uff09\u591a\u8bed\u8a00\u6a21\u578b\u5728\u521d\u59cb\u5c42\u7f16\u7801\u6e90\u8bed\u8a00\uff0c\u4e2d\u95f4\u5c42\u7528\u82f1\u8bed\u63a8\u7406\uff0c\u6700\u7ec8\u5c42\u7ffb\u8bd1\u56de\u6e90\u8bed\u8a00\u3002LayRA\u9009\u62e9\u6027\u5730\u5728\u521d\u59cb\u548c\u6700\u7ec8\u5c42\u6dfb\u52a0LoRA\u9002\u914d\u5668\uff0c\u5176\u4f59\u5c42\u4fdd\u6301\u51bb\u7ed3\u3002", "result": "\u5728\u6dfb\u52a0\u52a0\u5229\u897f\u4e9a\u8bed\u3001\u65af\u74e6\u5e0c\u91cc\u8bed\u548c\u4e4c\u5c14\u90fd\u8bed\u7684\u5b9e\u9a8c\u4e2d\uff0cLayRA\u5728\u4fdd\u6301\u539f\u6709\u8bed\u8a00\u80fd\u529b\u7684\u540c\u65f6\uff0c\u5728\u65b0\u8bed\u8a00\u5b66\u4e60\u65b9\u9762\u4e0e\u73b0\u6709\u65b9\u6cd5\uff08\u5982LoRA\uff09\u7ade\u4e89\u529b\u76f8\u5f53\uff0c\u63d0\u4f9b\u4e86\u6700\u4f73\u7684\u6574\u4f53\u6743\u8861\u3002\u901a\u8fc7\u6a21\u578b\u7b97\u672f\uff0c\u9002\u5e94\u540e\u7684\u6a21\u578b\u8fd8\u83b7\u5f97\u4e86\u5f3a\u5927\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u65e0\u9700\u76ee\u6807\u8bed\u8a00\u7684\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u3002", "conclusion": "LayRA\u662f\u4e00\u79cd\u6709\u6548\u7684\u591a\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u4ee5\u8f83\u4f4e\u6210\u672c\u6dfb\u52a0\u65b0\u8bed\u8a00\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u6709\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u591a\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.11443", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.11443", "abs": "https://arxiv.org/abs/2509.11443", "authors": ["Gaurab Chhetri", "Darrell Anderson", "Boniphace Kutela", "Subasish Das"], "title": "A Transformer-Based Cross-Platform Analysis of Public Discourse on the 15-Minute City Paradigm", "comment": "This is the author's preprint version of a paper accepted for\n  presentation at the 24th International Conference on Machine Learning and\n  Applications (ICMLA 2025), December 3-5, 2025, Florida, USA. The final\n  published version will appear in the official IEEE proceedings. Conference\n  site: https://www.icmla-conference.org/icmla25/", "summary": "This study presents the first multi-platform sentiment analysis of public\nopinion on the 15-minute city concept across Twitter, Reddit, and news media.\nUsing compressed transformer models and Llama-3-8B for annotation, we classify\nsentiment across heterogeneous text domains. Our pipeline handles long-form and\nshort-form text, supports consistent annotation, and enables reproducible\nevaluation. We benchmark five models (DistilRoBERTa, DistilBERT, MiniLM,\nELECTRA, TinyBERT) using stratified 5-fold cross-validation, reporting\nF1-score, AUC, and training time. DistilRoBERTa achieved the highest F1\n(0.8292), TinyBERT the best efficiency, and MiniLM the best cross-platform\nconsistency. Results show News data yields inflated performance due to class\nimbalance, Reddit suffers from summarization loss, and Twitter offers moderate\nchallenge. Compressed models perform competitively, challenging assumptions\nthat larger models are necessary. We identify platform-specific trade-offs and\npropose directions for scalable, real-world sentiment classification in urban\nplanning discourse.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5bf915\u5206\u949f\u57ce\u5e02\u6982\u5ff5\u5728Twitter\u3001Reddit\u548c\u65b0\u95fb\u5a92\u4f53\u4e0a\u7684\u516c\u4f17\u610f\u89c1\u8fdb\u884c\u591a\u5e73\u53f0\u60c5\u611f\u5206\u6790\uff0c\u4f7f\u7528\u538b\u7f29Transformer\u6a21\u578b\u548cLlama-3-8B\u8fdb\u884c\u6807\u6ce8\uff0c\u53d1\u73b0\u538b\u7f29\u6a21\u578b\u6027\u80fd\u5177\u6709\u7ade\u4e89\u529b\uff0c\u6311\u6218\u4e86\u5927\u6a21\u578b\u5fc5\u8981\u7684\u5047\u8bbe\u3002", "motivation": "\u7814\u7a7615\u5206\u949f\u57ce\u5e02\u6982\u5ff5\u7684\u516c\u4f17\u60c5\u611f\u6001\u5ea6\uff0c\u5f00\u53d1\u80fd\u591f\u5904\u7406\u591a\u5e73\u53f0\u5f02\u6784\u6587\u672c\u7684\u60c5\u611f\u5206\u6790\u6846\u67b6\uff0c\u63a2\u7d22\u538b\u7f29\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6548\u679c\u3002", "method": "\u4f7f\u7528\u538b\u7f29Transformer\u6a21\u578b\uff08DistilRoBERTa\u3001DistilBERT\u3001MiniLM\u3001ELECTRA\u3001TinyBERT\uff09\u548cLlama-3-8B\u8fdb\u884c\u6807\u6ce8\uff0c\u91c7\u7528\u5206\u5c425\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u8bc4\u4f30F1\u5206\u6570\u3001AUC\u548c\u8bad\u7ec3\u65f6\u95f4\u3002", "result": "DistilRoBERTa\u83b7\u5f97\u6700\u9ad8F1\u5206\u6570\uff080.8292\uff09\uff0cTinyBERT\u6548\u7387\u6700\u4f73\uff0cMiniLM\u8de8\u5e73\u53f0\u4e00\u81f4\u6027\u6700\u597d\u3002\u65b0\u95fb\u6570\u636e\u56e0\u7c7b\u522b\u4e0d\u5e73\u8861\u5bfc\u81f4\u6027\u80fd\u865a\u9ad8\uff0cReddit\u5b58\u5728\u6458\u8981\u635f\u5931\uff0cTwitter\u63d0\u4f9b\u4e2d\u7b49\u6311\u6218\u3002", "conclusion": "\u538b\u7f29\u6a21\u578b\u5728\u60c5\u611f\u5206\u7c7b\u4e2d\u8868\u73b0\u7ade\u4e89\u529b\u5f3a\uff0c\u6311\u6218\u4e86\u5927\u6a21\u578b\u5fc5\u8981\u6027\u7684\u5047\u8bbe\uff0c\u8bc6\u522b\u4e86\u5e73\u53f0\u7279\u5b9a\u7684\u6743\u8861\uff0c\u4e3a\u57ce\u5e02\u89c4\u5212\u8ba8\u8bba\u4e2d\u7684\u53ef\u6269\u5c55\u5b9e\u65f6\u60c5\u611f\u5206\u7c7b\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.11444", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.11444", "abs": "https://arxiv.org/abs/2509.11444", "authors": ["Gaurab Chhetri", "Anandi Dutta", "Subasish Das"], "title": "CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media", "comment": "This is the author's preprint version of a paper accepted for\n  presentation at HICSS 59 (Hawaii International Conference on System\n  Sciences), 2026, Hawaii, USA. The final published version will appear in the\n  official conference proceedings. Conference site: https://hicss.hawaii.edu/", "summary": "The emergence of decentralized social media platforms presents new\nopportunities and challenges for real-time analysis of public discourse. This\nstudy introduces CognitiveSky, an open-source and scalable framework designed\nfor sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter\nor X.com alternative. By ingesting data through Bluesky's Application\nProgramming Interface (API), CognitiveSky applies transformer-based models to\nannotate large-scale user-generated content and produces structured and\nanalyzable outputs. These summaries drive a dynamic dashboard that visualizes\nevolving patterns in emotion, activity, and conversation topics. Built entirely\non free-tier infrastructure, CognitiveSky achieves both low operational cost\nand high accessibility. While demonstrated here for monitoring mental health\ndiscourse, its modular design enables applications across domains such as\ndisinformation detection, crisis response, and civic sentiment analysis. By\nbridging large language models with decentralized networks, CognitiveSky offers\na transparent, extensible tool for computational social science in an era of\nshifting digital ecosystems.", "AI": {"tldr": "CognitiveSky\u662f\u4e00\u4e2a\u5f00\u6e90\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u53bb\u4e2d\u5fc3\u5316\u793e\u4ea4\u5a92\u4f53Bluesky\u4e0a\u8fdb\u884c\u60c5\u611f\u3001\u60c5\u7eea\u548c\u53d9\u4e8b\u5206\u6790\uff0c\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5904\u7406\u5927\u89c4\u6a21\u7528\u6237\u751f\u6210\u5185\u5bb9\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u4eea\u8868\u677f\u53ef\u89c6\u5316\u5206\u6790\u7ed3\u679c\u3002", "motivation": "\u968f\u7740\u53bb\u4e2d\u5fc3\u5316\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u7684\u51fa\u73b0\uff0c\u9700\u8981\u65b0\u7684\u5de5\u5177\u6765\u5b9e\u65f6\u5206\u6790\u516c\u5171\u8bdd\u8bed\u3002Bluesky\u4f5c\u4e3aTwitter/X\u7684\u8054\u90a6\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\u548c\u6311\u6218\u3002", "method": "\u901a\u8fc7Bluesky API\u83b7\u53d6\u6570\u636e\uff0c\u5e94\u7528\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5bf9\u5927\u89c4\u6a21\u7528\u6237\u751f\u6210\u5185\u5bb9\u8fdb\u884c\u6807\u6ce8\uff0c\u751f\u6210\u7ed3\u6784\u5316\u53ef\u5206\u6790\u8f93\u51fa\uff0c\u5e76\u6784\u5efa\u52a8\u6001\u4eea\u8868\u677f\u53ef\u89c6\u5316\u60c5\u7eea\u3001\u6d3b\u52a8\u548c\u8bdd\u9898\u7684\u6f14\u53d8\u6a21\u5f0f\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u5168\u57fa\u4e8e\u514d\u8d39\u5c42\u57fa\u7840\u8bbe\u65bd\u7684\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u4f4e\u8fd0\u8425\u6210\u672c\u548c\u9ad8\u53ef\u8bbf\u95ee\u6027\u3002\u867d\u7136\u4ee5\u5fc3\u7406\u5065\u5eb7\u8bdd\u8bed\u76d1\u6d4b\u4e3a\u4f8b\uff0c\u4f46\u5176\u6a21\u5757\u5316\u8bbe\u8ba1\u53ef\u5e94\u7528\u4e8e\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u3001\u5371\u673a\u54cd\u5e94\u548c\u516c\u6c11\u60c5\u7eea\u5206\u6790\u7b49\u591a\u4e2a\u9886\u57df\u3002", "conclusion": "CognitiveSky\u901a\u8fc7\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u53bb\u4e2d\u5fc3\u5316\u7f51\u7edc\u76f8\u7ed3\u5408\uff0c\u4e3a\u6570\u5b57\u751f\u6001\u7cfb\u7edf\u8f6c\u578b\u65f6\u4ee3\u7684\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u63d0\u4f9b\u4e86\u4e00\u4e2a\u900f\u660e\u3001\u53ef\u6269\u5c55\u7684\u5de5\u5177\u3002"}}
{"id": "2509.11465", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.11465", "abs": "https://arxiv.org/abs/2509.11465", "authors": ["Amirhossein Abaskohi", "Raymond Li", "Chuyuan Li", "Shafiq Joty", "Giuseppe Carenini"], "title": "CEMTM: Contextual Embedding-based Multimodal Topic Modeling", "comment": "EMNLP 2025", "summary": "We introduce CEMTM, a context-enhanced multimodal topic model designed to\ninfer coherent and interpretable topic structures from both short and long\ndocuments containing text and images. CEMTM builds on fine-tuned large vision\nlanguage models (LVLMs) to obtain contextualized embeddings, and employs a\ndistributional attention mechanism to weight token-level contributions to topic\ninference. A reconstruction objective aligns topic-based representations with\nthe document embedding, encouraging semantic consistency across modalities.\nUnlike existing approaches, CEMTM can process multiple images per document\nwithout repeated encoding and maintains interpretability through explicit\nword-topic and document-topic distributions. Extensive experiments on six\nmultimodal benchmarks show that CEMTM consistently outperforms unimodal and\nmultimodal baselines, achieving a remarkable average LLM score of 2.61. Further\nanalysis shows its effectiveness in downstream few-shot retrieval and its\nability to capture visually grounded semantics in complex domains such as\nscientific articles.", "AI": {"tldr": "CEMTM\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u589e\u5f3a\u7684\u591a\u6a21\u6001\u4e3b\u9898\u6a21\u578b\uff0c\u80fd\u591f\u4ece\u5305\u542b\u6587\u672c\u548c\u56fe\u50cf\u7684\u77ed/\u957f\u6587\u6863\u4e2d\u63a8\u65ad\u51fa\u8fde\u8d2f\u4e14\u53ef\u89e3\u91ca\u7684\u4e3b\u9898\u7ed3\u6784\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u4e3b\u9898\u6a21\u578b\u5728\u5904\u7406\u591a\u56fe\u50cf\u6587\u6863\u65f6\u9700\u8981\u91cd\u590d\u7f16\u7801\uff0c\u4e14\u96be\u4ee5\u4fdd\u6301\u8de8\u6a21\u6001\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u6a21\u6001\u5185\u5bb9\u5e76\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u5fae\u8c03\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u83b7\u53d6\u4e0a\u4e0b\u6587\u5d4c\u5165\uff0c\u91c7\u7528\u5206\u5e03\u6ce8\u610f\u529b\u673a\u5236\u52a0\u6743token\u7ea7\u8d21\u732e\uff0c\u901a\u8fc7\u91cd\u6784\u76ee\u6807\u5c06\u57fa\u4e8e\u4e3b\u9898\u7684\u8868\u5f81\u4e0e\u6587\u6863\u5d4c\u5165\u5bf9\u9f50\uff0c\u652f\u6301\u591a\u56fe\u50cf\u5904\u7406\u800c\u65e0\u9700\u91cd\u590d\u7f16\u7801\u3002", "result": "\u5728\u516d\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d consistently \u8d85\u8d8a\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u57fa\u7ebf\uff0c\u5e73\u5747LLM\u5f97\u5206\u8fbe\u52302.61\uff0c\u5728\u4e0b\u6e38\u5c11\u6837\u672c\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u6355\u6349\u590d\u6742\u9886\u57df\uff08\u5982\u79d1\u5b66\u6587\u7ae0\uff09\u4e2d\u7684\u89c6\u89c9\u57fa\u7840\u8bed\u4e49\u3002", "conclusion": "CEMTM\u901a\u8fc7\u4e0a\u4e0b\u6587\u589e\u5f3a\u548c\u5206\u5e03\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u591a\u6a21\u6001\u6587\u6863\u4e2d\u63a8\u65ad\u8fde\u8d2f\u4e3b\u9898\u7ed3\u6784\u7684\u80fd\u529b\uff0c\u5728\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u591a\u6a21\u6001\u4e3b\u9898\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.11466", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11466", "abs": "https://arxiv.org/abs/2509.11466", "authors": ["Yujian Gan", "Yuan Liang", "Yanni Lin", "Juntao Yu", "Massimo Poesio"], "title": "Improving LLMs' Learning for Coreference Resolution", "comment": null, "summary": "Coreference Resolution (CR) is crucial for many NLP tasks, but existing LLMs\nstruggle with hallucination and under-performance. In this paper, we\ninvestigate the limitations of existing LLM-based approaches to CR-specifically\nthe Question-Answering (QA) Template and Document Template methods and propose\ntwo novel techniques: Reversed Training with Joint Inference and Iterative\nDocument Generation. Our experiments show that Reversed Training improves the\nQA Template method, while Iterative Document Generation eliminates\nhallucinations in the generated source text and boosts coreference resolution.\nIntegrating these methods and techniques offers an effective and robust\nsolution to LLM-based coreference resolution.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6307\u4ee3\u6d88\u89e3\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u548c\u6027\u80fd\u4e0d\u8db3\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u6280\u672f\uff1a\u53cd\u5411\u8bad\u7ec3\u8054\u5408\u63a8\u7406\u548c\u8fed\u4ee3\u6587\u6863\u751f\u6210\uff0c\u6709\u6548\u63d0\u5347\u4e86QA\u6a21\u677f\u65b9\u6cd5\u7684\u6027\u80fd\u5e76\u6d88\u9664\u4e86\u751f\u6210\u6587\u672c\u4e2d\u7684\u5e7b\u89c9\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6307\u4ee3\u6d88\u89e3\u65b9\u6cd5\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\u548c\u6027\u80fd\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u95ee\u7b54\u6a21\u677f\u548c\u6587\u6863\u6a21\u677f\u65b9\u6cd5\u5b58\u5728\u660e\u663e\u5c40\u9650\u6027\uff0c\u9700\u8981\u65b0\u7684\u6280\u672f\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u6280\u672f\uff1a1\uff09\u53cd\u5411\u8bad\u7ec3\u8054\u5408\u63a8\u7406 - \u6539\u8fdb\u95ee\u7b54\u6a21\u677f\u65b9\u6cd5\uff1b2\uff09\u8fed\u4ee3\u6587\u6863\u751f\u6210 - \u6d88\u9664\u751f\u6210\u6e90\u6587\u672c\u4e2d\u7684\u5e7b\u89c9\u5e76\u63d0\u5347\u6307\u4ee3\u6d88\u89e3\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u53cd\u5411\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u95ee\u7b54\u6a21\u677f\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u8fed\u4ee3\u6587\u6863\u751f\u6210\u6709\u6548\u6d88\u9664\u4e86\u5e7b\u89c9\u95ee\u9898\u5e76\u63d0\u9ad8\u4e86\u6307\u4ee3\u6d88\u89e3\u51c6\u786e\u7387\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u8fd9\u4e24\u79cd\u65b9\u6cd5\uff0c\u4e3a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6307\u4ee3\u6d88\u89e3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.11492", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11492", "abs": "https://arxiv.org/abs/2509.11492", "authors": ["Anirban Saha Anik", "Md Fahimul Kabir Chowdhury", "Andrew Wyckoff", "Sagnik Ray Choudhury"], "title": "ClaimIQ at CheckThat! 2025: Comparing Prompted and Fine-Tuned Language Models for Verifying Numerical Claims", "comment": "Notebook for the CheckThat! Lab at CLEF 2025", "summary": "This paper presents our system for Task 3 of the CLEF 2025 CheckThat! Lab,\nwhich focuses on verifying numerical and temporal claims using retrieved\nevidence. We explore two complementary approaches: zero-shot prompting with\ninstruction-tuned large language models (LLMs) and supervised fine-tuning using\nparameter-efficient LoRA. To enhance evidence quality, we investigate several\nselection strategies, including full-document input and top-k sentence\nfiltering using BM25 and MiniLM. Our best-performing model LLaMA fine-tuned\nwith LoRA achieves strong performance on the English validation set. However, a\nnotable drop in the test set highlights a generalization challenge. These\nfindings underscore the importance of evidence granularity and model adaptation\nfor robust numerical fact verification.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CLEF 2025 CheckThat! Lab\u4efb\u52a13\u7684\u7cfb\u7edf\uff0c\u4f7f\u7528\u96f6\u6837\u672c\u63d0\u793a\u548cLoRA\u5fae\u8c03\u4e24\u79cd\u65b9\u6cd5\u8fdb\u884c\u6570\u503c\u548c\u65f6\u95f4\u58f0\u660e\u9a8c\u8bc1\uff0c\u901a\u8fc7\u4e0d\u540c\u8bc1\u636e\u9009\u62e9\u7b56\u7565\u63d0\u5347\u8bc1\u636e\u8d28\u91cf\uff0c\u6700\u4f73\u6a21\u578b\u5728\u9a8c\u8bc1\u96c6\u8868\u73b0\u826f\u597d\u4f46\u6d4b\u8bd5\u96c6\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u6570\u503c\u548c\u65f6\u95f4\u58f0\u660e\u7684\u81ea\u52a8\u9a8c\u8bc1\u95ee\u9898\uff0c\u63a2\u7d22\u5982\u4f55\u5229\u7528\u68c0\u7d22\u8bc1\u636e\u548c\u5927\u8bed\u8a00\u6a21\u578b\u6765\u6709\u6548\u9a8c\u8bc1\u4e8b\u5b9e\u58f0\u660e\uff0c\u7279\u522b\u5173\u6ce8\u8bc1\u636e\u7c92\u5ea6\u9009\u62e9\u5bf9\u9a8c\u8bc1\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u63d0\u793a\u65b9\u6cd5\uff1b2\uff09\u4f7f\u7528\u53c2\u6570\u9ad8\u6548\u7684LoRA\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u3002\u540c\u65f6\u7814\u7a76\u591a\u79cd\u8bc1\u636e\u9009\u62e9\u7b56\u7565\uff0c\u5305\u62ec\u5b8c\u6574\u6587\u6863\u8f93\u5165\u548c\u4f7f\u7528BM25\u548cMiniLM\u7684top-k\u53e5\u5b50\u8fc7\u6ee4\u3002", "result": "\u4f7f\u7528LoRA\u5fae\u8c03\u7684LLaMA\u6a21\u578b\u5728\u82f1\u8bed\u9a8c\u8bc1\u96c6\u4e0a\u53d6\u5f97\u4e86\u5f3a\u52b2\u6027\u80fd\uff0c\u4f46\u5728\u6d4b\u8bd5\u96c6\u4e0a\u51fa\u73b0\u663e\u8457\u6027\u80fd\u4e0b\u964d\uff0c\u8868\u660e\u5b58\u5728\u6cdb\u5316\u6311\u6218\u3002\u8bc1\u636e\u7c92\u5ea6\u9009\u62e9\u7b56\u7565\u5bf9\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\u3002", "conclusion": "\u8bc1\u636e\u7c92\u5ea6\u548c\u6a21\u578b\u9002\u5e94\u6027\u5bf9\u4e8e\u6784\u5efa\u9c81\u68d2\u7684\u6570\u5b57\u4e8b\u5b9e\u9a8c\u8bc1\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5982\u4f55\u63d0\u9ad8\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.11496", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11496", "abs": "https://arxiv.org/abs/2509.11496", "authors": ["Fabrycio Leite Nakano Almada", "Kauan Divino Pouso Mariano", "Maykon Adriell Dutra", "Victor Emanuel da Silva Monteiro", "Juliana Resplande Sant'Anna Gomes", "Arlindo Rodrigues Galv\u00e3o Filho", "Anderson da Silva Soares"], "title": "AKCIT-FN at CheckThat! 2025: Switching Fine-Tuned SLMs and LLM Prompting for Multilingual Claim Normalization", "comment": "15 pages, 2 figures", "summary": "Claim normalization, the transformation of informal social media posts into\nconcise, self-contained statements, is a crucial step in automated\nfact-checking pipelines. This paper details our submission to the CLEF-2025\nCheckThat! Task~2, which challenges systems to perform claim normalization\nacross twenty languages, divided into thirteen supervised (high-resource) and\nseven zero-shot (no training data) tracks.\n  Our approach, leveraging fine-tuned Small Language Models (SLMs) for\nsupervised languages and Large Language Model (LLM) prompting for zero-shot\nscenarios, achieved podium positions (top three) in fifteen of the twenty\nlanguages. Notably, this included second-place rankings in eight languages,\nfive of which were among the seven designated zero-shot languages, underscoring\nthe effectiveness of our LLM-based zero-shot strategy. For Portuguese, our\ninitial development language, our system achieved an average METEOR score of\n0.5290, ranking third. All implementation artifacts, including inference,\ntraining, evaluation scripts, and prompt configurations, are publicly available\nat https://github.com/ju-resplande/checkthat2025_normalization.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728CLEF-2025 CheckThat! Task 2\u4e2d\u4f7f\u7528\u7684\u58f0\u660e\u89c4\u8303\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u6709\u76d1\u7763\u8bed\u8a00\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u793a\u5904\u7406\u96f6\u6837\u672c\u8bed\u8a00\uff0c\u572820\u79cd\u8bed\u8a00\u4e2d\u768415\u79cd\u83b7\u5f97\u524d\u4e09\u540d\u6210\u7ee9", "motivation": "\u58f0\u660e\u89c4\u8303\u5316\u662f\u5c06\u975e\u6b63\u5f0f\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u8f6c\u5316\u4e3a\u7b80\u6d01\u3001\u81ea\u5305\u542b\u9648\u8ff0\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u662f\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u6d41\u7a0b\u4e2d\u7684\u91cd\u8981\u73af\u8282", "method": "\u9488\u5bf9\u6709\u76d1\u7763\u8bed\u8a00\u4f7f\u7528\u5fae\u8c03\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b(SLMs)\uff0c\u9488\u5bf9\u96f6\u6837\u672c\u8bed\u8a00\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u63d0\u793a\u6280\u672f", "result": "\u572820\u79cd\u8bed\u8a00\u4e2d\u768415\u79cd\u83b7\u5f97\u524d\u4e09\u540d\uff0c\u5176\u4e2d8\u79cd\u8bed\u8a00\u83b7\u5f97\u7b2c\u4e8c\u540d\uff08\u5305\u62ec5\u79cd\u96f6\u6837\u672c\u8bed\u8a00\uff09\uff0c\u8461\u8404\u7259\u8bed\u83b7\u5f97\u7b2c\u4e09\u540d\uff08METEOR\u5f97\u52060.5290\uff09", "conclusion": "LLM\u57fa\u7840\u7684\u96f6\u6837\u672c\u7b56\u7565\u975e\u5e38\u6709\u6548\uff0c\u7279\u522b\u662f\u5728\u7f3a\u4e4f\u8bad\u7ec3\u6570\u636e\u7684\u8bed\u8a00\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6240\u6709\u5b9e\u73b0\u4ee3\u7801\u548c\u914d\u7f6e\u5747\u5df2\u5f00\u6e90"}}
{"id": "2509.11498", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11498", "abs": "https://arxiv.org/abs/2509.11498", "authors": ["Zhuoxuan Ju", "Jingni Wu", "Abhishek Purushothama", "Amir Zeldes"], "title": "DeDisCo at the DISRPT 2025 Shared Task: A System for Discourse Relation Classification", "comment": "System submission for the DISRPT 2025 - Shared Task on Discourse\n  Relation Parsing and Treebanking In conjunction with CODI-CRAC & EMNLP 2025.\n  1st place in Task 3: relation classification", "summary": "This paper presents DeDisCo, Georgetown University's entry in the DISRPT 2025\nshared task on discourse relation classification. We test two approaches, using\nan mt5-based encoder and a decoder based approach using the openly available\nQwen model. We also experiment on training with augmented dataset for\nlow-resource languages using matched data translated automatically from\nEnglish, as well as using some additional linguistic features inspired by\nentries in previous editions of the Shared Task. Our system achieves a\nmacro-accuracy score of 71.28, and we provide some interpretation and error\nanalysis for our results.", "AI": {"tldr": "DeDisCo\u7cfb\u7edf\u5728DISRPT 2025\u8bed\u7bc7\u5173\u7cfb\u5206\u7c7b\u4efb\u52a1\u4e2d\u6d4b\u8bd5\u4e86mt5\u7f16\u7801\u5668\u548cQwen\u89e3\u7801\u5668\u4e24\u79cd\u65b9\u6cd5\uff0c\u4f7f\u7528\u81ea\u52a8\u7ffb\u8bd1\u7684\u6570\u636e\u589e\u5f3a\u548c\u989d\u5916\u8bed\u8a00\u5b66\u7279\u5f81\uff0c\u53d6\u5f97\u4e8671.28\u7684\u5b8f\u51c6\u786e\u7387", "motivation": "\u53c2\u4e0eDISRPT 2025\u5171\u4eab\u4efb\u52a1\uff0c\u63a2\u7d22\u5728\u8bed\u7bc7\u5173\u7cfb\u5206\u7c7b\u4e2d\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5904\u7406", "method": "\u4f7f\u7528\u57fa\u4e8emt5\u7684\u7f16\u7801\u5668\u65b9\u6cd5\u548c\u57fa\u4e8eQwen\u6a21\u578b\u7684\u89e3\u7801\u5668\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u7ffb\u8bd1\u82f1\u8bed\u6570\u636e\u6765\u589e\u5f3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8bad\u7ec3\u96c6\uff0c\u5e76\u52a0\u5165\u989d\u5916\u7684\u8bed\u8a00\u5b66\u7279\u5f81", "result": "\u7cfb\u7edf\u83b7\u5f97\u4e8671.28\u7684\u5b8f\u51c6\u786e\u7387\u5206\u6570\uff0c\u5e76\u5bf9\u7ed3\u679c\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u89e3\u91ca\u548c\u9519\u8bef\u5206\u6790", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8bed\u7bc7\u5173\u7cfb\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u6570\u636e\u589e\u5f3a\u548c\u8bed\u8a00\u5b66\u7279\u5f81\u7684\u52a0\u5165\u5bf9\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u6027\u80fd\u6709\u79ef\u6781\u4f5c\u7528"}}
{"id": "2509.11513", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11513", "abs": "https://arxiv.org/abs/2509.11513", "authors": ["Zhongyang Hu", "Naijie Gu", "Xiangzhi Tao", "Tianhui Gu", "Yibing Zhou"], "title": "Unsupervised Candidate Ranking for Lexical Substitution via Holistic Sentence Semantics", "comment": null, "summary": "A key subtask in lexical substitution is ranking the given candidate words. A\ncommon approach is to replace the target word with a candidate in the original\nsentence and feed the modified sentence into a model to capture semantic\ndifferences before and after substitution. However, effectively modeling the\nbidirectional influence of candidate substitution on both the target word and\nits context remains challenging. Existing methods often focus solely on\nsemantic changes at the target position or rely on parameter tuning over\nmultiple evaluation metrics, making it difficult to accurately characterize\nsemantic variation. To address this, we investigate two approaches: one based\non attention weights and another leveraging the more interpretable integrated\ngradients method, both designed to measure the influence of context tokens on\nthe target token and to rank candidates by incorporating semantic similarity\nbetween the original and substituted sentences. Experiments on the LS07 and\nSWORDS datasets demonstrate that both approaches improve ranking performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u6743\u91cd\u548c\u79ef\u5206\u68af\u5ea6\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u8bcd\u6c47\u66ff\u6362\u4efb\u52a1\u4e2d\u7684\u5019\u9009\u8bcd\u6392\u5e8f\uff0c\u901a\u8fc7\u5efa\u6a21\u76ee\u6807\u8bcd\u4e0e\u4e0a\u4e0b\u6587\u7684\u53cc\u5411\u5f71\u54cd\u6765\u66f4\u51c6\u786e\u5730\u8861\u91cf\u8bed\u4e49\u53d8\u5316\u3002", "motivation": "\u73b0\u6709\u8bcd\u6c47\u66ff\u6362\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u76ee\u6807\u4f4d\u7f6e\u7684\u8bed\u4e49\u53d8\u5316\u6216\u4f9d\u8d56\u591a\u6307\u6807\u53c2\u6570\u8c03\u4f18\uff0c\u96be\u4ee5\u51c6\u786e\u5efa\u6a21\u5019\u9009\u66ff\u6362\u5bf9\u76ee\u6807\u8bcd\u548c\u4e0a\u4e0b\u6587\u53cc\u5411\u5f71\u54cd\u7684\u8bed\u4e49\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a\u57fa\u4e8e\u6ce8\u610f\u529b\u6743\u91cd\u7684\u65b9\u6cd5\u548c\u57fa\u4e8e\u79ef\u5206\u68af\u5ea6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8861\u91cf\u4e0a\u4e0b\u6587\u8bcd\u5bf9\u76ee\u6807\u8bcd\u7684\u5f71\u54cd\uff0c\u5e76\u7ed3\u5408\u539f\u53e5\u4e0e\u66ff\u6362\u53e5\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u6765\u6392\u5e8f\u5019\u9009\u8bcd\u3002", "result": "\u5728LS07\u548cSWORDS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u79cd\u65b9\u6cd5\u90fd\u63d0\u5347\u4e86\u6392\u5e8f\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8e\u6ce8\u610f\u529b\u6743\u91cd\u548c\u79ef\u5206\u68af\u5ea6\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u5efa\u6a21\u8bcd\u6c47\u66ff\u6362\u4e2d\u7684\u8bed\u4e49\u53d8\u5316\uff0c\u63d0\u9ad8\u5019\u9009\u8bcd\u6392\u5e8f\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2509.11514", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11514", "abs": "https://arxiv.org/abs/2509.11514", "authors": ["Zhengxiang Wang", "Weiling Li", "Panagiotis Kaliosis", "Owen Rambow", "Susan E. Brennan"], "title": "LVLMs are Bad at Overhearing Human Referential Communication", "comment": "EMNLP 2025 (Main)", "summary": "During spontaneous conversations, speakers collaborate on novel referring\nexpressions, which they can then re-use in subsequent conversations.\nUnderstanding such referring expressions is an important ability for an\nembodied agent, so that it can carry out tasks in the real world. This requires\nintegrating and understanding language, vision, and conversational interaction.\nWe study the capabilities of seven state-of-the-art Large Vision Language\nModels (LVLMs) as overhearers to a corpus of spontaneous conversations between\npairs of human discourse participants engaged in a collaborative\nobject-matching task. We find that such a task remains challenging for current\nLVLMs and they all fail to show a consistent performance improvement as they\noverhear more conversations from the same discourse participants repeating the\nsame task for multiple rounds. We release our corpus and code for\nreproducibility and to facilitate future research.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e867\u79cd\u6700\u5148\u8fdb\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u65c1\u542c\u8005\u7406\u89e3\u4eba\u7c7b\u81ea\u53d1\u5bf9\u8bdd\u4e2d\u534f\u4f5c\u6307\u4ee3\u8868\u8fbe\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u4ecd\u6709\u6311\u6218\uff0c\u4e14\u65e0\u6cd5\u901a\u8fc7\u591a\u6b21\u5bf9\u8bdd\u5b66\u4e60\u6539\u8fdb\u8868\u73b0\u3002", "motivation": "\u7406\u89e3\u4eba\u7c7b\u81ea\u53d1\u5bf9\u8bdd\u4e2d\u7684\u534f\u4f5c\u6307\u4ee3\u8868\u8fbe\u5bf9\u4e8e\u5177\u8eab\u667a\u80fd\u4f53\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u6267\u884c\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u6574\u5408\u8bed\u8a00\u3001\u89c6\u89c9\u548c\u5bf9\u8bdd\u4ea4\u4e92\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u4f7f\u75287\u79cdstate-of-the-art\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u65c1\u542c\u8005\uff0c\u5206\u6790\u4eba\u7c7b\u5728\u534f\u4f5c\u5bf9\u8c61\u5339\u914d\u4efb\u52a1\u4e2d\u7684\u81ea\u53d1\u5bf9\u8bdd\u8bed\u6599\u5e93\uff0c\u8bc4\u4f30\u6a21\u578b\u7406\u89e3\u6307\u4ee3\u8868\u8fbe\u7684\u80fd\u529b\u3002", "result": "\u6240\u6709\u6d4b\u8bd5\u7684LVLM\u5728\u6b64\u4efb\u52a1\u4e0a\u90fd\u8868\u73b0\u4e0d\u4f73\uff0c\u672a\u80fd\u968f\u7740\u65c1\u542c\u66f4\u591a\u76f8\u540c\u53c2\u4e0e\u8005\u7684\u91cd\u590d\u5bf9\u8bdd\u800c\u663e\u793a\u51fa\u6301\u7eed\u7684\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "\u5f53\u524d\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u4eba\u7c7b\u81ea\u53d1\u5bf9\u8bdd\u4e2d\u7684\u534f\u4f5c\u6307\u4ee3\u8868\u8fbe\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u63d0\u5347\u8fd9\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2509.11517", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.11517", "abs": "https://arxiv.org/abs/2509.11517", "authors": ["Rodrigo M. Carrillo-Larco", "Jesus Lov\u00f3n Melgarejo", "Manuel Castillo-Cara", "Gusseppe Bravo-Rocca"], "title": "PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation", "comment": "https://github.com/rodrigo-carrillo/PeruMedQA", "summary": "BACKGROUND: Medical large language models (LLMS) have demonstrated remarkable\nperformance in answering medical examinations. However, the extent to which\nthis high performance is transferable to medical questions in Spanish and from\na Latin American country remains unexplored. This knowledge is crucial as\nLLM-based medical applications gain traction in Latin America. AIMS: to build a\ndataset of questions from medical examinations taken by Peruvian physicians\npursuing specialty training; to fine-tune a LLM on this dataset; to evaluate\nand compare the performance in terms of accuracy between vanilla LLMs and the\nfine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice\nquestion-answering (MCQA) datasets containing 8,380 questions spanning 12\nmedical domains (2018-2025). We selected eight medical LLMs including\nmedgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task-specific\nprompts to answer the questions appropriately. We employed parameter-efficient\nfine tuning (PEFT)and low-rant adaptation (LoRA) to fine-tune medgemma-4b-it\nutilizing all questions except those from 2025 (test set). RESULTS:\nmedgemma-27b-text-it outperformed all other models, achieving a proportion of\ncorrect answers exceeding 90% in several instances. LLMs with <10 billion\nparameters exhibited <60% of correct answers, while some exams yielded results\n<50%. The fine-tuned version of medgemma-4b-it emerged victorious agains all\nLLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters\nacross various examinations. CONCLUSIONS: For medical AI application and\nresearch that require knowledge bases from Spanish-speaking countries and those\nexhibiting similar epidemiological profiles to Peru's, interested parties\nshould utilize medgemma-27b-text-it or a fine-tuned version of medgemma-4b-it.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u79d8\u9c81\u533b\u5b66\u8003\u8bd5\u6570\u636e\u96c6PeruMedQA\uff0c\u8bc4\u4f30\u4e86\u591a\u4e2a\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u5728\u897f\u73ed\u7259\u8bed\u533b\u5b66\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0medgemma-27b-text-it\u8868\u73b0\u6700\u4f73\uff0c\u5fae\u8c03\u540e\u7684medgemma-4b-it\u4e5f\u80fd\u5ab2\u7f8e\u66f4\u5927\u53c2\u6570\u91cf\u7684\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u5728\u897f\u73ed\u7259\u8bed\u548c\u62c9\u4e01\u7f8e\u6d32\u56fd\u5bb6\u533b\u5b66\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u56e0\u4e3a\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u82f1\u8bed\u73af\u5883\uff0c\u800c\u62c9\u4e01\u7f8e\u6d32\u5730\u533a\u6b63\u5728\u5e7f\u6cdb\u5e94\u7528\u57fa\u4e8eLLM\u7684\u533b\u7597\u5e94\u7528\u3002", "method": "\u6784\u5efa\u5305\u542b8,380\u4e2a\u95ee\u9898\u7684\u79d8\u9c81\u533b\u5b66\u8003\u8bd5\u6570\u636e\u96c6PeruMedQA\uff0c\u9009\u62e98\u4e2a\u533b\u5b66LLM\u8fdb\u884c\u96f6\u6837\u672c\u6d4b\u8bd5\uff0c\u4f7f\u7528PEFT\u548cLoRA\u6280\u672f\u5bf9medgemma-4b-it\u8fdb\u884c\u5fae\u8c03\u3002", "result": "medgemma-27b-text-it\u5728\u591a\u4e2a\u8003\u8bd5\u4e2d\u6b63\u786e\u7387\u8d85\u8fc790%\uff0c\u53c2\u6570\u5c0f\u4e8e100\u4ebf\u7684\u6a21\u578b\u6b63\u786e\u7387\u4f4e\u4e8e60%\uff0c\u5fae\u8c03\u540e\u7684medgemma-4b-it\u8868\u73b0\u4f18\u4e8e\u6240\u6709\u5c0f\u4e8e100\u4ebf\u53c2\u6570\u7684\u6a21\u578b\uff0c\u5e76\u80fd\u4e0e700\u4ebf\u53c2\u6570\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002", "conclusion": "\u5bf9\u4e8e\u9700\u8981\u897f\u73ed\u7259\u8bed\u56fd\u5bb6\u548c\u7c7b\u4f3c\u79d8\u9c81\u6d41\u884c\u75c5\u5b66\u7279\u5f81\u77e5\u8bc6\u5e93\u7684\u533b\u5b66AI\u5e94\u7528\uff0c\u63a8\u8350\u4f7f\u7528medgemma-27b-text-it\u6216\u5fae\u8c03\u7248\u7684medgemma-4b-it\u3002"}}
{"id": "2509.11534", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11534", "abs": "https://arxiv.org/abs/2509.11534", "authors": ["Zhihan Cao", "Hiroaki Yamada", "Takenobu Tokunaga"], "title": "On the Distinctive Co-occurrence Characteristics of Antonymy", "comment": "Accepted by *SEM 2025", "summary": "Antonymy has long received particular attention in lexical semantics.\nPrevious studies have shown that antonym pairs frequently co-occur in text,\nacross genres and parts of speech, more often than would be expected by chance.\nHowever, whether this co-occurrence pattern is distinctive of antonymy remains\nunclear, due to a lack of comparison with other semantic relations. This work\nfills the gap by comparing antonymy with three other relations across parts of\nspeech using robust co-occurrence metrics. We find that antonymy is distinctive\nin three respects: antonym pairs co-occur with high strength, in a preferred\nlinear order, and within short spans. All results are available online.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6bd4\u8f83\u53cd\u4e49\u8bcd\u4e0e\u5176\u4ed6\u8bed\u4e49\u5173\u7cfb\u7684\u5171\u73b0\u6a21\u5f0f\uff0c\u53d1\u73b0\u53cd\u4e49\u8bcd\u5728\u5171\u73b0\u5f3a\u5ea6\u3001\u7ebf\u6027\u987a\u5e8f\u504f\u597d\u548c\u77ed\u8ddd\u79bb\u5171\u73b0\u4e09\u4e2a\u65b9\u9762\u5177\u6709\u72ec\u7279\u6027", "motivation": "\u53cd\u4e49\u8bcd\u5728\u6587\u672c\u4e2d\u9891\u7e41\u5171\u73b0\u5df2\u88ab\u8bc1\u5b9e\uff0c\u4f46\u7f3a\u4e4f\u4e0e\u5176\u4ed6\u8bed\u4e49\u5173\u7cfb\u7684\u6bd4\u8f83\uff0c\u65e0\u6cd5\u786e\u5b9a\u8fd9\u79cd\u5171\u73b0\u6a21\u5f0f\u662f\u5426\u4e3a\u53cd\u4e49\u8bcd\u6240\u7279\u6709", "method": "\u4f7f\u7528\u7a33\u5065\u7684\u5171\u73b0\u5ea6\u91cf\u65b9\u6cd5\uff0c\u6bd4\u8f83\u53cd\u4e49\u8bcd\u4e0e\u53e6\u5916\u4e09\u79cd\u8bed\u4e49\u5173\u7cfb\u5728\u4e0d\u540c\u8bcd\u6027\u95f4\u7684\u5171\u73b0\u6a21\u5f0f", "result": "\u53cd\u4e49\u8bcd\u5728\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u8868\u73b0\u51fa\u72ec\u7279\u6027\uff1a\u9ad8\u5f3a\u5ea6\u7684\u5171\u73b0\u3001\u504f\u597d\u7684\u7ebf\u6027\u987a\u5e8f\u4ee5\u53ca\u77ed\u8ddd\u79bb\u5185\u7684\u5171\u73b0", "conclusion": "\u53cd\u4e49\u8bcd\u7684\u5171\u73b0\u6a21\u5f0f\u786e\u5b9e\u5177\u6709\u72ec\u7279\u6027\uff0c\u8fd9\u4e3a\u53cd\u4e49\u8bcd\u8bc6\u522b\u548c\u8bed\u4e49\u5173\u7cfb\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u4f9d\u636e"}}
{"id": "2509.11536", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11536", "abs": "https://arxiv.org/abs/2509.11536", "authors": ["Junjie Hu", "Gang Tu", "ShengYu Cheng", "Jinxin Li", "Jinting Wang", "Rui Chen", "Zhilong Zhou", "Dongbo Shan"], "title": "HARP: Hallucination Detection via Reasoning Subspace Projection", "comment": null, "summary": "Hallucinations in Large Language Models (LLMs) pose a major barrier to their\nreliable use in critical decision-making. Although existing hallucination\ndetection methods have improved accuracy, they still struggle with\ndisentangling semantic and reasoning information and maintaining robustness. To\naddress these challenges, we propose HARP (Hallucination detection via\nreasoning subspace projection), a novel hallucination detection framework. HARP\nestablishes that the hidden state space of LLMs can be decomposed into a direct\nsum of a semantic subspace and a reasoning subspace, where the former encodes\nlinguistic expression and the latter captures internal reasoning processes.\nMoreover, we demonstrate that the Unembedding layer can disentangle these\nsubspaces, and by applying Singular Value Decomposition (SVD) to its\nparameters, the basis vectors spanning the semantic and reasoning subspaces are\nobtained. Finally, HARP projects hidden states onto the basis vectors of the\nreasoning subspace, and the resulting projections are then used as input\nfeatures for hallucination detection in LLMs. By using these projections, HARP\nreduces the dimension of the feature to approximately 5% of the original,\nfilters out most noise, and achieves enhanced robustness. Experiments across\nmultiple datasets show that HARP achieves state-of-the-art hallucination\ndetection performance; in particular, it achieves an AUROC of 92.8% on\nTriviaQA, outperforming the previous best method by 7.5%.", "AI": {"tldr": "HARP\u662f\u4e00\u4e2a\u65b0\u7684\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5c06LLM\u9690\u85cf\u72b6\u6001\u7a7a\u95f4\u5206\u89e3\u4e3a\u8bed\u4e49\u5b50\u7a7a\u95f4\u548c\u63a8\u7406\u5b50\u7a7a\u95f4\uff0c\u5229\u7528SVD\u83b7\u5f97\u57fa\u5411\u91cf\uff0c\u5e76\u5c06\u9690\u85cf\u72b6\u6001\u6295\u5f71\u5230\u63a8\u7406\u5b50\u7a7a\u95f4\u6765\u68c0\u6d4b\u5e7b\u89c9\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u5728\u5206\u79bb\u8bed\u4e49\u548c\u63a8\u7406\u4fe1\u606f\u4ee5\u53ca\u4fdd\u6301\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u68c0\u6d4b\u6846\u67b6\u3002", "method": "\u63d0\u51faHARP\u6846\u67b6\uff1a1\uff09\u8bc1\u660eLLM\u9690\u85cf\u72b6\u6001\u7a7a\u95f4\u53ef\u5206\u89e3\u4e3a\u8bed\u4e49\u5b50\u7a7a\u95f4\u548c\u63a8\u7406\u5b50\u7a7a\u95f4\uff1b2\uff09\u901a\u8fc7SVD\u5206\u89e3Unembedding\u5c42\u53c2\u6570\u83b7\u5f97\u57fa\u5411\u91cf\uff1b3\uff09\u5c06\u9690\u85cf\u72b6\u6001\u6295\u5f71\u5230\u63a8\u7406\u5b50\u7a7a\u95f4\u4f5c\u4e3a\u68c0\u6d4b\u7279\u5f81", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0cTriviaQA\u4e0aAUROC\u8fbe\u523092.8%\uff0c\u6bd4\u4e4b\u524d\u6700\u4f73\u65b9\u6cd5\u63d0\u53477.5%\uff0c\u7279\u5f81\u7ef4\u5ea6\u964d\u81f3\u539f\u59cb\u76845%\u5e76\u6709\u6548\u8fc7\u6ee4\u566a\u58f0", "conclusion": "HARP\u901a\u8fc7\u5b50\u7a7a\u95f4\u5206\u89e3\u548c\u6295\u5f71\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5e7b\u89c9\u68c0\u6d4b\u4e2d\u7684\u8bed\u4e49-\u63a8\u7406\u5206\u79bb\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u4e3aLLM\u53ef\u9760\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u652f\u6491"}}
{"id": "2509.11552", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11552", "abs": "https://arxiv.org/abs/2509.11552", "authors": ["Wensheng Lu", "Keyu Chen", "Ruizhi Qiao", "Xing Sun"], "title": "HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking", "comment": "17 pages, 5 figures, 6 tables", "summary": "Retrieval-Augmented Generation (RAG) enhances the response capabilities of\nlanguage models by integrating external knowledge sources. However, document\nchunking as an important part of RAG system often lacks effective evaluation\ntools. This paper first analyzes why existing RAG evaluation benchmarks are\ninadequate for assessing document chunking quality, specifically due to\nevidence sparsity. Based on this conclusion, we propose HiCBench, which\nincludes manually annotated multi-level document chunking points, synthesized\nevidence-dense quetion answer(QA) pairs, and their corresponding evidence\nsources. Additionally, we introduce the HiChunk framework, a multi-level\ndocument structuring framework based on fine-tuned LLMs, combined with the\nAuto-Merge retrieval algorithm to improve retrieval quality. Experiments\ndemonstrate that HiCBench effectively evaluates the impact of different\nchunking methods across the entire RAG pipeline. Moreover, HiChunk achieves\nbetter chunking quality within reasonable time consumption, thereby enhancing\nthe overall performance of RAG systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86HiCBench\u8bc4\u4f30\u57fa\u51c6\u548cHiChunk\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3RAG\u7cfb\u7edf\u4e2d\u6587\u6863\u5206\u5757\u8d28\u91cf\u8bc4\u4f30\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u7ea7\u5206\u5757\u6807\u6ce8\u548c\u8bc1\u636e\u5bc6\u96c6QA\u5bf9\u6765\u63d0\u5347\u8bc4\u4f30\u6548\u679c\u3002", "motivation": "\u73b0\u6709RAG\u8bc4\u4f30\u57fa\u51c6\u5728\u8bc4\u4f30\u6587\u6863\u5206\u5757\u8d28\u91cf\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u8bc1\u636e\u7a00\u758f\u6027\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u63d0\u51faHiCBench\u8bc4\u4f30\u57fa\u51c6\uff08\u5305\u542b\u624b\u52a8\u6807\u6ce8\u7684\u591a\u7ea7\u6587\u6863\u5206\u5757\u70b9\u3001\u5408\u6210\u7684\u8bc1\u636e\u5bc6\u96c6QA\u5bf9\u53ca\u5176\u8bc1\u636e\u6e90\uff09\u548cHiChunk\u6846\u67b6\uff08\u57fa\u4e8e\u5fae\u8c03LLM\u7684\u591a\u7ea7\u6587\u6863\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u7ed3\u5408Auto-Merge\u68c0\u7d22\u7b97\u6cd5\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660eHiCBench\u80fd\u6709\u6548\u8bc4\u4f30\u4e0d\u540c\u5206\u5757\u65b9\u6cd5\u5728\u6574\u4e2aRAG\u6d41\u7a0b\u4e2d\u7684\u5f71\u54cd\uff0cHiChunk\u5728\u5408\u7406\u65f6\u95f4\u6d88\u8017\u5185\u83b7\u5f97\u66f4\u597d\u7684\u5206\u5757\u8d28\u91cf\uff0c\u63d0\u5347RAG\u7cfb\u7edf\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "HiCBench\u548cHiChunk\u6846\u67b6\u4e3aRAG\u7cfb\u7edf\u7684\u6587\u6863\u5206\u5757\u8d28\u91cf\u8bc4\u4f30\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2509.11569", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11569", "abs": "https://arxiv.org/abs/2509.11569", "authors": ["Yue Ding", "Xiaofang Zhu", "Tianze Xia", "Junfei Wu", "Xinlong Chen", "Qiang Liu", "Liang Wang"], "title": "D$^2$HScore: Reasoning-Aware Hallucination Detection via Semantic Breadth and Depth Analysis in LLMs", "comment": "under review", "summary": "Although large Language Models (LLMs) have achieved remarkable success, their\npractical application is often hindered by the generation of non-factual\ncontent, which is called \"hallucination\". Ensuring the reliability of LLMs'\noutputs is a critical challenge, particularly in high-stakes domains such as\nfinance, security, and healthcare. In this work, we revisit hallucination\ndetection from the perspective of model architecture and generation dynamics.\nLeveraging the multi-layer structure and autoregressive decoding process of\nLLMs, we decompose hallucination signals into two complementary dimensions: the\nsemantic breadth of token representations within each layer, and the semantic\ndepth of core concepts as they evolve across layers. Based on this insight, we\npropose \\textbf{D$^2$HScore (Dispersion and Drift-based Hallucination Score)},\na training-free and label-free framework that jointly measures: (1)\n\\textbf{Intra-Layer Dispersion}, which quantifies the semantic diversity of\ntoken representations within each layer; and (2) \\textbf{Inter-Layer Drift},\nwhich tracks the progressive transformation of key token representations across\nlayers. To ensure drift reflects the evolution of meaningful semantics rather\nthan noisy or redundant tokens, we guide token selection using attention\nsignals. By capturing both the horizontal and vertical dynamics of\nrepresentation during inference, D$^2$HScore provides an interpretable and\nlightweight proxy for hallucination detection. Extensive experiments across\nfive open-source LLMs and five widely used benchmarks demonstrate that\nD$^2$HScore consistently outperforms existing training-free baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86D\u00b2HScore\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790LLM\u5185\u90e8\u8868\u793a\u5c42\u7684\u5206\u6563\u5ea6\u548c\u8de8\u5c42\u6f02\u79fb\u6765\u68c0\u6d4b\u5e7b\u89c9\uff0c\u65e0\u9700\u8bad\u7ec3\u548c\u6807\u7b7e\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u89e3\u51b3LLM\u751f\u6210\u975e\u4e8b\u5b9e\u5185\u5bb9\uff08\u5e7b\u89c9\uff09\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u91d1\u878d\u3001\u5b89\u5168\u548c\u533b\u7597\u7b49\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u9700\u8981\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u786e\u4fdd\u8f93\u51fa\u771f\u5b9e\u6027", "method": "\u57fa\u4e8eLLM\u7684\u591a\u5c42\u7ed3\u6784\u548c\u81ea\u56de\u5f52\u89e3\u7801\u8fc7\u7a0b\uff0c\u63d0\u51faD\u00b2HScore\u6846\u67b6\uff1a1\uff09\u5c42\u5185\u5206\u6563\u5ea6 - \u91cf\u5316\u6bcf\u5c42token\u8868\u793a\u7684\u8bed\u4e49\u591a\u6837\u6027\uff1b2\uff09\u8de8\u5c42\u6f02\u79fb - \u8ddf\u8e2a\u5173\u952etoken\u8868\u793a\u5728\u5c42\u95f4\u7684\u6e10\u8fdb\u53d8\u6362\uff0c\u4f7f\u7528\u6ce8\u610f\u529b\u4fe1\u53f7\u6307\u5bfctoken\u9009\u62e9", "result": "\u57285\u4e2a\u5f00\u6e90LLM\u548c5\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0cD\u00b2HScore\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u9700\u8bad\u7ec3\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "D\u00b2HScore\u901a\u8fc7\u6355\u6349\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8868\u793a\u7684\u6c34\u5e73\u548c\u5782\u76f4\u52a8\u6001\uff0c\u4e3a\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u8f7b\u91cf\u7ea7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u591a\u4e2a\u6a21\u578b\u548c\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272"}}
{"id": "2509.11570", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11570", "abs": "https://arxiv.org/abs/2509.11570", "authors": ["Sampoorna Poria", "Xiaolei Huang"], "title": "Bhaasha, Bhasa, Zaban: A Survey for Low-Resourced Languages in South Asia -- Current Stage and Challenges", "comment": null, "summary": "Rapid developments of large language models have revolutionized many NLP\ntasks for English data. Unfortunately, the models and their evaluations for\nlow-resource languages are being overlooked, especially for languages in South\nAsia. Although there are more than 650 languages in South Asia, many of them\neither have very limited computational resources or are missing from existing\nlanguage models. Thus, a concrete question to be answered is: Can we assess the\ncurrent stage and challenges to inform our NLP community and facilitate model\ndevelopments for South Asian languages? In this survey, we have comprehensively\nexamined current efforts and challenges of NLP models for South Asian languages\nby retrieving studies since 2020, with a focus on transformer-based models,\nsuch as BERT, T5, & GPT. We present advances and gaps across 3 essential\naspects: data, models, & tasks, such as available data sources, fine-tuning\nstrategies, & domain applications. Our findings highlight substantial issues,\nincluding missing data in critical domains (e.g., health), code-mixing, and\nlack of standardized evaluation benchmarks. Our survey aims to raise awareness\nwithin the NLP community for more targeted data curation, unify benchmarks\ntailored to cultural and linguistic nuances of South Asia, and encourage an\nequitable representation of South Asian languages. The complete list of\nresources is available at: https://github.com/trust-nlp/LM4SouthAsia-Survey.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u8c03\u67e5\u4e86\u5357\u4e9a\u8bed\u8a00NLP\u6a21\u578b\u7684\u73b0\u72b6\u548c\u6311\u6218\uff0c\u91cd\u70b9\u5173\u6ce8\u6570\u636e\u3001\u6a21\u578b\u548c\u4efb\u52a1\u4e09\u4e2a\u6838\u5fc3\u65b9\u9762\uff0c\u63ed\u793a\u4e86\u6570\u636e\u7f3a\u5931\u3001\u4ee3\u7801\u6df7\u5408\u548c\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u57fa\u51c6\u7b49\u5173\u952e\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u82f1\u8bed\u6570\u636e\u4e0a\u53d6\u5f97\u4e86\u9769\u547d\u6027\u8fdb\u5c55\uff0c\u4f46\u5357\u4e9a\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u88ab\u5ffd\u89c6\u3002\u5357\u4e9a\u6709650\u591a\u79cd\u8bed\u8a00\uff0c\u8bb8\u591a\u8bed\u8a00\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u6216\u5b8c\u5168\u7f3a\u5931\uff0c\u9700\u8981\u8bc4\u4f30\u5f53\u524d\u72b6\u51b5\u4ee5\u4fc3\u8fdb\u6a21\u578b\u53d1\u5c55\u3002", "method": "\u5168\u9762\u68c0\u7d222020\u5e74\u4ee5\u6765\u7684\u7814\u7a76\uff0c\u91cd\u70b9\u5173\u6ce8\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff08\u5982BERT\u3001T5\u3001GPT\uff09\uff0c\u4ece\u6570\u636e\u3001\u6a21\u578b\u548c\u4efb\u52a1\u4e09\u4e2a\u7ef4\u5ea6\u5206\u6790\u5357\u4e9a\u8bed\u8a00NLP\u7684\u8fdb\u5c55\u548c\u5dee\u8ddd\u3002", "result": "\u53d1\u73b0\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff1a\u5173\u952e\u9886\u57df\uff08\u5982\u533b\u7597\uff09\u6570\u636e\u7f3a\u5931\u3001\u4ee3\u7801\u6df7\u5408\u73b0\u8c61\u666e\u904d\u3001\u7f3a\u4e4f\u9488\u5bf9\u5357\u4e9a\u6587\u5316\u548c\u8bed\u8a00\u7279\u70b9\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u57fa\u51c6\u3002", "conclusion": "\u547c\u5401NLP\u793e\u533a\u63d0\u9ad8\u5bf9\u5357\u4e9a\u8bed\u8a00\u7684\u5173\u6ce8\uff0c\u8fdb\u884c\u66f4\u6709\u9488\u5bf9\u6027\u7684\u6570\u636e\u6574\u7406\uff0c\u5efa\u7acb\u9002\u5408\u5357\u4e9a\u8bed\u8a00\u6587\u5316\u7279\u70b9\u7684\u7edf\u4e00\u57fa\u51c6\uff0c\u4fc3\u8fdb\u5357\u4e9a\u8bed\u8a00\u7684\u516c\u5e73\u4ee3\u8868\u6027\u3002"}}
{"id": "2509.11591", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11591", "abs": "https://arxiv.org/abs/2509.11591", "authors": ["Chu-Hsuan Lee", "Chen-Chi Chang", "Hung-Shin Lee", "Yun-Hsiang Hsu", "Ching-Yuan Chen"], "title": "Analyzing Information-Seeking Behaviors in a Hakka AI Chatbot: A Cognitive-Pragmatic Study", "comment": "Accepted to HICSS-59 (2026)", "summary": "With many endangered languages at risk of disappearing, efforts to preserve\nthem now rely more than ever on using technology alongside culturally informed\nteaching strategies. This study examines user behaviors in TALKA, a generative\nAI-powered chatbot designed for Hakka language engagement, by employing a\ndual-layered analytical framework grounded in Bloom's Taxonomy of cognitive\nprocesses and dialogue act categorization. We analyzed 7,077 user utterances,\neach carefully annotated according to six cognitive levels and eleven dialogue\nact types. These included a variety of functions, such as asking for\ninformation, requesting translations, making cultural inquiries, and using\nlanguage creatively. Pragmatic classifications further highlight how different\ntypes of dialogue acts--such as feedback, control commands, and social\ngreetings--align with specific cognitive intentions. The results suggest that\ngenerative AI chatbots can support language learning in meaningful\nways--especially when they are designed with an understanding of how users\nthink and communicate. They may also help learners express themselves more\nconfidently and connect with their cultural identity. The TALKA case provides\nempirical insights into how AI-mediated dialogue facilitates cognitive\ndevelopment in low-resource language learners, as well as pragmatic negotiation\nand socio-cultural affiliation. By focusing on AI-assisted language learning,\nthis study offers new insights into how technology can support language\npreservation and educational practice.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790Hakka\u8bed\u8a00AI\u804a\u5929\u673a\u5668\u4ebaTALKA\u4e2d7077\u6761\u7528\u6237\u5bf9\u8bdd\uff0c\u7ed3\u5408\u5e03\u9c81\u59c6\u8ba4\u77e5\u5206\u7c7b\u548c\u5bf9\u8bdd\u884c\u4e3a\u5206\u7c7b\u6846\u67b6\uff0c\u53d1\u73b0\u751f\u6210\u5f0fAI\u80fd\u6709\u6548\u652f\u6301\u4f4e\u8d44\u6e90\u8bed\u8a00\u5b66\u4e60\uff0c\u4fc3\u8fdb\u8ba4\u77e5\u53d1\u5c55\u548c\u6587\u5316\u8ba4\u540c\u3002", "motivation": "\u968f\u7740\u6fd2\u5371\u8bed\u8a00\u9762\u4e34\u6d88\u5931\u98ce\u9669\uff0c\u9700\u8981\u7ed3\u5408\u6280\u672f\u548c\u6587\u5316\u6559\u5b66\u7b56\u7565\u8fdb\u884c\u4fdd\u62a4\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u751f\u6210\u5f0fAI\u804a\u5929\u673a\u5668\u4eba\u5982\u4f55\u652f\u6301\u8bed\u8a00\u5b66\u4e60\u548c\u6587\u5316\u4f20\u627f\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u6b21\u5206\u6790\u6846\u67b6\uff1a\u5e03\u9c81\u59c6\u8ba4\u77e5\u5206\u7c7b\uff086\u4e2a\u8ba4\u77e5\u5c42\u7ea7\uff09\u548c\u5bf9\u8bdd\u884c\u4e3a\u5206\u7c7b\uff0811\u79cd\u5bf9\u8bdd\u7c7b\u578b\uff09\uff0c\u5bf97077\u6761\u7528\u6237\u8bdd\u8bed\u8fdb\u884c\u7cbe\u7ec6\u6807\u6ce8\u548c\u5206\u6790\u3002", "result": "\u751f\u6210\u5f0fAI\u804a\u5929\u673a\u5668\u4eba\u80fd\u6709\u6548\u652f\u6301\u8bed\u8a00\u5b66\u4e60\uff0c\u5e2e\u52a9\u5b66\u4e60\u8005\u66f4\u81ea\u4fe1\u5730\u8868\u8fbe\u5e76\u4e0e\u6587\u5316\u8eab\u4efd\u5efa\u7acb\u8054\u7cfb\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u4fc3\u8fdb\u8ba4\u77e5\u53d1\u5c55\u548c\u8bed\u7528\u534f\u5546\u3002", "conclusion": "AI\u8f85\u52a9\u8bed\u8a00\u5b66\u4e60\u4e3a\u8bed\u8a00\u4fdd\u62a4\u548c\u6559\u80b2\u5b9e\u8df5\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u8bbe\u8ba1\u65f6\u9700\u8981\u7406\u89e3\u7528\u6237\u7684\u601d\u7ef4\u548c\u6c9f\u901a\u65b9\u5f0f\uff0c\u6280\u672f\u53ef\u4ee5\u6210\u4e3a\u8bed\u8a00\u4fdd\u5b58\u7684\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2509.11604", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11604", "abs": "https://arxiv.org/abs/2509.11604", "authors": ["Md. Mithun Hossain", "Sanjara", "Md. Shakil Hossain", "Sudipto Chaki"], "title": "Dynamic Span Interaction and Graph-Aware Memory for Entity-Level Sentiment Classification", "comment": null, "summary": "Entity-level sentiment classification involves identifying the sentiment\npolarity linked to specific entities within text. This task poses several\nchallenges: effectively modeling the subtle and complex interactions between\nentities and their surrounding sentiment expressions; capturing dependencies\nthat may span across sentences; and ensuring consistent sentiment predictions\nfor multiple mentions of the same entity through coreference resolution.\nAdditionally, linguistic phenomena such as negation, ambiguity, and overlapping\nopinions further complicate the analysis. These complexities make entity-level\nsentiment classification a difficult problem, especially in real-world, noisy\ntextual data. To address these issues, we propose SpanEIT, a novel framework\nintegrating dynamic span interaction and graph-aware memory mechanisms for\nenhanced entity-sentiment relational modeling. SpanEIT builds span-based\nrepresentations for entities and candidate sentiment phrases, employs\nbidirectional attention for fine-grained interactions, and uses a graph\nattention network to capture syntactic and co-occurrence relations. A\ncoreference-aware memory module ensures entity-level consistency across\ndocuments. Experiments on FSAD, BARU, and IMDB datasets show SpanEIT\noutperforms state-of-the-art transformer and hybrid baselines in accuracy and\nF1 scores. Ablation and interpretability analyses validate the effectiveness of\nour approach, underscoring its potential for fine-grained sentiment analysis in\napplications like social media monitoring and customer feedback analysis.", "AI": {"tldr": "SpanEIT\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u5b9e\u4f53\u7ea7\u60c5\u611f\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8de8\u5ea6\u4ea4\u4e92\u548c\u56fe\u611f\u77e5\u8bb0\u5fc6\u673a\u5236\u589e\u5f3a\u5b9e\u4f53-\u60c5\u611f\u5173\u7cfb\u5efa\u6a21\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5b9e\u4f53\u7ea7\u60c5\u611f\u5206\u7c7b\u9762\u4e34\u591a\u4e2a\u6311\u6218\uff1a\u9700\u8981\u5efa\u6a21\u5b9e\u4f53\u4e0e\u60c5\u611f\u8868\u8fbe\u7684\u590d\u6742\u4ea4\u4e92\u3001\u5904\u7406\u8de8\u53e5\u4f9d\u8d56\u5173\u7cfb\u3001\u786e\u4fdd\u540c\u4e00\u5b9e\u4f53\u591a\u6b21\u63d0\u53ca\u7684\u60c5\u611f\u4e00\u81f4\u6027\uff0c\u4ee5\u53ca\u5904\u7406\u5426\u5b9a\u3001\u6b67\u4e49\u7b49\u8bed\u8a00\u73b0\u8c61\u3002", "method": "\u63d0\u51faSpanEIT\u6846\u67b6\uff0c\u6784\u5efa\u57fa\u4e8e\u8de8\u5ea6\u7684\u5b9e\u4f53\u548c\u60c5\u611f\u77ed\u8bed\u8868\u793a\uff0c\u4f7f\u7528\u53cc\u5411\u6ce8\u610f\u529b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4ea4\u4e92\uff0c\u91c7\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u6355\u83b7\u53e5\u6cd5\u548c\u5171\u73b0\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5171\u6307\u611f\u77e5\u8bb0\u5fc6\u6a21\u5757\u786e\u4fdd\u6587\u6863\u7ea7\u4e00\u81f4\u6027\u3002", "result": "\u5728FSAD\u3001BARU\u548cIMDB\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSpanEIT\u5728\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684transformer\u548c\u6df7\u5408\u57fa\u7ebf\u65b9\u6cd5\u3002\u6d88\u878d\u5b9e\u9a8c\u548c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "SpanEIT\u6846\u67b6\u5728\u7ec6\u7c92\u5ea6\u60c5\u611f\u5206\u6790\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u7279\u522b\u9002\u7528\u4e8e\u793e\u4ea4\u5a92\u4f53\u76d1\u63a7\u548c\u5ba2\u6237\u53cd\u9988\u5206\u6790\u7b49\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2509.11619", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11619", "abs": "https://arxiv.org/abs/2509.11619", "authors": ["Spandan Anaokar", "Shrey Ganatra", "Harshvivek Kashid", "Swapnil Bhattacharyya", "Shruti Nair", "Reshma Sekhar", "Siddharth Manohar", "Rahul Hemrajani", "Pushpak Bhattacharyya"], "title": "HalluDetect: Detecting, Mitigating, and Benchmarking Hallucinations in Conversational Systems", "comment": "6 pages + references + appendix, 3 figures, 2 tables", "summary": "Large Language Models (LLMs) are widely used in industry but remain prone to\nhallucinations, limiting their reliability in critical applications. This work\naddresses hallucination reduction in consumer grievance chatbots built using\nLLaMA 3.1 8B Instruct, a compact model frequently used in industry. We develop\nHalluDetect, an LLM-based hallucination detection system that achieves an F1\nscore of 69% outperforming baseline detectors by 25.44%. Benchmarking five\nchatbot architectures, we find that out of them, AgentBot minimizes\nhallucinations to 0.4159 per turn while maintaining the highest token accuracy\n(96.13%), making it the most effective mitigation strategy. Our findings\nprovide a scalable framework for hallucination mitigation, demonstrating that\noptimized inference strategies can significantly improve factual accuracy.\nWhile applied to consumer law, our approach generalizes to other high-risk\ndomains, enhancing trust in LLM-driven assistants. We will release the code and\ndataset", "AI": {"tldr": "\u5f00\u53d1\u4e86\u57fa\u4e8eLLM\u7684\u5e7b\u89c9\u68c0\u6d4b\u7cfb\u7edfHalluDetect\uff0c\u5728\u6d88\u8d39\u8005\u6295\u8bc9\u804a\u5929\u673a\u5668\u4eba\u4e2d\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\uff0cF1\u5206\u6570\u8fbe69%\uff0c\u6bd4\u57fa\u7ebf\u63d0\u534725.44%\u3002AgentBot\u67b6\u6784\u5c06\u6bcf\u8f6e\u5e7b\u89c9\u964d\u81f30.4159\u4e2a\uff0c\u540c\u65f6\u4fdd\u630196.13%\u7684\u6700\u9ad8token\u51c6\u786e\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u4f46\u4ecd\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u9650\u5236\u4e86\u5176\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u6d88\u8d39\u8005\u6295\u8bc9\u5904\u7406\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u9700\u8981\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "method": "\u57fa\u4e8eLLaMA 3.1 8B Instruct\u6a21\u578b\u5f00\u53d1HalluDetect\u5e7b\u89c9\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5bf9\u4e94\u79cd\u804a\u5929\u673a\u5668\u4eba\u67b6\u6784\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91cd\u70b9\u8bc4\u4f30AgentBot\u67b6\u6784\u7684\u5e7b\u89c9\u7f13\u89e3\u6548\u679c\u3002", "result": "HalluDetect\u7cfb\u7edfF1\u5206\u6570\u8fbe\u523069%\uff0c\u6bd4\u57fa\u7ebf\u68c0\u6d4b\u5668\u63d0\u534725.44%\u3002AgentBot\u67b6\u6784\u8868\u73b0\u6700\u4f73\uff0c\u6bcf\u8f6e\u5e7b\u89c9\u964d\u81f30.4159\u4e2a\uff0ctoken\u51c6\u786e\u7387\u8fbe96.13%\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5e7b\u89c9\u7f13\u89e3\u6846\u67b6\uff0c\u8bc1\u660e\u4f18\u5316\u7684\u63a8\u7406\u7b56\u7565\u80fd\u663e\u8457\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u6d88\u8d39\u8005\u6cd5\u5f8b\u9886\u57df\uff0c\u8fd8\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u589e\u5f3a\u5bf9LLM\u9a71\u52a8\u52a9\u624b\u7684\u4fe1\u4efb\u3002"}}
{"id": "2509.11620", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.11620", "abs": "https://arxiv.org/abs/2509.11620", "authors": ["Kun Li", "Lai-Man Po", "Hongzheng Yang", "Xuyuan Xu", "Kangcheng Liu", "Yuzhi Zhao"], "title": "AesBiasBench: Evaluating Bias and Alignment in Multimodal Language Models for Personalized Image Aesthetic Assessment", "comment": "Accepted by EMNLP 2025", "summary": "Multimodal Large Language Models (MLLMs) are increasingly applied in\nPersonalized Image Aesthetic Assessment (PIAA) as a scalable alternative to\nexpert evaluations. However, their predictions may reflect subtle biases\ninfluenced by demographic factors such as gender, age, and education. In this\nwork, we propose AesBiasBench, a benchmark designed to evaluate MLLMs along two\ncomplementary dimensions: (1) stereotype bias, quantified by measuring\nvariations in aesthetic evaluations across demographic groups; and (2)\nalignment between model outputs and genuine human aesthetic preferences. Our\nbenchmark covers three subtasks (Aesthetic Perception, Assessment, Empathy) and\nintroduces structured metrics (IFD, NRD, AAS) to assess both bias and\nalignment. We evaluate 19 MLLMs, including proprietary models (e.g., GPT-4o,\nClaude-3.5-Sonnet) and open-source models (e.g., InternVL-2.5, Qwen2.5-VL).\nResults indicate that smaller models exhibit stronger stereotype biases,\nwhereas larger models align more closely with human preferences. Incorporating\nidentity information often exacerbates bias, particularly in emotional\njudgments. These findings underscore the importance of identity-aware\nevaluation frameworks in subjective vision-language tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86AesBiasBench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2a\u6027\u5316\u56fe\u50cf\u7f8e\u5b66\u8bc4\u4f30\u4e2d\u7684\u523b\u677f\u5370\u8c61\u504f\u89c1\u548c\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u6d4b\u8bd5\u4e8619\u4e2a\u6a21\u578b\u53d1\u73b0\u5c0f\u6a21\u578b\u504f\u89c1\u66f4\u5f3a\uff0c\u5927\u6a21\u578b\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2a\u6027\u5316\u56fe\u50cf\u7f8e\u5b66\u8bc4\u4f30\u4e2d\u5b58\u5728\u53d7\u4eba\u53e3\u7edf\u8ba1\u56e0\u7d20\uff08\u6027\u522b\u3001\u5e74\u9f84\u3001\u6559\u80b2\u7b49\uff09\u5f71\u54cd\u7684\u5fae\u5999\u504f\u89c1\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u8fd9\u4e9b\u504f\u89c1\u548c\u5bf9\u9f50\u7a0b\u5ea6\u3002", "method": "\u6784\u5efaAesBiasBench\u57fa\u51c6\uff0c\u5305\u542b\u4e09\u4e2a\u5b50\u4efb\u52a1\uff08\u7f8e\u5b66\u611f\u77e5\u3001\u8bc4\u4f30\u3001\u5171\u60c5\uff09\uff0c\u5f15\u5165\u7ed3\u6784\u5316\u6307\u6807\uff08IFD\u3001NRD\u3001AAS\uff09\u6765\u8bc4\u4f30\u504f\u89c1\u548c\u5bf9\u9f50\uff0c\u6d4b\u8bd5\u4e8619\u4e2aMLLM\u6a21\u578b\u3002", "result": "\u5c0f\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u523b\u677f\u5370\u8c61\u504f\u89c1\uff0c\u5927\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u66f4\u4e00\u81f4\uff1b\u52a0\u5165\u8eab\u4efd\u4fe1\u606f\u901a\u5e38\u4f1a\u52a0\u5267\u504f\u89c1\uff0c\u7279\u522b\u662f\u5728\u60c5\u611f\u5224\u65ad\u65b9\u9762\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u4e3b\u89c2\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u8eab\u4efd\u611f\u77e5\u8bc4\u4f30\u6846\u67b6\u7684\u91cd\u8981\u6027\uff0c\u9700\u8981\u5173\u6ce8\u6a21\u578b\u504f\u89c1\u548c\u5bf9\u9f50\u95ee\u9898\u3002"}}
{"id": "2509.11648", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.11648", "abs": "https://arxiv.org/abs/2509.11648", "authors": ["Sai Kartheek Reddy Kasu"], "title": "EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI", "comment": null, "summary": "The deployment of large language models (LLMs) in mental health and other\nsensitive domains raises urgent questions about ethical reasoning, fairness,\nand responsible alignment. Yet, existing benchmarks for moral and clinical\ndecision-making do not adequately capture the unique ethical dilemmas\nencountered in mental health practice, where confidentiality, autonomy,\nbeneficence, and bias frequently intersect. To address this gap, we introduce\nEthical Reasoning in Mental Health (EthicsMH), a pilot dataset of 125 scenarios\ndesigned to evaluate how AI systems navigate ethically charged situations in\ntherapeutic and psychiatric contexts. Each scenario is enriched with structured\nfields, including multiple decision options, expert-aligned reasoning, expected\nmodel behavior, real-world impact, and multi-stakeholder viewpoints. This\nstructure enables evaluation not only of decision accuracy but also of\nexplanation quality and alignment with professional norms. Although modest in\nscale and developed with model-assisted generation, EthicsMH establishes a task\nframework that bridges AI ethics and mental health decision-making. By\nreleasing this dataset, we aim to provide a seed resource that can be expanded\nthrough community and expert contributions, fostering the development of AI\nsystems capable of responsibly handling some of society's most delicate\ndecisions.", "AI": {"tldr": "\u63d0\u51fa\u4e86EthicsMH\u6570\u636e\u96c6\uff0c\u5305\u542b125\u4e2a\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u4f26\u7406\u56f0\u5883\u573a\u666f\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u7cfb\u7edf\u5728\u6cbb\u7597\u548c\u7cbe\u795e\u75c5\u5b66\u80cc\u666f\u4e0b\u7684\u4f26\u7406\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709\u9053\u5fb7\u548c\u4e34\u5e8a\u51b3\u7b56\u57fa\u51c6\u65e0\u6cd5\u5145\u5206\u6355\u6349\u5fc3\u7406\u5065\u5eb7\u5b9e\u8df5\u4e2d\u72ec\u7279\u7684\u4f26\u7406\u56f0\u5883\uff0c\u5982\u4fdd\u5bc6\u6027\u3001\u81ea\u4e3b\u6027\u3001\u4ec1\u6148\u548c\u504f\u89c1\u7b49\u95ee\u9898\u7684\u4ea4\u96c6", "method": "\u5f00\u53d1\u4e86\u5305\u542b125\u4e2a\u573a\u666f\u7684\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u573a\u666f\u90fd\u6709\u7ed3\u6784\u5316\u5b57\u6bb5\uff1a\u591a\u79cd\u51b3\u7b56\u9009\u9879\u3001\u4e13\u5bb6\u5bf9\u9f50\u7684\u63a8\u7406\u3001\u9884\u671f\u6a21\u578b\u884c\u4e3a\u3001\u73b0\u5b9e\u4e16\u754c\u5f71\u54cd\u548c\u591a\u5229\u76ca\u76f8\u5173\u8005\u89c2\u70b9", "result": "\u5efa\u7acb\u4e86\u8fde\u63a5AI\u4f26\u7406\u548c\u5fc3\u7406\u5065\u5eb7\u51b3\u7b56\u7684\u4efb\u52a1\u6846\u67b6\uff0c\u80fd\u591f\u8bc4\u4f30\u51b3\u7b56\u51c6\u786e\u6027\u3001\u89e3\u91ca\u8d28\u91cf\u548c\u4e13\u4e1a\u89c4\u8303\u5bf9\u9f50\u6027", "conclusion": "EthicsMH\u4f5c\u4e3a\u79cd\u5b50\u8d44\u6e90\u53d1\u5e03\uff0c\u65e8\u5728\u901a\u8fc7\u793e\u533a\u548c\u4e13\u5bb6\u8d21\u732e\u8fdb\u884c\u6269\u5c55\uff0c\u4fc3\u8fdb\u5f00\u53d1\u80fd\u591f\u8d1f\u8d23\u4efb\u5904\u7406\u793e\u4f1a\u6700\u5fae\u5999\u51b3\u7b56\u7684AI\u7cfb\u7edf"}}
{"id": "2509.11687", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11687", "abs": "https://arxiv.org/abs/2509.11687", "authors": ["Di Jin", "Jun Yang", "Xiaobao Wang", "Junwei Zhang", "Shuqi Li", "Dongxiao He"], "title": "A Dynamic Knowledge Update-Driven Model with Large Language Models for Fake News Detection", "comment": null, "summary": "As the Internet and social media evolve rapidly, distinguishing credible news\nfrom a vast amount of complex information poses a significant challenge. Due to\nthe suddenness and instability of news events, the authenticity labels of news\ncan potentially shift as events develop, making it crucial for fake news\ndetection to obtain the latest event updates. Existing methods employ\nretrieval-augmented generation to fill knowledge gaps, but they suffer from\nissues such as insufficient credibility of retrieved content and interference\nfrom noisy information. We propose a dynamic knowledge update-driven model for\nfake news detection (DYNAMO), which leverages knowledge graphs to achieve\ncontinuous updating of new knowledge and integrates with large language models\nto fulfill dual functions: news authenticity detection and verification of new\nknowledge correctness, solving the two key problems of ensuring the\nauthenticity of new knowledge and deeply mining news semantics. Specifically,\nwe first construct a news-domain-specific knowledge graph. Then, we use Monte\nCarlo Tree Search to decompose complex news and verify them step by step.\nFinally, we extract and update new knowledge from verified real news texts and\nreasoning paths. Experimental results demonstrate that DYNAMO achieves the best\nperformance on two real-world datasets.", "AI": {"tldr": "DYNAMO\u6a21\u578b\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u52a8\u6001\u66f4\u65b0\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96c6\u6210\uff0c\u89e3\u51b3\u5047\u65b0\u95fb\u68c0\u6d4b\u4e2d\u7684\u77e5\u8bc6\u771f\u5b9e\u6027\u548c\u8bed\u4e49\u6df1\u5ea6\u6316\u6398\u95ee\u9898\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73", "motivation": "\u4e92\u8054\u7f51\u548c\u793e\u4ea4\u5a92\u4f53\u5feb\u901f\u53d1\u5c55\uff0c\u6d77\u91cf\u590d\u6742\u4fe1\u606f\u4e2d\u533a\u5206\u53ef\u4fe1\u65b0\u95fb\u9762\u4e34\u6311\u6218\u3002\u65b0\u95fb\u4e8b\u4ef6\u7684\u7a81\u53d1\u6027\u548c\u4e0d\u7a33\u5b9a\u6027\u5bfc\u81f4\u771f\u5b9e\u6027\u6807\u7b7e\u53ef\u80fd\u53d8\u5316\uff0c\u9700\u8981\u83b7\u53d6\u6700\u65b0\u4e8b\u4ef6\u66f4\u65b0\u3002\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u68c0\u7d22\u5185\u5bb9\u53ef\u4fe1\u5ea6\u4e0d\u8db3\u548c\u566a\u58f0\u4fe1\u606f\u5e72\u6270\u95ee\u9898", "method": "\u6784\u5efa\u65b0\u95fb\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u56fe\u8c31\uff0c\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u5206\u89e3\u590d\u6742\u65b0\u95fb\u5e76\u9010\u6b65\u9a8c\u8bc1\uff0c\u4ece\u5df2\u9a8c\u8bc1\u7684\u771f\u5b9e\u65b0\u95fb\u6587\u672c\u548c\u63a8\u7406\u8def\u5f84\u4e2d\u63d0\u53d6\u66f4\u65b0\u65b0\u77e5\u8bc6", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f73\u6027\u80fd", "conclusion": "DYNAMO\u6a21\u578b\u901a\u8fc7\u52a8\u6001\u77e5\u8bc6\u66f4\u65b0\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u5047\u65b0\u95fb\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u786e\u4fdd\u4e86\u65b0\u77e5\u8bc6\u7684\u771f\u5b9e\u6027\u548c\u65b0\u95fb\u8bed\u4e49\u7684\u6df1\u5ea6\u6316\u6398"}}
{"id": "2509.11698", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "I.2.7; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.11698", "abs": "https://arxiv.org/abs/2509.11698", "authors": ["Wei-Hsin Yeh", "Yu-An Su", "Chih-Ning Chen", "Yi-Hsueh Lin", "Calvin Ku", "Wen-Hsin Chiu", "Min-Chun Hu", "Lun-Wei Ku"], "title": "CoachMe: Decoding Sport Elements with a Reference-Based Coaching Instruction Generation Model", "comment": "Published in Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025.\n  Official version: https://doi.org/10.18653/v1/2025.acl-long.1413", "summary": "Motion instruction is a crucial task that helps athletes refine their\ntechnique by analyzing movements and providing corrective guidance. Although\nrecent advances in multimodal models have improved motion understanding,\ngenerating precise and sport-specific instruction remains challenging due to\nthe highly domain-specific nature of sports and the need for informative\nguidance. We propose CoachMe, a reference-based model that analyzes the\ndifferences between a learner's motion and a reference under temporal and\nphysical aspects. This approach enables both domain-knowledge learning and the\nacquisition of a coach-like thinking process that identifies movement errors\neffectively and provides feedback to explain how to improve. In this paper, we\nillustrate how CoachMe adapts well to specific sports such as skating and\nboxing by learning from general movements and then leveraging limited data.\nExperiments show that CoachMe provides high-quality instructions instead of\ndirections merely in the tone of a coach but without critical information.\nCoachMe outperforms GPT-4o by 31.6% in G-Eval on figure skating and by 58.3% on\nboxing. Analysis further confirms that it elaborates on errors and their\ncorresponding improvement methods in the generated instructions. You can find\nCoachMe here: https://motionxperts.github.io/", "AI": {"tldr": "CoachMe\u662f\u4e00\u4e2a\u57fa\u4e8e\u53c2\u8003\u7684\u8fd0\u52a8\u6307\u5bfc\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u6790\u5b66\u4e60\u8005\u52a8\u4f5c\u4e0e\u53c2\u8003\u52a8\u4f5c\u5728\u65f6\u95f4\u548c\u7269\u7406\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u7ea0\u6b63\u6307\u5bfc\uff0c\u5728\u82b1\u6837\u6ed1\u51b0\u548c\u62f3\u51fb\u7b49\u7279\u5b9a\u8fd0\u52a8\u9879\u76ee\u4e2d\u8868\u73b0\u4f18\u4e8eGPT-4o\u3002", "motivation": "\u8fd0\u52a8\u6307\u5bfc\u5bf9\u4e8e\u8fd0\u52a8\u5458\u6280\u672f\u7cbe\u8fdb\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u591a\u6a21\u6001\u6a21\u578b\u5728\u751f\u6210\u7cbe\u786e\u3001\u8fd0\u52a8\u4e13\u9879\u7684\u6307\u5bfc\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u7531\u4e8e\u8fd0\u52a8\u7684\u9ad8\u5ea6\u9886\u57df\u7279\u5b9a\u6027\u548c\u9700\u8981\u63d0\u4f9b\u4fe1\u606f\u6027\u6307\u5bfc\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faCoachMe\u6a21\u578b\uff0c\u91c7\u7528\u57fa\u4e8e\u53c2\u8003\u7684\u65b9\u6cd5\u5206\u6790\u5b66\u4e60\u8005\u52a8\u4f5c\u4e0e\u53c2\u8003\u52a8\u4f5c\u5728\u65f6\u95f4\u548c\u7269\u7406\u7ef4\u5ea6\u4e0a\u7684\u5dee\u5f02\uff0c\u5b9e\u73b0\u9886\u57df\u77e5\u8bc6\u5b66\u4e60\u548c\u6559\u7ec3\u5f0f\u601d\u7ef4\u8fc7\u7a0b\u7684\u83b7\u53d6\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u8fd0\u52a8\u9519\u8bef\u5e76\u63d0\u4f9b\u6539\u8fdb\u53cd\u9988\u3002", "result": "\u5b9e\u9a8c\u663e\u793aCoachMe\u5728\u82b1\u6837\u6ed1\u51b0\u7684G-Eval\u8bc4\u4f30\u4e2d\u6bd4GPT-4o\u9ad8\u51fa31.6%\uff0c\u5728\u62f3\u51fb\u9879\u76ee\u4e2d\u9ad8\u51fa58.3%\uff0c\u80fd\u591f\u751f\u6210\u5305\u542b\u9519\u8bef\u5206\u6790\u548c\u5177\u4f53\u6539\u8fdb\u65b9\u6cd5\u7684\u9ad8\u8d28\u91cf\u6307\u5bfc\u3002", "conclusion": "CoachMe\u901a\u8fc7\u4ece\u901a\u7528\u8fd0\u52a8\u5b66\u4e60\u5e76\u5229\u7528\u6709\u9650\u6570\u636e\uff0c\u80fd\u591f\u5f88\u597d\u5730\u9002\u5e94\u7279\u5b9a\u8fd0\u52a8\u9879\u76ee\uff0c\u63d0\u4f9b\u5177\u6709\u5173\u952e\u4fe1\u606f\u7684\u4e13\u4e1a\u8fd0\u52a8\u6307\u5bfc\uff0c\u800c\u4e0d\u662f\u4ec5\u4ec5\u6a21\u4eff\u6559\u7ec3\u8bed\u6c14\u4f46\u7f3a\u4e4f\u5b9e\u8d28\u5185\u5bb9\u7684\u65b9\u5411\u6027\u5efa\u8bae\u3002"}}
{"id": "2509.11709", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.11709", "abs": "https://arxiv.org/abs/2509.11709", "authors": ["Robert Einig", "Stefan Janscha", "Jonas Schuster", "Julian Koch", "Martin Hagmueller", "Barbara Schuppler"], "title": "Room acoustics affect communicative success in hybrid meeting spaces: a pilot study", "comment": null, "summary": "Since the COVID-19 pandemic in 2020, universities and companies have\nincreasingly integrated hybrid features into their meeting spaces, or even\ncreated dedicated rooms for this purpose. While the importance of a fast and\nstable internet connection is often prioritized, the acoustic design of seminar\nrooms is frequently overlooked. Poor acoustics, particularly excessive\nreverberation, can lead to issues such as misunderstandings, reduced speech\nintelligibility or cognitive and vocal fatigue. This pilot study investigates\nwhether room acoustic interventions in a seminar room at Graz University of\nTechnology support better communication in hybrid meetings. For this purpose,\nwe recorded two groups of persons twice, once before and once after improving\nthe acoustics of the room. Our findings -- despite not reaching statistical\nsignificance due to the small sample size - indicate clearly that our spatial\ninterventions improve communicative success in hybrid meetings. To make the\npaper accessible also for readers from the speech communication community, we\nexplain room acoustics background, relevant for the interpretation of our\nresults.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u6df7\u5408\u4f1a\u8bae\u4e2d\u6539\u5584\u623f\u95f4\u58f0\u5b66\u6761\u4ef6\u5bf9\u6c9f\u901a\u6548\u679c\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u5728\u683c\u62c9\u8328\u6280\u672f\u5927\u5b66\u7814\u8ba8\u5ba4\u8fdb\u884c\u58f0\u5b66\u5e72\u9884\u524d\u540e\u7684\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u53d1\u73b0\u58f0\u5b66\u6539\u5584\u6709\u52a9\u4e8e\u63d0\u5347\u6df7\u5408\u4f1a\u8bae\u7684\u6c9f\u901a\u6210\u529f\u7387\u3002", "motivation": "COVID-19\u75ab\u60c5\u540e\u6df7\u5408\u4f1a\u8bae\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u7814\u8ba8\u5ba4\u7684\u58f0\u5b66\u8bbe\u8ba1\u5e38\u88ab\u5ffd\u89c6\u3002\u4e0d\u826f\u58f0\u5b66\u6761\u4ef6\uff08\u7279\u522b\u662f\u8fc7\u5ea6\u6df7\u54cd\uff09\u4f1a\u5bfc\u81f4\u8bef\u89e3\u3001\u8bed\u97f3\u6e05\u6670\u5ea6\u4e0b\u964d\u4ee5\u53ca\u8ba4\u77e5\u548c\u58f0\u97f3\u75b2\u52b3\u7b49\u95ee\u9898\u3002", "method": "\u5728\u683c\u62c9\u8328\u6280\u672f\u5927\u5b66\u7684\u4e00\u4e2a\u7814\u8ba8\u5ba4\u8fdb\u884c\u58f0\u5b66\u5e72\u9884\u524d\u540e\u5bf9\u6bd4\u7814\u7a76\u3002\u8bb0\u5f55\u4e24\u7ec4\u4eba\u5458\u5728\u58f0\u5b66\u6539\u5584\u524d\u540e\u7684\u8868\u73b0\uff0c\u5206\u6790\u6c9f\u901a\u6548\u679c\u7684\u53d8\u5316\u3002", "result": "\u5c3d\u7ba1\u7531\u4e8e\u6837\u672c\u91cf\u8f83\u5c0f\u672a\u8fbe\u5230\u7edf\u8ba1\u663e\u8457\u6027\uff0c\u4f46\u7814\u7a76\u7ed3\u679c\u660e\u786e\u663e\u793a\u7a7a\u95f4\u58f0\u5b66\u5e72\u9884\u6539\u5584\u4e86\u6df7\u5408\u4f1a\u8bae\u4e2d\u7684\u6c9f\u901a\u6210\u529f\u7387\u3002", "conclusion": "\u623f\u95f4\u58f0\u5b66\u8bbe\u8ba1\u5bf9\u6df7\u5408\u4f1a\u8bae\u7684\u6210\u529f\u81f3\u5173\u91cd\u8981\uff0c\u58f0\u5b66\u5e72\u9884\u80fd\u591f\u6709\u6548\u63d0\u5347\u6c9f\u901a\u6548\u679c\uff0c\u8fd9\u4e00\u53d1\u73b0\u5bf9\u5b66\u672f\u673a\u6784\u548c\u4f01\u4e1a\u7684\u4f1a\u8bae\u7a7a\u95f4\u8bbe\u8ba1\u5177\u6709\u91cd\u8981\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2509.11773", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11773", "abs": "https://arxiv.org/abs/2509.11773", "authors": ["Gaye Colakoglu", "G\u00fcrkan Solmaz", "Jonathan F\u00fcrst"], "title": "An Agentic Toolkit for Adaptive Information Extraction from Regulatory Documents", "comment": null, "summary": "Declaration of Performance (DoP) documents, mandated by EU regulation,\ncertify the performance of construction products. While some of their content\nis standardized, DoPs vary widely in layout, language, schema, and format,\nposing challenges for automated key-value pair extraction (KVP) and question\nanswering (QA). Existing static or LLM-only IE pipelines often hallucinate and\nfail to adapt to this structural diversity. Our domain-specific, stateful\nagentic system addresses these challenges through a planner-executor-responder\narchitecture. The system infers user intent, detects document modality, and\norchestrates tools dynamically for robust, traceable reasoning while avoiding\ntool misuse or execution loops. Evaluation on a curated DoP dataset\ndemonstrates improved robustness across formats and languages, offering a\nscalable solution for structured data extraction in regulated workflows.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u6b27\u76df\u5efa\u7b51\u4ea7\u54c1\u6027\u80fd\u58f0\u660e(DoP)\u6587\u6863\u7684\u9886\u57df\u7279\u5b9a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u89c4\u5212\u5668-\u6267\u884c\u5668-\u54cd\u5e94\u5668\u67b6\u6784\u89e3\u51b3\u6587\u6863\u683c\u5f0f\u591a\u6837\u6027\u5e26\u6765\u7684\u81ea\u52a8\u5316\u4fe1\u606f\u62bd\u53d6\u6311\u6218", "motivation": "\u6b27\u76df\u6cd5\u89c4\u8981\u6c42\u7684\u6027\u80fd\u58f0\u660e\u6587\u6863\u5728\u5e03\u5c40\u3001\u8bed\u8a00\u3001\u6a21\u5f0f\u548c\u683c\u5f0f\u4e0a\u5dee\u5f02\u5f88\u5927\uff0c\u73b0\u6709\u7684\u9759\u6001\u6216\u7eafLLM\u4fe1\u606f\u62bd\u53d6\u7ba1\u9053\u7ecf\u5e38\u4ea7\u751f\u5e7b\u89c9\u4e14\u65e0\u6cd5\u9002\u5e94\u8fd9\u79cd\u7ed3\u6784\u591a\u6837\u6027", "method": "\u91c7\u7528\u9886\u57df\u7279\u5b9a\u7684\u72b6\u6001\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u89c4\u5212\u5668-\u6267\u884c\u5668-\u54cd\u5e94\u5668\u67b6\u6784\uff0c\u63a8\u65ad\u7528\u6237\u610f\u56fe\u3001\u68c0\u6d4b\u6587\u6863\u6a21\u6001\uff0c\u5e76\u52a8\u6001\u7f16\u6392\u5de5\u5177\u8fdb\u884c\u7a33\u5065\u3001\u53ef\u8ffd\u6eaf\u7684\u63a8\u7406", "result": "\u5728\u7cbe\u5fc3\u7b56\u5212\u7684DoP\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u5728\u591a\u79cd\u683c\u5f0f\u548c\u8bed\u8a00\u4e0b\u90fd\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u9c81\u68d2\u6027", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u53d7\u76d1\u7ba1\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u7ed3\u6784\u5316\u6570\u636e\u63d0\u53d6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u907f\u514d\u5de5\u5177\u8bef\u7528\u548c\u6267\u884c\u5faa\u73af"}}
{"id": "2509.11777", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.11777", "abs": "https://arxiv.org/abs/2509.11777", "authors": ["Mikhail Kulyabin", "Jan Joosten", "Choro Ulan uulu", "Nuno Miguel Martins Pacheco", "Fabian Ries", "Filippos Petridis", "Jan Bosch", "Helena Holmstr\u00f6m Olsson"], "title": "User eXperience Perception Insights Dataset (UXPID): Synthetic User Feedback from Public Industrial Forums", "comment": null, "summary": "Customer feedback in industrial forums reflect a rich but underexplored\nsource of insight into real-world product experience. These publicly shared\ndiscussions offer an organic view of user expectations, frustrations, and\nsuccess stories shaped by the specific contexts of use. Yet, harnessing this\ninformation for systematic analysis remains challenging due to the unstructured\nand domain-specific nature of the content. The lack of structure and\nspecialized vocabulary makes it difficult for traditional data analysis\ntechniques to accurately interpret, categorize, and quantify the feedback,\nthereby limiting its potential to inform product development and support\nstrategies. To address these challenges, this paper presents the User\neXperience Perception Insights Dataset (UXPID), a collection of 7130\nartificially synthesized and anonymized user feedback branches extracted from a\npublic industrial automation forum. Each JavaScript object notation (JSON)\nrecord contains multi-post comments related to specific hardware and software\nproducts, enriched with metadata and contextual conversation data. Leveraging a\nlarge language model (LLM), each branch is systematically analyzed and\nannotated for UX insights, user expectations, severity and sentiment ratings,\nand topic classifications. The UXPID dataset is designed to facilitate research\nin user requirements, user experience (UX) analysis, and AI-driven feedback\nprocessing, particularly where privacy and licensing restrictions limit access\nto real-world data. UXPID supports the training and evaluation of\ntransformer-based models for tasks such as issue detection, sentiment analysis,\nand requirements extraction in the context of technical forums.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UXPID\u6570\u636e\u96c6\uff0c\u5305\u542b7130\u6761\u5408\u6210\u7684\u5de5\u4e1a\u81ea\u52a8\u5316\u8bba\u575b\u7528\u6237\u53cd\u9988\uff0c\u7528\u4e8e\u4fc3\u8fdb\u7528\u6237\u4f53\u9a8c\u5206\u6790\u548cAI\u53cd\u9988\u5904\u7406\u7814\u7a76", "motivation": "\u5de5\u4e1a\u8bba\u575b\u4e2d\u7684\u5ba2\u6237\u53cd\u9988\u662f\u5b9d\u8d35\u4f46\u672a\u88ab\u5145\u5206\u6316\u6398\u7684\u4fe1\u606f\u6e90\uff0c\u4f20\u7edf\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u8fd9\u79cd\u975e\u7ed3\u6784\u5316\u548c\u9886\u57df\u7279\u5b9a\u7684\u5185\u5bb9", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u4ece\u516c\u5f00\u5de5\u4e1a\u81ea\u52a8\u5316\u8bba\u575b\u63d0\u53d6\u76847130\u6761\u5408\u6210\u533f\u540d\u7528\u6237\u53cd\u9988\u5206\u652f\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\u548c\u6807\u6ce8", "result": "\u521b\u5efa\u4e86\u5305\u542b\u591a\u5e16\u5b50\u8bc4\u8bba\u3001\u5143\u6570\u636e\u548c\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u6570\u636e\u7684JSON\u683c\u5f0f\u6570\u636e\u96c6\uff0c\u652f\u6301\u7528\u6237\u4f53\u9a8c\u6d1e\u5bdf\u3001\u7528\u6237\u671f\u671b\u3001\u4e25\u91cd\u6027\u8bc4\u5206\u548c\u4e3b\u9898\u5206\u7c7b", "conclusion": "UXPID\u6570\u636e\u96c6\u4e3a\u6280\u672f\u8bba\u575b\u4e2d\u7684\u95ee\u9898\u68c0\u6d4b\u3001\u60c5\u611f\u5206\u6790\u548c\u9700\u6c42\u63d0\u53d6\u7b49\u4efb\u52a1\u63d0\u4f9b\u4e86\u8bad\u7ec3\u548c\u8bc4\u4f30\u57fa\u7840\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9690\u79c1\u548c\u8bb8\u53ef\u9650\u5236\u4e0b\u7684\u7814\u7a76"}}
{"id": "2509.11802", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11802", "abs": "https://arxiv.org/abs/2509.11802", "authors": ["Dvora Goncharok", "Arbel Shifman", "Alexander Apartsin", "Yehudit Aperstein"], "title": "When Curiosity Signals Danger: Predicting Health Crises Through Online Medication Inquiries", "comment": "5 pages, 2 figures", "summary": "Online medical forums are a rich and underutilized source of insight into\npatient concerns, especially regarding medication use. Some of the many\nquestions users pose may signal confusion, misuse, or even the early warning\nsigns of a developing health crisis. Detecting these critical questions that\nmay precede severe adverse events or life-threatening complications is vital\nfor timely intervention and improving patient safety. This study introduces a\nnovel annotated dataset of medication-related questions extracted from online\nforums. Each entry is manually labelled for criticality based on clinical risk\nfactors. We benchmark the performance of six traditional machine learning\nclassifiers using TF-IDF textual representations, alongside three\nstate-of-the-art large language model (LLM)-based classification approaches\nthat leverage deep contextual understanding. Our results highlight the\npotential of classical and modern methods to support real-time triage and alert\nsystems in digital health spaces. The curated dataset is made publicly\navailable to encourage further research at the intersection of\npatient-generated data, natural language processing, and early warning systems\nfor critical health events. The dataset and benchmark are available at:\nhttps://github.com/Dvora-coder/LLM-Medication-QA-Risk-Classifier-MediGuard.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86\u4e00\u4e2a\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u68c0\u6d4b\u5728\u7ebf\u533b\u7597\u8bba\u575b\u4e2d\u836f\u7269\u76f8\u5173\u95ee\u9898\u7684\u5173\u952e\u6027\uff0c\u5e76\u6bd4\u8f83\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u9ad8\u98ce\u9669\u95ee\u9898\u65b9\u9762\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u7ebf\u533b\u7597\u8bba\u575b\u5305\u542b\u5927\u91cf\u60a3\u8005\u5173\u4e8e\u836f\u7269\u4f7f\u7528\u7684\u7591\u95ee\uff0c\u5176\u4e2d\u4e00\u4e9b\u95ee\u9898\u53ef\u80fd\u9884\u793a\u7740\u6df7\u6dc6\u3001\u8bef\u7528\u751a\u81f3\u5065\u5eb7\u5371\u673a\u3002\u53ca\u65f6\u68c0\u6d4b\u8fd9\u4e9b\u5173\u952e\u95ee\u9898\u5bf9\u4e8e\u5e72\u9884\u548c\u63d0\u9ad8\u60a3\u8005\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u624b\u52a8\u6807\u6ce8\u7684\u836f\u7269\u76f8\u5173\u95ee\u9898\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u4e34\u5e8a\u98ce\u9669\u56e0\u7d20\u6807\u6ce8\u5173\u952e\u6027\u3002\u4f7f\u7528\u4e866\u79cd\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\uff08TF-IDF\u7279\u5f81\uff09\u548c3\u79cd\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u8fdb\u884c\u6027\u80fd\u5bf9\u6bd4\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u4ee3\u65b9\u6cd5\u90fd\u6709\u6f5c\u529b\u652f\u6301\u6570\u5b57\u5065\u5eb7\u7a7a\u95f4\u7684\u5b9e\u65f6\u5206\u8bca\u548c\u8b66\u62a5\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u9f13\u52b1\u5728\u60a3\u8005\u751f\u6210\u6570\u636e\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u5065\u5eb7\u4e8b\u4ef6\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u4ea4\u53c9\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2509.11803", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11803", "abs": "https://arxiv.org/abs/2509.11803", "authors": ["Eden Mama", "Liel Sheri", "Yehudit Aperstein", "Alexander Apartsin"], "title": "From Fuzzy Speech to Medical Insight: Benchmarking LLMs on Noisy Patient Narratives", "comment": "6 pages, 1 figure", "summary": "The widespread adoption of large language models (LLMs) in healthcare raises\ncritical questions about their ability to interpret patient-generated\nnarratives, which are often informal, ambiguous, and noisy. Existing benchmarks\ntypically rely on clean, structured clinical text, offering limited insight\ninto model performance under realistic conditions. In this work, we present a\nnovel synthetic dataset designed to simulate patient self-descriptions\ncharacterized by varying levels of linguistic noise, fuzzy language, and\nlayperson terminology. Our dataset comprises clinically consistent scenarios\nannotated with ground-truth diagnoses, spanning a spectrum of communication\nclarity to reflect diverse real-world reporting styles. Using this benchmark,\nwe fine-tune and evaluate several state-of-the-art models (LLMs), including\nBERT-based and encoder-decoder T5 models. To support reproducibility and future\nresearch, we release the Noisy Diagnostic Benchmark (NDB), a structured dataset\nof noisy, synthetic patient descriptions designed to stress-test and compare\nthe diagnostic capabilities of large language models (LLMs) under realistic\nlinguistic conditions. We made the benchmark available for the community:\nhttps://github.com/lielsheri/PatientSignal", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aNoisy Diagnostic Benchmark (NDB)\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6d4b\u8bd5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5608\u6742\u3001\u975e\u6b63\u5f0f\u7684\u533b\u7597\u53d9\u8ff0\u6587\u672c\u4e2d\u7684\u8bca\u65ad\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u4f7f\u7528\u6e05\u6d01\u3001\u7ed3\u6784\u5316\u7684\u4e34\u5e8a\u6587\u672c\uff0c\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620LLM\u5728\u5b9e\u9645\u533b\u7597\u73af\u5883\u4e2d\u5904\u7406\u60a3\u8005\u81ea\u8ff0\uff08\u901a\u5e38\u662f\u975e\u6b63\u5f0f\u3001\u6a21\u7cca\u548c\u5608\u6742\u7684\uff09\u7684\u80fd\u529b\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u4e0d\u540c\u8bed\u8a00\u566a\u58f0\u6c34\u5e73\u3001\u6a21\u7cca\u8bed\u8a00\u548c\u901a\u4fd7\u672f\u8bed\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e34\u5e8a\u4e00\u81f4\u7684\u573a\u666f\u548c\u771f\u5b9e\u8bca\u65ad\u6807\u6ce8\u3002\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u5bf9BERT\u548cT5\u7b49\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u548c\u8bc4\u4f30\u3002", "result": "\u5f00\u53d1\u5e76\u53d1\u5e03\u4e86NDB\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd9\u662f\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u5608\u6742\u5408\u6210\u60a3\u8005\u63cf\u8ff0\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5728\u771f\u5b9e\u8bed\u8a00\u6761\u4ef6\u4e0b\u538b\u529b\u6d4b\u8bd5\u548c\u6bd4\u8f83LLM\u7684\u8bca\u65ad\u80fd\u529b\u3002", "conclusion": "NDB\u57fa\u51c6\u6d4b\u8bd5\u586b\u8865\u4e86\u73b0\u6709\u533b\u7597NLP\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u5728\u66f4\u771f\u5b9e\u6761\u4ef6\u4e0b\u8bc4\u4f30LLM\u8bca\u65ad\u80fd\u529b\u7684\u5de5\u5177\uff0c\u652f\u6301\u53ef\u91cd\u590d\u6027\u548c\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2509.11804", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11804", "abs": "https://arxiv.org/abs/2509.11804", "authors": ["Yulong Chen", "Michael Sejr Schlichtkrull", "Zhenyun Deng", "David Corney", "Nasim Asl", "Joshua Salisbury", "Andrew Dudfield", "Andreas Vlachos"], "title": "PledgeTracker: A System for Monitoring the Fulfilment of Pledges", "comment": "EMNLP 2025 demo", "summary": "Political pledges reflect candidates' policy commitments, but tracking their\nfulfilment requires reasoning over incremental evidence distributed across\nmultiple, dynamically updated sources. Existing methods simplify this task into\na document classification task, overlooking its dynamic, temporal and\nmulti-document nature. To address this issue, we introduce\n\\textsc{PledgeTracker}, a system that reformulates pledge verification into\nstructured event timeline construction. PledgeTracker consists of three core\ncomponents: (1) a multi-step evidence retrieval module; (2) a timeline\nconstruction module and; (3) a fulfilment filtering module, allowing the\ncapture of the evolving nature of pledge fulfilment and producing interpretable\nand structured timelines. We evaluate PledgeTracker in collaboration with\nprofessional fact-checkers in real-world workflows, demonstrating its\neffectiveness in retrieving relevant evidence and reducing human verification\neffort.", "AI": {"tldr": "PledgeTracker\u662f\u4e00\u4e2a\u5c06\u653f\u6cbb\u627f\u8bfa\u9a8c\u8bc1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7ed3\u6784\u5316\u4e8b\u4ef6\u65f6\u95f4\u7ebf\u6784\u5efa\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6b65\u9aa4\u8bc1\u636e\u68c0\u7d22\u3001\u65f6\u95f4\u7ebf\u6784\u5efa\u548c\u5c65\u884c\u8fc7\u6ee4\u6a21\u5757\u6765\u8ddf\u8e2a\u653f\u6cbb\u627f\u8bfa\u7684\u5c65\u884c\u60c5\u51b5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u653f\u6cbb\u627f\u8bfa\u9a8c\u8bc1\u7b80\u5316\u4e3a\u6587\u6863\u5206\u7c7b\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u5176\u52a8\u6001\u6027\u3001\u65f6\u95f4\u6027\u548c\u591a\u6587\u6863\u7279\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u589e\u91cf\u8bc1\u636e\u548c\u52a8\u6001\u66f4\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1)\u591a\u6b65\u9aa4\u8bc1\u636e\u68c0\u7d22\u6a21\u5757\uff1b(2)\u65f6\u95f4\u7ebf\u6784\u5efa\u6a21\u5757\uff1b(3)\u5c65\u884c\u8fc7\u6ee4\u6a21\u5757\uff0c\u901a\u8fc7\u6784\u5efa\u7ed3\u6784\u5316\u4e8b\u4ef6\u65f6\u95f4\u7ebf\u6765\u6355\u6349\u627f\u8bfa\u5c65\u884c\u7684\u6f14\u53d8\u8fc7\u7a0b\u3002", "result": "\u4e0e\u4e13\u4e1a\u4e8b\u5b9e\u6838\u67e5\u4eba\u5458\u5728\u771f\u5b9e\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u5408\u4f5c\u8bc4\u4f30\uff0c\u8bc1\u660e\u7cfb\u7edf\u5728\u68c0\u7d22\u76f8\u5173\u8bc1\u636e\u548c\u51cf\u5c11\u4eba\u5de5\u9a8c\u8bc1\u5de5\u4f5c\u91cf\u65b9\u9762\u5177\u6709\u6709\u6548\u6027\u3002", "conclusion": "PledgeTracker\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u95ee\u9898\u4e3a\u4e8b\u4ef6\u65f6\u95f4\u7ebf\u6784\u5efa\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u653f\u6cbb\u627f\u8bfa\u9a8c\u8bc1\u7684\u52a8\u6001\u6027\u548c\u590d\u6742\u6027\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u7ed3\u6784\u5316\u65f6\u95f4\u7ebf\u3002"}}
{"id": "2509.11818", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11818", "abs": "https://arxiv.org/abs/2509.11818", "authors": ["Taichi Aida", "Danushka Bollegala"], "title": "SCDTour: Embedding Axis Ordering and Merging for Interpretable Semantic Change Detection", "comment": "Findings of EMNLP2025", "summary": "In Semantic Change Detection (SCD), it is a common problem to obtain\nembeddings that are both interpretable and high-performing. However, improving\ninterpretability often leads to a loss in the SCD performance, and vice versa.\nTo address this problem, we propose SCDTour, a method that orders and merges\ninterpretable axes to alleviate the performance degradation of SCD. SCDTour\nconsiders both (a) semantic similarity between axes in the embedding space, as\nwell as (b) the degree to which each axis contributes to semantic change.\nExperimental results show that SCDTour preserves performance in semantic change\ndetection while maintaining high interpretability. Moreover, agglomerating the\nsorted axes produces a more refined set of word senses, which achieves\ncomparable or improved performance against the original full-dimensional\nembeddings in the SCD task. These findings demonstrate that SCDTour effectively\nbalances interpretability and SCD performance, enabling meaningful\ninterpretation of semantic shifts through a small number of refined axes.\nSource code is available at https://github.com/LivNLP/svp-tour .", "AI": {"tldr": "SCDTour\u65b9\u6cd5\u901a\u8fc7\u6392\u5e8f\u548c\u5408\u5e76\u53ef\u89e3\u91ca\u8f74\u6765\u89e3\u51b3\u8bed\u4e49\u53d8\u5316\u68c0\u6d4b\u4e2d\u53ef\u89e3\u91ca\u6027\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u7ef4\u6301\u68c0\u6d4b\u6027\u80fd", "motivation": "\u89e3\u51b3\u8bed\u4e49\u53d8\u5316\u68c0\u6d4b\u4e2d\u53ef\u89e3\u91ca\u6027\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u56fa\u6709\u77db\u76fe\u2014\u2014\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u901a\u5e38\u4f1a\u5bfc\u81f4\u68c0\u6d4b\u6027\u80fd\u4e0b\u964d\uff0c\u53cd\u4e4b\u4ea6\u7136", "method": "SCDTour\u65b9\u6cd5\u901a\u8fc7\u8003\u8651(a)\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8f74\u4e4b\u95f4\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u4ee5\u53ca(b)\u6bcf\u4e2a\u8f74\u5bf9\u8bed\u4e49\u53d8\u5316\u7684\u8d21\u732e\u7a0b\u5ea6\uff0c\u6765\u6392\u5e8f\u548c\u5408\u5e76\u53ef\u89e3\u91ca\u8f74", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eSCDTour\u5728\u4fdd\u6301\u8bed\u4e49\u53d8\u5316\u68c0\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u7ef4\u6301\u9ad8\u53ef\u89e3\u91ca\u6027\uff0c\u805a\u5408\u6392\u5e8f\u540e\u7684\u8f74\u80fd\u4ea7\u751f\u66f4\u7cbe\u7ec6\u7684\u8bcd\u4e49\u96c6\u5408\uff0c\u5728SCD\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e0e\u539f\u59cb\u5168\u7ef4\u5d4c\u5165\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd", "conclusion": "SCDTour\u6709\u6548\u5e73\u8861\u4e86\u53ef\u89e3\u91ca\u6027\u548cSCD\u6027\u80fd\uff0c\u901a\u8fc7\u5c11\u91cf\u7cbe\u70bc\u8f74\u5b9e\u73b0\u5bf9\u8bed\u4e49\u53d8\u5316\u7684\u6709\u610f\u4e49\u89e3\u91ca"}}
{"id": "2509.11860", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11860", "abs": "https://arxiv.org/abs/2509.11860", "authors": ["Weishu Chen", "Jinyi Tang", "Zhouhui Hou", "Shihao Han", "Mingjie Zhan", "Zhiyuan Huang", "Delong Liu", "Jiawei Guo", "Zhicheng Zhao", "Fei Su"], "title": "MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing Dialogues", "comment": null, "summary": "Memory extraction is crucial for maintaining coherent ultra-long dialogues in\nhuman-robot role-playing scenarios. However, existing methods often exhibit\nuncontrolled memory growth. To address this, we propose MOOM, the first\ndual-branch memory plugin that leverages literary theory by modeling plot\ndevelopment and character portrayal as core storytelling elements.\nSpecifically, one branch summarizes plot conflicts across multiple time scales,\nwhile the other extracts the user's character profile. MOOM further integrates\na forgetting mechanism, inspired by the ``competition-inhibition'' memory\ntheory, to constrain memory capacity and mitigate uncontrolled growth.\nFurthermore, we present ZH-4O, a Chinese ultra-long dialogue dataset\nspecifically designed for role-playing, featuring dialogues that average 600\nturns and include manually annotated memory information. Experimental results\ndemonstrate that MOOM outperforms all state-of-the-art memory extraction\nmethods, requiring fewer large language model invocations while maintaining a\ncontrollable memory capacity.", "AI": {"tldr": "MOOM\u662f\u4e00\u4e2a\u57fa\u4e8e\u6587\u5b66\u7406\u8bba\u7684\u53cc\u5206\u652f\u8bb0\u5fc6\u63d2\u4ef6\uff0c\u901a\u8fc7\u5efa\u6a21\u60c5\u8282\u53d1\u5c55\u548c\u89d2\u8272\u523b\u753b\u6765\u63a7\u5236\u8d85\u957f\u5bf9\u8bdd\u4e2d\u7684\u8bb0\u5fc6\u589e\u957f\uff0c\u5e76\u6574\u5408\u9057\u5fd8\u673a\u5236\u6765\u7ea6\u675f\u8bb0\u5fc6\u5bb9\u91cf\u3002", "motivation": "\u89e3\u51b3\u4eba\u673a\u89d2\u8272\u626e\u6f14\u573a\u666f\u4e2d\u8d85\u957f\u5bf9\u8bdd\u4e2d\u8bb0\u5fc6\u63d0\u53d6\u7684\u4e0d\u53ef\u63a7\u589e\u957f\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u6709\u6548\u63a7\u5236\u8bb0\u5fc6\u5bb9\u91cf\u3002", "method": "\u63d0\u51faMOOM\u53cc\u5206\u652f\u8bb0\u5fc6\u63d2\u4ef6\uff1a\u4e00\u4e2a\u5206\u652f\u603b\u7ed3\u591a\u65f6\u95f4\u5c3a\u5ea6\u7684\u60c5\u8282\u51b2\u7a81\uff0c\u53e6\u4e00\u4e2a\u5206\u652f\u63d0\u53d6\u7528\u6237\u89d2\u8272\u753b\u50cf\uff1b\u6574\u5408\u57fa\u4e8e\"\u7ade\u4e89\u6291\u5236\"\u8bb0\u5fc6\u7406\u8bba\u7684\u9057\u5fd8\u673a\u5236\u6765\u7ea6\u675f\u8bb0\u5fc6\u5bb9\u91cf\u3002", "result": "MOOM\u5728ZH-4O\u4e2d\u6587\u8d85\u957f\u5bf9\u8bdd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u6240\u6709\u6700\u5148\u8fdb\u7684\u8bb0\u5fc6\u63d0\u53d6\u65b9\u6cd5\uff0c\u9700\u8981\u66f4\u5c11\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8c03\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u63a7\u7684\u8bb0\u5fc6\u5bb9\u91cf\u3002", "conclusion": "MOOM\u901a\u8fc7\u7ed3\u5408\u6587\u5b66\u7406\u8bba\u548c\u8bb0\u5fc6\u7406\u8bba\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89d2\u8272\u626e\u6f14\u5bf9\u8bdd\u4e2d\u7684\u8bb0\u5fc6\u4e0d\u53ef\u63a7\u589e\u957f\u95ee\u9898\uff0c\u4e3a\u8d85\u957f\u5bf9\u8bdd\u8bb0\u5fc6\u7ba1\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.11868", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.RO", "I.2; I.2.7; I.2.10; J.4"], "pdf": "https://arxiv.org/pdf/2509.11868", "abs": "https://arxiv.org/abs/2509.11868", "authors": ["Sabrina Patania", "Luca Annese", "Anna Lambiase", "Anita Pellegrini", "Tom Foulsham", "Azzurra Ruggeri", "Silvia Rossi", "Silvia Serino", "Dimitri Ognibene"], "title": "Growing Perspectives: Modelling Embodied Perspective Taking and Inner Narrative Development Using Large Language Models", "comment": "Accepted at ICDL https://icdl2025.fel.cvut.cz/", "summary": "Language and embodied perspective taking are essential for human\ncollaboration, yet few computational models address both simultaneously. This\nwork investigates the PerspAct system [1], which integrates the ReAct (Reason\nand Act) paradigm with Large Language Models (LLMs) to simulate developmental\nstages of perspective taking, grounded in Selman's theory [2]. Using an\nextended director task, we evaluate GPT's ability to generate internal\nnarratives aligned with specified developmental stages, and assess how these\ninfluence collaborative performance both qualitatively (action selection) and\nquantitatively (task efficiency). Results show that GPT reliably produces\ndevelopmentally-consistent narratives before task execution but often shifts\ntowards more advanced stages during interaction, suggesting that language\nexchanges help refine internal representations. Higher developmental stages\ngenerally enhance collaborative effectiveness, while earlier stages yield more\nvariable outcomes in complex contexts. These findings highlight the potential\nof integrating embodied perspective taking and language in LLMs to better model\ndevelopmental dynamics and stress the importance of evaluating internal speech\nduring combined linguistic and embodied tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8PerspAct\u7cfb\u7edf\u5982\u4f55\u5c06ReAct\u8303\u5f0f\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u6a21\u62dfSelman\u7406\u8bba\u4e2d\u7684\u89c2\u70b9\u91c7\u62e9\u53d1\u5c55\u9636\u6bb5\uff0c\u901a\u8fc7\u5bfc\u6f14\u4efb\u52a1\u8bc4\u4f30GPT\u751f\u6210\u53d1\u5c55\u4e00\u81f4\u6027\u53d9\u4e8b\u7684\u80fd\u529b\u53ca\u5176\u5bf9\u534f\u4f5c\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u8bed\u8a00\u548c\u5177\u8eab\u89c2\u70b9\u91c7\u62e9\u5bf9\u4eba\u7c7b\u534f\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u5f88\u5c11\u6709\u8ba1\u7b97\u6a21\u578b\u80fd\u540c\u65f6\u5904\u7406\u8fd9\u4e24\u4e2a\u65b9\u9762\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5c06\u5177\u8eab\u89c2\u70b9\u91c7\u62e9\u4e0e\u8bed\u8a00\u80fd\u529b\u6574\u5408\u5230LLMs\u4e2d\uff0c\u4ee5\u66f4\u597d\u5730\u6a21\u62df\u53d1\u5c55\u52a8\u6001\u3002", "method": "\u4f7f\u7528\u6269\u5c55\u7684\u5bfc\u6f14\u4efb\u52a1\uff0c\u8bc4\u4f30GPT\u751f\u6210\u4e0e\u6307\u5b9a\u53d1\u5c55\u9636\u6bb5\u4e00\u81f4\u7684\u5185\u5728\u53d9\u4e8b\u80fd\u529b\uff0c\u5e76\u5206\u6790\u8fd9\u4e9b\u53d9\u4e8b\u5982\u4f55\u5f71\u54cd\u534f\u4f5c\u6027\u80fd\uff08\u5305\u62ec\u5b9a\u6027\u884c\u52a8\u9009\u62e9\u548c\u5b9a\u91cf\u4efb\u52a1\u6548\u7387\uff09\u3002\u57fa\u4e8eSelman\u7684\u89c2\u70b9\u91c7\u62e9\u7406\u8bba\u548cReAct\u8303\u5f0f\u3002", "result": "GPT\u80fd\u5728\u4efb\u52a1\u6267\u884c\u524d\u53ef\u9760\u5730\u751f\u6210\u53d1\u5c55\u4e00\u81f4\u6027\u7684\u53d9\u4e8b\uff0c\u4f46\u5728\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u5f80\u5f80\u5411\u66f4\u9ad8\u7ea7\u9636\u6bb5\u8f6c\u53d8\u3002\u9ad8\u7ea7\u53d1\u5c55\u9636\u6bb5\u901a\u5e38\u80fd\u63d0\u9ad8\u534f\u4f5c\u6548\u679c\uff0c\u800c\u65e9\u671f\u9636\u6bb5\u5728\u590d\u6742\u60c5\u5883\u4e2d\u4ea7\u751f\u66f4\u591a\u53d8\u7684\u7ed3\u679c\u3002\u8bed\u8a00\u4ea4\u6d41\u6709\u52a9\u4e8e\u7cbe\u70bc\u5185\u5728\u8868\u5f81\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5c06\u5177\u8eab\u89c2\u70b9\u91c7\u62e9\u4e0e\u8bed\u8a00\u6574\u5408\u5230LLMs\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u66f4\u597d\u5730\u5efa\u6a21\u53d1\u5c55\u52a8\u6001\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u7ed3\u5408\u8bed\u8a00\u548c\u5177\u8eab\u4efb\u52a1\u65f6\u8bc4\u4f30\u5185\u5728\u8a00\u8bed\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.11915", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11915", "abs": "https://arxiv.org/abs/2509.11915", "authors": ["Aadil Gani Ganie"], "title": "Uncertainty in Authorship: Why Perfect AI Detection Is Mathematically Impossible", "comment": null, "summary": "As large language models (LLMs) become more advanced, it is increasingly\ndifficult to distinguish between human-written and AI-generated text. This\npaper draws a conceptual parallel between quantum uncertainty and the limits of\nauthorship detection in natural language. We argue that there is a fundamental\ntrade-off: the more confidently one tries to identify whether a text was\nwritten by a human or an AI, the more one risks disrupting the text's natural\nflow and authenticity. This mirrors the tension between precision and\ndisturbance found in quantum systems. We explore how current detection\nmethods--such as stylometry, watermarking, and neural classifiers--face\ninherent limitations. Enhancing detection accuracy often leads to changes in\nthe AI's output, making other features less reliable. In effect, the very act\nof trying to detect AI authorship introduces uncertainty elsewhere in the text.\nOur analysis shows that when AI-generated text closely mimics human writing,\nperfect detection becomes not just technologically difficult but theoretically\nimpossible. We address counterarguments and discuss the broader implications\nfor authorship, ethics, and policy. Ultimately, we suggest that the challenge\nof AI-text detection is not just a matter of better tools--it reflects a\ndeeper, unavoidable tension in the nature of language itself.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u91cf\u5b50\u4e0d\u786e\u5b9a\u6027\u7c7b\u6bd4\uff0c\u8bba\u8bc1\u4e86AI\u6587\u672c\u68c0\u6d4b\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff1a\u68c0\u6d4b\u7cbe\u5ea6\u4e0e\u6587\u672c\u771f\u5b9e\u6027\u4e4b\u95f4\u5b58\u5728\u4e0d\u53ef\u8c03\u548c\u7684\u77db\u76fe\uff0c\u5b8c\u7f8e\u68c0\u6d4b\u5728\u7406\u8bba\u4e0a\u662f\u4e0d\u53ef\u80fd\u7684", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\uff0c\u533a\u5206\u4eba\u7c7b\u5199\u4f5c\u548cAI\u751f\u6210\u6587\u672c\u53d8\u5f97\u8d8a\u6765\u8d8a\u56f0\u96be\uff0c\u9700\u8981\u63a2\u8ba8\u68c0\u6d4b\u65b9\u6cd5\u7684\u7406\u8bba\u6781\u9650", "method": "\u91c7\u7528\u6982\u5ff5\u7c7b\u6bd4\u65b9\u6cd5\uff0c\u5c06\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u6d4b\u4e0d\u51c6\u539f\u7406\u4e0e\u6587\u672c\u68c0\u6d4b\u4e2d\u7684\u7cbe\u5ea6-\u5e72\u6270\u6743\u8861\u8fdb\u884c\u5e73\u884c\u6bd4\u8f83\uff0c\u5206\u6790\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\uff08\u98ce\u683c\u8ba1\u91cf\u3001\u6c34\u5370\u6280\u672f\u3001\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668\uff09\u7684\u5185\u5728\u5c40\u9650\u6027", "result": "\u53d1\u73b0\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u6027\u5f80\u5f80\u4f1a\u5bfc\u81f4AI\u8f93\u51fa\u53d1\u751f\u53d8\u5316\uff0c\u4f7f\u5176\u4ed6\u7279\u5f81\u53ef\u9760\u6027\u964d\u4f4e\uff0c\u68c0\u6d4b\u884c\u4e3a\u672c\u8eab\u4f1a\u5728\u6587\u672c\u4e2d\u5f15\u5165\u4e0d\u786e\u5b9a\u6027", "conclusion": "AI\u6587\u672c\u68c0\u6d4b\u7684\u6311\u6218\u4e0d\u4ec5\u4ec5\u662f\u6280\u672f\u5de5\u5177\u95ee\u9898\uff0c\u800c\u662f\u53cd\u6620\u4e86\u8bed\u8a00\u672c\u8d28\u4e2d\u66f4\u6df1\u5c42\u6b21\u3001\u4e0d\u53ef\u907f\u514d\u7684\u5f20\u529b\uff0c\u5bf9\u4f5c\u8005\u8eab\u4efd\u3001\u4f26\u7406\u548c\u653f\u7b56\u5177\u6709\u5e7f\u6cdb\u5f71\u54cd"}}
{"id": "2509.11921", "categories": ["cs.CL", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.11921", "abs": "https://arxiv.org/abs/2509.11921", "authors": ["Helene Tenzer", "Oumnia Abidi", "Stefan Feuerriegel"], "title": "Designing LLMs for cultural sensitivity: Evidence from English-Japanese translation", "comment": null, "summary": "Large language models (LLMs) are increasingly used in everyday communication,\nincluding multilingual interactions across different cultural contexts. While\nLLMs can now generate near-perfect literal translations, it remains unclear\nwhether LLMs support culturally appropriate communication. In this paper, we\nanalyze the cultural sensitivity of different LLM designs when applied to\nEnglish-Japanese translations of workplace e-mails. Here, we vary the prompting\nstrategies: (1) naive \"just translate\" prompts, (2) audience-targeted prompts\nspecifying the recipient's cultural background, and (3) instructional prompts\nwith explicit guidance on Japanese communication norms. Using a mixed-methods\nstudy, we then analyze culture-specific language patterns to evaluate how well\ntranslations adapt to cultural norms. Further, we examine the appropriateness\nof the tone of the translations as perceived by native speakers. We find that\nculturally-tailored prompting can improve cultural fit, based on which we offer\nrecommendations for designing culturally inclusive LLMs in multilingual\nsettings.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u4e0d\u540cLLM\u63d0\u793a\u7b56\u7565\u5728\u82f1\u65e5\u804c\u573a\u90ae\u4ef6\u7ffb\u8bd1\u4e2d\u7684\u6587\u5316\u654f\u611f\u6027\uff0c\u53d1\u73b0\u6587\u5316\u5b9a\u5236\u63d0\u793a\u80fd\u6539\u5584\u6587\u5316\u9002\u5e94\u6027", "motivation": "\u867d\u7136LLM\u80fd\u751f\u6210\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u5b57\u9762\u7ffb\u8bd1\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u662f\u5426\u652f\u6301\u6587\u5316\u9002\u5b9c\u7684\u6c9f\u901a\uff0c\u7279\u522b\u662f\u5728\u8de8\u6587\u5316\u804c\u573a\u4ea4\u6d41\u4e2d", "method": "\u91c7\u7528\u6df7\u5408\u7814\u7a76\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff1a\u6734\u7d20\u7ffb\u8bd1\u63d0\u793a\u3001\u53d7\u4f17\u5b9a\u5411\u63d0\u793a\uff08\u6307\u5b9a\u6587\u5316\u80cc\u666f\uff09\u3001\u6559\u5b66\u63d0\u793a\uff08\u660e\u786e\u65e5\u672c\u6c9f\u901a\u89c4\u8303\u6307\u5bfc\uff09\uff0c\u5206\u6790\u6587\u5316\u7279\u5b9a\u8bed\u8a00\u6a21\u5f0f\u5e76\u8bc4\u4f30\u6bcd\u8bed\u8005\u5bf9\u7ffb\u8bd1\u8bed\u6c14\u9002\u5b9c\u6027\u7684\u611f\u77e5", "result": "\u7814\u7a76\u53d1\u73b0\u6587\u5316\u5b9a\u5236\u7684\u63d0\u793a\u7b56\u7565\u80fd\u591f\u63d0\u9ad8\u7ffb\u8bd1\u7684\u6587\u5316\u9002\u5e94\u6027", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u8bbe\u8ba1\u6587\u5316\u5305\u5bb9\u6027LLM\u63d0\u4f9b\u4e86\u5efa\u8bae"}}
{"id": "2509.11961", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11961", "abs": "https://arxiv.org/abs/2509.11961", "authors": ["Mingxiao Huo", "Jiayi Zhang", "Hewei Wang", "Jinfeng Xu", "Zheyu Chen", "Huilin Tai", "Yijun Chen"], "title": "Spec-LLaVA: Accelerating Vision-Language Models with Dynamic Tree-Based Speculative Decoding", "comment": "7pages, accepted by ICML TTODLer-FM workshop", "summary": "Vision-Language Models (VLMs) enable powerful multimodal reasoning but suffer\nfrom slow autoregressive inference, limiting their deployment in real-time\napplications. We introduce Spec-LLaVA, a system that applies speculative\ndecoding to accelerate VLMs without sacrificing output quality. Spec-LLaVA\npairs a lightweight draft VLM with a large target model: the draft speculates\nfuture tokens, which the target verifies in parallel, allowing multiple tokens\nto be generated per step. To maximize efficiency, we design a dynamic\ntree-based verification algorithm that adaptively expands and prunes\nspeculative branches using draft model confidence. On MS COCO out-of-domain\nimages, Spec-LLaVA achieves up to 3.28$\\times$ faster decoding on LLaVA-1.5\n(7B, 13B) with no loss in generation quality. This work presents a lossless\nacceleration framework for VLMs using dynamic tree-structured speculative\ndecoding, opening a path toward practical real-time multimodal assistants.\nImportantly, the lightweight draft model design makes the framework amenable to\nresource-constrained or on-device deployment settings.", "AI": {"tldr": "Spec-LLaVA\u4f7f\u7528\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u52a0\u901f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8349\u7a3f\u6a21\u578b\u9884\u6d4btoken\uff0c\u76ee\u6807\u6a21\u578b\u5e76\u884c\u9a8c\u8bc1\uff0c\u5b9e\u73b03.28\u500d\u52a0\u901f\u4e14\u4e0d\u635f\u5931\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u7684\u81ea\u56de\u5f52\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u635f\u52a0\u901f\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u52a8\u6001\u6811\u5f62\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\uff0c\u914d\u5bf9\u8f7b\u91cf\u7ea7\u8349\u7a3fVLM\u548c\u5927\u578b\u76ee\u6807\u6a21\u578b\uff0c\u8349\u7a3f\u6a21\u578b\u63a8\u6d4b\u672a\u6765token\uff0c\u76ee\u6807\u6a21\u578b\u5e76\u884c\u9a8c\u8bc1\uff0c\u5e76\u6839\u636e\u7f6e\u4fe1\u5ea6\u81ea\u9002\u5e94\u6269\u5c55\u548c\u4fee\u526a\u63a8\u6d4b\u5206\u652f\u3002", "result": "\u5728MS COCO\u57df\u5916\u56fe\u50cf\u4e0a\uff0cSpec-LLaVA\u5728LLaVA-1.5(7B, 13B)\u4e0a\u5b9e\u73b0\u6700\u9ad83.28\u500d\u7684\u89e3\u7801\u52a0\u901f\uff0c\u751f\u6210\u8d28\u91cf\u65e0\u635f\u5931\u3002", "conclusion": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u52a8\u6001\u6811\u5f62\u63a8\u6d4b\u89e3\u7801\u7684\u65e0\u635fVLM\u52a0\u901f\u6846\u67b6\uff0c\u4e3a\u5b9e\u7528\u5b9e\u65f6\u591a\u6a21\u6001\u52a9\u624b\u5f00\u8f9f\u4e86\u9053\u8def\uff0c\u8f7b\u91cf\u7ea7\u8349\u7a3f\u6a21\u578b\u8bbe\u8ba1\u4f7f\u5176\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u6216\u8bbe\u5907\u7aef\u90e8\u7f72\u573a\u666f\u3002"}}
{"id": "2509.11963", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11963", "abs": "https://arxiv.org/abs/2509.11963", "authors": ["Mayank Agarwal", "Ibrahim Abdelaziz", "Kinjal Basu", "Merve Unuvar", "Luis A. Lastras", "Yara Rizk", "Pavan Kapanipathi"], "title": "ToolRM: Outcome Reward Models for Tool-Calling Large Language Models", "comment": null, "summary": "As large language models (LLMs) increasingly interact with external tools,\nreward modeling for tool use has become a critical yet underexplored area.\nExisting reward models, trained primarily on natural language outputs, struggle\nto evaluate tool-based reasoning and execution. To quantify this gap, we\nintroduce FC-RewardBench, the first benchmark designed to systematically assess\nreward models' performance in tool-calling scenarios. Our analysis shows that\ncurrent reward models often miss key signals of effective tool use,\nhighlighting the need for domain-specific modeling. To address this, we propose\na training framework for outcome-based reward models using data synthesized\nfrom permissively licensed, open-weight LLMs. We train models ranging from 1.7B\nto 14B parameters and evaluate them across seven out-of-domain benchmarks.\nThese models consistently outperform general-purpose baselines, achieving up to\n25\\% average improvement in downstream task performance and enabling\ndata-efficient fine-tuning through reward-guided filtering.", "AI": {"tldr": "\u63d0\u51fa\u4e86FC-RewardBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u73b0\u6709\u5956\u52b1\u6a21\u578b\u5728\u8bc4\u4f30\u5de5\u5177\u8c03\u7528\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u57287\u4e2a\u57df\u5916\u57fa\u51c6\u4e0a\u5e73\u5747\u63d0\u534725%\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u4e0e\u5916\u90e8\u5de5\u5177\u4ea4\u4e92\uff0c\u4f46\u73b0\u6709\u7684\u5956\u52b1\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u81ea\u7136\u8bed\u8a00\u8f93\u51fa\u8bad\u7ec3\uff0c\u96be\u4ee5\u6709\u6548\u8bc4\u4f30\u57fa\u4e8e\u5de5\u5177\u7684\u63a8\u7406\u548c\u6267\u884c\u8fc7\u7a0b\uff0c\u5b58\u5728\u660e\u663e\u7684\u8bc4\u4f30\u5dee\u8ddd\u3002", "method": "\u5f15\u5165FC-RewardBench\u57fa\u51c6\u7cfb\u7edf\u8bc4\u4f30\u5956\u52b1\u6a21\u578b\u5728\u5de5\u5177\u8c03\u7528\u573a\u666f\u7684\u6027\u80fd\uff1b\u63d0\u51fa\u4f7f\u7528\u5f00\u6e90LLM\u5408\u6210\u6570\u636e\u8bad\u7ec3\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u6a21\u578b\u7684\u6846\u67b6\uff1b\u8bad\u7ec3\u4e861.7B\u523014B\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\u3002", "result": "\u8bad\u7ec3\u7684\u5956\u52b1\u6a21\u578b\u57287\u4e2a\u57df\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u4f18\u4e8e\u901a\u7528\u57fa\u7ebf\u6a21\u578b\uff0c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u5e73\u5747\u63d0\u5347\u8fbe25%\uff0c\u5e76\u901a\u8fc7\u5956\u52b1\u5f15\u5bfc\u8fc7\u6ee4\u5b9e\u73b0\u4e86\u6570\u636e\u9ad8\u6548\u7684\u5fae\u8c03\u3002", "conclusion": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u5728\u5de5\u5177\u4f7f\u7528\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u9700\u8981\u9886\u57df\u7279\u5b9a\u7684\u5efa\u6a21\u65b9\u6cd5\uff1b\u63d0\u51fa\u7684\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u5de5\u5177\u8c03\u7528\u573a\u666f\u7684\u8bc4\u4f30\u6027\u80fd\u3002"}}
{"id": "2509.11989", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.11989", "abs": "https://arxiv.org/abs/2509.11989", "authors": ["Ahmed Moubtahij", "Sylvie Ratt\u00e9", "Yazid Attabi", "Maxime Dumas"], "title": "Query-Focused Extractive Summarization for Sentiment Explanation", "comment": null, "summary": "Constructive analysis of feedback from clients often requires determining the\ncause of their sentiment from a substantial amount of text documents. To assist\nand improve the productivity of such endeavors, we leverage the task of\nQuery-Focused Summarization (QFS). Models of this task are often impeded by the\nlinguistic dissonance between the query and the source documents. We propose\nand substantiate a multi-bias framework to help bridge this gap at a\ndomain-agnostic, generic level; we then formulate specialized approaches for\nthe problem of sentiment explanation through sentiment-based biases and query\nexpansion. We achieve experimental results outperforming baseline models on a\nreal-world proprietary sentiment-aware QFS dataset.", "AI": {"tldr": "\u63d0\u51fa\u591a\u504f\u5dee\u6846\u67b6\u89e3\u51b3\u67e5\u8be2\u805a\u7126\u6458\u8981\u4e2d\u7684\u8bed\u8a00\u5dee\u5f02\u95ee\u9898\uff0c\u901a\u8fc7\u60c5\u611f\u504f\u5dee\u548c\u67e5\u8be2\u6269\u5c55\u65b9\u6cd5\u5728\u60c5\u611f\u89e3\u91ca\u4efb\u52a1\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b", "motivation": "\u5ba2\u6237\u53cd\u9988\u5206\u6790\u9700\u8981\u4ece\u5927\u91cf\u6587\u672c\u4e2d\u786e\u5b9a\u60c5\u611f\u539f\u56e0\uff0c\u67e5\u8be2\u805a\u7126\u6458\u8981\u6a21\u578b\u5e38\u56e0\u67e5\u8be2\u4e0e\u6e90\u6587\u6863\u95f4\u7684\u8bed\u8a00\u5dee\u5f02\u800c\u53d7\u9650", "method": "\u63d0\u51fa\u591a\u504f\u5dee\u6846\u67b6\uff08\u9886\u57df\u65e0\u5173\u901a\u7528\u65b9\u6cd5\uff09\uff0c\u4e13\u95e8\u9488\u5bf9\u60c5\u611f\u89e3\u91ca\u95ee\u9898\u8bbe\u8ba1\u57fa\u4e8e\u60c5\u611f\u7684\u504f\u5dee\u548c\u67e5\u8be2\u6269\u5c55\u65b9\u6cd5", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u4e13\u6709\u60c5\u611f\u611f\u77e5QFS\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u7684\u5b9e\u9a8c\u7ed3\u679c", "conclusion": "\u591a\u504f\u5dee\u6846\u67b6\u80fd\u6709\u6548\u5f25\u5408\u67e5\u8be2\u4e0e\u6587\u6863\u95f4\u7684\u8bed\u8a00\u9e3f\u6c9f\uff0c\u63d0\u5347\u60c5\u611f\u89e3\u91ca\u4efb\u52a1\u7684\u6027\u80fd"}}
{"id": "2509.11991", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11991", "abs": "https://arxiv.org/abs/2509.11991", "authors": ["Jes\u00fas Calleja", "David Ponce", "Thierry Etchegoyhen"], "title": "Text Adaptation to Plain Language and Easy Read via Automatic Post-Editing Cycles", "comment": null, "summary": "We describe Vicomtech's participation in the CLEARS challenge on text\nadaptation to Plain Language and Easy Read in Spanish. Our approach features\nautomatic post-editing of different types of initial Large Language Model\nadaptations, where successive adaptations are generated iteratively until\nreadability and similarity metrics indicate that no further adaptation\nrefinement can be successfully performed. Taking the average of all official\nmetrics, our submissions achieved first and second place in Plain language and\nEasy Read adaptation, respectively.", "AI": {"tldr": "Vicomtech\u56e2\u961f\u5728CLEARS\u6311\u6218\u8d5b\u4e2d\u91c7\u7528\u81ea\u52a8\u540e\u7f16\u8f91\u548c\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u897f\u73ed\u7259\u8bed\u7b80\u5316\u6587\u672c\u548c\u6613\u8bfb\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u5206\u522b\u83b7\u5f97\u7b2c\u4e00\u548c\u7b2c\u4e8c\u540d", "motivation": "\u53c2\u4e0eCLEARS\u6311\u6218\u8d5b\uff0c\u65e8\u5728\u5f00\u53d1\u6709\u6548\u7684\u6587\u672c\u7b80\u5316\u6280\u672f\uff0c\u5c06\u590d\u6742\u6587\u672c\u8f6c\u6362\u4e3a\u666e\u901a\u8bed\u8a00\u548c\u6613\u8bfb\u683c\u5f0f\uff0c\u63d0\u9ad8\u6587\u672c\u53ef\u8bfb\u6027", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u81ea\u52a8\u540e\u7f16\u8f91\uff0c\u901a\u8fc7\u8fed\u4ee3\u751f\u6210\u8fde\u7eed\u7684\u6587\u672c\u7b80\u5316\u7248\u672c\uff0c\u76f4\u5230\u53ef\u8bfb\u6027\u548c\u76f8\u4f3c\u6027\u6307\u6807\u663e\u793a\u65e0\u6cd5\u8fdb\u4e00\u6b65\u4f18\u5316", "result": "\u5728\u5b98\u65b9\u8bc4\u4f30\u6307\u6807\u7684\u5e73\u5747\u5f97\u5206\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u666e\u901a\u8bed\u8a00\u7b80\u5316\u4efb\u52a1\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0c\u5728\u6613\u8bfb\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u83b7\u5f97\u7b2c\u4e8c\u540d", "conclusion": "\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8fed\u4ee3\u540e\u7f16\u8f91\u65b9\u6cd5\u5728\u6587\u672c\u7b80\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u63d0\u9ad8\u6587\u672c\u53ef\u8bfb\u6027\u65b9\u9762\u7684\u6709\u6548\u6027"}}
{"id": "2509.12065", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.12065", "abs": "https://arxiv.org/abs/2509.12065", "authors": ["Alina Klerings", "Jannik Brinkmann", "Daniel Ruffinelli", "Simone Ponzetto"], "title": "Steering Language Models in Multi-Token Generation: A Case Study on Tense and Aspect", "comment": "to be published in The 2025 Conference on Empirical Methods in\n  Natural Language Processing", "summary": "Large language models (LLMs) are able to generate grammatically well-formed\ntext, but how do they encode their syntactic knowledge internally? While prior\nwork has focused largely on binary grammatical contrasts, in this work, we\nstudy the representation and control of two multidimensional hierarchical\ngrammar phenomena - verb tense and aspect - and for each, identify distinct,\northogonal directions in residual space using linear discriminant analysis.\nNext, we demonstrate causal control over both grammatical features through\nconcept steering across three generation tasks. Then, we use these identified\nfeatures in a case study to investigate factors influencing effective steering\nin multi-token generation. We find that steering strength, location, and\nduration are crucial parameters for reducing undesirable side effects such as\ntopic shift and degeneration. Our findings suggest that models encode tense and\naspect in structurally organized, human-like ways, but effective control of\nsuch features during generation is sensitive to multiple factors and requires\nmanual tuning or automated optimization.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ebf\u6027\u5224\u522b\u5206\u6790\u8bc6\u522b\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u52a8\u8bcd\u65f6\u6001\u548c\u4f53\u4e24\u4e2a\u591a\u7ef4\u8bed\u6cd5\u73b0\u8c61\u7684\u6b63\u4ea4\u65b9\u5411\uff0c\u5e76\u901a\u8fc7\u6982\u5ff5\u5bfc\u5411\u5b9e\u73b0\u4e86\u5bf9\u8fd9\u4e24\u4e2a\u8bed\u6cd5\u7279\u5f81\u7684\u56e0\u679c\u63a7\u5236\uff0c\u53d1\u73b0\u5bfc\u5411\u5f3a\u5ea6\u3001\u4f4d\u7f6e\u548c\u6301\u7eed\u65f6\u95f4\u662f\u51cf\u5c11\u526f\u4f5c\u7528\u7684\u5173\u952e\u53c2\u6570\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5185\u90e8\u7f16\u7801\u53e5\u6cd5\u77e5\u8bc6\uff0c\u7279\u522b\u662f\u591a\u7ef4\u5c42\u6b21\u8bed\u6cd5\u73b0\u8c61\uff08\u52a8\u8bcd\u65f6\u6001\u548c\u4f53\uff09\u7684\u8868\u5f81\u548c\u63a7\u5236\u65b9\u5f0f\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u5224\u522b\u5206\u6790\u8bc6\u522b\u8bed\u6cd5\u7279\u5f81\u7684\u6b63\u4ea4\u65b9\u5411\uff0c\u901a\u8fc7\u6982\u5ff5\u5bfc\u5411\u5728\u4e09\u4e2a\u751f\u6210\u4efb\u52a1\u4e2d\u8fdb\u884c\u56e0\u679c\u63a7\u5236\u5b9e\u9a8c\uff0c\u5e76\u7814\u7a76\u591a\u6807\u8bb0\u751f\u6210\u4e2d\u6709\u6548\u5bfc\u5411\u7684\u5f71\u54cd\u56e0\u7d20\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u4ee5\u7ed3\u6784\u5316\u7684\u3001\u7c7b\u4f3c\u4eba\u7c7b\u7684\u65b9\u5f0f\u7f16\u7801\u65f6\u6001\u548c\u4f53\uff0c\u4f46\u751f\u6210\u8fc7\u7a0b\u4e2d\u5bf9\u8fd9\u4e9b\u7279\u5f81\u7684\u6709\u6548\u63a7\u5236\u5bf9\u591a\u4e2a\u56e0\u7d20\u654f\u611f\uff0c\u9700\u8981\u624b\u52a8\u8c03\u4f18\u6216\u81ea\u52a8\u4f18\u5316\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u8bed\u6cd5\u77e5\u8bc6\u7684\u65b9\u5f0f\u5177\u6709\u7ed3\u6784\u6027\uff0c\u4f46\u8bed\u6cd5\u7279\u5f81\u7684\u63a7\u5236\u9700\u8981\u7cbe\u7ec6\u7684\u53c2\u6570\u8c03\u8282\uff0c\u5bfc\u5411\u5f3a\u5ea6\u3001\u4f4d\u7f6e\u548c\u6301\u7eed\u65f6\u95f4\u662f\u5f71\u54cd\u63a7\u5236\u6548\u679c\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2509.12093", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12093", "abs": "https://arxiv.org/abs/2509.12093", "authors": ["Salima Mdhaffar", "Haroun Elleuch", "Chaimae Chellaf", "Ha Nguyen", "Yannick Est\u00e8ve"], "title": "SENSE models: an open source solution for multilingual and multimodal semantic-based tasks", "comment": "Accepted to IEEE ASRU 2025", "summary": "This paper introduces SENSE (Shared Embedding for N-lingual Speech and tExt),\nan open-source solution inspired by the SAMU-XLSR framework and conceptually\nsimilar to Meta AI's SONAR models. These approaches rely on a teacher-student\nframework to align a self-supervised speech encoder with the language-agnostic\ncontinuous representations of a text encoder at the utterance level. We\ndescribe how the original SAMU-XLSR method has been updated by selecting a\nstronger teacher text model and a better initial speech encoder. The source\ncode for training and using SENSE models has been integrated into the\nSpeechBrain toolkit, and the first SENSE model we trained has been publicly\nreleased. We report experimental results on multilingual and multimodal\nsemantic tasks, where our SENSE model achieves highly competitive performance.\nFinally, this study offers new insights into how semantics are captured in such\nsemantically aligned speech encoders.", "AI": {"tldr": "SENSE\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u8bed\u8a00\u8bed\u97f3-\u6587\u672c\u5171\u4eab\u5d4c\u5165\u6a21\u578b\uff0c\u57fa\u4e8eSAMU-XLSR\u6846\u67b6\uff0c\u901a\u8fc7\u5e08\u751f\u6846\u67b6\u5c06\u81ea\u76d1\u7763\u8bed\u97f3\u7f16\u7801\u5668\u4e0e\u8bed\u8a00\u65e0\u5173\u7684\u6587\u672c\u7f16\u7801\u5668\u8868\u793a\u5bf9\u9f50\u3002", "motivation": "\u53d7\u5230SAMU-XLSR\u548cMeta AI\u7684SONAR\u6a21\u578b\u542f\u53d1\uff0c\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u66f4\u597d\u7684\u591a\u8bed\u8a00\u8bed\u97f3-\u6587\u672c\u8bed\u4e49\u5bf9\u9f50\u6a21\u578b\uff0c\u63d0\u5347\u8de8\u8bed\u8a00\u548c\u8de8\u6a21\u6001\u8bed\u4e49\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u5e08\u751f\u6846\u67b6\uff0c\u9009\u62e9\u66f4\u5f3a\u7684\u6559\u5e08\u6587\u672c\u6a21\u578b\u548c\u66f4\u597d\u7684\u521d\u59cb\u8bed\u97f3\u7f16\u7801\u5668\u6765\u6539\u8fdb\u539f\u59cbSAMU-XLSR\u65b9\u6cd5\uff0c\u5e76\u5c06\u4ee3\u7801\u96c6\u6210\u5230SpeechBrain\u5de5\u5177\u5305\u4e2d\u3002", "result": "\u5728\u591a\u8bed\u8a00\u548c\u591a\u6a21\u6001\u8bed\u4e49\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6781\u5177\u7ade\u4e89\u529b\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u8bed\u4e49\u5728\u8bed\u97f3-\u6587\u672c\u5bf9\u9f50\u7f16\u7801\u5668\u4e2d\u7684\u6355\u83b7\u65b9\u5f0f\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u516c\u5f00\u53ef\u7528\u7684SENSE\u6a21\u578b\u3002"}}
{"id": "2509.12098", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12098", "abs": "https://arxiv.org/abs/2509.12098", "authors": ["Payam Latifi"], "title": "Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing traditional NLP tools and large language models on ambiguous entities", "comment": "14 pages, 9 figures, 2 tables. This is a pilot study evaluating six\n  NER systems -- three traditional tools (NLTK, spaCy, Stanza) and three LLMs\n  (Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B) -- on a small, ambiguity-rich\n  dataset of 119 tokens. The annotated dataset, prompts are provided in\n  appendices for full reproducibility. All experiments were conducted on 14 May\n  2025", "summary": "This pilot study presents a small-scale but carefully annotated benchmark of\nNamed Entity Recognition (NER) performance across six systems: three non-LLM\nNLP tools (NLTK, spaCy, Stanza) and three general-purpose large language models\n(LLMs: Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The dataset contains 119\ntokens covering five entity types (PERSON, LOCATION, ORGANIZATION, DATE, TIME).\nWe evaluated each system's output against the manually annotated gold standard\ndataset using F1-score. The results show that LLMs generally outperform\nconventional tools in recognizing context-sensitive entities like person names,\nwith Gemini achieving the highest average F1-score. However, traditional\nsystems like Stanza demonstrate greater consistency in structured tags such as\nLOCATION and DATE. We also observed variability among LLMs, particularly in\nhandling temporal expressions and multi-word organizations. Our findings\nhighlight that while LLMs offer improved contextual understanding, traditional\ntools remain competitive in specific tasks, informing model selection.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e866\u4e2aNER\u7cfb\u7edf\uff083\u4e2a\u4f20\u7edfNLP\u5de5\u5177\u548c3\u4e2aLLM\uff09\u5728\u5c0f\u578b\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u53d1\u73b0LLM\u5728\u4e0a\u4e0b\u6587\u654f\u611f\u5b9e\u4f53\u8bc6\u522b\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u4f20\u7edf\u5de5\u5177\u5728\u7ed3\u6784\u5316\u6807\u7b7e\u4e0a\u66f4\u7a33\u5b9a\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u4f20\u7edfNLP\u5de5\u5177\u5728\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4e3a\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\u3002", "method": "\u4f7f\u7528\u5305\u542b119\u4e2a\u6807\u8bb0\u30015\u79cd\u5b9e\u4f53\u7c7b\u578b\u7684\u624b\u52a8\u6807\u6ce8\u9ec4\u91d1\u6807\u51c6\u6570\u636e\u96c6\uff0c\u5bf96\u4e2a\u7cfb\u7edf\uff08NLTK\u3001spaCy\u3001Stanza\u3001Gemini\u3001DeepSeek\u3001Qwen\uff09\u8fdb\u884cF1\u5206\u6570\u8bc4\u4f30\u3002", "result": "LLM\u6574\u4f53\u4f18\u4e8e\u4f20\u7edf\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u4eba\u540d\u8bc6\u522b\u4e0a\uff1bGemini\u83b7\u5f97\u6700\u9ad8\u5e73\u5747F1\u5206\u6570\uff1b\u4f20\u7edf\u5de5\u5177\u5982Stanza\u5728LOCATION\u548cDATE\u7b49\u7ed3\u6784\u5316\u6807\u7b7e\u4e0a\u8868\u73b0\u66f4\u4e00\u81f4\uff1bLLM\u5728\u5904\u7406\u65f6\u95f4\u8868\u8fbe\u5f0f\u548c\u591a\u8bcd\u7ec4\u7ec7\u540d\u65f6\u5b58\u5728\u53d8\u5f02\u6027\u3002", "conclusion": "LLM\u63d0\u4f9b\u66f4\u597d\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u4f20\u7edf\u5de5\u5177\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u4ecd\u5177\u7ade\u4e89\u529b\uff0c\u6a21\u578b\u9009\u62e9\u5e94\u6839\u636e\u5177\u4f53\u9700\u6c42\u51b3\u5b9a\u3002"}}
{"id": "2509.12101", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12101", "abs": "https://arxiv.org/abs/2509.12101", "authors": ["Jarod Duret", "Salima Mdhaffar", "Ga\u00eblle Laperri\u00e8re", "Ryan Whetten", "Audrey Galametz", "Catherine Kobus", "Marion-C\u00e9cile Martin", "Jo Oleiwan", "Yannick Est\u00e8ve"], "title": "In-domain SSL pre-training and streaming ASR", "comment": "Accepted to SPECOM 2025", "summary": "In this study, we investigate the benefits of domain-specific self-supervised\npre-training for both offline and streaming ASR in Air Traffic Control (ATC)\nenvironments. We train BEST-RQ models on 4.5k hours of unlabeled ATC data, then\nfine-tune on a smaller supervised ATC set. To enable real-time processing, we\npropose using chunked attention and dynamic convolutions, ensuring low-latency\ninference. We compare these in-domain SSL models against state-of-the-art,\ngeneral-purpose speech encoders such as w2v-BERT 2.0 and HuBERT. Results show\nthat domain-adapted pre-training substantially improves performance on standard\nATC benchmarks, significantly reducing word error rates when compared to models\ntrained on broad speech corpora. Furthermore, the proposed streaming approach\nfurther improves word error rate under tighter latency constraints, making it\nparticularly suitable for safety-critical aviation applications. These findings\nhighlight that specializing SSL representations for ATC data is a practical\npath toward more accurate and efficient ASR systems in real-world operational\nsettings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5728\u822a\u7a7a\u4ea4\u901a\u7ba1\u5236\u9886\u57df\u4f7f\u7528\u9886\u57df\u7279\u5b9a\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u6d41\u5f0f\u5904\u7406\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u8bc6\u522b\u51c6\u786e\u7387\u548c\u5b9e\u65f6\u6027\u80fd", "motivation": "\u822a\u7a7a\u4ea4\u901a\u7ba1\u5236\u73af\u5883\u5bf9\u8bed\u97f3\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\u8981\u6c42\u6781\u9ad8\uff0c\u901a\u7528\u8bed\u97f3\u7f16\u7801\u5668\u5728\u8be5\u9886\u57df\u8868\u73b0\u6709\u9650\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9ATC\u6570\u636e\u7684\u4f18\u5316\u65b9\u6848", "method": "\u4f7f\u75284.5\u5343\u5c0f\u65f6\u65e0\u6807\u6ce8ATC\u6570\u636e\u8bad\u7ec3BEST-RQ\u6a21\u578b\uff0c\u7136\u540e\u5728\u6709\u76d1\u7763ATC\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\uff1b\u63d0\u51fa\u5206\u5757\u6ce8\u610f\u529b\u548c\u52a8\u6001\u5377\u79ef\u6280\u672f\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u6d41\u5f0f\u5904\u7406", "result": "\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u663e\u8457\u964d\u4f4e\u4e86\u8bcd\u9519\u8bef\u7387\uff0c\u5728\u6807\u51c6ATC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8ew2v-BERT 2.0\u548cHuBERT\u7b49\u901a\u7528\u8bed\u97f3\u7f16\u7801\u5668\uff1b\u6d41\u5f0f\u65b9\u6cd5\u5728\u4e25\u683c\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd", "conclusion": "\u9488\u5bf9ATC\u6570\u636e\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u4e13\u4e1a\u5316\u662f\u63d0\u9ad8\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2dASR\u7cfb\u7edf\u51c6\u786e\u6027\u548c\u6548\u7387\u7684\u6709\u6548\u9014\u5f84\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u7684\u822a\u7a7a\u5e94\u7528"}}
{"id": "2509.12108", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12108", "abs": "https://arxiv.org/abs/2509.12108", "authors": ["Min Zeng", "Jinfei Sun", "Xueyou Luo", "Caiquan Liu", "Shiqi Zhang", "Li Xie", "Xiaoxin Chen"], "title": "GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models", "comment": "Accepted at EMNLP 2025", "summary": "In natural language processing tasks, pure reinforcement learning (RL)\nfine-tuning methods often suffer from inefficient exploration and slow\nconvergence; while supervised fine-tuning (SFT) methods, although efficient in\ntraining, have limited performance ceiling and less solid theoretical\nfoundation compared to RL. To address efficiency-capability trade-off, we\npropose the Guess-Think-Answer (GTA) framework that combines the efficiency of\nSFT with the capability gains of RL in a unified training paradigm. GTA works\nby having the model first produce a provisional guess (optimized via\ncross-entropy loss), then reflect on this guess before generating the final\nanswer, with RL rewards shaping both the final output and the format of the\nentire GTA structure. This hybrid approach achieves both faster convergence\nthan pure RL and higher performance ceiling than pure SFT. To mitigate gradient\nconflicts between the two training signals, we employ loss masking and gradient\nconstraints. Empirical results on four text classification benchmarks\ndemonstrate that GTA substantially accelerates convergence while outperforming\nboth standalone SFT and RL baselines.", "AI": {"tldr": "GTA\u6846\u67b6\u7ed3\u5408\u76d1\u7763\u5fae\u8c03(SFT)\u7684\u6548\u7387\u4e0e\u5f3a\u5316\u5b66\u4e60(RL)\u7684\u80fd\u529b\u63d0\u5347\uff0c\u901a\u8fc7\u5148\u731c\u6d4b\u540e\u53cd\u601d\u7684\u6d41\u7a0b\uff0c\u5728\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u7eafSFT\u548cRL\u66f4\u597d\u7684\u6027\u80fd\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u7eafRL\u65b9\u6cd5\u63a2\u7d22\u6548\u7387\u4f4e\u3001\u6536\u655b\u6162\uff0c\u800cSFT\u65b9\u6cd5\u6027\u80fd\u4e0a\u9650\u6709\u9650\u3001\u7406\u8bba\u57fa\u7840\u8584\u5f31\u7684\u95ee\u9898\uff0c\u5bfb\u6c42\u6548\u7387\u4e0e\u80fd\u529b\u7684\u6700\u4f73\u5e73\u8861\u3002", "method": "\u63d0\u51faGuess-Think-Answer\u6846\u67b6\uff1a\u6a21\u578b\u5148\u4ea7\u751f\u4e34\u65f6\u731c\u6d4b\uff08\u4ea4\u53c9\u71b5\u635f\u5931\u4f18\u5316\uff09\uff0c\u7136\u540e\u53cd\u601d\u731c\u6d4b\uff0c\u6700\u540e\u751f\u6210\u7b54\u6848\uff0cRL\u5956\u52b1\u540c\u65f6\u4f18\u5316\u6700\u7ec8\u8f93\u51fa\u548c\u6574\u4e2aGTA\u7ed3\u6784\u683c\u5f0f\u3002\u4f7f\u7528\u635f\u5931\u63a9\u7801\u548c\u68af\u5ea6\u7ea6\u675f\u7f13\u89e3\u8bad\u7ec3\u4fe1\u53f7\u51b2\u7a81\u3002", "result": "\u5728\u56db\u4e2a\u6587\u672c\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGTA\u663e\u8457\u52a0\u901f\u6536\u655b\u901f\u5ea6\uff0c\u540c\u65f6\u8d85\u8d8a\u5355\u72ec\u7684SFT\u548cRL\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GTA\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86SFT\u6548\u7387\u4e0eRL\u80fd\u529b\u7684\u7ed3\u5408\uff0c\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6df7\u5408\u8bad\u7ec3\u8303\u5f0f\u3002"}}
{"id": "2509.12112", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12112", "abs": "https://arxiv.org/abs/2509.12112", "authors": ["Jiaxuan Zhao", "Naibin Gu", "Yuchen Feng", "Xiyu Liu", "Peng Fu", "Zheng Lin", "Weiping Wang"], "title": "CBP-Tuning: Efficient Local Customization for Black-box Large Language Models", "comment": null, "summary": "The high costs of customizing large language models (LLMs) fundamentally\nlimit their adaptability to user-specific needs. Consequently, LLMs are\nincreasingly offered as cloud-based services, a paradigm that introduces\ncritical limitations: providers struggle to support personalized customization\nat scale, while users face privacy risks when exposing sensitive data. To\naddress this dual challenge, we propose Customized Black-box Prompt Tuning\n(CBP-Tuning), a novel framework that facilitates efficient local customization\nwhile preserving bidirectional privacy. Specifically, we design a two-stage\nframework: (1) a prompt generator trained on the server-side to capture\ndomain-specific and task-agnostic capabilities, and (2) user-side gradient-free\noptimization that tailors soft prompts for individual tasks. This approach\neliminates the need for users to access model weights or upload private data,\nrequiring only a single customized vector per task while achieving effective\nadaptation. Furthermore, the evaluation of CBP-Tuning in the commonsense\nreasoning, medical and financial domain settings demonstrates superior\nperformance compared to baselines, showcasing its advantages in task-agnostic\nprocessing and privacy preservation.", "AI": {"tldr": "CBP-Tuning\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u9ed1\u76d2\u63d0\u793a\u8c03\u4f18\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u6548\u672c\u5730\u5b9a\u5236\uff0c\u540c\u65f6\u4fdd\u62a4\u53cc\u5411\u9690\u79c1\uff0c\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u6743\u91cd\u6216\u4e0a\u4f20\u79c1\u6709\u6570\u636e\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b9a\u5236\u6210\u672c\u9ad8\uff0c\u4e91\u670d\u52a1\u6a21\u5f0f\u5b58\u5728\u4e2a\u6027\u5316\u5b9a\u5236\u56f0\u96be\u548c\u7528\u6237\u9690\u79c1\u98ce\u9669\u7684\u53cc\u91cd\u6311\u6218\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u670d\u52a1\u5668\u7aef\u8bad\u7ec3\u63d0\u793a\u751f\u6210\u5668\u6355\u83b7\u9886\u57df\u7279\u5b9a\u80fd\u529b\uff1b2\uff09\u7528\u6237\u7aef\u8fdb\u884c\u65e0\u68af\u5ea6\u4f18\u5316\uff0c\u4e3a\u5355\u4e2a\u4efb\u52a1\u5b9a\u5236\u8f6f\u63d0\u793a\u3002", "result": "\u5728\u5e38\u8bc6\u63a8\u7406\u3001\u533b\u7597\u548c\u91d1\u878d\u9886\u57df\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u57fa\u7ebf\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u4efb\u52a1\u65e0\u5173\u5904\u7406\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u4f18\u52bf\u3002", "conclusion": "CBP-Tuning\u6709\u6548\u89e3\u51b3\u4e86LLM\u5b9a\u5236\u5316\u7684\u6210\u672c\u548c\u9690\u79c1\u95ee\u9898\uff0c\u4e3a\u4e2a\u6027\u5316\u5b9a\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.12130", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12130", "abs": "https://arxiv.org/abs/2509.12130", "authors": ["Ariana Sahitaj", "Jiaao Li", "Pia Wenzel Neves", "Fedor Splitt", "Premtim Sahitaj", "Charlott Jakob", "Veronika Solopova", "Vera Schmitt"], "title": "XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with Finetuned Transformers and Prompt-Based Inference with Large Language Models", "comment": null, "summary": "This notebook reports the XplaiNLP submission to the CheckThat! 2025 shared\ntask on multilingual subjectivity detection. We evaluate two approaches: (1)\nsupervised fine-tuning of transformer encoders, EuroBERT, XLM-RoBERTa, and\nGerman-BERT, on monolingual and machine-translated training data; and (2)\nzero-shot prompting using two LLMs: o3-mini for Annotation (rule-based\nlabelling) and gpt-4.1-mini for DoubleDown (contrastive rewriting) and\nPerspective (comparative reasoning). The Annotation Approach achieves 1st place\nin the Italian monolingual subtask with an F_1 score of 0.8104, outperforming\nthe baseline of 0.6941. In the Romanian zero-shot setting, the fine-tuned\nXLM-RoBERTa model obtains an F_1 score of 0.7917, ranking 3rd and exceeding the\nbaseline of 0.6461. The same model also performs reliably in the multilingual\ntask and improves over the baseline in Greek. For German, a German-BERT model\nfine-tuned on translated training data from typologically related languages\nyields competitive performance over the baseline. In contrast, performance in\nthe Ukrainian and Polish zero-shot settings falls slightly below the respective\nbaselines, reflecting the challenge of generalization in low-resource\ncross-lingual scenarios.", "AI": {"tldr": "XplaiNLP\u5728CheckThat! 2025\u591a\u8bed\u8a00\u4e3b\u89c2\u6027\u68c0\u6d4b\u4efb\u52a1\u4e2d\u63d0\u4ea4\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a\u57fa\u4e8eTransformer\u7684\u76d1\u7763\u5fae\u8c03\u548c\u96f6\u6837\u672c\u63d0\u793aLLM\uff0c\u5728\u610f\u5927\u5229\u8bed\u4efb\u52a1\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0c\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u6392\u540d\u7b2c\u4e09\uff0c\u4f46\u5728\u4e4c\u514b\u5170\u8bed\u548c\u6ce2\u5170\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u8868\u73b0\u7565\u4f4e\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u4e3b\u89c2\u6027\u68c0\u6d4b\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0b\u5982\u4f55\u6709\u6548\u8fdb\u884c\u8de8\u8bed\u8a00\u6cdb\u5316\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u65b9\u6cd5\uff1a(1) \u5bf9EuroBERT\u3001XLM-RoBERTa\u548cGerman-BERT\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u4f7f\u7528\u5355\u8bed\u548c\u673a\u5668\u7ffb\u8bd1\u7684\u8bad\u7ec3\u6570\u636e\uff1b(2) \u96f6\u6837\u672c\u63d0\u793a\u4f7f\u7528o3-mini\u8fdb\u884c\u57fa\u4e8e\u89c4\u5219\u7684\u6807\u6ce8\uff0c\u4ee5\u53cagpt-4.1-mini\u8fdb\u884c\u5bf9\u6bd4\u91cd\u5199\u548c\u6bd4\u8f83\u63a8\u7406\u3002", "result": "\u610f\u5927\u5229\u8bed\u5355\u8bed\u4efb\u52a1F1\u5f97\u52060.8104\uff08\u7b2c\u4e00\u540d\uff0c\u57fa\u7ebf0.6941\uff09\uff1b\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2dXLM-RoBERTa\u83b7\u5f970.7917 F1\u5f97\u5206\uff08\u7b2c\u4e09\u540d\uff0c\u57fa\u7ebf0.6461\uff09\uff1b\u5fb7\u8bed\u4f7f\u7528\u7ffb\u8bd1\u6570\u636e\u5fae\u8c03\u7684German-BERT\u8868\u73b0\u7ade\u4e89\u529b\uff1b\u4f46\u4e4c\u514b\u5170\u8bed\u548c\u6ce2\u5170\u8bed\u96f6\u6837\u672c\u8bbe\u7f6e\u7565\u4f4e\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u76d1\u7763\u5fae\u8c03\u5728\u591a\u8bed\u8a00\u4e3b\u89c2\u6027\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u76f8\u5bf9\u4e30\u5bcc\u7684\u8bed\u8a00\u4e0a\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8de8\u8bed\u8a00\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002"}}
{"id": "2509.12158", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.12158", "abs": "https://arxiv.org/abs/2509.12158", "authors": ["Alessandro Zangari", "Matteo Marcuzzo", "Andrea Albarelli", "Mohammad Taher Pilehvar", "Jose Camacho-Collados"], "title": "Pun Unintended: LLMs and the Illusion of Humor Understanding", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Puns are a form of humorous wordplay that exploits polysemy and phonetic\nsimilarity. While LLMs have shown promise in detecting puns, we show in this\npaper that their understanding often remains shallow, lacking the nuanced grasp\ntypical of human interpretation. By systematically analyzing and reformulating\nexisting pun benchmarks, we demonstrate how subtle changes in puns are\nsufficient to mislead LLMs. Our contributions include comprehensive and nuanced\npun detection benchmarks, human evaluation across recent LLMs, and an analysis\nof the robustness challenges these models face in processing puns.", "AI": {"tldr": "LLMs\u5728\u53cc\u5173\u8bed\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u6709\u9650\uff0c\u5bb9\u6613\u88ab\u7ec6\u5fae\u53d8\u5316\u8bef\u5bfc\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u822c\u7684\u6df1\u5c42\u7406\u89e3", "motivation": "\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u53cc\u5173\u8bed\u68c0\u6d4b\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u7406\u89e3\u5f80\u5f80\u505c\u7559\u5728\u8868\u9762\u5c42\u6b21\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u822c\u7684\u7ec6\u817b\u628a\u63e1\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5176\u9c81\u68d2\u6027\u6311\u6218", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u548c\u91cd\u6784\u73b0\u6709\u7684\u53cc\u5173\u8bed\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c55\u793a\u53cc\u5173\u8bed\u7684\u7ec6\u5fae\u53d8\u5316\u5982\u4f55\u8bef\u5bfcLLMs\uff0c\u5305\u62ec\u521b\u5efa\u5168\u9762\u7684\u53cc\u5173\u8bed\u68c0\u6d4b\u57fa\u51c6\u548c\u8fdb\u884c\u4eba\u7c7b\u8bc4\u4f30", "result": "\u7814\u7a76\u8868\u660eLLMs\u5bf9\u53cc\u5173\u8bed\u7684\u7406\u89e3\u8f83\u4e3a\u80a4\u6d45\uff0c\u5bb9\u6613\u88ab\u5fae\u5999\u7684\u53d8\u5316\u6240\u6b3a\u9a97\uff0c\u63ed\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u5904\u7406\u53cc\u5173\u8bed\u65f6\u9762\u4e34\u7684\u9c81\u68d2\u6027\u6311\u6218", "conclusion": "\u5f53\u524dLLMs\u5728\u7406\u89e3\u53cc\u5173\u8bed\u8fd9\u79cd\u9700\u8981\u6df1\u5c42\u8bed\u4e49\u548c\u8bed\u97f3\u76f8\u4f3c\u6027\u628a\u63e1\u7684\u5e7d\u9ed8\u5f62\u5f0f\u65b9\u9762\u4ecd\u6709\u660e\u663e\u5c40\u9650\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u6a21\u578b\u7684\u7406\u89e3\u6df1\u5ea6\u548c\u9c81\u68d2\u6027"}}
{"id": "2509.12168", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12168", "abs": "https://arxiv.org/abs/2509.12168", "authors": ["Timothy Rupprecht", "Enfu Nan", "Arash Akbari", "Arman Akbari", "Lei Lu", "Priyanka Maan", "Sean Duffy", "Pu Zhao", "Yumei He", "David Kaeli", "Yanzhi Wang"], "title": "RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing", "comment": null, "summary": "Role-playing Large language models (LLMs) are increasingly deployed in\nhigh-stakes domains such as healthcare, education, and governance, where\nfailures can directly impact user trust and well-being. A cost effective\nparadigm for LLM role-playing is few-shot learning, but existing approaches\noften cause models to break character in unexpected and potentially harmful\nways, especially when interacting with hostile users. Inspired by\nRetrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a\ntext retrieval problem and propose a new prompting framework called\nRAGs-to-Riches, which leverages curated reference demonstrations to condition\nLLM responses. We evaluate our framework with LLM-as-a-judge preference voting\nand introduce two novel token-level ROUGE metrics: Intersection over Output\n(IOO) to quantity how much an LLM improvises and Intersection over References\n(IOR) to measure few-shot demonstrations utilization rate during the evaluation\ntasks. When simulating interactions with a hostile user, our prompting strategy\nincorporates in its responses during inference an average of 35% more tokens\nfrom the reference demonstrations. As a result, across 453 role-playing\ninteractions, our models are consistently judged as being more authentic, and\nremain in-character more often than zero-shot and in-context Learning (ICL)\nmethods. Our method presents a scalable strategy for building robust,\nhuman-aligned LLM role-playing frameworks.", "AI": {"tldr": "RAGs-to-Riches\u6846\u67b6\u901a\u8fc7\u5c06LLM\u89d2\u8272\u626e\u6f14\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6587\u672c\u68c0\u7d22\u95ee\u9898\uff0c\u5229\u7528\u7cbe\u5fc3\u7b56\u5212\u7684\u53c2\u8003\u6f14\u793a\u6765\u589e\u5f3a\u6a21\u578b\u54cd\u5e94\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u654c\u5bf9\u7528\u6237\u4ea4\u4e92\u4e2d\u7684\u89d2\u8272\u4fdd\u6301\u80fd\u529b\u3002", "motivation": "LLM\u5728\u533b\u7597\u3001\u6559\u80b2\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u90e8\u7f72\u65f6\uff0c\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u7ecf\u5e38\u5bfc\u81f4\u6a21\u578b\u5728\u654c\u5bf9\u7528\u6237\u4ea4\u4e92\u4e2d\u610f\u5916\u8131\u620f\uff0c\u53ef\u80fd\u5bf9\u7528\u6237\u4fe1\u4efb\u548c\u798f\u7949\u9020\u6210\u5371\u5bb3\u3002", "method": "\u53d7RAG\u542f\u53d1\uff0c\u5c06LLM\u89d2\u8272\u626e\u6f14\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6587\u672c\u68c0\u7d22\u95ee\u9898\uff0c\u63d0\u51faRAGs-to-Riches\u63d0\u793a\u6846\u67b6\uff0c\u5229\u7528\u53c2\u8003\u6f14\u793a\u6765\u8c03\u8282LLM\u54cd\u5e94\uff0c\u5e76\u5f15\u5165IOO\u548cIOR\u4e24\u4e2a\u65b0\u7684token\u7ea7ROUGE\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u4e0e\u654c\u5bf9\u7528\u6237\u6a21\u62df\u4ea4\u4e92\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4ece\u53c2\u8003\u6f14\u793a\u4e2d\u591a\u7eb3\u5165\u4e8635%\u7684token\uff0c\u5728453\u6b21\u89d2\u8272\u626e\u6f14\u4ea4\u4e92\u4e2d\u88ab\u4e00\u81f4\u8bc4\u4e3a\u66f4\u771f\u5b9e\u3001\u66f4\u5c11\u8131\u620f\uff0c\u4f18\u4e8e\u96f6\u6837\u672c\u548cICL\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u7a33\u5065\u3001\u4eba\u7c7b\u5bf9\u9f50\u7684LLM\u89d2\u8272\u626e\u6f14\u6846\u67b6\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u654c\u5bf9\u73af\u5883\u4e0b\u7684\u89d2\u8272\u4fdd\u6301\u80fd\u529b\u3002"}}
{"id": "2509.12171", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12171", "abs": "https://arxiv.org/abs/2509.12171", "authors": ["Marek Kubis", "Pawe\u0142 Sk\u00f3rzewski", "Iwona Christop", "Mateusz Czy\u017cnikiewicz", "Jakub Kubiak", "\u0141ukasz Bondaruk", "Marcin Lewandowski"], "title": "Preservation of Language Understanding Capabilities in Speech-aware Large Language Models", "comment": "5 pages, 1 figure", "summary": "The paper presents C3T (Cross-modal Capabilities Conservation Test), a new\nbenchmark for assessing the performance of speech-aware large language models.\nThe benchmark utilizes textual tasks and a voice cloning text-to-speech model\nto quantify the extent to which language understanding capabilities are\npreserved when the model is accessed via speech input. C3T quantifies the\nfairness of the model for different categories of speakers and its robustness\nacross text and speech modalities.", "AI": {"tldr": "C3T\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u97f3\u611f\u77e5\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u6a21\u6001\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u901a\u8fc7\u6587\u672c\u4efb\u52a1\u548c\u8bed\u97f3\u514b\u9686\u6280\u672f\u6765\u91cf\u5316\u6a21\u578b\u5728\u8bed\u97f3\u8f93\u5165\u65f6\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u4fdd\u6301\u7a0b\u5ea6\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\u8bed\u97f3\u611f\u77e5\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u97f3\u548c\u6587\u672c\u6a21\u6001\u4e0b\u6027\u80fd\u4e00\u81f4\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9700\u8981\u91cf\u5316\u6a21\u578b\u5728\u4e0d\u540c\u8bf4\u8bdd\u4eba\u7fa4\u4f53\u95f4\u7684\u516c\u5e73\u6027\u548c\u8de8\u6a21\u6001\u9c81\u68d2\u6027\u3002", "method": "\u5229\u7528\u6587\u672c\u4efb\u52a1\u548c\u8bed\u97f3\u514b\u9686\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\uff0c\u5c06\u6587\u672c\u8f93\u5165\u8f6c\u6362\u4e3a\u8bed\u97f3\u8f93\u5165\uff0c\u6d4b\u8bd5\u6a21\u578b\u5728\u8bed\u97f3\u6a21\u6001\u4e0b\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u4e0e\u6587\u672c\u6a21\u6001\u4e0b\u7684\u6027\u80fd\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u63d0\u51fa\u4e86C3T\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u80fd\u591f\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u8bf4\u8bdd\u4eba\u7fa4\u4f53\u95f4\u7684\u516c\u5e73\u6027\u548c\u8de8\u6587\u672c-\u8bed\u97f3\u6a21\u6001\u7684\u9c81\u68d2\u6027\u8868\u73b0\u3002", "conclusion": "C3T\u57fa\u51c6\u4e3a\u8bc4\u4f30\u8bed\u97f3\u611f\u77e5\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u6a21\u6001\u80fd\u529b\u4fdd\u6301\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6d4b\u8bd5\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u6a21\u578b\u5728\u8bed\u97f3\u8f93\u5165\u573a\u666f\u4e0b\u7684\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u53d1\u5c55\u3002"}}
