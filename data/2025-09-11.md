<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 33]
- [cs.SD](#cs.SD) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [CommonVoice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction with a New Dataset and Multi-Order Generative Framework](https://arxiv.org/abs/2509.08438)
*Jinzhong Ning,Paerhati Tulajiang,Yingying Le,Yijia Zhang,Yuanyuan Sun,Hongfei Lin,Haifeng Liu*

Main category: cs.CL

TL;DR: 提出了CommonVoice-SpeechRE大规模真实语音数据集和RPG-MoGe框架，通过多序三元组生成和关系提示引导，显著提升了语音关系抽取性能。


<details>
  <summary>Details</summary>
Motivation: 现有SpeechRE基准数据集严重依赖合成数据，缺乏真实人类语音的多样性和数量；现有模型存在单一生成模板和弱语义对齐问题，限制了性能。

Method: 1) 构建包含近2万条真实人类语音样本的CommonVoice-SpeechRE数据集；2) 提出RPG-MoGe框架，包含多序三元组生成集成策略和基于CNN的潜在关系预测头生成关系提示。

Result: 实验表明该方法优于现有最先进方法，为SpeechRE研究提供了新的基准数据集和有效解决方案。

Conclusion: 该工作解决了SpeechRE领域的数据稀缺和模型性能限制问题，通过真实数据集和创新框架推动了该领域的发展。

Abstract: Speech Relation Extraction (SpeechRE) aims to extract relation triplets
directly from speech. However, existing benchmark datasets rely heavily on
synthetic data, lacking sufficient quantity and diversity of real human speech.
Moreover, existing models also suffer from rigid single-order generation
templates and weak semantic alignment, substantially limiting their
performance. To address these challenges, we introduce CommonVoice-SpeechRE, a
large-scale dataset comprising nearly 20,000 real-human speech samples from
diverse speakers, establishing a new benchmark for SpeechRE research.
Furthermore, we propose the Relation Prompt-Guided Multi-Order Generative
Ensemble (RPG-MoGe), a novel framework that features: (1) a multi-order triplet
generation ensemble strategy, leveraging data diversity through diverse element
orders during both training and inference, and (2) CNN-based latent relation
prediction heads that generate explicit relation prompts to guide cross-modal
alignment and accurate triplet generation. Experiments show our approach
outperforms state-of-the-art methods, providing both a benchmark dataset and an
effective solution for real-world SpeechRE. The source code and dataset are
publicly available at https://github.com/NingJinzhong/SpeechRE_RPG_MoGe.

</details>


### [2] [Bilingual Word Level Language Identification for Omotic Languages](https://arxiv.org/abs/2509.07998)
*Mesay Gemeda Yigezu,Girma Yohannis Bade,Atnafu Lambebo Tonja,Olga Kolesnikova,Grigori Sidorov,Alexander Gelbukh*

Main category: cs.CL

TL;DR: 本文提出了针对埃塞俄比亚南部Wolaita和Gofa语言的双语识别(BLID)方法，结合BERT预训练模型和LSTM取得了0.72的F1分数


<details>
  <summary>Details</summary>
Motivation: 在多语言社区中，文本常包含多种语言，而Wolaita和Gofa语言之间的相似性和差异性使得语言识别任务具有挑战性

Method: 采用多种实验方法，最终确定BERT预训练语言模型与LSTM相结合的方法

Result: 在测试集上获得了0.72的F1分数，表现最佳

Conclusion: 这项工作能有效处理社交媒体问题，并为该领域的进一步研究提供基础

Abstract: Language identification is the task of determining the languages for a given
text. In many real world scenarios, text may contain more than one language,
particularly in multilingual communities. Bilingual Language Identification
(BLID) is the task of identifying and distinguishing between two languages in a
given text. This paper presents BLID for languages spoken in the southern part
of Ethiopia, namely Wolaita and Gofa. The presence of words similarities and
differences between the two languages makes the language identification task
challenging. To overcome this challenge, we employed various experiments on
various approaches. Then, the combination of the BERT based pretrained language
model and LSTM approach performed better, with an F1 score of 0.72 on the test
set. As a result, the work will be effective in tackling unwanted social media
issues and providing a foundation for further research in this area.

</details>


### [3] [AntiDote: Bi-level Adversarial Training for Tamper-Resistant LLMs](https://arxiv.org/abs/2509.08000)
*Debdeep Sanyal,Manodeep Ray,Murari Mandal*

Main category: cs.CL

TL;DR: AntiDote是一种双层优化方法，通过对抗性超网络训练LLM抵抗恶意微调攻击，在保持模型性能的同时显著提升安全性


<details>
  <summary>Details</summary>
Motivation: 开源权重LLM面临恶意微调生成有害内容的风险，现有安全措施难以在保持模型能力的同时抵御完全访问模型权重的攻击者

Method: 使用辅助对抗性超网络生成恶意LoRA权重，通过双层优化训练防御模型来抵消这些对抗性权重的影响，保持安全对齐

Result: 在52种红队攻击测试中，AntiDote比现有方法鲁棒性提升27.4%，在MMLU等基准测试中性能下降小于0.5%

Conclusion: 该方法为构建安全性和实用性兼备的开源权重模型提供了实用且计算高效的方法论

Abstract: The release of open-weight large language models (LLMs) creates a tension
between advancing accessible research and preventing misuse, such as malicious
fine-tuning to elicit harmful content. Current safety measures struggle to
preserve the general capabilities of the LLM while resisting a determined
adversary with full access to the model's weights and architecture, who can use
full-parameter fine-tuning to erase existing safeguards. To address this, we
introduce AntiDote, a bi-level optimization procedure for training LLMs to be
resistant to such tampering. AntiDote involves an auxiliary adversary
hypernetwork that learns to generate malicious Low-Rank Adaptation (LoRA)
weights conditioned on the defender model's internal activations. The defender
LLM is then trained with an objective to nullify the effect of these
adversarial weight additions, forcing it to maintain its safety alignment. We
validate this approach against a diverse suite of 52 red-teaming attacks,
including jailbreak prompting, latent space manipulation, and direct
weight-space attacks. AntiDote is upto 27.4\% more robust against adversarial
attacks compared to both tamper-resistance and unlearning baselines. Crucially,
this robustness is achieved with a minimal trade-off in utility, incurring a
performance degradation of upto less than 0.5\% across capability benchmarks
including MMLU, HellaSwag, and GSM8K. Our work offers a practical and compute
efficient methodology for building open-weight models where safety is a more
integral and resilient property.

</details>


### [4] [MVPBench: A Benchmark and Fine-Tuning Framework for Aligning Large Language Models with Diverse Human Values](https://arxiv.org/abs/2509.08022)
*Yao Liang,Dongcheng Zhao,Feifei Zhao,Guobin Shen,Yuwei Wang,Dongqi Liang,Yi Zeng*

Main category: cs.CL

TL;DR: MVPBench是一个包含24,020个实例的基准测试，用于评估LLM在75个国家中的多维人类价值观对齐表现，揭示了地理和人口统计上的显著差异，并证明轻量级微调方法可以显著提升对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试往往忽视文化和人口多样性，导致对价值观对齐在全球范围内泛化能力的理解有限。

Method: 开发MVPBench基准测试，包含精细价值观标签、个性化问题和丰富人口统计元数据；使用LoRA和DPO等轻量级微调方法进行实验。

Result: 发现不同地理和人口统计群体间存在显著的对齐性能差异；轻量级微调方法在域内和域外设置中都能显著提升价值观对齐效果。

Conclusion: 需要进行群体感知的对齐评估，MVPBench为全球对齐、个性化价值观建模和公平AI发展提供了实用基础。

Abstract: The alignment of large language models (LLMs) with human values is critical
for their safe and effective deployment across diverse user populations.
However, existing benchmarks often neglect cultural and demographic diversity,
leading to limited understanding of how value alignment generalizes globally.
In this work, we introduce MVPBench, a novel benchmark that systematically
evaluates LLMs' alignment with multi-dimensional human value preferences across
75 countries. MVPBench contains 24,020 high-quality instances annotated with
fine-grained value labels, personalized questions, and rich demographic
metadata, making it the most comprehensive resource of its kind to date. Using
MVPBench, we conduct an in-depth analysis of several state-of-the-art LLMs,
revealing substantial disparities in alignment performance across geographic
and demographic lines. We further demonstrate that lightweight fine-tuning
methods, such as Low-Rank Adaptation (LoRA) and Direct Preference Optimization
(DPO), can significantly enhance value alignment in both in-domain and
out-of-domain settings. Our findings underscore the necessity for
population-aware alignment evaluation and provide actionable insights for
building culturally adaptive and value-sensitive LLMs. MVPBench serves as a
practical foundation for future research on global alignment, personalized
value modeling, and equitable AI development.

</details>


### [5] [NOWJ@COLIEE 2025: A Multi-stage Framework Integrating Embedding Models and Large Language Models for Legal Retrieval and Entailment](https://arxiv.org/abs/2509.08025)
*Hoang-Trung Nguyen,Tan-Minh Nguyen,Xuan-Bach Le,Tuan-Kiet Le,Khanh-Huyen Nguyen,Ha-Thanh Nguyen,Thi-Hai-Yen Vuong,Le-Minh Nguyen*

Main category: cs.CL

TL;DR: NOWJ团队在COLIEE 2025竞赛中采用混合方法，结合传统信息检索技术和现代生成模型，在Legal Case Entailment任务中获得第一名，并在其他法律信息处理任务中展现强劲性能。


<details>
  <summary>Details</summary>
Motivation: 探索将传统信息检索技术与先进大语言模型相结合的方法，以提升法律信息处理的准确性和效率，特别是在法律案例蕴含任务中的表现。

Method: 采用两阶段检索系统：第一阶段使用预排序模型（BM25、BERT、monoT5）和基于嵌入的语义表示（BGE-m3、LLM2Vec）进行词汇语义过滤；第二阶段使用大型语言模型（Qwen-2、QwQ-32B、DeepSeek-V3）进行摘要、相关性评分和上下文重排序。

Result: 在Legal Case Entailment任务（Task 2）中获得第一名，F1分数达到0.3195。在其他任务（法律案例检索、法规检索、法律文本蕴含和法律判决预测）中也表现出稳健的性能。

Conclusion: 混合模型整合传统IR技术和现代生成模型具有巨大潜力，为法律信息处理的未来发展提供了有价值的参考，证明了集成方法在法律AI应用中的有效性。

Abstract: This paper presents the methodologies and results of the NOWJ team's
participation across all five tasks at the COLIEE 2025 competition, emphasizing
advancements in the Legal Case Entailment task (Task 2). Our comprehensive
approach systematically integrates pre-ranking models (BM25, BERT, monoT5),
embedding-based semantic representations (BGE-m3, LLM2Vec), and advanced Large
Language Models (Qwen-2, QwQ-32B, DeepSeek-V3) for summarization, relevance
scoring, and contextual re-ranking. Specifically, in Task 2, our two-stage
retrieval system combined lexical-semantic filtering with contextualized LLM
analysis, achieving first place with an F1 score of 0.3195. Additionally, in
other tasks--including Legal Case Retrieval, Statute Law Retrieval, Legal
Textual Entailment, and Legal Judgment Prediction--we demonstrated robust
performance through carefully engineered ensembles and effective prompt-based
reasoning strategies. Our findings highlight the potential of hybrid models
integrating traditional IR techniques with contemporary generative models,
providing a valuable reference for future advancements in legal information
processing.

</details>


### [6] [SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery](https://arxiv.org/abs/2509.08032)
*Fengyu She,Nan Wang,Hongfei Wu,Ziyi Wan,Jingmian Wang,Chang Wang*

Main category: cs.CL

TL;DR: SciGPT是一个针对科学文献理解的领域自适应基础模型，通过低成本的领域蒸馏、稀疏专家混合注意力机制和知识感知适应等创新技术，在科学任务上超越了GPT-4o的表现。


<details>
  <summary>Details</summary>
Motivation: 科学文献呈指数级增长，研究人员需要高效的知识合成工具。通用大语言模型在科学领域存在技术术语理解不足、方法严谨性把握困难等问题，限制了其在跨学科研究中的实用性。

Method: 基于Qwen3架构，采用三阶段创新：1）低成本领域蒸馏的两阶段流水线；2）稀疏专家混合注意力机制，在32,000词长文档推理中减少55%内存消耗；3）知识感知适应，整合领域本体以弥合跨学科知识鸿沟。

Result: 在ScienceBench基准测试中，SciGPT在序列标注、生成和推理等核心科学任务上超越了GPT-4o，并在未见过的科学任务上表现出强大的鲁棒性。

Conclusion: SciGPT验证了其在促进AI增强科学发现方面的潜力，为科学文献理解提供了有效的领域自适应解决方案。

Abstract: Scientific literature is growing exponentially, creating a critical
bottleneck for researchers to efficiently synthesize knowledge. While
general-purpose Large Language Models (LLMs) show potential in text processing,
they often fail to capture scientific domain-specific nuances (e.g., technical
jargon, methodological rigor) and struggle with complex scientific tasks,
limiting their utility for interdisciplinary research. To address these gaps,
this paper presents SciGPT, a domain-adapted foundation model for scientific
literature understanding and ScienceBench, an open source benchmark tailored to
evaluate scientific LLMs.
  Built on the Qwen3 architecture, SciGPT incorporates three key innovations:
(1) low-cost domain distillation via a two-stage pipeline to balance
performance and efficiency; (2) a Sparse Mixture-of-Experts (SMoE) attention
mechanism that cuts memory consumption by 55\% for 32,000-token long-document
reasoning; and (3) knowledge-aware adaptation integrating domain ontologies to
bridge interdisciplinary knowledge gaps.
  Experimental results on ScienceBench show that SciGPT outperforms GPT-4o in
core scientific tasks including sequence labeling, generation, and inference.
It also exhibits strong robustness in unseen scientific tasks, validating its
potential to facilitate AI-augmented scientific discovery.

</details>


### [7] [No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal in Language Models](https://arxiv.org/abs/2509.08075)
*Flor Miriam Plaza-del-Arco,Paul Röttger,Nino Scherrer,Emanuele Borgonovo,Elmar Plischke,Dirk Hovy*

Main category: cs.CL

TL;DR: 研究发现LLM个性化中的虚假拒绝问题被高估，模型能力和任务类型对拒绝率的影响比人口统计特征更重要


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型日益个性化和融入日常生活，需要量化个性化可能带来的副作用，特别是虚假拒绝请求的问题

Method: 使用15种社会人口特征角色、16个不同模型、3种任务类型和9种提示变体，提出基于蒙特卡洛的样本高效量化方法

Result: 模型能力越强，角色对拒绝率的影响越小；某些社会人口特征角色在某些模型中会增加虚假拒绝，但模型选择和任务类型对虚假拒绝的影响更显著

Conclusion: 角色效应被高估，虚假拒绝问题更多源于模型选择和任务类型等其他因素，而非人口统计特征本身

Abstract: Large language models (LLMs) are increasingly integrated into our daily lives
and personalized. However, LLM personalization might also increase unintended
side effects. Recent work suggests that persona prompting can lead models to
falsely refuse user requests. However, no work has fully quantified the extent
of this issue. To address this gap, we measure the impact of 15
sociodemographic personas (based on gender, race, religion, and disability) on
false refusal. To control for other factors, we also test 16 different models,
3 tasks (Natural Language Inference, politeness, and offensiveness
classification), and nine prompt paraphrases. We propose a Monte Carlo-based
method to quantify this issue in a sample-efficient manner. Our results show
that as models become more capable, personas impact the refusal rate less and
less. Certain sociodemographic personas increase false refusal in some models,
which suggests underlying biases in the alignment strategies or safety
mechanisms. However, we find that the model choice and task significantly
influence false refusals, especially in sensitive content tasks. Our findings
suggest that persona effects have been overestimated, and might be due to other
factors.

</details>


### [8] [Culturally transmitted color categories in LLMs reflect a learning bias toward efficient compression](https://arxiv.org/abs/2509.08093)
*Nathaniel Imel,Noga Zaslavsky*

Main category: cs.CL

TL;DR: 研究发现大型语言模型能够通过信息瓶颈原则演化出类似人类的高效语义系统，特别是在颜色命名领域，表现出与人类相似的归纳偏好的文化演化模式。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型是否能够演化出类似人类的高效语义分类系统，特别是验证它们是否遵循信息瓶颈的复杂度-准确性优化原则，而非仅仅模仿训练数据中的模式。

Method: 使用Gemini 2.0-flash和Llama 3.3-70B-Instruct模型，复现两个重要的人类行为研究：英语颜色命名研究和通过上下文语言学习模拟文化演化过程。

Result: Gemini与英语母语者的命名模式高度一致且达到高IB效率分数，Llama表现出高效但复杂度较低的系统。通过迭代学习，两种模型都能将初始随机系统重构为更高IB效率的系统，并与世界语言模式更加一致。

Conclusion: 大型语言模型能够演化出感知基础的人类化语义系统，其驱动机制与支配人类语言语义效率的基本原理相同，表明它们具备类似人类的归纳偏好。

Abstract: Converging evidence suggests that systems of semantic categories across human
languages achieve near-optimal compression via the Information Bottleneck (IB)
complexity-accuracy principle. Large language models (LLMs) are not trained for
this objective, which raises the question: are LLMs capable of evolving
efficient human-like semantic systems? To address this question, we focus on
the domain of color as a key testbed of cognitive theories of categorization
and replicate with LLMs (Gemini 2.0-flash and Llama 3.3-70B-Instruct) two
influential human behavioral studies. First, we conduct an English color-naming
study, showing that Gemini aligns well with the naming patterns of native
English speakers and achieves a significantly high IB-efficiency score, while
Llama exhibits an efficient but lower complexity system compared to English.
Second, to test whether LLMs simply mimic patterns in their training data or
actually exhibit a human-like inductive bias toward IB-efficiency, we simulate
cultural evolution of pseudo color-naming systems in LLMs via iterated
in-context language learning. We find that akin to humans, LLMs iteratively
restructure initially random systems towards greater IB-efficiency and
increased alignment with patterns observed across the world's languages. These
findings demonstrate that LLMs are capable of evolving perceptually grounded,
human-like semantic systems, driven by the same fundamental principle that
governs semantic efficiency across human languages.

</details>


### [9] [MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder and LLM Fusion](https://arxiv.org/abs/2509.08105)
*Kosei Uemura,David Guzmán,Quang Phuoc Nguyen,Jesujoba Oluwadara Alabi,En-shiun Annie Lee,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: MERLIN是一个两阶段模型堆叠框架，通过课程学习策略（从通用双语文本到任务特定数据）并仅适配少量DoRA权重，显著提升低资源语言的复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在英语表现优异，但在低资源语言的复杂推理方面仍有困难。现有的编码器-解码器方法对中高资源语言有效，但对低资源语言效果有限。

Method: 提出两阶段模型堆叠框架，采用课程学习策略：从通用双语文本到任务特定数据，仅适配少量DoRA权重。

Result: 在AfriMGSM基准上比MindMerger准确率提升12.9个百分点，优于GPT-4o-mini；在MGSM和MSVAMP上也分别获得0.9和2.8个百分点的提升。

Conclusion: MERLIN框架在低资源和高资源环境下都表现出有效性，显著改善了低资源语言的复杂推理能力。

Abstract: Large language models excel in English but still struggle with complex
reasoning in many low-resource languages (LRLs). Existing encoder-plus-decoder
methods such as LangBridge and MindMerger raise accuracy on mid and
high-resource languages, yet they leave a large gap on LRLs. We present MERLIN,
a two-stage model-stacking framework that applies a curriculum learning
strategy -- from general bilingual bitext to task-specific data -- and adapts
only a small set of DoRA weights. On the AfriMGSM benchmark MERLIN improves
exact-match accuracy by +12.9 pp over MindMerger and outperforms GPT-4o-mini.
It also yields consistent gains on MGSM and MSVAMP (+0.9 and +2.8 pp),
demonstrating effectiveness across both low and high-resource settings.

</details>


### [10] [Bias after Prompting: Persistent Discrimination in Large Language Models](https://arxiv.org/abs/2509.08146)
*Nivedha Sivakumar,Natalie Mackraz,Samira Khorshidi,Krishna Patel,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Main category: cs.CL

TL;DR: 研究发现偏见可以通过提示从预训练大语言模型转移到适应模型，现有提示去偏方法无法一致阻止偏见转移，需要在内在模型中纠正偏见以防止向下游任务传播


<details>
  <summary>Details</summary>
Motivation: 先前关于偏见转移假设的研究可能错误地假设偏见不会从预训练LLM转移到适应模型，需要验证提示适应策略下的偏见转移情况

Method: 研究因果模型在提示适应下的偏见转移，分析不同少样本组合参数（样本大小、刻板内容、职业分布等）的影响，评估多种提示去偏策略

Result: 偏见通过提示转移的相关性很强（性别rho≥0.94，年龄rho≥0.98，宗教rho≥0.69），不同去偏方法各有优势但无一能一致减少偏见转移

Conclusion: 需要在内在模型中纠正偏见和改进推理能力，以防止偏见向下游任务传播

Abstract: A dangerous assumption that can be made from prior work on the bias transfer
hypothesis (BTH) is that biases do not transfer from pre-trained large language
models (LLMs) to adapted models. We invalidate this assumption by studying the
BTH in causal models under prompt adaptations, as prompting is an extremely
popular and accessible adaptation strategy used in real-world applications. In
contrast to prior work, we find that biases can transfer through prompting and
that popular prompt-based mitigation methods do not consistently prevent biases
from transferring. Specifically, the correlation between intrinsic biases and
those after prompt adaptation remain moderate to strong across demographics and
tasks -- for example, gender (rho >= 0.94) in co-reference resolution, and age
(rho >= 0.98) and religion (rho >= 0.69) in question answering. Further, we
find that biases remain strongly correlated when varying few-shot composition
parameters, such as sample size, stereotypical content, occupational
distribution and representational balance (rho >= 0.90). We evaluate several
prompt-based debiasing strategies and find that different approaches have
distinct strengths, but none consistently reduce bias transfer across models,
tasks or demographics. These results demonstrate that correcting bias, and
potentially improving reasoning ability, in intrinsic models may prevent
propagation of biases to downstream tasks.

</details>


### [11] [Verbalized Algorithms](https://arxiv.org/abs/2509.08150)
*Supriya Lall,Christian Farrell,Hari Pathanjaly,Marko Pavic,Sarvesh Chezhian,Masataro Asai*

Main category: cs.CL

TL;DR: 提出verbalized algorithms（VAs）范式，将复杂任务分解为LLM可可靠执行的简单自然语言操作，使用经典算法框架来保证理论可靠性


<details>
  <summary>Details</summary>
Motivation: 传统一次性查询LLM的方式不可靠，希望通过结合经典算法的理论保证和LLM的自然语言处理能力，提高推理任务的可靠性

Method: 使用verbalized sorting方法，将排序任务分解为二元比较操作，让LLM作为比较oracle，在已知的排序算法框架（如双调排序网络）中执行

Result: 在排序和聚类任务上证明了该方法的有效性

Conclusion: VAs范式通过限制LLM只执行简单可靠的操作，结合经典算法的理论保证，能够提高复杂推理任务的可靠性

Abstract: Instead of querying LLMs in a one-shot manner and hoping to get the right
answer for a reasoning task, we propose a paradigm we call \emph{verbalized
algorithms} (VAs), which leverage classical algorithms with established
theoretical understanding. VAs decompose a task into simple elementary
operations on natural language strings that they should be able to answer
reliably, and limit the scope of LLMs to only those simple tasks. For example,
for sorting a series of natural language strings, \emph{verbalized sorting}
uses an LLM as a binary comparison oracle in a known and well-analyzed sorting
algorithm (e.g., bitonic sorting network). We demonstrate the effectiveness of
this approach on sorting and clustering tasks.

</details>


### [12] [Balancing Quality and Variation: Spam Filtering Distorts Data Label Distributions](https://arxiv.org/abs/2509.08217)
*Eve Fleisig,Matthias Orlikowski,Philipp Cimiano,Dan Klein*

Main category: cs.CL

TL;DR: 现有标注者过滤方法在主观任务中会错误移除意见不同的标注者而非垃圾标注者，导致准确性和标签多样性之间的次优权衡。研究发现保守的标注者移除比例（<5%）最佳，且垃圾标注者通常比非垃圾标注者更不随机。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器学习数据集中如何在过滤垃圾或低质量响应的同时保持标签多样性，平衡标注者可靠性和代表性的问题。

Method: 通过实证评估一系列标注者过滤启发式方法对主观任务中变异保留的影响，并分析合成垃圾数据上的性能表现。

Result: 现有方法经常移除意见不同的标注者而非垃圾标注者，保守设置（<5%移除率）效果最好。垃圾标注者通常给出固定答案而非随机答案，与现有方法的假设相反。

Conclusion: 需要开发能够考虑标签多样性的垃圾过滤方法，因为现有方法基于变异即垃圾的直觉在需要保持变异性的任务中效果不佳。

Abstract: For machine learning datasets to accurately represent diverse opinions in a
population, they must preserve variation in data labels while filtering out
spam or low-quality responses. How can we balance annotator reliability and
representation? We empirically evaluate how a range of heuristics for annotator
filtering affect the preservation of variation on subjective tasks. We find
that these methods, designed for contexts in which variation from a single
ground-truth label is considered noise, often remove annotators who disagree
instead of spam annotators, introducing suboptimal tradeoffs between accuracy
and label diversity. We find that conservative settings for annotator removal
(<5%) are best, after which all tested methods increase the mean absolute error
from the true average label. We analyze performance on synthetic spam to
observe that these methods often assume spam annotators are less random than
real spammers tend to be: most spammers are distributionally indistinguishable
from real annotators, and the minority that are distinguishable tend to give
fixed answers, not random ones. Thus, tasks requiring the preservation of
variation reverse the intuition of existing spam filtering methods: spammers
tend to be less random than non-spammers, so metrics that assume variation is
spam fare worse. These results highlight the need for spam removal methods that
account for label diversity.

</details>


### [13] [Towards Knowledge-Aware Document Systems: Modeling Semantic Coverage Relations via Answerability Detection](https://arxiv.org/abs/2509.08304)
*Yehudit Aperstein,Alon Gottlib,Gal Benita,Alexander Apartsin*

Main category: cs.CL

TL;DR: 该论文提出了一个基于问答的语义覆盖关系(SCR)分类框架，通过分析文档间共享问题的可回答性来判断信息对齐关系，包括等价、包含和语义重叠三种类型。


<details>
  <summary>Details</summary>
Motivation: 理解不同格式文档间的信息共享对于信息检索、摘要和内容对齐等任务至关重要，需要能够识别文档间语义内容对齐关系的方法。

Method: 采用基于问答的方法，使用SQuAD语料库构建合成数据集，通过改写源段落和选择性省略信息来控制内容重叠，使用生成式语言模型和基于transformer的分类器进行SCR预测。

Result: 判别式模型显著优于生成式方法，RoBERTa-base模型达到61.4%的最高准确率，基于随机森林的模型在macro-F1分数上表现最佳(52.9%)。

Conclusion: 问答方法为评估风格多样化文本间的语义关系提供了有效视角，揭示了当前模型在超越表面相似性进行信息推理方面的能力。

Abstract: Understanding how information is shared across documents, regardless of the
format in which it is expressed, is critical for tasks such as information
retrieval, summarization, and content alignment. In this work, we introduce a
novel framework for modelling Semantic Coverage Relations (SCR), which
classifies document pairs based on how their informational content aligns. We
define three core relation types: equivalence, where both texts convey the same
information using different textual forms or styles; inclusion, where one
document fully contains the information of another and adds more; and semantic
overlap, where each document presents partially overlapping content. To capture
these relations, we adopt a question answering (QA)-based approach, using the
answerability of shared questions across documents as an indicator of semantic
coverage. We construct a synthetic dataset derived from the SQuAD corpus by
paraphrasing source passages and selectively omitting information, enabling
precise control over content overlap. This dataset allows us to benchmark
generative language models and train transformer-based classifiers for SCR
prediction. Our findings demonstrate that discriminative models significantly
outperform generative approaches, with the RoBERTa-base model achieving the
highest accuracy of 61.4% and the Random Forest-based model showing the best
balance with a macro-F1 score of 52.9%. The results show that QA provides an
effective lens for assessing semantic relations across stylistically diverse
texts, offering insights into the capacity of current models to reason about
information beyond surface similarity. The dataset and code developed in this
study are publicly available to support reproducibility.

</details>


### [14] [Toward Subtrait-Level Model Explainability in Automated Writing Evaluation](https://arxiv.org/abs/2509.08345)
*Alejandro Andrade-Lotero,Lee Becker,Joshua Southerland,Scott Hellman*

Main category: cs.CL

TL;DR: 使用生成式语言模型进行写作评分中的子特质评估，提供更透明的自动化评分解释


<details>
  <summary>Details</summary>
Motivation: 提高自动化写作评分的透明度，为教育者和学生提供更详细的评分解释

Method: 使用生成式语言模型进行子特质评分原型开发，分析人类子特质评分与总体特质评分的相关性

Result: 人类子特质评分与总体特质评分之间存在适度相关性，自动化子特质评分与人类评分也呈现适度相关

Conclusion: 该方法为教育者和学生提供了详细的评分解释，有助于消除自动化评分的黑箱问题

Abstract: Subtrait (latent-trait components) assessment presents a promising path
toward enhancing transparency of automated writing scores. We prototype
explainability and subtrait scoring with generative language models and show
modest correlation between human subtrait and trait scores, and between
automated and human subtrait scores. Our approach provides details to demystify
scores for educators and students.

</details>


### [15] [Automatic Detection of Inauthentic Templated Responses in English Language Assessments](https://arxiv.org/abs/2509.08355)
*Yashad Samant,Lee Becker,Scott Hellman,Bradley Behan,Sarah Hughes,Joshua Southerland*

Main category: cs.CL

TL;DR: 论文提出了自动化检测模板化应试作文的任务(AuDITR)，开发了机器学习方法来识别低水平考生使用的记忆模板，并强调了在生产环境中定期更新模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 在英语语言评估中，低水平考生可能使用记忆的模板材料来欺骗自动化评分系统，需要开发检测方法来确保评估的公正性和有效性。

Method: 采用机器学习方法来自动检测不真实的模板化应试作文响应。

Result: 成功开发了自动化检测模板化响应的方法，并证明了该方法在识别作弊行为方面的有效性。

Conclusion: 定期更新检测模型对于维持自动化评分系统的可靠性和防止考生通过模板作弊至关重要。

Abstract: In high-stakes English Language Assessments, low-skill test takers may employ
memorized materials called ``templates'' on essay questions to ``game'' or fool
the automated scoring system. In this study, we introduce the automated
detection of inauthentic, templated responses (AuDITR) task, describe a machine
learning-based approach to this task and illustrate the importance of regularly
updating these models in production.

</details>


### [16] [<think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs](https://arxiv.org/abs/2509.08358)
*Sergey Pletenev,Daniil Moskovskiy,Alexander Panchenko*

Main category: cs.CL

TL;DR: 使用LLM生成的合成毒性数据训练去毒模型效果较差，性能下降达30%，主要原因是词汇多样性不足


<details>
  <summary>Details</summary>
Motivation: 探索LLM生成的合成毒性数据是否可以代替人工标注数据用于文本去毒模型训练

Method: 使用Llama 3和Qwen激活补丁模型为ParaDetox和SST-2数据集中的中性文本生成合成毒性对应文本，然后训练去毒模型

Result: 基于合成数据训练的模型表现一致比基于人工数据的模型更差，聚合指标下降达30%

Conclusion: 当前LLM在毒性内容生成方面存在限制，词汇多样性不足，强调了人工标注数据对建立健壮去毒系统的重要性

Abstract: Modern Large Language Models (LLMs) are excellent at generating synthetic
data. However, their performance in sensitive domains such as text
detoxification has not received proper attention from the scientific community.
This paper explores the possibility of using LLM-generated synthetic toxic data
as an alternative to human-generated data for training models for
detoxification. Using Llama 3 and Qwen activation-patched models, we generated
synthetic toxic counterparts for neutral texts from ParaDetox and SST-2
datasets. Our experiments show that models fine-tuned on synthetic data
consistently perform worse than those trained on human data, with a drop in
performance of up to 30% in joint metrics. The root cause is identified as a
critical lexical diversity gap: LLMs generate toxic content using a small,
repetitive vocabulary of insults that fails to capture the nuances and variety
of human toxicity. These findings highlight the limitations of current LLMs in
this domain and emphasize the continued importance of diverse, human-annotated
data for building robust detoxification systems.

</details>


### [17] [Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model](https://arxiv.org/abs/2509.08381)
*Yu Cheng Chih,Yong Hao Hou*

Main category: cs.CL

TL;DR: ETLCH是一个基于LLaMA的10亿参数小模型，通过少量样本微调在结构化数据提取任务中表现优异，成本效益高


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在结构化数据提取领域部署成本高且需要大量高质量数据，小团队难以承受。目前缺乏小模型在低资源多任务条件下可靠性的研究

Method: 使用低秩适应(LoRA)技术对LLaMA基础模型进行微调，每个任务仅使用几百到一千个样本，用于JSON提取、知识图谱提取和命名实体识别

Result: ETLCH在大多数评估指标上超越强基线模型，即使在最低数据规模下也观察到显著增益

Conclusion: 精心调优的小模型能够以极低的计算成本提供稳定准确的结构化输出，使资源受限环境中的信息提取管道既经济又可靠

Abstract: Deploying large language models (LLMs) for structured data extraction in
domains such as financial compliance reporting, legal document analytics, and
multilingual knowledge base construction is often impractical for smaller teams
due to the high cost of running large architectures and the difficulty of
preparing large, high-quality datasets. Most recent instruction-tuning studies
focus on seven-billion-parameter or larger models, leaving limited evidence on
whether much smaller models can work reliably under low-resource, multi-task
conditions. This work presents ETLCH, a billion-parameter LLaMA-based model
fine-tuned with low-rank adaptation on only a few hundred to one thousand
samples per task for JSON extraction, knowledge graph extraction, and named
entity recognition. Despite its small scale, ETLCH outperforms strong baselines
across most evaluation metrics, with substantial gains observed even at the
lowest data scale. These findings demonstrate that well-tuned small models can
deliver stable and accurate structured outputs at a fraction of the
computational cost, enabling cost-effective and reliable information extraction
pipelines in resource-constrained environments.

</details>


### [18] [Adversarial Attacks Against Automated Fact-Checking: A Survey](https://arxiv.org/abs/2509.08463)
*Fanzhen Liu,Alsharif Abuadbba,Kristen Moore,Surya Nepal,Cecile Paris,Jia Wu,Jian Yang,Quan Z. Sheng*

Main category: cs.CL

TL;DR: 对抗性攻击对自动事实核查系统构成严重威胁，本调查首次全面综述了针对事实核查的攻击方法、防御策略和关键挑战


<details>
  <summary>Details</summary>
Motivation: 在错误信息泛滥的时代，现有自动事实核查系统容易受到对抗性攻击，这些攻击会操纵或生成虚假声明、证据或声明-证据对，从而破坏事实核查模型的可靠性

Method: 对对抗性攻击进行深入综述，分类现有攻击方法学，评估其对自动事实核查系统的影响，并检查最新的对抗感知防御进展

Result: 研究发现当前事实核查系统在面对对抗性攻击时存在脆弱性，需要更强大的防御机制

Conclusion: 迫切需要开发能够抵御对抗性操纵的弹性事实核查框架，以保持高验证准确性

Abstract: In an era where misinformation spreads freely, fact-checking (FC) plays a
crucial role in verifying claims and promoting reliable information. While
automated fact-checking (AFC) has advanced significantly, existing systems
remain vulnerable to adversarial attacks that manipulate or generate claims,
evidence, or claim-evidence pairs. These attacks can distort the truth, mislead
decision-makers, and ultimately undermine the reliability of FC models. Despite
growing research interest in adversarial attacks against AFC systems, a
comprehensive, holistic overview of key challenges remains lacking. These
challenges include understanding attack strategies, assessing the resilience of
current models, and identifying ways to enhance robustness. This survey
provides the first in-depth review of adversarial attacks targeting FC,
categorizing existing attack methodologies and evaluating their impact on AFC
systems. Additionally, we examine recent advancements in adversary-aware
defenses and highlight open research questions that require further
exploration. Our findings underscore the urgent need for resilient FC
frameworks capable of withstanding adversarial manipulations in pursuit of
preserving high verification accuracy.

</details>


### [19] [Acquiescence Bias in Large Language Models](https://arxiv.org/abs/2509.08480)
*Daniel Braun*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Acquiescence bias, i.e. the tendency of humans to agree with statements in
surveys, independent of their actual beliefs, is well researched and
documented. Since Large Language Models (LLMs) have been shown to be very
influenceable by relatively small changes in input and are trained on
human-generated data, it is reasonable to assume that they could show a similar
tendency. We present a study investigating the presence of acquiescence bias in
LLMs across different models, tasks, and languages (English, German, and
Polish). Our results indicate that, contrary to humans, LLMs display a bias
towards answering no, regardless of whether it indicates agreement or
disagreement.

</details>


### [20] [Simulating Identity, Propagating Bias: Abstraction and Stereotypes in LLM-Generated Text](https://arxiv.org/abs/2509.08484)
*Pia Sommerauer,Giulia Rambelli,Tommaso Caselli*

Main category: cs.CL

TL;DR: 研究发现人物角色提示并不能有效调节语言抽象程度，反而可能传播刻板印象，即使是在模拟边缘群体声音时也存在风险


<details>
  <summary>Details</summary>
Motivation: 探索人物角色提示方法对LLMs在表示社会群体时语言抽象程度（刻板印象标记）的影响，该方法在个性化输出中的应用日益增长但其社会影响尚未充分研究

Method: 基于语言期望偏差框架，分析6个开源LLM在三种提示条件下的输出，比较11种人物角色驱动响应与通用AI助手的响应，使用具体性、特异性和否定性三个指标测量抽象程度，并引入Self-Stereo新数据集

Result: 人物角色提示在调节语言抽象程度方面存在局限性，证实了人物角色作为社会人口群体代表的有效性批评，并引发了对即使看似唤起边缘群体声音时也可能传播刻板印象的担忧

Conclusion: 人物角色提示方法在调节语言抽象和避免刻板印象方面效果有限，需要更谨慎地使用这种方法以避免强化社会偏见

Abstract: Persona-prompting is a growing strategy to steer LLMs toward simulating
particular perspectives or linguistic styles through the lens of a specified
identity. While this method is often used to personalize outputs, its impact on
how LLMs represent social groups remains underexplored. In this paper, we
investigate whether persona-prompting leads to different levels of linguistic
abstraction - an established marker of stereotyping - when generating short
texts linking socio-demographic categories with stereotypical or
non-stereotypical attributes. Drawing on the Linguistic Expectancy Bias
framework, we analyze outputs from six open-weight LLMs under three prompting
conditions, comparing 11 persona-driven responses to those of a generic AI
assistant. To support this analysis, we introduce Self-Stereo, a new dataset of
self-reported stereotypes from Reddit. We measure abstraction through three
metrics: concreteness, specificity, and negation. Our results highlight the
limits of persona-prompting in modulating abstraction in language, confirming
criticisms about the ecology of personas as representative of socio-demographic
groups and raising concerns about the risk of propagating stereotypes even when
seemingly evoking the voice of a marginalized group.

</details>


### [21] [Too Helpful, Too Harmless, Too Honest or Just Right?](https://arxiv.org/abs/2509.08486)
*Gautam Siddharth Kashyap,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: TrinityX是一个模块化对齐框架，通过校准专家混合(MoCaE)机制在Transformer架构中分别处理Helpfulness、Harmlessness、Honesty三个对齐维度，实现了更好的性能表现和效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常单独优化各个对齐维度，导致权衡和不一致行为。虽然MoE架构提供模块化，但路由校准不佳限制了在对齐任务中的有效性。

Method: 提出TrinityX框架，包含Mixture of Calibrated Experts (MoCaE)，为每个HHH维度分别训练专家，通过校准的任务自适应路由机制整合专家输出。

Result: 在三个标准对齐基准测试中，TrinityX相比基线方法在胜率上提升32.5%，安全分数提升33.9%，真实性提升28.4%，同时内存使用和推理延迟降低40%以上。

Conclusion: TrinityX通过校准路由机制有效解决了LLM对齐问题，在多个维度上实现显著性能提升，同时提高效率，具有良好的泛化能力。

Abstract: Large Language Models (LLMs) exhibit strong performance across a wide range
of NLP tasks, yet aligning their outputs with the principles of Helpfulness,
Harmlessness, and Honesty (HHH) remains a persistent challenge. Existing
methods often optimize for individual alignment dimensions in isolation,
leading to trade-offs and inconsistent behavior. While Mixture-of-Experts (MoE)
architectures offer modularity, they suffer from poorly calibrated routing,
limiting their effectiveness in alignment tasks. We propose TrinityX, a modular
alignment framework that incorporates a Mixture of Calibrated Experts (MoCaE)
within the Transformer architecture. TrinityX leverages separately trained
experts for each HHH dimension, integrating their outputs through a calibrated,
task-adaptive routing mechanism that combines expert signals into a unified,
alignment-aware representation. Extensive experiments on three standard
alignment benchmarks-Alpaca (Helpfulness), BeaverTails (Harmlessness), and
TruthfulQA (Honesty)-demonstrate that TrinityX outperforms strong baselines,
achieving relative improvements of 32.5% in win rate, 33.9% in safety score,
and 28.4% in truthfulness. In addition, TrinityX reduces memory usage and
inference latency by over 40% compared to prior MoE-based approaches. Ablation
studies highlight the importance of calibrated routing, and cross-model
evaluations confirm TrinityX's generalization across diverse LLM backbones.

</details>


### [22] [CM-Align: Consistency-based Multilingual Alignment for Large Language Models](https://arxiv.org/abs/2509.08541)
*Xue Zhang,Yunlong Liang,Fandong Meng,Songming Zhang,Yufeng Chen,Jinan Xu,Jie Zhou*

Main category: cs.CL

TL;DR: 提出CM-Align方法，通过一致性引导的数据选择构建高质量多语言偏好数据，解决现有方法中英语参考质量不高和偏好对构建有偏差的问题，显著提升多语言对齐性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在英语和其他语言之间的对齐性能存在显著差距。现有方法使用英语回答作为参考来构建多语言偏好数据，但存在两个问题：1）并非所有英语回答都是高质量的，低质量回答会误导其他语言的对齐；2）当前方法通常使用有偏见或启发式的方法构建多语言偏好对。

Method: 设计了基于一致性的数据选择方法（CM-Align），包括两个部分：一致性引导的英语参考选择和跨语言一致性为基础的多语言偏好数据构建。通过一致性标准来筛选高质量的英语回答作为参考，并基于跨语言一致性构建更可靠的多语言偏好对。

Result: 在三个大语言模型和三个常见任务上的实验结果表明，该方法具有有效性和优越性，显著提升了多语言对齐性能。

Conclusion: 该方法证明了构建高质量偏好数据的必要性，为多语言对齐提供了更有效的解决方案，通过一致性标准解决了现有方法中的噪声和数据质量问题。

Abstract: Current large language models (LLMs) generally show a significant performance
gap in alignment between English and other languages. To bridge this gap,
existing research typically leverages the model's responses in English as a
reference to select the best/worst responses in other languages, which are then
used for Direct Preference Optimization (DPO) training. However, we argue that
there are two limitations in the current methods that result in noisy
multilingual preference data and further limited alignment performance: 1) Not
all English responses are of high quality, and using a response with low
quality may mislead the alignment for other languages. 2) Current methods
usually use biased or heuristic approaches to construct multilingual preference
pairs. To address these limitations, we design a consistency-based data
selection method to construct high-quality multilingual preference data for
improving multilingual alignment (CM-Align). Specifically, our method includes
two parts: consistency-guided English reference selection and cross-lingual
consistency-based multilingual preference data construction. Experimental
results on three LLMs and three common tasks demonstrate the effectiveness and
superiority of our method, which further indicates the necessity of
constructing high-quality preference data.

</details>


### [23] [LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question Answering for BioASQ Challenge](https://arxiv.org/abs/2509.08596)
*Dima Galat,Diego Molla-Aliod*

Main category: cs.CL

TL;DR: 本文提出使用大型语言模型集成和检索增强生成(RAG)的零样本方法，在生物医学问答任务中达到最先进性能，无需微调或标注数据。


<details>
  <summary>Details</summary>
Motivation: 生物医学问答面临专业知识复杂、语料庞大且快速演变的挑战，需要探索LLM如何用于信息检索，以及集成方法能否在领域特定任务中超越传统微调系统。

Method: 采用多个LLM变体(包括Anthropic和Google的模型)的输出集成，结合检索增强生成(RAG)管道，通过零样本方式合成更准确和鲁棒的答案。

Result: 集成方法在BioASQ挑战任务中表现优于单个LLM，在某些情况下可媲美或超越领域调优系统，同时保持通用性。研究发现上下文长度与性能存在关联关系。

Conclusion: 基于集成的零样本方法结合有效的RAG管道，为生物医学问答提供了实用且可扩展的替代方案，精确检索对于确保LLM在相关信息边界内生成答案至关重要。

Abstract: Biomedical question answering (QA) poses significant challenges due to the
need for precise interpretation of specialized knowledge drawn from a vast,
complex, and rapidly evolving corpus. In this work, we explore how large
language models (LLMs) can be used for information retrieval (IR), and an
ensemble of zero-shot models can accomplish state-of-the-art performance on a
domain-specific Yes/No QA task. Evaluating our approach on the BioASQ challenge
tasks, we show that ensembles can outperform individual LLMs and in some cases
rival or surpass domain-tuned systems - all while preserving generalizability
and avoiding the need for costly fine-tuning or labeled data. Our method
aggregates outputs from multiple LLM variants, including models from Anthropic
and Google, to synthesize more accurate and robust answers. Moreover, our
investigation highlights a relationship between context length and performance:
while expanded contexts are meant to provide valuable evidence, they
simultaneously risk information dilution and model disorientation. These
findings emphasize IR as a critical foundation in Retrieval-Augmented
Generation (RAG) approaches for biomedical QA systems. Precise, focused
retrieval remains essential for ensuring LLMs operate within relevant
information boundaries when generating answers from retrieved documents. Our
results establish that ensemble-based zero-shot approaches, when paired with
effective RAG pipelines, constitute a practical and scalable alternative to
domain-tuned systems for biomedical question answering.

</details>


### [24] [Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications](https://arxiv.org/abs/2509.08604)
*Anran Li,Lingfei Qian,Mengmeng Du,Yu Yin,Yan Hu,Zihao Sun,Yihang Fu,Erica Stutz,Xuguang Ai,Qianqian Xie,Rui Zhu,Jimin Huang,Yifan Yang,Siru Liu,Yih-Chung Tham,Lucila Ohno-Machado,Hyunghoon Cho,Zhiyong Lu,Hua Xu,Qingyu Chen*

Main category: cs.CL

TL;DR: 大语言模型在医学领域存在显著的记忆问题，记忆率高于普通领域，包含有益、无意义和有害三类记忆，需要针对性策略来优化医疗LLM应用。


<details>
  <summary>Details</summary>
Motivation: 识别大语言模型在医学领域记忆训练数据的程度和特征，以估计其对医疗应用的潜在影响。

Method: 系统分析三种适配场景：医学语料续预训练、标准医学指标微调、真实临床数据微调（包含超13,000份病人记录）。评估记忆的普遍性、特征、量级和下游影响。

Result: 记忆现象在所有适配场景中都很普遍，记忆率显著高于普通领域。记忆可分为三类：有益记忆（临床指南、生物医学参考）、无意义记忆（模板化语言）、有害记忆（敏感临床信息泄漏）。

Conclusion: 记忆问题影响医疗LLM的发展和采用，需要通过实践建议来促进有益记忆、减少无意义记忆、减轻有害记忆，以保护病人隐私和提升模型性能。

Abstract: Large Language Models (LLMs) have demonstrated significant potential in
medicine. To date, LLMs have been widely applied to tasks such as diagnostic
assistance, medical question answering, and clinical information synthesis.
However, a key open question remains: to what extent do LLMs memorize medical
training data. In this study, we present the first comprehensive evaluation of
memorization of LLMs in medicine, assessing its prevalence (how frequently it
occurs), characteristics (what is memorized), volume (how much content is
memorized), and potential downstream impacts (how memorization may affect
medical applications). We systematically analyze common adaptation scenarios:
(1) continued pretraining on medical corpora, (2) fine-tuning on standard
medical benchmarks, and (3) fine-tuning on real-world clinical data, including
over 13,000 unique inpatient records from Yale New Haven Health System. The
results demonstrate that memorization is prevalent across all adaptation
scenarios and significantly higher than reported in the general domain.
Memorization affects both the development and adoption of LLMs in medicine and
can be categorized into three types: beneficial (e.g., accurate recall of
clinical guidelines and biomedical references), uninformative (e.g., repeated
disclaimers or templated medical document language), and harmful (e.g.,
regeneration of dataset-specific or sensitive clinical content). Based on these
findings, we offer practical recommendations to facilitate beneficial
memorization that enhances domain-specific reasoning and factual accuracy,
minimize uninformative memorization to promote deeper learning beyond
surface-level patterns, and mitigate harmful memorization to prevent the
leakage of sensitive or identifiable patient information.

</details>


### [25] [OTESGN:Optimal Transport Enhanced Syntactic-Semantic Graph Networks for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2509.08612)
*Xinfeng Liao,Xuanqi Chen,Lianxi Wang,Jiahuan Yang,Zhuowei Chen,Ziying Rong*

Main category: cs.CL

TL;DR: 提出OTESGN模型，通过最优传输增强的语法-语义图网络，结合语法图感知注意力和语义最优传输注意力，在ABSA任务中实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有基于语法树和方面感知注意力的方法难以建模复杂语义关系，线性点积特征无法捕获非线性关联，导致无关词汇的噪声相似性掩盖关键观点词

Method: OTESGN模型包含语法-语义协作注意力机制：语法图感知注意力挖掘潜在语法依赖和全局语法拓扑；语义最优传输注意力在文本噪声中发现细粒度语义对齐；自适应注意力融合模块整合异构特征；对比正则化提升鲁棒性

Result: 在Twitter数据集上F1提升1.01%，在Laptop14数据集上F1提升1.30%，达到最先进水平。消融研究和可视化分析证实其在精确定位观点词和抗噪声方面的有效性

Conclusion: OTESGN通过最优传输技术有效解决了ABSA任务中语义噪声问题，显著提升了情感分析的准确性和鲁棒性

Abstract: Aspect-based sentiment analysis (ABSA) aims to identify aspect terms and
determine their sentiment polarity. While dependency trees combined with
contextual semantics effectively identify aspect sentiment, existing methods
relying on syntax trees and aspect-aware attention struggle to model complex
semantic relationships. Their dependence on linear dot-product features fails
to capture nonlinear associations, allowing noisy similarity from irrelevant
words to obscure key opinion terms. Motivated by Differentiable Optimal
Matching, we propose the Optimal Transport Enhanced Syntactic-Semantic Graph
Network (OTESGN), which introduces a Syntactic-Semantic Collaborative
Attention. It comprises a Syntactic Graph-Aware Attention for mining latent
syntactic dependencies and modeling global syntactic topology, as well as a
Semantic Optimal Transport Attention designed to uncover fine-grained semantic
alignments amidst textual noise, thereby accurately capturing sentiment signals
obscured by irrelevant tokens. A Adaptive Attention Fusion module integrates
these heterogeneous features, and contrastive regularization further improves
robustness. Experiments demonstrate that OTESGN achieves state-of-the-art
results, outperforming previous best models by +1.01% F1 on Twitter and +1.30%
F1 on Laptop14 benchmarks. Ablative studies and visual analyses corroborate its
efficacy in precise localization of opinion words and noise resistance.

</details>


### [26] [X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates](https://arxiv.org/abs/2509.08729)
*Hyunjun Kim,Junwoo Ha,Sangyoon Yu,Haon Park*

Main category: cs.CL

TL;DR: X-Teaming Evolutionary M2S是一个自动化框架，通过语言模型引导的进化来发现和优化M2S模板，将多轮红队测试压缩为单轮结构化提示


<details>
  <summary>Details</summary>
Motivation: 现有的M2S方法依赖少量手动编写的模板，需要自动化方法来发现和优化更有效的模板结构

Method: 使用语言模型引导的进化算法，结合12个来源的智能采样和基于StrongREJECT的LLM-as-judge评估，设置成功阈值θ=0.70进行选择

Result: 获得了5个进化世代、2个新模板家族，在GPT-4.1上达到44.8%的总体成功率（103/230）。跨模型测试显示结构增益可迁移但随目标模型变化

Conclusion: 结构级搜索是可复现的强化单轮探测的有效途径，强调了阈值校准和跨模型评估的重要性

Abstract: Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one
structured prompt, but prior work relied on a handful of manually written
templates. We present X-Teaming Evolutionary M2S, an automated framework that
discovers and optimizes M2S templates through language-model-guided evolution.
The system pairs smart sampling from 12 sources with an LLM-as-judge inspired
by StrongREJECT and records fully auditable logs.
  Maintaining selection pressure by setting the success threshold to $\theta =
0.70$, we obtain five evolutionary generations, two new template families, and
44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of
2,500 trials (judge fixed) shows that structural gains transfer but vary by
target; two models score zero at the same threshold. We also find a positive
coupling between prompt length and score, motivating length-aware judging.
  Our results demonstrate that structure-level search is a reproducible route
to stronger single-turn probes and underscore the importance of threshold
calibration and cross-model evaluation. Code, configurations, and artifacts are
available at https://github.com/hyunjun1121/M2S-x-teaming.

</details>


### [27] [Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling](https://arxiv.org/abs/2509.08753)
*Neil Zeghidour,Eugene Kharitonov,Manu Orsini,Václav Volhejn,Gabriel de Marmiesse,Edouard Grave,Patrick Pérez,Laurent Mazaré,Alexandre Défossez*

Main category: cs.CL

TL;DR: DSM是一种流式多模态序列到序列学习框架，通过延迟对齐流实现任意输出序列的流式推理，在ASR和TTS任务上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 传统序列到序列生成要么是离线方式（完整输入后才输出），要么需要学习复杂的流控制策略，DSM旨在提供更灵活高效的流式处理方案

Method: 使用仅解码器语言模型，通过预处理步骤对齐流并引入适当的延迟，使模型能够处理任意组合的输入输出流

Result: 在自动语音识别和文本转语音任务上实现了最先进的性能和延迟，支持任意长序列，甚至能与离线基线竞争

Conclusion: DSM为多模态流式序列到序列学习提供了简单而有效的解决方案，具有广泛的应用潜力

Abstract: We introduce Delayed Streams Modeling (DSM), a flexible formulation for
streaming, multimodal sequence-to-sequence learning. Sequence-to-sequence
generation is often cast in an offline manner, where the model consumes the
complete input sequence before generating the first output timestep.
Alternatively, streaming sequence-to-sequence rely on learning a policy for
choosing when to advance on the input stream, or write to the output stream.
DSM instead models already time-aligned streams with a decoder-only language
model. By moving the alignment to a pre-processing step,and introducing
appropriate delays between streams, DSM provides streaming inference of
arbitrary output sequences, from any input combination, making it applicable to
many sequence-to-sequence problems. In particular, given text and audio
streams, automatic speech recognition (ASR) corresponds to the text stream
being delayed, while the opposite gives a text-to-speech (TTS) model. We
perform extensive experiments for these two major sequence-to-sequence tasks,
showing that DSM provides state-of-the-art performance and latency while
supporting arbitrary long sequences, being even competitive with offline
baselines. Code, samples and demos are available at
https://github.com/kyutai-labs/delayed-streams-modeling

</details>


### [28] [Do All Autoregressive Transformers Remember Facts the Same Way? A Cross-Architecture Analysis of Recall Mechanisms](https://arxiv.org/abs/2509.08778)
*Minyeong Choe,Haehyun Cho,Changho Seo,Hyunil Kim*

Main category: cs.CL

TL;DR: 本文研究发现不同自回归Transformer架构在事实回忆机制上存在显著差异，特别是Qwen模型在早期注意力层而非MLP层进行事实回忆，这与GPT等模型的模式不同


<details>
  <summary>Details</summary>
Motivation: 理解Transformer语言模型如何存储和检索事实关联对于提高可解释性和实现针对性模型编辑至关重要，但先前研究主要基于GPT风格模型，不清楚这些发现是否适用于不同自回归架构

Method: 对多个模型（包括GPT、LLaMA、Qwen和DeepSeek）进行事实回忆的全面评估，分析事实信息在何处以及如何被编码和访问

Result: 发现基于Qwen的模型表现出与先前模式不同的行为：最早层的注意力模块比MLP模块对事实回忆的贡献更大

Conclusion: 即使在自回归Transformer家族内部，架构变化也可能导致根本不同的事实回忆机制

Abstract: Understanding how Transformer-based language models store and retrieve
factual associations is critical for improving interpretability and enabling
targeted model editing. Prior work, primarily on GPT-style models, has
identified MLP modules in early layers as key contributors to factual recall.
However, it remains unclear whether these findings generalize across different
autoregressive architectures. To address this, we conduct a comprehensive
evaluation of factual recall across several models -- including GPT, LLaMA,
Qwen, and DeepSeek -- analyzing where and how factual information is encoded
and accessed. Consequently, we find that Qwen-based models behave differently
from previous patterns: attention modules in the earliest layers contribute
more to factual recall than MLP modules. Our findings suggest that even within
the autoregressive Transformer family, architectural variations can lead to
fundamentally different mechanisms of factual recall.

</details>


### [29] [Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation Through Unsupervised Consistency Signals](https://arxiv.org/abs/2509.08809)
*Cheng Chen,Haiyan Yin,Ivor Tsang*

Main category: cs.CL

TL;DR: 提出一种新的无监督注释评估方法CAI比率，通过学生模型与噪声教师(LLM)协作，在无监督环境中评估和提升LLM注释质量


<details>
  <summary>Details</summary>
Motivation: 在动态无监督环境中，缺乏真实标签反馈，传统方法难以评估LLM注释质量，需要新的无监督评估方法

Method: 采用学生模型与噪声教师(LLM)协作模式，学生模型作为无监督反馈机制，使用用户偏好多数投票策略评估LLM输出一致性，提出CAI比率指标

Result: 在10个开放域NLP数据集和4个LLM上应用，CAI比率与LLM准确率呈强正相关关系，证明其在无监督评估和模型选择中的有效性

Conclusion: CAI比率是一种可靠的无监督评估指标，能够在真实环境中有效评估LLM注释质量和选择稳健的模型

Abstract: Large Language Models (LLMs), when paired with prompt-based tasks, have
significantly reduced data annotation costs and reliance on human annotators.
However, evaluating the quality of their annotations remains challenging in
dynamic, unsupervised environments where oracle feedback is scarce and
conventional methods fail. To address this challenge, we propose a novel
agentic annotation paradigm, where a student model collaborates with a noisy
teacher (the LLM) to assess and refine annotation quality without relying on
oracle feedback. The student model, acting as an unsupervised feedback
mechanism, employs a user preference-based majority voting strategy to evaluate
the consistency of the LLM outputs. To systematically measure the reliability
of LLM-generated annotations, we introduce the Consistent and Inconsistent
(CAI) Ratio, a novel unsupervised evaluation metric. The CAI Ratio not only
quantifies the annotation quality of the noisy teacher under limited user
preferences but also plays a critical role in model selection, enabling the
identification of robust LLMs in dynamic, unsupervised environments. Applied to
ten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates a
strong positive correlation with LLM accuracy, establishing it as an essential
tool for unsupervised evaluation and model selection in real-world settings.

</details>


### [30] [MoVoC: Morphology-Aware Subword Construction for Geez Script Languages](https://arxiv.org/abs/2509.08812)
*Hailay Kidu Teklehaymanot,Dren Fazlija,Wolfgang Nejdl*

Main category: cs.CL

TL;DR: 通过融合质基分析和BPE的MoVoC-Tok切分方法，为缀基米写作语言提供保持语法边界的切分，在内部评测指标上显示一致改善


<details>
  <summary>Details</summary>
Motivation: 解决子词切分方法在质源稀缺、语法复杂的缀基米写作语言中无法保持语法边界的问题

Method: 提出MoVoC方法，训练MoVoC-Tok切分器，结合监督语法分析和BPE切分，使用融合语素基和BPE标记来保持语法完整性

Result: 虽然自动翻译质量没有显著提升，但在MorphoScore和Boundary Precision等内部评测指标上取得一致改善，并发布了四种缀基米语言的手动注释语素数据集

Conclusion: 语法意识切分在提高语言信实性和标记效率方面具有价值，并为质源稀缺语言研究提供了重要资源

Abstract: Subword-based tokenization methods often fail to preserve morphological
boundaries, a limitation especially pronounced in low-resource, morphologically
complex languages such as those written in the Geez script. To address this, we
present MoVoC (Morpheme-aware Subword Vocabulary Construction) and train
MoVoC-Tok, a tokenizer that integrates supervised morphological analysis into
the subword vocabulary. This hybrid segmentation approach combines
morpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological
integrity while maintaining lexical meaning. To tackle resource scarcity, we
curate and release manually annotated morpheme data for four Geez script
languages and a morpheme-aware vocabulary for two of them. While the proposed
tokenization method does not lead to significant gains in automatic translation
quality, we observe consistent improvements in intrinsic metrics, MorphoScore,
and Boundary Precision, highlighting the value of morphology-aware segmentation
in enhancing linguistic fidelity and token efficiency. Our morpheme-annotated
datasets and tokenizer will be publicly available to support further research
in low-resource, morphologically rich languages. Our code and data are
available on GitHub: https://github.com/hailaykidu/MoVoC

</details>


### [31] [Building High-Quality Datasets for Portuguese LLMs: From Common Crawl Snapshots to Industrial-Grade Corpora](https://arxiv.org/abs/2509.08824)
*Thales Sales Almeida,Rodrigo Nogueira,Helio Pedrini*

Main category: cs.CL

TL;DR: 通过可扩展的网络数据收集方法，构建了120B标记的葡萄牙语语料库，并研究了从英语到其他语言的持续预训练策略对大语言模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然现有研究主要集中于英语，但对如何构建其他语言的高效训练语料库仍然存在知识空白，需要研究可扩展的多语言语料构建方法。

Method: 采用持续预训练设置，研究不同数据选择和前处理策略对模型性能的影响，包括语言特定的过滤流水线、STEM分类器和有害内容识别等方法。

Result: 构建的120B标记葡萄牙语语料库达到了与商业级语料库竞争的效果，证明了语言特定过滤管道的价值，模型适应目标语言后性能显著提升。

Conclusion: 高质量的语言特定数据对多语言大语言模型发展至关重要，虽然案例研究以葡萄牙语为主，但方法可扩展到其他语言，为多语言LLM开发提供了有价值的见解。

Abstract: The performance of large language models (LLMs) is deeply influenced by the
quality and composition of their training data. While much of the existing work
has centered on English, there remains a gap in understanding how to construct
effective training corpora for other languages. We explore scalable methods for
building web-based corpora for LLMs. We apply them to build a new 120B token
corpus in Portuguese that achieves competitive results to an industrial-grade
corpus. Using a continual pretraining setup, we study how different data
selection and preprocessing strategies affect LLM performance when
transitioning a model originally trained in English to another language. Our
findings demonstrate the value of language-specific filtering pipelines,
including classifiers for education, science, technology, engineering, and
mathematics (STEM), as well as toxic content. We show that adapting a model to
the target language leads to performance improvements, reinforcing the
importance of high-quality, language-specific data. While our case study
focuses on Portuguese, our methods are applicable to other languages, offering
insights for multilingual LLM development.

</details>


### [32] [Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation](https://arxiv.org/abs/2509.08825)
*Joachim Baumann,Paul Röttger,Aleksandra Urman,Albert Wendsjö,Flor Miriam Plaza-del-Arco,Johannes B. Gruber,Dirk Hovy*

Main category: cs.CL

TL;DR: LLM在社会科学研究中存在"LLM hacking"风险，研究人员的不同实现选择会导致系统性偏差和随机错误，约1/3的假设会得出错误结论，即使高性能模型也无法完全消除风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型正在改变社会科学研究，但研究人员在模型选择、提示策略等实现选择上的差异会引入系统偏差和随机错误，影响下游分析的统计结论可靠性。

Method: 复制37个数据标注任务，使用18个不同模型生成1300万标签，测试2361个现实假设，分析研究人员选择对统计结论的影响。

Result: 最先进模型约1/3假设得出错误结论，小语言模型一半假设错误；高风险出现在效应值较小的情况；人类标注能减少假阳性但常见校正技术效果有限；故意操纵极其简单。

Conclusion: LLM hacking风险真实存在，需要更严格的验证机制，特别是对接近显著性阈值的研究发现；人类标注在减少错误发现方面至关重要；需要开发更有效的风险缓解技术。

Abstract: Large language models (LLMs) are rapidly transforming social science research
by enabling the automation of labor-intensive tasks like data annotation and
text analysis. However, LLM outputs vary significantly depending on the
implementation choices made by researchers (e.g., model selection, prompting
strategy, or temperature settings). Such variation can introduce systematic
biases and random errors, which propagate to downstream analyses and cause Type
I, Type II, Type S, or Type M errors. We call this LLM hacking.
  We quantify the risk of LLM hacking by replicating 37 data annotation tasks
from 21 published social science research studies with 18 different models.
Analyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure
how plausible researcher choices affect statistical conclusions. We find
incorrect conclusions based on LLM-annotated data in approximately one in three
hypotheses for state-of-the-art models, and in half the hypotheses for small
language models. While our findings show that higher task performance and
better general model capabilities reduce LLM hacking risk, even highly accurate
models do not completely eliminate it. The risk of LLM hacking decreases as
effect sizes increase, indicating the need for more rigorous verification of
findings near significance thresholds. Our extensive analysis of LLM hacking
mitigation techniques emphasizes the importance of human annotations in
reducing false positive findings and improving model selection. Surprisingly,
common regression estimator correction techniques are largely ineffective in
reducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.
  Beyond accidental errors, we find that intentional LLM hacking is
unacceptably simple. With few LLMs and just a handful of prompt paraphrases,
anything can be presented as statistically significant.

</details>


### [33] [A Survey of Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2509.08827)
*Kaiyan Zhang,Yuxin Zuo,Bingxiang He,Youbang Sun,Runze Liu,Che Jiang,Yuchen Fan,Kai Tian,Guoli Jia,Pengfei Li,Yu Fu,Xingtai Lv,Yuchen Zhang,Sihang Zeng,Shang Qu,Haozhan Li,Shijie Wang,Yuru Wang,Xinwei Long,Fangfu Liu,Xiang Xu,Jiaze Ma,Xuekai Zhu,Ermo Hua,Yihao Liu,Zonglin Li,Huayu Chen,Xiaoye Qu,Yafu Li,Weize Chen,Zhenzhao Yuan,Junqi Gao,Dong Li,Zhiyuan Ma,Ganqu Cui,Zhiyuan Liu,Biqing Qi,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: 这篇论文综述了强化学习在大型语言模型推理能力方面的最新进展，重点分析了RL在数学和编程等复杂逻辑任务中的应用，并探讨了该领域面临的计算资源、算法设计等挑战。


<details>
  <summary>Details</summary>
Motivation: 随着RL在提升LLM推理能力方面取得显著成功，该领域面临计算资源、算法设计、训练数据和基础设施等方面的基础性挑战，需要重新评估发展轨迹并探索增强RL可扩展性的策略。

Method: 通过系统调研将RL应用于LLM和LRM推理能力的研究工作，包括基础组件、核心问题、训练资源和下游应用等方面的分析。

Result: 识别了RL for LRMs领域的关键发展趋势和技术挑战，为该快速发展的领域指明了未来的机遇和研究方向。

Conclusion: 这篇综述旨在促进RL在更广泛推理模型中的未来研究，为通向人工超级智能(ASI)的可扩展RL发展提供指导。

Abstract: In this paper, we survey recent advances in Reinforcement Learning (RL) for
reasoning with Large Language Models (LLMs). RL has achieved remarkable success
in advancing the frontier of LLM capabilities, particularly in addressing
complex logical tasks such as mathematics and coding. As a result, RL has
emerged as a foundational methodology for transforming LLMs into LRMs. With the
rapid progress of the field, further scaling of RL for LRMs now faces
foundational challenges not only in computational resources but also in
algorithm design, training data, and infrastructure. To this end, it is timely
to revisit the development of this domain, reassess its trajectory, and explore
strategies to enhance the scalability of RL toward Artificial SuperIntelligence
(ASI). In particular, we examine research applying RL to LLMs and LRMs for
reasoning abilities, especially since the release of DeepSeek-R1, including
foundational components, core problems, training resources, and downstream
applications, to identify future opportunities and directions for this rapidly
evolving area. We hope this review will promote future research on RL for
broader reasoning models. Github:
https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [34] [PianoVAM: A Multimodal Piano Performance Dataset](https://arxiv.org/abs/2509.08800)
*Yonghyun Kim,Junhyung Park,Joonhyung Bae,Kirak Kim,Taegyun Kwon,Alexander Lerch,Juhan Nam*

Main category: cs.SD

TL;DR: PianoVAM是一个包含视频、音频、MIDI、手部关键点、指法标注和丰富元数据的钢琴演奏数据集，通过Disklavier钢琴采集业余钢琴家的日常练习数据，支持多模态音乐信息检索研究。


<details>
  <summary>Details</summary>
Motivation: 音乐表演的多模态特性促使MIR社区对音频以外数据的需求增加，需要更全面的钢琴演奏数据集来支持多模态研究。

Method: 使用Disklavier钢琴采集业余钢琴家的日常练习数据，包括同步的音频、MIDI和俯视视频；使用预训练手部姿态估计模型提取手部关键点，采用半自动指法标注算法进行指法标注。

Result: 成功构建了包含多模态数据的PianoVAM数据集，并提供了音频转录和视听钢琴转录的基准测试结果。

Conclusion: PianoVAM数据集为多模态音乐信息检索研究提供了宝贵资源，支持音频转录、视听分析等多种应用，并展示了在真实多样化表演条件下的数据采集方法。

Abstract: The multimodal nature of music performance has driven increasing interest in
data beyond the audio domain within the music information retrieval (MIR)
community. This paper introduces PianoVAM, a comprehensive piano performance
dataset that includes videos, audio, MIDI, hand landmarks, fingering labels,
and rich metadata. The dataset was recorded using a Disklavier piano, capturing
audio and MIDI from amateur pianists during their daily practice sessions,
alongside synchronized top-view videos in realistic and varied performance
conditions. Hand landmarks and fingering labels were extracted using a
pretrained hand pose estimation model and a semi-automated fingering annotation
algorithm. We discuss the challenges encountered during data collection and the
alignment process across different modalities. Additionally, we describe our
fingering annotation method based on hand landmarks extracted from videos.
Finally, we present benchmarking results for both audio-only and audio-visual
piano transcription using the PianoVAM dataset and discuss additional potential
applications.

</details>


### [35] [LALM-Eval: An Open-Source Toolkit for Holistic Evaluation of Large Audio Language Models](https://arxiv.org/abs/2509.08031)
*Sidharth Surapaneni,Hoang Nguyen,Jash Mehta,Aman Tiwari,Oluwanifemi Bamgbose,Akshay Kalkunte,Sai Rajeswar,Sathwik Tejaswi Madhusudhan*

Main category: cs.SD

TL;DR: LALM-Eval是一个高效的大音频语言模型评估框架，解决了现有工具包处理慢、提示不一致和任务覆盖窄的问题，实现了127%的速度提升，并引入了新的评估类别。


<details>
  <summary>Details</summary>
Motivation: 当前大音频语言模型评估工具存在处理效率低、提示不一致和任务覆盖范围有限的问题，限制了公平比较和系统评估。

Method: 开发LALM-Eval框架，通过优化的批处理和并行执行实现高效评估，提供标准化提示协议，并引入LLM-Adaptive Diarization和Spoken Language Reasoning两个新评估类别。

Result: 在380多个任务上的评估显示，现有LALM在时间理解和复杂语音推理任务上存在显著差距，指令模态缺乏标准化导致性能差异高达9.5个百分点。

Conclusion: LALM-Eval提供了实用的评估工具和模型局限性的深入见解，推动了大音频语言模型的系统性发展。

Abstract: Large Audio Language Models (LALMs) are rapidly advancing, but evaluating
them remains challenging due to inefficient toolkits that limit fair comparison
and systematic assessment. Current frameworks suffer from three critical
issues: slow processing that bottlenecks large-scale studies, inconsistent
prompting that hurts reproducibility, and narrow task coverage that misses
important audio reasoning capabilities. We introduce LALM-Eval, an efficient
and comprehensive evaluation framework for LALMs. Our system achieves a speedup
of up to 127% over existing toolkits through optimized batch processing and
parallel execution, enabling large-scale evaluations previously impractical. We
provide standardized prompting protocols and flexible configurations for fair
model comparison across diverse scenarios. Additionally, we introduce two new
evaluation categories: LLM-Adaptive Diarization for temporal audio
understanding and Spoken Language Reasoning for complex audio-based cognitive
tasks. Through evaluation across 380+ tasks, we reveal significant gaps in
current LALMs, particularly in temporal understanding and complex spoken
language reasoning tasks. Our findings also highlight a lack of standardization
in instruction modality existent across audio benchmarks, which can lead up
performance differences up to 9.5 absolute points on the challenging complex
instruction following downstream tasks. LALM-Eval provides both practical
evaluation tools and insights into model limitations, advancing systematic LALM
development.

</details>


### [36] [Segment Transformer: AI-Generated Music Detection via Music Structural Analysis](https://arxiv.org/abs/2509.08283)
*Yumin Kim,Seonghyeon Go*

Main category: cs.SD

TL;DR: 提出基于音乐片段结构分析的AI生成音乐检测方法，使用预训练模型提取特征，通过transformer框架处理短音频和长音频，在FakeMusicCaps和SONICS数据集上取得高准确率


<details>
  <summary>Details</summary>
Motivation: 解决AI生成音乐(AIGM)的版权归属问题，因为AI生成音乐与人类创作音乐难以区分，需要开发准确的检测技术

Method: 集成多种预训练模型（自监督学习模型和音频效果编码器）提取音乐特征，提出基于transformer的框架处理短音频片段，开发segment transformer分析长音频的片段间关系

Result: 在FakeMusicCaps和SONICS数据集上，短音频和完整音频检测实验都获得了高准确率

Conclusion: 将片段级音乐特征整合到长时域分析中，可以有效提升AIGM检测系统的性能和鲁棒性

Abstract: Audio and music generation systems have been remarkably developed in the
music information retrieval (MIR) research field. The advancement of these
technologies raises copyright concerns, as ownership and authorship of
AI-generated music (AIGM) remain unclear. Also, it can be difficult to
determine whether a piece was generated by AI or composed by humans clearly. To
address these challenges, we aim to improve the accuracy of AIGM detection by
analyzing the structural patterns of music segments. Specifically, to extract
musical features from short audio clips, we integrated various pre-trained
models, including self-supervised learning (SSL) models or an audio effect
encoder, each within our suggested transformer-based framework. Furthermore,
for long audio, we developed a segment transformer that divides music into
segments and learns inter-segment relationships. We used the FakeMusicCaps and
SONICS datasets, achieving high accuracy in both the short-audio and full-audio
detection experiments. These findings suggest that integrating segment-level
musical features into long-range temporal analysis can effectively enhance both
the performance and robustness of AIGM detection systems.

</details>


### [37] [LatentVoiceGrad: Nonparallel Voice Conversion with Latent Diffusion/Flow-Matching Models](https://arxiv.org/abs/2509.08379)
*Hirokazu Kameoka,Takuhiro Kaneko,Kou Tanaka,Yuto Kondo*

Main category: cs.SD

TL;DR: VoiceGrad改进版：通过潜在沉种模型和流匹配技术提升语音转换的音频质量和转换速度


<details>
  <summary>Details</summary>
Motivation: 原始VoiceGrad在音频质量和转换速度方面仍有不足，需要提升以跟上现代高速语音转换方法的要求

Method: 1）在自动编码器瓶颈层中引入潜在沉种模型进行反向沉种 2）使用流匹配模型替代沉种模型以加速转换过程

Result: 实验结果显示改进版在语音质量和转换速度方面都超过原始方法

Conclusion: 通过潜在沉种和流匹配技术的结合，成功提升了非平行语音转换的性能，为高质量高速转换提供了有效解决方案

Abstract: Previously, we introduced VoiceGrad, a nonparallel voice conversion (VC)
technique enabling mel-spectrogram conversion from source to target speakers
using a score-based diffusion model. The concept involves training a score
network to predict the gradient of the log density of mel-spectrograms from
various speakers. VC is executed by iteratively adjusting an input
mel-spectrogram until resembling the target speaker's. However, challenges
persist: audio quality needs improvement, and conversion is slower compared to
modern VC methods designed to operate at very high speeds. To address these, we
introduce latent diffusion models into VoiceGrad, proposing an improved version
with reverse diffusion in the autoencoder bottleneck. Additionally, we propose
using a flow matching model as an alternative to the diffusion model to further
speed up the conversion process without compromising the conversion quality.
Experimental results show enhanced speech quality and accelerated conversion
compared to the original.

</details>


### [38] [Behind the Scenes: Mechanistic Interpretability of LoRA-adapted Whisper for Speech Emotion Recognition](https://arxiv.org/abs/2509.08454)
*Yujian Ma,Jinqiu Sang,Ruizhe Li*

Main category: cs.SD

TL;DR: 对Whisper语音模型在语音情感识别任务中LoRA微调机制的首个系统性可解释性研究，揭示了延迟特化过程和前向对齐-后向分化动态


<details>
  <summary>Details</summary>
Motivation: 大型预训练语音模型如Whisper具有强大泛化能力，但资源高效的适配面临挑战。LoRA已成为流行的参数高效微调方法，但其在语音任务中的工作机制尚不明确

Method: 使用层贡献探测、logit-lens检查、SVD和CKA表示相似性分析等工具，系统研究Whisper编码器中LoRA的机制

Result: 发现了两个关键机制：早期层保留通用特征、后期层整合任务特定信息的延迟特化过程，以及LoRA矩阵间的前向对齐-后向分化动态

Conclusion: 阐明了LoRA如何重塑编码器层次结构，为设计高效可解释的大型语音模型适配策略提供了实证见解和机制理解

Abstract: Large pre-trained speech models such as Whisper offer strong generalization
but pose significant challenges for resource-efficient adaptation. Low-Rank
Adaptation (LoRA) has become a popular parameter-efficient fine-tuning method,
yet its underlying mechanisms in speech tasks remain poorly understood. In this
work, we conduct the first systematic mechanistic interpretability study of
LoRA within the Whisper encoder for speech emotion recognition (SER). Using a
suite of analytical tools, including layer contribution probing, logit-lens
inspection, and representational similarity via singular value decomposition
(SVD) and centered kernel alignment (CKA), we reveal two key mechanisms: a
delayed specialization process that preserves general features in early layers
before consolidating task-specific information, and a forward alignment,
backward differentiation dynamic between LoRA's matrices. Our findings clarify
how LoRA reshapes encoder hierarchies, providing both empirical insights and a
deeper mechanistic understanding for designing efficient and interpretable
adaptation strategies in large speech models.

</details>


### [39] [Explainability of CNN Based Classification Models for Acoustic Signal](https://arxiv.org/abs/2509.08717)
*Zubair Faruqui,Mackenzie S. McIntire,Rahul Dubey,Jay McEntee*

Main category: cs.SD

TL;DR: 本研究应用多种可解释AI技术分析鸟类鸣声分类模型，结合模型无关和模型特定的XAI方法获得更完整的解释，在生物声学领域展示了XAI的重要价值。


<details>
  <summary>Details</summary>
Motivation: 虽然XAI在声学领域应用日益广泛，但在生物声学（分析生物音频信号）中的应用仍相对不足。本研究旨在探索XAI在鸟类地理变异鸣声分析中的解释能力。

Method: 将音频录音转换为频谱图图像，训练深度卷积神经网络进行分类（准确率94.8%），并应用模型无关（LIME、SHAP）和模型特定（DeepLIFT、Grad-CAM）XAI技术进行解释。

Result: 不同XAI技术产生了互补的解释，当综合考虑这些解释时，能够提供更完整和可解释的模型决策洞察。

Conclusion: 研究表明结合多种XAI技术可以提高模型的可信度和互操作性，不仅在声学信号分析中，在其他领域特定任务中也具有广泛适用性。

Abstract: Explainable Artificial Intelligence (XAI) has emerged as a critical tool for
interpreting the predictions of complex deep learning models. While XAI has
been increasingly applied in various domains within acoustics, its use in
bioacoustics, which involves analyzing audio signals from living organisms,
remains relatively underexplored. In this paper, we investigate the
vocalizations of a bird species with strong geographic variation throughout its
range in North America. Audio recordings were converted into spectrogram images
and used to train a deep Convolutional Neural Network (CNN) for classification,
achieving an accuracy of 94.8\%. To interpret the model's predictions, we
applied both model-agnostic (LIME, SHAP) and model-specific (DeepLIFT,
Grad-CAM) XAI techniques. These techniques produced different but complementary
explanations, and when their explanations were considered together, they
provided more complete and interpretable insights into the model's
decision-making. This work highlights the importance of using a combination of
XAI techniques to improve trust and interoperability, not only in broader
acoustics signal analysis but also argues for broader applicability in
different domain specific tasks.

</details>
