<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 51]
- [cs.SD](#cs.SD) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum](https://arxiv.org/abs/2508.18673)
*Xinglong Yang,Quan Feng,Zhongying Pan,Xiang Chen,Yu Tian,Wentong Li,Shuofei Qiao,Yuxia Geng,Xingyu Zhao,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 提出了一种基于难度平衡采样的多模态思维链提示选择框架，通过联合分析模型感知难度和样本内在复杂度来优化示例选择，显著提升多模态推理性能


<details>
  <summary>Details</summary>
Motivation: 传统多模态思维链提示中随机或手动选择示例的方法无法考虑模型特定知识分布和任务内在复杂度，导致性能次优且不稳定

Method: 将提示选择重构为课程设计问题，结合模型感知难度（通过主动学习中的预测分歧量化）和样本内在复杂度，开发难度平衡采样策略

Result: 在五个挑战性基准测试和多个流行MLLM上的实验表明，该方法带来显著且一致的性能提升，大大减少随机采样导致的性能差异

Conclusion: 该方法为增强多模态推理提供了一种原则性和鲁棒性的方法，通过难度平衡的示例选择优化模型性能

Abstract: The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often
limited by the use of randomly or manually selected examples. These examples
fail to account for both model-specific knowledge distributions and the
intrinsic complexity of the tasks, resulting in suboptimal and unstable model
performance. To address this, we propose a novel framework inspired by the
pedagogical principle of "tailored teaching with balanced difficulty". We
reframe prompt selection as a prompt curriculum design problem: constructing a
well ordered set of training examples that align with the model's current
capabilities. Our approach integrates two complementary signals: (1)
model-perceived difficulty, quantified through prediction disagreement in an
active learning setup, capturing what the model itself finds challenging; and
(2) intrinsic sample complexity, which measures the inherent difficulty of each
question-image pair independently of any model. By jointly analyzing these
signals, we develop a difficulty-balanced sampling strategy that ensures the
selected prompt examples are diverse across both dimensions. Extensive
experiments conducted on five challenging benchmarks and multiple popular
Multimodal Large Language Models (MLLMs) demonstrate that our method yields
substantial and consistent improvements and greatly reduces performance
discrepancies caused by random sampling, providing a principled and robust
approach for enhancing multimodal reasoning.

</details>


### [2] [Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI](https://arxiv.org/abs/2508.18290)
*Hans-Joachim Rudolph*

Main category: cs.CL

TL;DR: 提出基于复数空间语义吸引子的AGI理论框架，通过张量变换和虚数运算构建语义旋转结构，用意向性算子替代统计预测来实现语义收敛


<details>
  <summary>Details</summary>
Motivation: 突破当前基于统计的transformer模型，探索从概率推断转向递归张量变换的语义形成机制，解决反讽、同音异义和歧义等复杂语义问题

Method: 使用复数空间中的循环运算和虚数单位i，构建旋转语义结构；采用梯度流、张量变形和迭代矩阵动力学来描述语义吸引子这一目的论算子

Result: 建立了能够建模复杂语义现象的数学框架，提出了不同于统计计算的意向性语义收敛机制

Conclusion: 真正的意义源于向语义一致性的递归收敛而非模拟，这需要设计新型认知架构来塑造语言而不仅仅是预测语言

Abstract: This essay develops a theoretical framework for a semantic Artificial General
Intelligence (AGI) based on the notion of semantic attractors in complex-valued
meaning spaces. Departing from current transformer-based language models, which
operate on statistical next-token prediction, we explore a model in which
meaning is not inferred probabilistically but formed through recursive
tensorial transformation. Using cyclic operations involving the imaginary unit
\emph{i}, we describe a rotational semantic structure capable of modeling
irony, homonymy, and ambiguity. At the center of this model, however, is a
semantic attractor -- a teleological operator that, unlike statistical
computation, acts as an intentional agent (Microvitum), guiding meaning toward
stability, clarity, and expressive depth. Conceived in terms of gradient flows,
tensor deformations, and iterative matrix dynamics, the attractor offers a
model of semantic transformation that is not only mathematically suggestive,
but also philosophically significant. We argue that true meaning emerges not
from simulation, but from recursive convergence toward semantic coherence, and
that this requires a fundamentally new kind of cognitive architecture -- one
designed to shape language, not just predict it.

</details>


### [3] [LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions](https://arxiv.org/abs/2508.18321)
*Maojia Song,Tej Deep Pala,Weisheng Jin,Amir Zadeh,Chuan Li,Dorien Herremans,Soujanya Poria*

Main category: cs.CL

TL;DR: KAIROS基准测试研究LLM在多智能体系统中如何基于历史印象建立信任、抵抗错误信息并整合同伴输入，发现GRPO强化学习方法在提升性能的同时降低了社交影响的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注从众偏见，但需要更全面地分析LLM在多智能体协作中如何形成信任、抵抗错误信息以及整合同伴输入，这些因素对实现复杂社交动态下的集体智能至关重要。

Method: 提出KAIROS基准测试，模拟具有不同可靠性同伴的问答竞赛，控制专家-新手角色、噪声群体和对抗性同伴等条件。评估提示工程、监督微调和强化学习（特别是GRPO）等多种缓解策略。

Result: GRPO结合多智能体上下文、基于结果的奖励和无约束推理实现了最佳整体性能，但与基础模型相比降低了对社交影响的鲁棒性。

Conclusion: 多智能体强化学习能有效提升LLM在协作环境中的决策性能，但需要平衡性能提升与社交影响鲁棒性之间的关系，为构建更可靠的集体智能系统提供重要见解。

Abstract: Large language models (LLMs) are increasingly deployed in multi-agent systems
(MAS) as components of collaborative intelligence, where peer interactions
dynamically shape individual decision-making. Although prior work has focused
on conformity bias, we extend the analysis to examine how LLMs form trust from
previous impressions, resist misinformation, and integrate peer input during
interaction, key factors for achieving collective intelligence under complex
social dynamics. We present KAIROS, a benchmark simulating quiz contests with
peer agents of varying reliability, offering fine-grained control over
conditions such as expert-novice roles, noisy crowds, and adversarial peers.
LLMs receive both historical interactions and current peer responses, allowing
systematic investigation into how trust, peer action, and self-confidence
influence decisions. As for mitigation strategies, we evaluate prompting,
supervised fine-tuning, and reinforcement learning, Group Relative Policy
Optimisation (GRPO), across multiple models. Our results reveal that GRPO with
multi-agent context combined with outcome-based rewards and unconstrained
reasoning achieves the best overall performance, but also decreases the
robustness to social influence compared to Base models. The code and datasets
are available at: https://github.com/declare-lab/KAIROS.

</details>


### [4] [Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective](https://arxiv.org/abs/2508.18328)
*Masudul Hasan Masud Bhuiyan,Matteo Varvello,Yasir Zaki,Cristian-Alexandru Staicu*

Main category: cs.CL

TL;DR: 本文介绍了LangCrUX数据集，这是首个大规模多语言网站数据集，包含12种非拉丁语系的12万个流行网站。研究发现网页可访问性提示普遍忽视语言多样性，导致屏幕阅读器效果不佳，并提出了Kizuki语言感知自动化测试工具来解决此问题。


<details>
  <summary>Details</summary>
Motivation: 网络内容日益多语言化，但辅助技术（如屏幕阅读器）对非拉丁语系支持不足，导致视觉障碍用户面临严重的可访问性挑战。现有研究缺乏全面的多语言网页内容数据集，限制了大规模分析。

Method: 构建LangCrUX数据集（12种非拉丁语系的12万个流行网站），进行系统性多语言网页可访问性分析，并开发Kizuki语言感知自动化可访问性测试扩展工具。

Result: 发现可访问性提示普遍未能反映可见内容的语言多样性，降低了屏幕阅读器的有效性，限制了网页可访问性。

Conclusion: 多语言网页可访问性存在严重问题，需要语言感知的自动化测试工具来改善辅助技术对非拉丁语系内容的支持。

Abstract: English is the predominant language on the web, powering nearly half of the
world's top ten million websites. Support for multilingual content is
nevertheless growing, with many websites increasingly combining English with
regional or native languages in both visible content and hidden metadata. This
multilingualism introduces significant barriers for users with visual
impairments, as assistive technologies like screen readers frequently lack
robust support for non-Latin scripts and misrender or mispronounce non-English
text, compounding accessibility challenges across diverse linguistic contexts.
Yet, large-scale studies of this issue have been limited by the lack of
comprehensive datasets on multilingual web content. To address this gap, we
introduce LangCrUX, the first large-scale dataset of 120,000 popular websites
across 12 languages that primarily use non-Latin scripts. Leveraging this
dataset, we conduct a systematic analysis of multilingual web accessibility and
uncover widespread neglect of accessibility hints. We find that these hints
often fail to reflect the language diversity of visible content, reducing the
effectiveness of screen readers and limiting web accessibility. We finally
propose Kizuki, a language-aware automated accessibility testing extension to
account for the limited utility of language-inconsistent accessibility hints.

</details>


### [5] [Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models](https://arxiv.org/abs/2508.18381)
*Yuchun Fan,Yilin Wang,Yongyu Mu,Lei Huang,Bei Li,Xiaocheng Feng,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: PLAST是一种通过精确语言特定层微调来高效提升大型视觉语言模型多语言能力的训练方法，仅需调整14%参数即可显著改善多语言理解性能


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多语言能力上存在不平衡问题，研究发现多语言理解能力与浅层语言特定神经元激活存在显著相关性

Method: 首先通过监控语言特定神经元激活来识别参与多语言理解的层，然后使用问题翻译对精确微调这些层以实现多语言对齐

Result: 在MM-Bench和MMMB基准测试中有效提升多语言能力，仅调整14%参数即可实现显著效率提升，并能泛化到低资源和复杂视觉推理任务

Conclusion: PLAST方法通过浅层语言特定视觉信息参与，为大型视觉语言模型提供了高效的多语言能力增强方案

Abstract: Large vision-language models (LVLMs) have demonstrated exceptional
capabilities in understanding visual information with human languages but also
exhibit an imbalance in multilingual capabilities. In this work, we delve into
the multilingual working pattern of LVLMs and identify a salient correlation
between the multilingual understanding ability of LVLMs and language-specific
neuron activations in shallow layers. Building on this insight, we introduce
PLAST, a training recipe that achieves efficient multilingual enhancement for
LVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies
layers involved in multilingual understanding by monitoring language-specific
neuron activations. These layers are then precisely fine-tuned with
question-translation pairs to achieve multilingual alignment. Our empirical
results on MM-Bench and MMMB demonstrate that PLAST effectively improves the
multilingual capabilities of LVLMs and achieves significant efficiency with
only 14% of the parameters tuned. Further analysis reveals that PLAST can be
generalized to low-resource and complex visual reasoning tasks, facilitating
the language-specific visual information engagement in shallow layers.

</details>


### [6] [Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models](https://arxiv.org/abs/2508.18655)
*Haoyu Wang,Guangyan Zhang,Jiale Chen,Jingyu Li,Yuehai Wang,Yiwen Guo*

Main category: cs.CL

TL;DR: 提出了Emotion Omni模型，能够在有限数据下理解用户语音中的情感并生成共情语音响应，无需大规模训练


<details>
  <summary>Details</summary>
Motivation: 现有语音大语言模型仅简单转换文本为语音，无法理解用户查询中的丰富情感和副语言线索，而情感理解对于提升人机交互体验至关重要

Method: 提出新颖的模型架构Emotion Omni，并基于开源TTS框架开发数据生成管道，构建了20万条情感对话数据集

Result: 成功构建了支持共情语音助手的数据集和模型架构，演示样例已公开

Conclusion: 该方法为开发具有情感理解能力的语音助手提供了一种数据高效且计算资源友好的解决方案

Abstract: With the development of speech large language models (speech LLMs), users can
now interact directly with assistants via speech. However, most existing models
simply convert the response content into speech without fully understanding the
rich emotional and paralinguistic cues embedded in the user's query. In many
cases, the same sentence can have different meanings depending on the emotional
expression. Furthermore, emotional understanding is essential for improving
user experience in human-machine interaction. Currently, most speech LLMs with
empathetic capabilities are trained on massive datasets. This approach requires
vast amounts of data and significant computational resources. Therefore, a key
challenge lies in how to develop a speech LLM capable of generating empathetic
responses with limited data and without the need for large-scale training. To
address this challenge, we propose Emotion Omni, a novel model architecture
designed to understand the emotional content of user speech input and generate
empathetic speech responses. Additionally, we developed a data generation
pipeline based on an open-source TTS framework to construct a 200k emotional
dialogue dataset, which supports the construction of an empathetic speech
assistant. The demos are available at https://w311411.github.io/omni_demo/

</details>


### [7] [Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails](https://arxiv.org/abs/2508.18384)
*Kellen Tan Cheng,Anna Lisa Gentile,Chad DeLuca,Guang-Jie Ren*

Main category: cs.CL

TL;DR: 这篇论文提出了backprompting方法，通过生成生产级别标注数据来改善健康建议监测器的性能，在参数较少的情况下超过GPT-4o的表现。


<details>
  <summary>Details</summary>
Motivation: 企业环境中大语言模型的普及带来了风险，而开发稳健的监测器面临生产级别标数据获取困难的挑战。

Method: 提出backprompting方法生成生产级别标注数据，结合稀疏人巡环聚类技术进行标注，构建与原始数据集并行但类似真实LLM输出的语料库。

Result: 在健康建议识别这个难题上，监测器性能超过GPT-4o达到3.73%，而参数数量只有其的1/400。

Conclusion: backprompting方法能够有效生成高质量标注数据，为开发稳健的LLM拦截器提供了可靠的数据支持。

Abstract: The pervasiveness of large language models (LLMs) in enterprise settings has
also brought forth a significant amount of risks associated with their usage.
Guardrails technologies aim to mitigate this risk by filtering LLMs'
input/output text through various detectors. However, developing and
maintaining robust detectors faces many challenges, one of which is the
difficulty in acquiring production-quality labeled data on real LLM outputs
prior to deployment. In this work, we propose backprompting, a simple yet
intuitive solution to generate production-like labeled data for health advice
guardrails development. Furthermore, we pair our backprompting method with a
sparse human-in-the-loop clustering technique to label the generated data. Our
aim is to construct a parallel corpus roughly representative of the original
dataset yet resembling real LLM output. We then infuse existing datasets with
our synthetic examples to produce robust training data for our detector. We
test our technique in one of the most difficult and nuanced guardrails: the
identification of health advice in LLM output, and demonstrate improvement
versus other solutions. Our detector is able to outperform GPT-4o by up to
3.73%, despite having 400x less parameters.

</details>


### [8] [VibeVoice Technical Report](https://arxiv.org/abs/2508.19205)
*Zhiliang Peng,Jianwei Yu,Wenhui Wang,Yaoyao Chang,Yutao Sun,Li Dong,Yi Zhu,Weijiang Xu,Hangbo Bao,Zehua Wang,Shaohan Huang,Yan Xia,Furu Wei*

Main category: cs.CL

TL;DR: VibeVoice是一个使用next-token扩散技术合成多说话人长语音的新模型，通过新型连续语音分词器实现80倍数据压缩，能在64K上下文窗口中合成长达90分钟的4人对话语音。


<details>
  <summary>Details</summary>
Motivation: 现有的语音合成模型在处理长序列多说话人对话时面临计算效率和数据压缩的挑战，需要开发能够高效处理长语音并保持音频保真度的解决方案。

Method: 采用next-token扩散方法，通过自回归生成潜在向量来建模连续数据；引入新型连续语音分词器，相比Encodec模型实现80倍数据压缩；支持64K上下文窗口长度。

Result: 模型能够合成长达90分钟的长语音，最多支持4个说话人，在保持音频保真度的同时显著提升计算效率，超越了开源和专有对话模型的表现。

Conclusion: VibeVoice通过创新的分词器和扩散方法，成功解决了长序列多说话人语音合成的挑战，实现了高效且高质量的对话语音生成，捕捉了真实的对话氛围。

Abstract: This report presents VibeVoice, a novel model designed to synthesize
long-form speech with multiple speakers by employing next-token diffusion,
which is a unified method for modeling continuous data by autoregressively
generating latent vectors via diffusion. To enable this, we introduce a novel
continuous speech tokenizer that, when compared to the popular Encodec model,
improves data compression by 80 times while maintaining comparable performance.
The tokenizer effectively preserves audio fidelity while significantly boosting
computational efficiency for processing long sequences. Thus, VibeVoice can
synthesize long-form speech for up to 90 minutes (in a 64K context window
length) with a maximum of 4 speakers, capturing the authentic conversational
``vibe'' and surpassing open-source and proprietary dialogue models.

</details>


### [9] [Integral Transformer: Denoising Attention, Not Too Much Not Too Little](https://arxiv.org/abs/2508.18387)
*Ivan Kobyzev,Abbas Ghaddar,Dingtao Hu,Boxing Chen*

Main category: cs.CL

TL;DR: 提出Integral Transformer，通过积分采样logit分布来降噪注意力，在保持特殊令牌作用的同时解决注意力噪声问题，在多个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统softmax自注意力机制会给语义信息较少的特殊令牌和标点符号分配过多权重，形成注意力噪声。现有方法如Cog Attention和Differential Transformer虽然引入负注意力分数，但可能丢失有用信息。

Method: 提出Integral Transformer自注意力机制，通过从logit分布中积分采样信号来降噪注意力，同时保留对模型性能至关重要的特殊令牌的贡献。

Result: 在成熟的知识和推理语言基准测试中，该模型优于vanilla、Cog和Differential注意力变体。分析表明，在底层使用vanilla自注意力可提升性能，Integral Transformer能有效平衡注意力分布并减少上层秩崩溃。

Conclusion: Integral Transformer是一种有效的自注意力降噪方法，既能缓解注意力噪声问题，又能保持关键信息，在多个维度上优于现有方法。

Abstract: Softmax self-attention often assigns disproportionate weight to semantically
uninformative tokens such as special tokens and punctuation, a phenomenon known
as attention noise. While recent methods like Cog Attention and the
Differential Transformer have addressed this by introducing negative attention
scores, they risk discarding useful information. In this paper, we propose the
Integral Transformer, a novel self-attention mechanism that denoises attention
by integrating signals sampled from the logit distribution. Our approach
mitigates noise while preserving the contributions of special tokens critical
for model performance. Extensive experiments demonstrate that our model
outperforms vanilla, Cog, and Differential attention variants on
well-established knowledge and reasoning language benchmarks. Moreover, our
analysis reveals that employing vanilla self-attention in the lower Transformer
layers enhances performance and that the Integral Transformer effectively
balances attention distributions and reduces rank collapse in upper layers.

</details>


### [10] [Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning](https://arxiv.org/abs/2508.18395)
*Jeong-seok Oh,Jay-yoon Lee*

Main category: cs.CL

TL;DR: 提出了Latent Self-Consistency (LSC)方法，使用可学习的token嵌入选择语义最一致的回答，在短式和长式推理基准测试中均优于现有方法，且计算开销极小。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在复杂或长式问题上的概率解码输出不一致问题，现有方法如SC、USC和WUCS在短式和长式回答格式之间存在准确度权衡。

Method: 使用可学习的token嵌入来选择语义最一致的回答，通过轻量级的前向生成摘要token，推理时间增加不到1%，无需改变模型架构。

Result: 在6个短式和5个长式推理基准测试（如MATH、MMLU、TruthfulQA）中，LSC在所有短式和长式测试上平均优于SC、USC和WUCS，同时保持可忽略的计算开销。

Conclusion: LSC是一种实用的跨答案格式一致性选择方法，提供良好校准的置信度估计，在两个答案格式上都保持低期望校准误差。

Abstract: Probabilistic decoding in Large Language Models (LLMs) often yields
inconsistent outputs, particularly on complex or long-form questions.
Self-Consistency (SC) mitigates this for short-form QA by majority voting over
exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram
Consistency Score (WUCS) extend to long-form responses but lose accuracy on
short-form benchmarks.
  We introduce Latent Self-Consistency (LSC), which selects the most
semantically consistent response using learnable token embeddings. A
lightweight forward generation of summary tokens increases inference time by
less than 1% and requires no changes to the model architecture.
  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,
TruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form
ones on average, while maintaining negligible computational overhead. These
results position LSC as a practical consistency-selection method that works
reliably across answer formats. Additionally, LSC provides well-calibrated
confidence estimates, maintaining low Expected Calibration Error across both
answer formats.

</details>


### [11] [Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering](https://arxiv.org/abs/2508.18407)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Michal Spiegel,Josef Kuchař*

Main category: cs.CL

TL;DR: 本文挖掘了OOD评估方法在评估问答模型健壮性时的局限性，发现不同数据集对模型偏好短便式的评估质量差异很大，并提出了更健壮的评估方法建议。


<details>
  <summary>Details</summary>
Motivation: 当前AI领域依赖OOD数据集评估模型渗透性，但这个假设可能无法真实反映模型在实际部署中的失败模式。本文要挖掘OOD评估在评估问答模型偏好短便式时的有效性问题。

Method: 将OOD评估结果与现有问答模型中已文档化的具体失败模式进行对比，分析不同OOD数据集对模型偏好短便式评估的质量差异。

Result: 发现不同OOD数据集对模型偏好短便式的评估质量差异显著，某些数据集的评估效果甚至较简单的内部分布评估更差。这部分归因于短便式在ID和OOD数据集中的共享性，以及数据集训练质量与评估质量的脱节。

Conclusion: 常见的OOD基于渗透性评估方法存在显著局限性，本研究为在问答及更广泛领域更健壮地评估模型渗透性提供了方法论和建议。

Abstract: A majority of recent work in AI assesses models' generalization capabilities
through the lens of performance on out-of-distribution (OOD) datasets. Despite
their practicality, such evaluations build upon a strong assumption: that OOD
evaluations can capture and reflect upon possible failures in a real-world
deployment.
  In this work, we challenge this assumption and confront the results obtained
from OOD evaluations with a set of specific failure modes documented in
existing question-answering (QA) models, referred to as a reliance on spurious
features or prediction shortcuts.
  We find that different datasets used for OOD evaluations in QA provide an
estimate of models' robustness to shortcuts that have a vastly different
quality, some largely under-performing even a simple, in-distribution
evaluation. We partially attribute this to the observation that spurious
shortcuts are shared across ID+OOD datasets, but also find cases where a
dataset's quality for training and evaluation is largely disconnected. Our work
underlines limitations of commonly-used OOD-based evaluations of
generalization, and provides methodology and recommendations for evaluating
generalization within and beyond QA more robustly.

</details>


### [12] [How Reliable are LLMs for Reasoning on the Re-ranking task?](https://arxiv.org/abs/2508.18444)
*Nafis Tanveer Islam,Zhiming Zhao*

Main category: cs.CL

TL;DR: \u8fd9\u7bc7\u8bba\u6587\u5206\u6790\u4e86\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91cd\u6392\u540d\u4efb\u52a1\u4e2d\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u7814\u7a76\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\u3002


<details>
  <summary>Details</summary>
Motivation: \u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u7684\u63d0\u5347\uff0c\u5b83\u4eec\u66f4\u52a0\u7b26\u5408\u4eba\u7c7b\u4ef7\u503c\u89c2\uff0c\u4f46\u4ee5\u900f\u660e\u6027\u4e3a\u4ee3\u4ef7\u3002\u9700\u8981\u6df1\u5165\u7406\u89e3\u6a21\u578b\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u6765\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u91cd\u6392\u540d\u7406\u7531\uff0c\u5e76\u89e3\u51b3\u65b0\u7cfb\u7edf\u4e2d\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u6311\u6218\u3002

Method: \u4f7f\u7528\u73af\u5883\u548c\u5730\u7403\u79d1\u5b66\u9886\u57df\u7684\u8f83\u5c0f\u6392\u540d\u6570\u636e\u96c6\uff0c\u5206\u6790\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u4e0bLLM\u5728\u91cd\u6392\u540n\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7814\u7a76\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u63a8\u7406\u548c\u53ef\u89e3\u91ca\u6027\u4fe1\u606f\u3002

Result: \u53d1\u73b0\u67d0\u4e9b\u8bad\u7ec3\u65b9\u6cd5\u6bd4\u5176\u4ed6\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u8868\u660e\u4e0d\u662f\u6240\u6709\u8bad\u7ec3\u65b9\u6cd5\u90fd\u80fd\u5b66\u4e60\u5230\u51c6\u786e\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u800c\u662f\u83b7\u5f97\u4e86\u62bd\u8c61\u77e5\u8bc6\u6765\u4f18\u5316\u8bc4\u4f30\u3002

Conclusion: \u7814\u7a76\u5bf9\u4e8e\u7406\u89e3LLM\u5728\u91cd\u6392\u540d\u4efb\u52a1\u4e2d\u7684\u771f\u5b9e\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4e0d\u540c\u8bad\u7ec3\u65b6\u65b9\u6cd5\u5bf9\u6a21\u578b\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u6df1\u5165\u7684\u5206\u6790\u6765\u786e\u4fdd\u6a21\u578b\u7684\u900f\u660e\u6027\u548c\u53ef\u9760\u6027\u3002

Abstract: With the improving semantic understanding capability of Large Language Models
(LLMs), they exhibit a greater awareness and alignment with human values, but
this comes at the cost of transparency. Although promising results are achieved
via experimental analysis, an in-depth understanding of the LLM's internal
workings is unavoidable to comprehend the reasoning behind the re-ranking,
which provides end users with an explanation that enables them to make an
informed decision. Moreover, in newly developed systems with limited user
engagement and insufficient ranking data, accurately re-ranking content remains
a significant challenge. While various training methods affect the training of
LLMs and generate inference, our analysis has found that some training methods
exhibit better explainability than others, implying that an accurate semantic
understanding has not been learned through all training methods; instead,
abstract knowledge has been gained to optimize evaluation, which raises
questions about the true reliability of LLMs. Therefore, in this work, we
analyze how different training methods affect the semantic understanding of the
re-ranking task in LLMs and investigate whether these models can generate more
informed textual reasoning to overcome the challenges of transparency or LLMs
and limited training data. To analyze the LLMs for re-ranking tasks, we utilize
a relatively small ranking dataset from the environment and the Earth science
domain to re-rank retrieved content. Furthermore, we also analyze the
explainable information to see if the re-ranking can be reasoned using
explainability.

</details>


### [13] [Integrating gender inclusivity into large language models via instruction tuning](https://arxiv.org/abs/2508.18466)
*Alina Wróblewska,Bartosz Żuk*

Main category: cs.CL

TL;DR: 该研究通过IPIS数据集微调大语言模型，为波兰语设计性别包容性系统提示，旨在解决波兰语中男性偏见问题。


<details>
  <summary>Details</summary>
Motivation: 波兰语由于历史和政治惯例，主要使用男性形式指代所有人，导致大语言模型继承并强化了这种性别偏见，产生性别不平衡的输出。

Method: 使用IPIS数据集（包含人工制作的性别包容性校对和波兰语到英语翻译指令）微调多语言LLMs和波兰语专用LLMs，并设计包含明确性别包容性指南的系统提示。

Result: 研究对Llama-8B、Mistral-7B、Mistral-Nemo等多语言模型以及Bielik、PLLuM等波兰语专用模型进行了IPIS微调。

Conclusion: 该方法旨在将性别包容性作为模型的内在特征，为缓解波兰语生成中的性别偏见提供系统性解决方案。

Abstract: Imagine a language with masculine, feminine, and neuter grammatical genders,
yet, due to historical and political conventions, masculine forms are
predominantly used to refer to men, women and mixed-gender groups. This is the
reality of contemporary Polish. A social consequence of this unfair linguistic
system is that large language models (LLMs) trained on Polish texts inherit and
reinforce this masculine bias, generating gender-imbalanced outputs. This study
addresses this issue by tuning LLMs using the IPIS dataset, a collection of
human-crafted gender-inclusive proofreading in Polish and Polish-to-English
translation instructions. Grounded in a theoretical linguistic framework, we
design a system prompt with explicit gender-inclusive guidelines for Polish. In
our experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and
Mistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to
integrate gender inclusivity as an inherent feature of these models, offering a
systematic solution to mitigate gender bias in Polish language generation.

</details>


### [14] [Principled Detection of Hallucinations in Large Language Models via Multiple Testing](https://arxiv.org/abs/2508.18473)
*Jiawei Li,Akshayaa Magesh,Venugopal V. Veeravalli*

Main category: cs.CL

TL;DR: 本文提出了一种基于多重检验假设的幻觉检测方法，将LLM幻觉检测问题建模为假设检验问题，并与机器学习中的分布外检测问题进行类比。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然功能强大，但容易产生幻觉（生成看似自信但实际错误或无意义的回答），需要有效的检测方法来识别这些不可靠的输出。

Method: 将幻觉检测问题形式化为假设检验问题，借鉴多重检验的思想，提出了一种统计方法来检测LLM生成的回答中的幻觉内容。

Result: 通过大量实验验证，该方法在鲁棒性方面优于现有的最先进方法，能够有效识别LLM生成的幻觉内容。

Conclusion: 提出的多重检验启发式方法为LLM幻觉检测提供了一种有效的统计框架，实验结果表明该方法具有优越的性能和鲁棒性。

Abstract: While Large Language Models (LLMs) have emerged as powerful foundational
models to solve a variety of tasks, they have also been shown to be prone to
hallucinations, i.e., generating responses that sound confident but are
actually incorrect or even nonsensical. In this work, we formulate the problem
of detecting hallucinations as a hypothesis testing problem and draw parallels
to the problem of out-of-distribution detection in machine learning models. We
propose a multiple-testing-inspired method to solve the hallucination detection
problem, and provide extensive experimental results to validate the robustness
of our approach against state-of-the-art methods.

</details>


### [15] [COMET-poly: Machine Translation Metric Grounded in Other Candidates](https://arxiv.org/abs/2508.18549)
*Maike Züfle,Vilém Zouhar,Tu Anh Dinh,Felipe Maia Polo,Jan Niehues,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 提出两种新的机器翻译自动评估指标COMET-polycand和COMET-polyic，通过引入多个备选翻译或相似文本的翻译示例来提升评估性能，相比仅使用单一翻译的传统方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统机器翻译自动评估指标只考虑源句子和单个翻译，而人类评估时通常会对比多个备选翻译。这种评估设置上的差异可能影响自动指标的性能。

Method: COMET-polycand使用同一源句的多个备选翻译进行比较；COMET-polyic借鉴检索式上下文学习，输入相似源文本的翻译及其人工标注质量分数来指导评估。

Result: COMET-polycand加入单个额外翻译即可提升段级指标性能（Kendall's tau-b相关性从0.079提升至0.118），加入更多翻译效果更好。COMET-polyic通过检索示例获得类似改进（0.079到0.116）。

Conclusion: 在自动翻译评估中引入多个翻译或相似示例信息能够显著提升评估指标与人工判断的相关性，模型已公开发布。

Abstract: Automated metrics for machine translation attempt to replicate human
judgment. Unlike humans, who often assess a translation in the context of
multiple alternatives, these metrics typically consider only the source
sentence and a single translation. This discrepancy in the evaluation setup may
negatively impact the performance of automated metrics. We propose two
automated metrics that incorporate additional information beyond the single
translation. COMET-polycand uses alternative translations of the same source
sentence to compare and contrast with the translation at hand, thereby
providing a more informed assessment of its quality. COMET-polyic, inspired by
retrieval-based in-context learning, takes in translations of similar source
texts along with their human-labeled quality scores to guide the evaluation. We
find that including a single additional translation in COMET-polycand improves
the segment-level metric performance (0.079 to 0.118 Kendall's tau-b
correlation), with further gains when more translations are added.
Incorporating retrieved examples in COMET-polyic yields similar improvements
(0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.

</details>


### [16] [The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation](https://arxiv.org/abs/2508.18569)
*Girish A. Koushik,Fatemeh Nazarieh,Katherine Birch,Shenbin Qian,Diptesh Kanojia*

Main category: cs.CL

TL;DR: 这篇论文提出了一种自我评估的视觉隐喻生成框架，通过语言分解和轻量级强化学习来提高隐喻对齐效果，在多个指标上超越了GPT-4o和Imagen等基线模型。


<details>
  <summary>Details</summary>
Motivation: 视觉隐喻生成需要同时满足语言理解和视觉一致性，当前模型在隐喻对齐方面遇到挑战，需要更好的方法来绑定源概念与目标概念的含义关联。

Method: 提出了自我评估框架，结合新提出的隐喻分解指标和含义对齐指标。包含两种方法：一种是无需训练的流水线，通过显式将提示分解为源-目标-含义（S-T-M）映射进行图像合成；另一种是基于训练的流水线，使用自我评估奖励机制改善对齐效果。

Result: 在测试集上，无需训练方法在分解、CLIP和含义对齐指标上超越了GPT-4o和Imagen等强劲基线模型。用户研究显示用户更偏好GPT-4o，但无需训练流水线在开源方法中领先并在抽象隐喻上微超Imagen。

Conclusion: 结构化提示和轻量级强化学习在计算资源有限的情况下能够有效实现隐喻对齐，与人类偏好的差距主要来自美学质量和采样设置的敏感性。

Abstract: Visual metaphor generation is a challenging task that aims to generate an
image given an input text metaphor. Inherently, it needs language understanding
to bind a source concept with a target concept, in a way that preserves meaning
while ensuring visual coherence. We propose a self-evaluating visual metaphor
generation framework that focuses on metaphor alignment. Our self-evaluation
approach combines existing metrics with our newly proposed metaphor
decomposition score and a meaning alignment (MA) metric. Within this setup, we
explore two novel approaches: a training-free pipeline that explicitly
decomposes prompts into source-target-meaning (S-T-M) mapping for image
synthesis, and a complementary training-based pipeline that improves alignment
using our proposed self-evaluation reward schema, without any large-scale
retraining. On the held-out test set, the training-free approach surpasses
strong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores,
with the training-based approach close behind. We evaluate our framework output
using a user-facing study, and observed that participants preferred GPT-4o
overall, while our training-free pipeline led open-source methods and edged
Imagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or
more abstract metaphors, with closed models excelling on short, concrete cases;
we also observe sensitivity to sampler settings. Overall, structured prompting
and lightweight RL perform metaphor alignment well under modest compute, and
remaining gaps to human preference appear driven by aesthetics and sampling.

</details>


### [17] [What do language models model? Transformers, automata, and the format of thought](https://arxiv.org/abs/2508.18598)
*Colin Klein*

Main category: cs.CL

TL;DR: 这篇论文主张大型语言模型主要是对训练语料库的模拟，而非人类认知能力的模型，因为Transformer架构的线性计算格式与人类超线性计算格式存在根本差异


<details>
  <summary>Details</summary>
Motivation: 辨析大型语言模型的本质，明确其是否真正模拟了人类认知能力，还是仅仅对训练语料库的统计学习

Method: 通过对比Transformer架构的线性计算格式与人类超线性计算格式的差异，以及基于Liu等人(2022)关于短接自动机的理论分析

Result: 证明了LLM主要是对语料库的模拟，而非人类认知能力的直接模型，但认为这并不是气荼的观点

Conclusion: 语言不仅仅是内部状态的表达工具，更是一种"话语机器"，LLM通过不同方式学会了使用这种技术

Abstract: What do large language models actually model? Do they tell us something about
human capacities, or are they models of the corpus we've trained them on? I
give a non-deflationary defence of the latter position. Cognitive science tells
us that linguistic capabilities in humans rely supralinear formats for
computation. The transformer architecture, by contrast, supports at best a
linear formats for processing. This argument will rely primarily on certain
invariants of the computational architecture of transformers. I then suggest a
positive story about what transformers are doing, focusing on Liu et al.
(2022)'s intriguing speculations about shortcut automata. I conclude with why I
don't think this is a terribly deflationary story. Language is not (just) a
means for expressing inner state but also a kind of 'discourse machine' that
lets us make new language given appropriate context. We have learned to use
this technology in one way; LLMs have also learned to use it too, but via very
different means.

</details>


### [18] [A New NMT Model for Translating Clinical Texts from English to Spanish](https://arxiv.org/abs/2508.18607)
*Rumeng Li,Xun Wang,Hong Yu*

Main category: cs.CL

TL;DR: NOOV是一个新的神经机器翻译系统，专门用于将电子健康记录从英文翻译到西班牙文，通过整合双语词典和短语查找表来解决未知词汇和训练数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录(EHR)的英文到西班牙文翻译具有重要临床意义，但面临缺乏平行语料库和大量未知词汇的挑战。

Method: 提出NOOV系统，整合从平行语料库自动学习的双语词典和从大型生物医学知识资源提取的短语查找表，缓解NMT中的未知词问题和词汇重复挑战。

Result: 评估显示NOOV能够生成更好的EHR翻译，在准确性和流畅性方面都有改进。

Conclusion: NOOV系统有效解决了EHR翻译中的关键挑战，为临床医疗中的跨语言信息交流提供了实用解决方案。

Abstract: Translating electronic health record (EHR) narratives from English to Spanish
is a clinically important yet challenging task due to the lack of a
parallel-aligned corpus and the abundant unknown words contained. To address
such challenges, we propose \textbf{NOOV} (for No OOV), a new neural machine
translation (NMT) system that requires little in-domain parallel-aligned corpus
for training. NOOV integrates a bilingual lexicon automatically learned from
parallel-aligned corpora and a phrase look-up table extracted from a large
biomedical knowledge resource, to alleviate both the unknown word problem and
the word-repeat challenge in NMT, enhancing better phrase generation of NMT
systems. Evaluation shows that NOOV is able to generate better translation of
EHR with improvement in both accuracy and fluency.

</details>


### [19] [Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models](https://arxiv.org/abs/2508.18609)
*Chenxi Zhou,Pengfei Cao,Jiang Li,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 这篇论文通过实验研究建立了大语言模型后训练量化的任务分层缩放律，发现知识记忆比知识利用对量化参数更敏感，为知识感知量化策略提供指导。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型后训练量化如何准确影响多样化知识能力的理解不充分，现有缩放律忽视了PTQ特定参数和任务特定敏感性。

Method: 进行广泛的实证研究，将LLM知识解构为记忆和利用能力，开发包含模型大小、有效位宽、校准集大小和组大小的统一量化框架。

Result: 核心发现：知识记忆在有效位宽、校准集大小和模型大小的变化上显示出比知识利用更明显的敏感性，而知识利用更加稳健。

Conclusion: 这些发现提供了对PTQ影响的细粒度理解，为开发能更好保留目标认知功能的知识感知量化策略提供了指导。

Abstract: Large language models (LLMs) present significant deployment challenges due to
their scale, with post-training quantization (PTQ) emerging as a practical
compression solution. However, a comprehensive understanding of how PTQ
precisely impacts diverse LLM knowledge capabilities remains elusive, and
existing scaling laws for quantized models often overlook crucial PTQ-specific
parameters and task-specific sensitivities. This paper addresses these gaps by
conducting an extensive empirical investigation to establish task-stratified
scaling laws. We disentangle LLM knowledge into memorization and utilization
capabilities and develop a unified quantitative framework that incorporates
model size, effective bit-width, calibration set size, and group size. Our
central finding reveals that knowledge memorization exhibits markedly greater
sensitivity to variations in effective bit-width, calibration set size, and
model size compared to the more robust knowledge utilization. These findings
offer a fine-grained understanding of PTQ's impact and provide guidance for
developing knowledge-aware quantization strategies that can better preserve
targeted cognitive functions.

</details>


### [20] [Thinking Before You Speak: A Proactive Test-time Scaling Approach](https://arxiv.org/abs/2508.18648)
*Cong Li,Wenchang Chai,Hejun Wu,Yan Pan,Pengxu Wei,Liang Lin*

Main category: cs.CL

TL;DR: TBYS框架通过在推理步骤间插入主动生成的insights来指导LLM的复杂推理过程，解决了训练数据中人类内在思维缺失的问题


<details>
  <summary>Details</summary>
Motivation: LLM在复杂推理任务（如数学）上表现不佳，因为人类在思考时不会表达内在思维过程，导致训练数据中缺少连接推理步骤的关键insights

Method: 提出Thinking Before You Speak (TBYS)框架，在连续推理步骤间主动生成insights来回顾状态并启动下一步推理，通过自动化流水线收集和过滤上下文示例

Result: 在具有挑战性的数学数据集上的实验验证了TBYS的有效性

Conclusion: 通过主动生成insights来指导推理过程，可以有效提升LLM在复杂推理任务上的表现，且减少了人工标注和微调的开销

Abstract: Large Language Models (LLMs) often exhibit deficiencies with complex
reasoning tasks, such as maths, which we attribute to the discrepancy between
human reasoning patterns and those presented in the LLMs' training data. When
dealing with complex problems, humans tend to think carefully before expressing
solutions. However, they often do not articulate their inner thoughts,
including their intentions and chosen methodologies. Consequently, critical
insights essential for bridging reasoning steps may be absent in training data
collected from human sources. To bridge this gap, we proposes inserting
\emph{insight}s between consecutive reasoning steps, which review the status
and initiate the next reasoning steps. Unlike prior prompting strategies that
rely on a single or a workflow of static prompts to facilitate reasoning,
\emph{insight}s are \emph{proactively} generated to guide reasoning processes.
We implement our idea as a reasoning framework, named \emph{Thinking Before You
Speak} (TBYS), and design a pipeline for automatically collecting and filtering
in-context examples for the generation of \emph{insight}s, which alleviates
human labeling efforts and fine-tuning overheads. Experiments on challenging
mathematical datasets verify the effectiveness of TBYS. Project website:
https://gitee.com/jswrt/TBYS

</details>


### [21] [Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models](https://arxiv.org/abs/2508.18651)
*Chenxu Yang,Qingyi Si,Zheng Lin*

Main category: cs.CL

TL;DR: 通过动态整合外部知识和模型内部参数的输出概率，突破了大语言模型在忠实性和表达性之间的交易，提升了回答的可靠性和自然性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在整合外部知识时无法同时保持忠实性和表达性，导致输出或缺乏知识支持或过于繁琐不自然。

Method: 提出协同解码(CoDe)方法，通过分布散度和模型信心指导动态整合有无外部知识的输出概率，并使用知识感知重排机制防止过度依赖参数知识。

Result: 完整实验表明CoDe框架在各种大语言模型和评估指标上都显示出优异性能，在提升忠实性的同时不損害表达性。

Conclusion: CoDe是一种插件式的高效解决方案，有效突破了忠实性与表达性的交易关系，验证了其有效性和普适性。

Abstract: Grounding responses in external knowledge represents an effective strategy
for mitigating hallucinations in Large Language Models (LLMs). However, current
LLMs struggle to seamlessly integrate knowledge while simultaneously
maintaining faithfulness (or fidelity) and expressiveness, capabilities that
humans naturally possess. This limitation results in outputs that either lack
support from external knowledge, thereby compromising faithfulness, or appear
overly verbose and unnatural, thus sacrificing expressiveness. In this work, to
break the trade-off between faithfulness and expressiveness, we propose
Collaborative Decoding (CoDe), a novel approach that dynamically integrates
output probabilities generated with and without external knowledge. This
integration is guided by distribution divergence and model confidence, enabling
the selective activation of relevant and reliable expressions from the model's
internal parameters. Furthermore, we introduce a knowledge-aware reranking
mechanism that prevents over-reliance on prior parametric knowledge while
ensuring proper utilization of provided external information. Through
comprehensive experiments, our plug-and-play CoDe framework demonstrates
superior performance in enhancing faithfulness without compromising
expressiveness across diverse LLMs and evaluation metrics, validating both its
effectiveness and generalizability.

</details>


### [22] [Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning](https://arxiv.org/abs/2508.18687)
*Songtao Jiang,Yuxi Chen,Sibo Song,Yan Zhang,Yeying Jin,Yang Feng,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: 本文揭示了当前医学视觉语言模型在医学视觉问答中存在答案不一致的脆弱性问题，构建了RoMed数据集进行评估，并提出了CCL方法显著提升了模型的鲁棒性和一致性。


<details>
  <summary>Details</summary>
Motivation: 在高风险医疗应用中，模型需要对语义相同但表述不同的医学问题给出一致的回答，但现有Med-VLMs在面对问题重述时表现出令人担忧的脆弱性，答案波动显著。

Method: 构建了包含14.4万个问题的RoMed数据集（包含词汇级、句子级和语义级扰动），提出了CCL方法，包含知识锚定一致性学习和偏差感知对比学习两个关键组件。

Result: 在RoMed上评估SOTA模型（如LLaVA-Med）发现性能显著下降（如召回率下降40%），CCL方法在三个流行VQA基准上达到SOTA性能，并在RoMed测试集上将答案一致性提高了50%。

Conclusion: CCL方法通过整合医学知识和对齐表示学习，有效解决了Med-VLMs的脆弱性问题，显著增强了模型在医学视觉问答中的鲁棒性和一致性。

Abstract: In high-stakes medical applications, consistent answering across diverse
question phrasings is essential for reliable diagnosis. However, we reveal that
current Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility
in Medical Visual Question Answering, as their answers fluctuate significantly
when faced with semantically equivalent rephrasings of medical questions. We
attribute this to two limitations: (1) insufficient alignment of medical
concepts, leading to divergent reasoning patterns, and (2) hidden biases in
training data that prioritize syntactic shortcuts over semantic understanding.
To address these challenges, we construct RoMed, a dataset built upon original
VQA datasets containing 144k questions with variations spanning word-level,
sentence-level, and semantic-level perturbations. When evaluating
state-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarming
performance drops (e.g., a 40\% decline in Recall) compared to original VQA
benchmarks, exposing critical robustness gaps. To bridge this gap, we propose
Consistency and Contrastive Learning (CCL), which integrates two key
components: (1) knowledge-anchored consistency learning, aligning Med-VLMs with
medical knowledge rather than shallow feature patterns, and (2) bias-aware
contrastive learning, mitigating data-specific priors through discriminative
representation refinement. CCL achieves SOTA performance on three popular VQA
benchmarks and notably improves answer consistency by 50\% on the challenging
RoMed test set, demonstrating significantly enhanced robustness. Code will be
released.

</details>


### [23] [Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System](https://arxiv.org/abs/2508.18701)
*Yanfan Du,Jun Zhang,Bin Wang,Jin Qiu,Lu Huang,Yuan Ge,Xiaoqian Liu,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 提出了Attention2Probability方法，通过注意力权重转换和课程学习提升语音大模型中专业术语的识别准确率，在中文和英文测试中分别达到92.57%和86.83%的召回率，延迟仅8.71ms。


<details>
  <summary>Details</summary>
Motivation: 当前语音大模型在通用领域表现良好，但在处理领域特定术语和新词时准确率不足，需要专门的方法来提升术语识别能力。

Method: 提出Attention2Probability方法，将语音与术语之间的交叉注意力权重转换为存在概率，并采用课程学习提高检索准确性。同时构建了包含术语的语音数据集。

Result: 在测试集上显著优于VectorDB方法，中文最大召回率92.57%，英文86.83%，查询延迟仅8.71ms。术语干预使识别和翻译任务准确率提升6-17%。

Conclusion: 该方法轻量、灵活且准确，有效提升了语音大模型的术语处理能力，同时揭示了当前模型在术语利用方面仍存在局限性。

Abstract: Recent advances in speech large language models (SLMs) have improved speech
recognition and translation in general domains, but accurately generating
domain-specific terms or neologisms remains challenging. To address this, we
propose Attention2Probability: attention-driven terminology probability
estimation for robust speech-to-text system, which is lightweight, flexible,
and accurate. Attention2Probability converts cross-attention weights between
speech and terminology into presence probabilities, and it further employs
curriculum learning to enhance retrieval accuracy. Furthermore, to tackle the
lack of data for speech-to-text tasks with terminology intervention, we create
and release a new speech dataset with terminology to support future research in
this area. Experimental results show that Attention2Probability significantly
outperforms the VectorDB method on our test set. Specifically, its maximum
recall rates reach 92.57% for Chinese and 86.83% for English. This high recall
is achieved with a latency of only 8.71ms per query. Intervening in SLMs'
recognition and translation tasks using Attention2Probability-retrieved terms
improves terminology accuracy by 6-17%, while revealing that the current
utilization of terminology by SLMs has limitations.

</details>


### [24] [Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs](https://arxiv.org/abs/2508.18709)
*Duy Le,Kent Ziti,Evan Girard-Sun,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: 该文章提出了适应性原创性筛选(AOF)提示框架，通过余弦相似性拔消和词法新颖性约束，提升多语言谜语生成的创造性和文化适应性。


<details>
  <summary>Details</summary>
Motivation: 多语言谜语生成面临的挑战是需要在文化流利性和创造性抽象之间取得平衡。标准提示策略容易重复记忆的谜语或进行浅层的反义转换。

Method: 适应性原创性筛选(AOF)框架，使用余弦相似性拔消来筛选重复生成内容，同时强化词法新颖性和跨语言信度要求。

Result: 在三种大语言模型和四组语言对中评估，AOF增强的GPT-4o在日语中达到了0.177的Self-BLEU和0.915的Distinct-2指标，显示出词法多样性提升和重复性降低。

Conclusion: 语义拔消可以在不需任务特定微调的情况下，指导基于文化根基的创造性生成。

Abstract: Multilingual riddle generation challenges large language models (LLMs) to
balance cultural fluency with creative abstraction. Standard prompting
strategies -- zero-shot, few-shot, chain-of-thought -- tend to reuse memorized
riddles or perform shallow paraphrasing. We introduce Adaptive Originality
Filtering (AOF), a prompting framework that filters redundant generations using
cosine-based similarity rejection, while enforcing lexical novelty and
cross-lingual fidelity. Evaluated across three LLMs and four language pairs,
AOF-enhanced GPT-4o achieves \texttt{0.177} Self-BLEU and \texttt{0.915}
Distinct-2 in Japanese, signaling improved lexical diversity and reduced
redundancy compared to other prompting methods and language pairs. Our findings
show that semantic rejection can guide culturally grounded, creative generation
without task-specific fine-tuning.

</details>


### [25] [EMMM, Explain Me My Model! Explainable Machine Generated Text Detection in Dialogues](https://arxiv.org/abs/2508.18715)
*Angela Yifei Yuan,Haoyi Li,Soyeon Caren Han,Christopher Leckie*

Main category: cs.CL

TL;DR: 提出了EMMM框架，通过先解释后检测的方法，在客户服务场景中平衡延迟、准确性和非专家可解释性，有效检测机器生成文本的冒充行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在客户服务中的快速应用带来了新的风险，恶意行为者可能利用机器生成文本进行大规模用户冒充。现有检测方法在在线对话环境中表现不佳，缺乏可靠性和可解释性，特别是在非专家用户操作的客户服务场景中。

Method: 提出EMMM框架，采用先解释后检测的方法，为非专家用户提供可理解的解释，同时保持低延迟和高准确性。

Result: 实验结果显示，EMMM为非专家用户提供了可访问的解释，70%的人类评估者偏好其输出，在保持1秒内生成输出的低延迟的同时，达到了与最先进模型相当的准确性。

Conclusion: EMMM框架成功解决了客户服务场景中机器生成文本检测的可解释性问题，为非专家用户提供了可信赖的AI部署方案，代码和数据集已开源。

Abstract: The rapid adoption of large language models (LLMs) in customer service
introduces new risks, as malicious actors can exploit them to conduct
large-scale user impersonation through machine-generated text (MGT). Current
MGT detection methods often struggle in online conversational settings,
reducing the reliability and interpretability essential for trustworthy AI
deployment. In customer service scenarios where operators are typically
non-expert users, explanation become crucial for trustworthy MGT detection. In
this paper, we propose EMMM, an explanation-then-detection framework that
balances latency, accuracy, and non-expert-oriented interpretability.
Experimental results demonstrate that EMMM provides explanations accessible to
non-expert users, with 70\% of human evaluators preferring its outputs, while
achieving competitive accuracy compared to state-of-the-art models and
maintaining low latency, generating outputs within 1 second. Our code and
dataset are open-sourced at
https://github.com/AngieYYF/EMMM-explainable-chatbot-detection.

</details>


### [26] [Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models](https://arxiv.org/abs/2508.18739)
*Chang Wang,Siyu Yan,Depeng Yuan,Yuqi Chen,Yanhua Huang,Yuanhang Zheng,Shuhao Li,Yinqi Zhang,Kedi Chen,Mingrui Zhu,Ruiwen Xu*

Main category: cs.CL

TL;DR: DIVER是一个基于大语言模型的广告标题生成框架，通过多阶段多目标优化同时提升标题质量和多样性，在工业数据集上验证了效果，部署后ADVV和CTR分别提升4.0%和1.4%。


<details>
  <summary>Details</summary>
Motivation: 当前广告标题生成方法主要优化质量或点击率，忽视了多样性需求，导致输出同质化严重，无法满足不同受众群体的需求。

Method: 提出语义和风格感知的数据生成管道自动创建高质量训练对，采用多阶段多目标优化框架，结合监督微调(SFT)和强化学习(RL)，在单次前向传播中生成高质量多样化标题。

Result: 在真实工业数据集上验证了DIVER能有效平衡质量和多样性，部署到亿级用户的内容分享平台后，广告主价值(ADVV)提升4.0%，点击率(CTR)提升1.4%。

Conclusion: DIVER框架成功解决了广告标题生成中质量与多样性的平衡问题，通过联合优化实现了实际业务指标的显著提升，证明了多目标优化在广告生成任务中的有效性。

Abstract: The generation of ad headlines plays a vital role in modern advertising,
where both quality and diversity are essential to engage a broad range of
audience segments. Current approaches primarily optimize language models for
headline quality or click-through rates (CTR), often overlooking the need for
diversity and resulting in homogeneous outputs. To address this limitation, we
propose DIVER, a novel framework based on large language models (LLMs) that are
jointly optimized for both diversity and quality. We first design a semantic-
and stylistic-aware data generation pipeline that automatically produces
high-quality training pairs with ad content and multiple diverse headlines. To
achieve the goal of generating high-quality and diversified ad headlines within
a single forward pass, we propose a multi-stage multi-objective optimization
framework with supervised fine-tuning (SFT) and reinforcement learning (RL).
Experiments on real-world industrial datasets demonstrate that DIVER
effectively balances quality and diversity. Deployed on a large-scale
content-sharing platform serving hundreds of millions of users, our framework
improves advertiser value (ADVV) and CTR by 4.0% and 1.4%.

</details>


### [27] [M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations](https://arxiv.org/abs/2508.18740)
*Qiao Liang,Ying Shen,Tiantian Chen,Lin Zhang*

Main category: cs.CL

TL;DR: 提出了M3HG模型和MECAD数据集，用于多模态对话中的情感原因三元组提取，通过多模态异构图显式建模情感和因果上下文，在跨话语和话语内层面融合语义信息


<details>
  <summary>Details</summary>
Motivation: 现有MECTEC领域数据集稀缺且对话场景单一，现有方法未能显式建模情感和因果上下文，且忽略了不同层次语义信息的融合，导致性能下降

Method: 提出M3HG模型，使用多模态异构图显式捕获情感和因果上下文，在跨话语和话语内两个层面有效融合上下文信息

Result: 大量实验证明M3HG相比现有最先进方法具有更好的效果

Conclusion: M3HG模型和多模态多场景的MECAD数据集有效解决了MECTEC任务中的关键挑战，为多模态对话情感分析提供了新的解决方案

Abstract: Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has
recently gained significant attention in social media analysis, aiming to
extract emotion utterances, cause utterances, and emotion categories
simultaneously. However, the scarcity of related datasets, with only one
published dataset featuring highly uniform dialogue scenarios, hinders model
development in this field. To address this, we introduce MECAD, the first
multimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56
TV series spanning a wide range of dialogue contexts. In addition, existing
MECTEC methods fail to explicitly model emotional and causal contexts and
neglect the fusion of semantic information at different levels, leading to
performance degradation. In this paper, we propose M3HG, a novel model that
explicitly captures emotional and causal contexts and effectively fuses
contextual information at both inter- and intra-utterance levels via a
multimodal heterogeneous graph. Extensive experiments demonstrate the
effectiveness of M3HG compared with existing state-of-the-art methods. The
codes and dataset are available at https://github.com/redifinition/M3HG.

</details>


### [28] [Chronological Passage Assembling in RAG framework for Temporal Question Answering](https://arxiv.org/abs/2508.18748)
*Byeongjeong Kim,Jeonghyun Park,Joonho Yang,Hwanhee Lee*

Main category: cs.CL

TL;DR: ChronoRAG是一个专门针对叙事文本的新型RAG框架，通过重构连贯段落和保持时间顺序来改进叙事问答任务


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在处理叙事文本时效果有限，因为叙事理解需要更广泛的上下文和段落间的时序关系，而不仅仅是孤立片段

Method: 提出ChronoRAG框架，专注于两个关键方面：将分散的文档信息重构为连贯的结构化段落；通过显式捕获和维护检索段落的时间顺序来保持叙事流

Result: 在NarrativeQA数据集上的实验显示，ChronoRAG在需要事实识别和复杂时序关系理解的任务上取得了显著改进

Conclusion: 时序推理对于解决叙事问答任务至关重要，ChronoRAG通过专门处理叙事文本的时序特性有效提升了性能

Abstract: Long-context question answering over narrative tasks is challenging because
correct answers often hinge on reconstructing a coherent timeline of events
while preserving contextual flow in a limited context window.
Retrieval-augmented generation (RAG) indexing methods aim to address this
challenge by selectively retrieving only necessary document segments. However,
narrative texts possess unique characteristics that limit the effectiveness of
these existing approaches. Specifically, understanding narrative texts requires
more than isolated segments, as the broader context and sequential
relationships between segments are crucial for comprehension. To address these
limitations, we propose ChronoRAG, a novel RAG framework specialized for
narrative texts. This approach focuses on two essential aspects: refining
dispersed document information into coherent and structured passages, and
preserving narrative flow by explicitly capturing and maintaining the temporal
order among retrieved passages. We empirically demonstrate the effectiveness of
ChronoRAG through experiments on the NarrativeQA dataset, showing substantial
improvements in tasks requiring both factual identification and comprehension
of complex sequential relationships, underscoring that reasoning over temporal
order is crucial in resolving narrative QA.

</details>


### [29] [ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models](https://arxiv.org/abs/2508.18773)
*Qianyu He,Siyu Yuan,Xuefeng Li,Mingxuan Wang,Jiangjie Chen*

Main category: cs.CL

TL;DR: ThinkDial是首个开源端到端框架，通过离散操作模式实现GPT-style可控推理，提供高中低三种推理模式，在显著减少计算token的同时保持性能阈值。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然具备强大的推理能力，但计算成本控制仍是实际部署的重大挑战。开源社区缺乏像GPT系列那样的可控推理能力，需要开发能够灵活控制计算开销的解决方案。

Method: 采用端到端训练范式：1）预算模式监督微调，将可控推理能力直接嵌入学习过程；2）两阶段预算感知强化学习，使用自适应奖励塑形技术。

Result: 成功实现三种推理模式：高模式（完整能力）、中模式（减少50%token，性能下降<10%）、低模式（减少75%token，性能下降<15%）。在分布外任务上表现出强泛化能力。

Conclusion: ThinkDial框架有效解决了LLM推理计算成本控制问题，为开源社区提供了实用的可控推理解决方案，在保持性能的同时显著降低了计算开销。

Abstract: Large language models (LLMs) with chain-of-thought reasoning have
demonstrated remarkable problem-solving capabilities, but controlling their
computational effort remains a significant challenge for practical deployment.
Recent proprietary systems like OpenAI's gpt-oss series have introduced
discrete operational modes for intuitive reasoning control, but the open-source
community has largely failed to achieve such capabilities. In this paper, we
introduce ThinkDial, the first open-recipe end-to-end framework that
successfully implements gpt-oss-style controllable reasoning through discrete
operational modes. Our system enables seamless switching between three distinct
reasoning regimes: High mode (full reasoning capability), Medium mode (50
percent token reduction with <10 percent performance degradation), and Low mode
(75 percent token reduction with <15 percent performance degradation). We
achieve this through an end-to-end training paradigm that integrates
budget-mode control throughout the entire pipeline: budget-mode supervised
fine-tuning that embeds controllable reasoning capabilities directly into the
learning process, and two-phase budget-aware reinforcement learning with
adaptive reward shaping. Extensive experiments demonstrate that ThinkDial
achieves target compression-performance trade-offs with clear response length
reductions while maintaining performance thresholds. The framework also
exhibits strong generalization capabilities on out-of-distribution tasks.

</details>


### [30] [Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction](https://arxiv.org/abs/2508.18780)
*Yilin Li,Xunjian Yin,Yilin Chen,Xiaojun Wan*

Main category: cs.CL

TL;DR: 基于规则强化学习的新框架在中文语法错误编正任务中达到最先进性能，显著提升了可叐回率


<details>
  <summary>Details</summary>
Motivation: 传统的直接生成模型限制了大语言模型的推理能力，需要更好地利用RL来导向LLMs以获得更可控和可靠的方案

Method: 提出了一种基于规则强化学习（Rule-Based RL）的新框架，通过强化学习方式来导向大语言模型进行语法错误编正

Result: 在中文数据集上达到了state-of-the-art性能，尤其是可叐回率（recall）有显著提升

Conclusion: 强化学习方法能够更好地发挥大语言模型的推理能力，为语法错误编正领域提供了更可控和可靠的发展方向

Abstract: Grammatical error correction is a significant task in NLP. Traditional
methods based on encoder-decoder models have achieved certain success, but the
application of LLMs in this field is still underexplored. Current research
predominantly relies on supervised fine-tuning to train LLMs to directly
generate the corrected sentence, which limits the model's powerful reasoning
ability. To address this limitation, we propose a novel framework based on
Rule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL
framework achieves \textbf{state-of-the-art }performance, with a notable
increase in \textbf{recall}. This result clearly highlights the advantages of
using RL to steer LLMs, offering a more controllable and reliable paradigm for
future development in GEC.

</details>


### [31] [Controllable Conversational Theme Detection Track at DSTC 12](https://arxiv.org/abs/2508.18783)
*Igor Shalyminov,Hang Su,Jake Vincent,Siffi Singh,Jason Cai,James Gung,Raphael Shu,Saab Mansour*

Main category: cs.CL

TL;DR: 本文介绍了对话分析中的主题检测任务，作为DSTC 12的竞赛赛道，旨在通过可控聚类方法自动识别和分类对话主题，减少人工分析工作量。


<details>
  <summary>Details</summary>
Motivation: 随着语音和自然语言处理技术的发展，对话分析领域需要自动化处理更复杂和规模更大的问题。传统对话意图检测依赖固定意图集，而主题检测提供更灵活的用户面向对话摘要。

Method: 提出可控对话主题检测问题，采用联合聚类和主题标注的方法，通过用户偏好数据控制主题簇的粒度。

Result: 在DSTC 12竞赛中设立了公开赛道，提供了相关数据集和评估指标（自动和人工），并分析了参赛团队的提交结果。

Conclusion: 主题检测是对话分析中的关键任务，可控聚类方法能够有效满足用户特定的粒度需求，相关材料和代码已在GitHub开源。

Abstract: Conversational analytics has been on the forefront of transformation driven
by the advances in Speech and Natural Language Processing techniques. Rapid
adoption of Large Language Models (LLMs) in the analytics field has taken the
problems that can be automated to a new level of complexity and scale. In this
paper, we introduce Theme Detection as a critical task in conversational
analytics, aimed at automatically identifying and categorizing topics within
conversations. This process can significantly reduce the manual effort involved
in analyzing expansive dialogs, particularly in domains like customer support
or sales. Unlike traditional dialog intent detection, which often relies on a
fixed set of intents for downstream system logic, themes are intended as a
direct, user-facing summary of the conversation's core inquiry. This
distinction allows for greater flexibility in theme surface forms and
user-specific customizations. We pose Controllable Conversational Theme
Detection problem as a public competition track at Dialog System Technology
Challenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of
dialog utterances, with the distinctive aspect being controllability of the
resulting theme clusters' granularity achieved via the provided user preference
data. We give an overview of the problem, the associated dataset and the
evaluation metrics, both automatic and human. Finally, we discuss the
participant teams' submissions and provide insights from those. The track
materials (data and code) are openly available in the GitHub repository.

</details>


### [32] [LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination](https://arxiv.org/abs/2508.18791)
*Ziming Zhu,Chenglong Wang,Shunjie Xing,Yifu Huo,Fengning Tian,Quan Du,Di Yang,Chunliang Zhang,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: LaTeXTrans是一个多智能体系统，专门用于翻译LaTeX格式文档，通过六个专业代理确保格式保持、结构保真和术语一致性，在翻译准确性和结构保真度方面优于主流机器翻译系统。


<details>
  <summary>Details</summary>
Motivation: 现代机器翻译系统在通用文本上取得了显著进展，但在翻译结构化LaTeX格式文档时仍面临重大挑战。这些文档通常交织自然语言和领域特定语法（如数学公式、表格、图表和交叉引用），需要准确保持以维护语义完整性和可编译性。

Method: LaTeXTrans采用协作式多智能体系统，包含六个专门代理：1)解析器通过占位符替换和语法过滤将LaTeX分解为翻译友好单元；2)翻译器、验证器、总结器和术语提取器协作确保上下文感知、自我纠正和术语一致的翻译；3)生成器将翻译内容重构为结构良好的LaTeX文档。

Result: 实验结果表明，LaTeXTrans在翻译准确性和结构保真度方面能够超越主流机器翻译系统。

Conclusion: LaTeXTrans为翻译LaTeX格式文档提供了一个有效且实用的解决方案，解决了结构化文档翻译中的格式保持和语义完整性挑战。

Abstract: Despite the remarkable progress of modern machine translation (MT) systems on
general-domain texts, translating structured LaTeX-formatted documents remains
a significant challenge. These documents typically interleave natural language
with domain-specific syntax, such as mathematical equations, tables, figures,
and cross-references, all of which must be accurately preserved to maintain
semantic integrity and compilability. In this paper, we introduce LaTeXTrans, a
collaborative multi-agent system designed to address this challenge. LaTeXTrans
ensures format preservation, structural fidelity, and terminology consistency
through six specialized agents: 1) a Parser that decomposes LaTeX into
translation-friendly units via placeholder substitution and syntax filtering;
2) a Translator, Validator, Summarizer, and Terminology Extractor that work
collaboratively to ensure context-aware, self-correcting, and
terminology-consistent translations; 3) a Generator that reconstructs the
translated content into well-structured LaTeX documents. Experimental results
demonstrate that LaTeXTrans can outperform mainstream MT systems in both
translation accuracy and structural fidelity, offering an effective and
practical solution for translating LaTeX-formatted documents.

</details>


### [33] [LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection](https://arxiv.org/abs/2508.18819)
*Shubham Gupta,Shraban Kumar Chatterjee,Suman Kundu*

Main category: cs.CL

TL;DR: 提出了一种结合语义关系和传播动态的自监督虚假信息检测框架，使用LLM生成对比损失和多重图掩码自编码器，在零样本和有限标注数据下实现优异性能


<details>
  <summary>Details</summary>
Motivation: 数字时代虚假信息泛滥带来重大社会挑战，现有方法难以捕获长距离依赖、复杂语义关系和新闻传播的社会动态，且需要大量标注数据导致部署成本高

Method: 提出自监督虚假信息检测框架：1) 使用抽象意义表示(AMR)捕获复杂语义关系；2) 基于LLM的图对比损失(LGCL)利用大语言模型生成负锚点增强特征可分性；3) 多重图掩码自编码器从社交上下文图中学习新闻传播特征

Result: 大量实验表明，该自监督框架在性能上优于其他最先进方法，即使在有限标注数据集下也能实现优异表现，同时提高了泛化能力

Conclusion: 该研究成功整合了语义和传播特征，提出了一种有效的自监督虚假信息检测方法，解决了现有方法对标注数据的依赖问题，在零样本和有限数据场景下表现出色

Abstract: The proliferation of misinformation in the digital age has led to significant
societal challenges. Existing approaches often struggle with capturing
long-range dependencies, complex semantic relations, and the social dynamics
influencing news dissemination. Furthermore, these methods require extensive
labelled datasets, making their deployment resource-intensive. In this study,
we propose a novel self-supervised misinformation detection framework that
integrates both complex semantic relations using Abstract Meaning
Representation (AMR) and news propagation dynamics. We introduce an LLM-based
graph contrastive loss (LGCL) that utilizes negative anchor points generated by
a Large Language Model (LLM) to enhance feature separability in a zero-shot
manner. To incorporate social context, we employ a multi view graph masked
autoencoder, which learns news propagation features from social context graph.
By combining these semantic and propagation-based features, our approach
effectively differentiates between fake and real news in a self-supervised
manner. Extensive experiments demonstrate that our self-supervised framework
achieves superior performance compared to other state-of-the-art methodologies,
even with limited labelled datasets while improving generalizability.

</details>


### [34] [Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness](https://arxiv.org/abs/2508.18824)
*Sirui Chen,Changxin Tian,Binbin Hu,Kunlong Chen,Ziqi Liu,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 通过程序辅助合成框架生成120万高质量数学训练数据，提升大语言模型的数学推理能力


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在生成高质量数学训练数据时遇到的可扩展性、成本和数据可靠性问题

Method: 程序辅助合成框架，整合数学知识系统和领域特定工具生成可执行程序，通过双向验证机制确保解题正确性和程序-问题一致性

Result: 生成了12.3百万个问题-解决三元组，微调后的模型在多个标准数据集上达到最先进性能

Conclusion: 该合成框架能够生成高质量、多样化的数学训练数据，有效提升LLM的数学推理能力

Abstract: Enhancing the mathematical reasoning of large language models (LLMs) demands
high-quality training data, yet conventional methods face critical challenges
in scalability, cost, and data reliability. To address these limitations, we
propose a novel program-assisted synthesis framework that systematically
generates a high-quality mathematical corpus with guaranteed diversity,
complexity, and correctness. This framework integrates mathematical knowledge
systems and domain-specific tools to create executable programs. These programs
are then translated into natural language problem-solution pairs and vetted by
a bilateral validation mechanism that verifies solution correctness against
program outputs and ensures program-problem consistency. We have generated 12.3
million such problem-solving triples. Experiments demonstrate that models
fine-tuned on our data significantly improve their inference capabilities,
achieving state-of-the-art performance on several benchmark datasets and
showcasing the effectiveness of our synthesis approach.

</details>


### [35] [ConfTuner: Training Large Language Models to Express Their Confidence Verbally](https://arxiv.org/abs/2508.18847)
*Yibo Li,Miao Xiong,Jiaying Wu,Bryan Hooi*

Main category: cs.CL

TL;DR: ConfTuner是一种通过tokenized Brier score损失函数微调LLM的新方法，能够有效校准大语言模型的置信度表达，解决过度自信问题，无需真实置信度标签。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在科学、法律、医疗等高风险领域部署时存在过度自信问题，生成错误答案时仍表现出高置信度，现有校准方法效果有限且泛化性差。

Method: 提出ConfTuner微调方法，使用理论证明为proper scoring rule的tokenized Brier score损失函数，无需真实置信度分数或代理置信度估计。

Result: 在多种推理任务上显著改善校准效果，并能泛化到GPT-4o等黑盒模型，更好的校准置信度带来了自校正和模型级联等下游性能提升。

Conclusion: ConfTuner推进了可信赖LLM系统的发展，通过简单高效的微调方法有效解决了LLM置信度校准问题，代码已开源。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes domains
such as science, law, and healthcare, where accurate expressions of uncertainty
are essential for reliability and trust. However, current LLMs are often
observed to generate incorrect answers with high confidence, a phenomenon known
as "overconfidence". Recent efforts have focused on calibrating LLMs'
verbalized confidence: i.e., their expressions of confidence in text form, such
as "I am 80% confident that...". Existing approaches either rely on prompt
engineering or fine-tuning with heuristically generated uncertainty estimates,
both of which have limited effectiveness and generalizability. Motivated by the
notion of proper scoring rules for calibration in classical machine learning
models, we introduce ConfTuner, a simple and efficient fine-tuning method that
introduces minimal overhead and does not require ground-truth confidence scores
or proxy confidence estimates. ConfTuner relies on a new loss function,
tokenized Brier score, which we theoretically prove to be a proper scoring
rule, intuitively meaning that it "correctly incentivizes the model to report
its true probability of being correct". ConfTuner improves calibration across
diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our
results further show that better-calibrated confidence enables downstream gains
in self-correction and model cascade, advancing the development of trustworthy
LLM systems. The code is available at
https://github.com/liushiliushi/ConfTuner.

</details>


### [36] [ReflectivePrompt: Reflective evolution in autoprompting algorithms](https://arxiv.org/abs/2508.18870)
*Viktor N. Zhuravlev,Artur R. Khairullin,Ernest A. Dyagin,Alena N. Sitkina,Nikita I. Kulin*

Main category: cs.CL

TL;DR: ReflectivePrompt是一种基于进化算法的自动提示方法，通过短期和长期反思操作来优化提示选择，在33个数据集上相比现有方法平均提升28%性能


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，自动提示选择技术变得越来越重要。现有方法在搜索最优提示方面仍有改进空间，需要更精确和全面的搜索策略

Method: 基于进化算法的自反进化方法，在交叉和精英变异前使用短期和长期反思操作来提升修改质量。该方法能够积累进化过程中获得的知识并在每个epoch基于当前种群进行更新

Result: 在33个分类和文本生成数据集上测试，使用t-lite-instruct-0.1和gemma3-27b-it模型，相比最先进方法平均显著提升28%（在BBH数据集上）

Conclusion: ReflectivePrompt成为基于进化算法的自动提示方法中最有效的解决方案之一，通过反思机制显著提升了提示优化的效果

Abstract: Autoprompting is the process of automatically selecting optimized prompts for
language models, which has been gaining popularity with the rapid advancement
of prompt engineering, driven by extensive research in the field of large
language models (LLMs). This paper presents ReflectivePrompt - a novel
autoprompting method based on evolutionary algorithms that employs a reflective
evolution approach for more precise and comprehensive search of optimal
prompts. ReflectivePrompt utilizes short-term and long-term reflection
operations before crossover and elitist mutation to enhance the quality of the
modifications they introduce. This method allows for the accumulation of
knowledge obtained throughout the evolution process and updates it at each
epoch based on the current population. ReflectivePrompt was tested on 33
datasets for classification and text generation tasks using open-access large
language models: t-lite-instruct-0.1 and gemma3-27b-it. The method
demonstrates, on average, a significant improvement (e.g., 28% on BBH compared
to EvoPrompt) in metrics relative to current state-of-the-art approaches,
thereby establishing itself as one of the most effective solutions in
evolutionary algorithm-based autoprompting.

</details>


### [37] [Empowering Computing Education Researchers Through LLM-Assisted Content Analysis](https://arxiv.org/abs/2508.18872)
*Laurie Gale,Sebastian Mateos Nicolajsen*

Main category: cs.CL

TL;DR: 本文提出了一种LLM辅助内容分析(LACA)方法，用于处理计算教育研究中的大规模文本数据，帮助研究者进行更严谨和可推广的研究。


<details>
  <summary>Details</summary>
Motivation: 计算教育研究(CER)领域的研究者往往缺乏资源进行大规模严谨研究，需要能够处理大量定性数据而不增加研究负担的方法。

Method: 提出LLM辅助内容分析(LACA)方法，结合内容分析和大型语言模型，使研究者能够进行原本无法完成的大规模研究。

Result: 通过计算教育数据集展示了LACA方法的应用，证明该方法可以在可复现和严谨的方式下处理大规模文本数据。

Conclusion: LACA方法在CER领域具有重要潜力，能够从更广泛的研究中获得更可推广的发现，有助于提升CER学科的研究质量和实践水平。

Abstract: Computing education research (CER) is often instigated by practitioners
wanting to improve both their own and the wider discipline's teaching practice.
However, the latter is often difficult as many researchers lack the colleagues,
resources, or capacity to conduct research that is generalisable or rigorous
enough to advance the discipline. As a result, research methods that enable
sense-making with larger volumes of qualitative data, while not increasing the
burden on the researcher, have significant potential within CER.
  In this discussion paper, we propose such a method for conducting rigorous
analysis on large volumes of textual data, namely a variation of LLM-assisted
content analysis (LACA). This method combines content analysis with the use of
large language models, empowering researchers to conduct larger-scale research
which they would otherwise not be able to perform. Using a computing education
dataset, we illustrate how LACA could be applied in a reproducible and rigorous
manner. We believe this method has potential in CER, enabling more
generalisable findings from a wider range of research. This, together with the
development of similar methods, can help to advance both the practice and
research quality of the CER discipline.

</details>


### [38] [Affective Polarization across European Parliaments](https://arxiv.org/abs/2508.18916)
*Bojan Evkoski,Igor Mozetič,Nikola Ljubešić,Petra Kralj Novak*

Main category: cs.CL

TL;DR: 使用自然语言处理技术分析欧洲六国议会演讲，发现普遍存在情感极化现象，即议员对反对派表现出更多负面情绪，且互惠机制是情感极化的重要驱动因素


<details>
  <summary>Details</summary>
Motivation: 研究情感极化（对反对群体表现出负面情绪和敌意）在欧洲议会中的存在情况，这种极化现象已成为全球政治话语的显著特征

Method: 收集六国欧洲议会的演讲语料库，运用自然语言处理技术估计议员情感，通过比较对反对派和己方人士的负面情绪水平来识别情感极化模式

Result: 在所有六个欧洲议会中都发现了一致的情感极化现象；活动程度与负面情绪相关，但活跃度不同的议员之间在情感极化方面没有差异；互惠机制是跨议会情感极化的贡献机制

Conclusion: 欧洲议会普遍存在情感极化，互惠行为是推动这种极化的重要机制，研究为理解政治环境中的情感动态提供了重要见解

Abstract: Affective polarization, characterized by increased negativity and hostility
towards opposing groups, has become a prominent feature of political discourse
worldwide. Our study examines the presence of this type of polarization in a
selection of European parliaments in a fully automated manner. Utilizing a
comprehensive corpus of parliamentary speeches from the parliaments of six
European countries, we employ natural language processing techniques to
estimate parliamentarian sentiment. By comparing the levels of negativity
conveyed in references to individuals from opposing groups versus one's own, we
discover patterns of affectively polarized interactions. The findings
demonstrate the existence of consistent affective polarization across all six
European parliaments. Although activity correlates with negativity, there is no
observed difference in affective polarization between less active and more
active members of parliament. Finally, we show that reciprocity is a
contributing mechanism in affective polarization between parliamentarians
across all six parliaments.

</details>


### [39] [Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework](https://arxiv.org/abs/2508.18929)
*Ilias Driouich,Hongliu Cao,Eoin Thomas*

Main category: cs.CL

TL;DR: 提出了一种新的多段代理框架，用于生成具有语义多样性和隐私保护的合成QA数据集，以改善RAG系统的评估质量。


<details>
  <summary>Details</summary>
Motivation: 目前RAG系统评估主要集中在性能指标上，对基础评估数据集的设计和质量关注不够，尤其缺乏对隐私保护等实际约束的考虑。

Method: 设计了一个多段代理框架：(1)多样性段代理利用聚类技术完善主题覆盖和语义变化性；(2)隐私段代理检测和隐藏多领域敏感信息；(3)QA精选段代理合成具有隐私保护和多样性的QA对。

Result: 大量实验表明，该方法生成的评估集在多样性方面超过基准方法，并在领域特定数据集上实现了稳健的隐私隐藏效果。

Conclusion: 该工作提供了一条实用且符合道德规范的路径，实现更安全、更全面的RAG系统评估，为未来人工智能监管和遵循标准的发展奠定基础。

Abstract: Retrieval-augmented generation (RAG) systems improve large language model
outputs by incorporating external knowledge, enabling more informed and
context-aware responses. However, the effectiveness and trustworthiness of
these systems critically depends on how they are evaluated, particularly on
whether the evaluation process captures real-world constraints like protecting
sensitive information. While current evaluation efforts for RAG systems have
primarily focused on the development of performance metrics, far less attention
has been given to the design and quality of the underlying evaluation datasets,
despite their pivotal role in enabling meaningful, reliable assessments. In
this work, we introduce a novel multi-agent framework for generating synthetic
QA datasets for RAG evaluation that prioritize semantic diversity and privacy
preservation. Our approach involves: (1) a Diversity agent leveraging
clustering techniques to maximize topical coverage and semantic variability,
(2) a Privacy Agent that detects and mask sensitive information across multiple
domains and (3) a QA curation agent that synthesizes private and diverse QA
pairs suitable as ground truth for RAG evaluation. Extensive experiments
demonstrate that our evaluation sets outperform baseline methods in diversity
and achieve robust privacy masking on domain-specific datasets. This work
offers a practical and ethically aligned pathway toward safer, more
comprehensive RAG system evaluation, laying the foundation for future
enhancements aligned with evolving AI regulations and compliance standards.

</details>


### [40] [Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models](https://arxiv.org/abs/2508.18988)
*Hung Ming Liu*

Main category: cs.CL

TL;DR: 提出了AI母语框架，让神经网络模型发展原生符号语言，同时支持直觉推理、组合符号链和内在可解释性，无需后处理解释方法


<details>
  <summary>Details</summary>
Motivation: 现有神经网络缺乏内在可解释性和符号推理能力，需要一种能够将推理直接嵌入模型表示的方法，实现透明且灵活的可解释AI

Method: 通过符号捕获语义模式、符号链追踪决策路径、门控归纳机制引导选择性关注；引入互补训练目标增强符号纯度和决策稀疏性；采用顺序专业化策略先建立广泛符号能力再精炼直觉判断

Result: 在AI任务上展示了具有竞争力的准确性，同时提供可验证的推理轨迹

Conclusion: AI母语可以作为神经网络中可解释性、直觉和符号推理的统一机制

Abstract: We present a framework where neural models develop an AI Mother Tongue, a
native symbolic language that simultaneously supports intuitive reasoning,
compositional symbol chains, and inherent interpretability. Unlike post-hoc
explanation methods, our approach embeds reasoning directly into the model's
representations: symbols capture meaningful semantic patterns, chains trace
decision paths, and gated induction mechanisms guide selective focus, yielding
transparent yet flexible reasoning. We introduce complementary training
objectives to enhance symbol purity and decision sparsity, and employ a
sequential specialization strategy to first build broad symbolic competence and
then refine intuitive judgments. Experiments on AI tasks demonstrate
competitive accuracy alongside verifiable reasoning traces, showing that AI
Mother Tongue can serve as a unified mechanism for interpretability, intuition,
and symbolic reasoning in neural models.

</details>


### [41] [Automatic Prompt Optimization with Prompt Distillation](https://arxiv.org/abs/2508.18992)
*Viktor N. Zhuravlev,Artur R. Khairullin,Ernest A. Dyagin,Alena N. Sitkina,Nikita I. Kulin*

Main category: cs.CL

TL;DR: DistillPrompt是一种基于大语言模型的新型自动提示方法，通过多阶段集成任务特定信息到提示中，在文本分类和生成任务上相比现有方法有显著提升


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型研究的快速发展，自动选择优化提示的需求日益增长，需要更有效的自动提示方法来提升语言模型性能

Method: 采用蒸馏、压缩和聚合操作的多阶段集成方法，将任务特定信息整合到提示中，更全面地探索提示空间

Result: 在文本分类和生成任务的不同数据集上测试，相比现有方法（如Grips）在关键指标上平均提升20.12%

Conclusion: DistillPrompt被证明是自动提示领域中最有效的非梯度方法之一

Abstract: Autoprompting is the process of automatically selecting optimized prompts for
language models, which is gaining popularity due to the rapid development of
prompt engineering driven by extensive research in the field of large language
models (LLMs). This paper presents DistillPrompt -- a novel autoprompting
method based on large language models that employs a multi-stage integration of
task-specific information into prompts using training data. DistillPrompt
utilizes distillation, compression, and aggregation operations to explore the
prompt space more thoroughly. The method was tested on different datasets for
text classification and generation tasks using the t-lite-instruct-0.1 language
model. The results demonstrate a significant average improvement (e.g., 20.12%
across the entire dataset compared to Grips) in key metrics over existing
methods in the field, establishing DistillPrompt as one of the most effective
non-gradient approaches in autoprompting.

</details>


### [42] [MovieCORE: COgnitive REasoning in Movies](https://arxiv.org/abs/2508.19026)
*Gueter Josmy Faure,Min-Hung Chen,Jia-Fong Yeh,Ying Cheng,Hung-Ting Su,Yung-Hao Tang,Shang-Hong Lai,Winston H. Hsu*

Main category: cs.CL

TL;DR: MovieCORE是一个新颖的视频问答数据集，专注于电影内容的深层认知理解，通过多智能体头脑风暴方法生成高质量问题，并提出ACE模块提升模型推理能力25%。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答数据集主要关注表层理解，缺乏对电影内容深层认知理解的评估，需要开发能够激发系统2思维的高质量数据集。

Method: 采用多大型语言模型作为思考智能体进行头脑风暴，生成和精炼高质量问答对；开发认知测试评估数据集质量；提出Agentic Choice Enhancement (ACE)模块增强模型推理能力。

Result: 创建了MovieCORE数据集，包含深度认知问题；ACE模块使模型推理能力提升高达25%；为评估VQA模型在深层认知任务上的性能提供了全面方案。

Conclusion: 该工作推动了AI系统对电影理解的发展，揭示了当前VQA模型在处理具有挑战性的电影内容问题时的能力和局限，为未来研究提供了宝贵资源。

Abstract: This paper introduces MovieCORE, a novel video question answering (VQA)
dataset designed to probe deeper cognitive understanding of movie content.
Unlike existing datasets that focus on surface-level comprehension, MovieCORE
emphasizes questions that engage System-2 thinking while remaining specific to
the video material. We present an innovative agentic brainstorming approach,
utilizing multiple large language models (LLMs) as thought agents to generate
and refine high-quality question-answer pairs. To evaluate dataset quality, we
develop a set of cognitive tests assessing depth, thought-provocation
potential, and syntactic complexity. We also propose a comprehensive evaluation
scheme for assessing VQA model performance on deeper cognitive tasks. To
address the limitations of existing video-language models (VLMs), we introduce
an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves
model reasoning capabilities post-training by up to 25%. Our work contributes
to advancing movie understanding in AI systems and provides valuable insights
into the capabilities and limitations of current VQA models when faced with
more challenging, nuanced questions about cinematic content. Our project page,
dataset and code can be found at
https://joslefaure.github.io/assets/html/moviecore.html.

</details>


### [43] [HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance](https://arxiv.org/abs/2508.19076)
*Ziyue Li,Yuan Chang,Gaihong Yu,Xiaoqiu Le*

Main category: cs.CL

TL;DR: HiPlan是一个分层规划框架，通过全局-局部指导提升LLM智能体的决策能力，在复杂长时程规划任务中显著优于现有基线方法


<details>
  <summary>Details</summary>
Motivation: LLM智能体在复杂长时程规划场景中缺乏宏观指导和持续监督，容易迷失方向和偏离目标

Method: 分层规划框架：离线阶段构建里程碑库重用专家经验，执行阶段动态生成逐步提示来对齐观测与目标

Result: 在两个挑战性基准测试中大幅超越强基线方法，消融研究验证了分层组件的互补效益

Conclusion: HiPlan通过分层规划有效解决了LLM智能体在复杂任务中的导向和监督问题，为智能体决策提供了新思路

Abstract: Large language model (LLM)-based agents have demonstrated remarkable
capabilities in decision-making tasks, but struggle significantly with complex,
long-horizon planning scenarios. This arises from their lack of macroscopic
guidance, causing disorientation and failures in complex tasks, as well as
insufficient continuous oversight during execution, rendering them unresponsive
to environmental changes and prone to deviations. To tackle these challenges,
we introduce HiPlan, a hierarchical planning framework that provides adaptive
global-local guidance to boost LLM-based agents'decision-making. HiPlan
decomposes complex tasks into milestone action guides for general direction and
step-wise hints for detailed actions. During the offline phase, we construct a
milestone library from expert demonstrations, enabling structured experience
reuse by retrieving semantically similar tasks and milestones. In the execution
phase, trajectory segments from past milestones are dynamically adapted to
generate step-wise hints that align current observations with the milestone
objectives, bridging gaps and correcting deviations. Extensive experiments
across two challenging benchmarks demonstrate that HiPlan substantially
outperforms strong baselines, and ablation studies validate the complementary
benefits of its hierarchical components.

</details>


### [44] ["Where does it hurt?" -- Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues](https://arxiv.org/abs/2508.19077)
*Tom Röhr,Soumyadeep Roy,Fares Al Mohamad,Jens-Michalis Papaioannou,Wolfgang Nejdl,Felix Gers,Alexander Löser*

Main category: cs.CL

TL;DR: 本研究首次分析医患对话中的医生意图轨迹，开发了基于SOAP框架的细粒度意图分类法，构建了大规模标注数据集，并评估了最先进的医疗意图分类模型。


<details>
  <summary>Details</summary>
Motivation: 医患对话中医生通过针对性提问来收集信息以进行诊断和治疗，但缺乏对医生意图轨迹的系统研究，需要开发专门的分类体系和评估方法。

Method: 使用ACI-bench数据集，与医学专家合作开发基于SOAP框架的细粒度意图分类法，通过众包平台招募医学专家标注5000多个医患对话轮次，评估生成式和编码器模型在医疗意图分类任务上的表现。

Result: 模型能够高精度理解医疗对话的一般结构，但在识别SOAP类别转换方面存在困难；首次报告了医疗对话结构的常见轨迹；意图过滤显著提升了医疗对话摘要性能。

Conclusion: 该研究为医疗对话分析提供了重要资源和见解，开发的分类体系和数据集有助于设计差分诊断系统，意图过滤技术可有效提升医疗对话摘要质量。

Abstract: In a doctor-patient dialogue, the primary objective of physicians is to
diagnose patients and propose a treatment plan. Medical doctors guide these
conversations through targeted questioning to efficiently gather the
information required to provide the best possible outcomes for patients. To the
best of our knowledge, this is the first work that studies physician intent
trajectories in doctor-patient dialogues. We use the `Ambient Clinical
Intelligence Benchmark' (Aci-bench) dataset for our study. We collaborate with
medical professionals to develop a fine-grained taxonomy of physician intents
based on the SOAP framework (Subjective, Objective, Assessment, and Plan). We
then conduct a large-scale annotation effort to label over 5000 doctor-patient
turns with the help of a large number of medical experts recruited using
Prolific, a popular crowd-sourcing platform. This large labeled dataset is an
important resource contribution that we use for benchmarking the
state-of-the-art generative and encoder models for medical intent
classification tasks. Our findings show that our models understand the general
structure of medical dialogues with high accuracy, but often fail to identify
transitions between SOAP categories. We also report for the first time common
trajectories in medical dialogue structures that provide valuable insights for
designing `differential diagnosis' systems. Finally, we extensively study the
impact of intent filtering for medical dialogue summarization and observe a
significant boost in performance. We make the codes and data, including
annotation guidelines, publicly available at
https://github.com/DATEXIS/medical-intent-classification.

</details>


### [45] [It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs](https://arxiv.org/abs/2508.19089)
*Yue Li,Zhixue Zhao,Carolina Scarton*

Main category: cs.CL

TL;DR: 本文首次全面分析了大语言模型能否通过上下文学习（ICL）来获取极低资源语言，比较了ICL与参数高效微调（PEFT）的效果。研究发现ICL在极低资源语言上表现优异，而PEFT在语言和文字都极度缺乏的情况下效果有限。


<details>
  <summary>Details</summary>
Motivation: 极低资源语言，特别是使用罕见文字的语言，在大语言模型中缺乏支持。主要原因是训练数据匮乏等复合因素。需要探索LLM是否能够通过上下文学习来获取这些语言。

Method: 系统评估了20种代表性不足的语言，在三个最先进的多语言LLM上测试了零样本ICL、少样本ICL和PEFT方法，比较了有无辅助对齐信号的效果。

Result: 当语言及其文字都极度缺乏时，PEFT效果有限。零样本ICL配合语言对齐在极低资源语言上效果显著，而少样本ICL或PEFT对相对较好的语言更有效。

Conclusion: 为LLM从业者提供了基于结果的指导原则：避免在多语言模型上对未见文字的语言进行微调，ICL是处理极低资源语言的更有效方法。

Abstract: Extremely low-resource languages, especially those written in rare scripts,
as shown in Figure 1, remain largely unsupported by large language models
(LLMs). This is due in part to compounding factors such as the lack of training
data. This paper delivers the first comprehensive analysis of whether LLMs can
acquire such languages purely via in-context learning (ICL), with or without
auxiliary alignment signals, and how these methods compare to
parameter-efficient fine-tuning (PEFT). We systematically evaluate 20
under-represented languages across three state-of-the-art multilingual LLMs.
Our findings highlight the limitation of PEFT when both language and its script
are extremely under-represented by the LLM. In contrast, zero-shot ICL with
language alignment is impressively effective on extremely low-resource
languages, while few-shot ICL or PEFT is more beneficial for languages
relatively better represented by LLMs. For LLM practitioners working on
extremely low-resource languages, we summarise guidelines grounded by our
results on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning
a multilingual model on languages of unseen scripts.

</details>


### [46] [Retrieval-Augmented Generation for Natural Language Art Provenance Searches in the Getty Provenance Index](https://arxiv.org/abs/2508.19093)
*Mathew Henrickson*

Main category: cs.CL

TL;DR: 基于检索增强生成的方法，为艺术品来源研究提供多语言自然语言检索和语义摘要功能，解决档案数据分散性问题


<details>
  <summary>Details</summary>
Motivation: 艺术品来源研究对验证真伪、支持还原和法律纳预重要，但现有检索工具依赖精确元数据，限制了探索性检索

Method: 采用检索增强生成(RAG)框架，通过语义检索和上下文摘要实现自然语言和多语言检索，减少对元数据结构的依赖

Result: 在Getty来源索引-德国销售数据库的10,000条记录上评估，证明该方法能够有效检索和摘要拍卖记录

Conclusion: 该方法为导航艺术市场档案提供了可扩展的解决方案，为历史学家和文化遗产专业人员提供了实用工具

Abstract: This research presents a Retrieval-Augmented Generation (RAG) framework for
art provenance studies, focusing on the Getty Provenance Index. Provenance
research establishes the ownership history of artworks, which is essential for
verifying authenticity, supporting restitution and legal claims, and
understanding the cultural and historical context of art objects. The process
is complicated by fragmented, multilingual archival data that hinders efficient
retrieval. Current search portals require precise metadata, limiting
exploratory searches. Our method enables natural-language and multilingual
searches through semantic retrieval and contextual summarization, reducing
dependence on metadata structures. We assess RAG's capability to retrieve and
summarize auction records using a 10,000-record sample from the Getty
Provenance Index - German Sales. The results show this approach provides a
scalable solution for navigating art market archives, offering a practical tool
for historians and cultural heritage professionals conducting historically
sensitive research.

</details>


### [47] [Beyond the Black Box: Integrating Lexical and Semantic Methods in Quantitative Discourse Analysis with BERTopic](https://arxiv.org/abs/2508.19099)
*Thomas Compton*

Main category: cs.CL

TL;DR: 提出一个结合词汇和语义方法的透明QDA框架，使用Python工具链实现可复现、可解释的混合分析流程


<details>
  <summary>Details</summary>
Motivation: 解决当前定量话语分析中黑盒软件（如MAXQDA、NVivo）导致的方法透明性不足和研究目标对齐问题

Method: 使用Python工具链（NLTK、spaCy、Sentence Transformers）构建自定义处理流程，结合BERTopic建模（UMAP降维、HDBSCAN聚类、c-TF-IDF关键词提取）进行迭代优化

Result: 通过历史政治话语案例研究，展示了词汇精确搜索与语义聚类相结合的多层次分析方法，提高了主题一致性和覆盖度

Conclusion: 强调代码级透明度、研究者自主权和方法三角验证在计算话语研究中的重要性，提倡混合方法以克服单一方法的局限性

Abstract: Quantitative Discourse Analysis has seen growing adoption with the rise of
Large Language Models and computational tools. However, reliance on black box
software such as MAXQDA and NVivo risks undermining methodological transparency
and alignment with research goals. This paper presents a hybrid, transparent
framework for QDA that combines lexical and semantic methods to enable
triangulation, reproducibility, and interpretability. Drawing from a case study
in historical political discourse, we demonstrate how custom Python pipelines
using NLTK, spaCy, and Sentence Transformers allow fine-grained control over
preprocessing, lemmatisation, and embedding generation. We further detail our
iterative BERTopic modelling process, incorporating UMAP dimensionality
reduction, HDBSCAN clustering, and c-TF-IDF keyword extraction, optimised
through parameter tuning and multiple runs to enhance topic coherence and
coverage. By juxtaposing precise lexical searches with context-aware semantic
clustering, we argue for a multi-layered approach that mitigates the
limitations of either method in isolation. Our workflow underscores the
importance of code-level transparency, researcher agency, and methodological
triangulation in computational discourse studies. Code and supplementary
materials are available via GitHub.

</details>


### [48] [Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs](https://arxiv.org/abs/2508.19111)
*Zhikai Ding,Shiyu Ni,Keping Bi*

Main category: cs.CL

TL;DR: 大型视觉语言模型在视觉问答中存在幻觉问题，本文研究其知识边界感知能力，评估三种置信度信号，发现概率和一致性置信度更可靠，并提出改进方法。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在视觉问答中表现出色但存在幻觉，需要可靠的模型来感知其知识边界，了解知道什么和不知道什么。

Method: 评估三种置信度信号：概率置信度、答案一致性置信度和语言化置信度，在三个LVLM模型和三个VQA数据集上进行实验，并采用LLM的置信度校准方法进行改进。

Result: LVLM具有一定的知识边界感知能力但仍有改进空间，概率和一致性置信度信号更可靠，语言化置信度容易导致过度自信。视觉和文本联合处理降低了问答性能但提高了感知水平。

Conclusion: LVLM的知识边界感知能力需要进一步提升，概率和一致性置信度是更可靠的指标，提出的校准方法能有效改善感知能力，多模态处理虽然降低性能但提高了感知准确性。

Abstract: Large vision-language models (LVLMs) demonstrate strong visual question
answering (VQA) capabilities but are shown to hallucinate. A reliable model
should perceive its knowledge boundaries-knowing what it knows and what it does
not. This paper investigates LVLMs' perception of their knowledge boundaries by
evaluating three types of confidence signals: probabilistic confidence, answer
consistency-based confidence, and verbalized confidence. Experiments on three
LVLMs across three VQA datasets show that, although LVLMs possess a reasonable
perception level, there is substantial room for improvement. Among the three
confidences, probabilistic and consistency-based signals are more reliable
indicators, while verbalized confidence often leads to overconfidence. To
enhance LVLMs' perception, we adapt several established confidence calibration
methods from Large Language Models (LLMs) and propose three effective methods.
Additionally, we compare LVLMs with their LLM counterparts, finding that
jointly processing visual and textual inputs decreases question-answering
performance but reduces confidence, resulting in an improved perception level
compared to LLMs.

</details>


### [49] [Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning](https://arxiv.org/abs/2508.19202)
*Alan Li,Yixin Liu,Arpan Sarkar,Doug Downey,Arman Cohan*

Main category: cs.CL

TL;DR: SciReas和SciReas-Pro科学推理评测套件，结合KRUX探测框架分析LLM在科学推理中知识与推理的作用，发现知识检索是瓶颈，外部知识能提升推理能力，增强推理表达能改善知识提取。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏全面评估科学推理的基准测试，且很少方法系统性地分析知识与推理在科学任务中的不同作用。

Method: 引入SciReas评测套件和SciReas-Pro子集，提出KRUX探测框架来分离知识与推理的作用，并进行深入分析。

Result: 发现知识检索是LLM科学推理的关键瓶颈；外部知识能进一步提升推理能力；增强推理表达能改善相关知识提取。

Conclusion: 科学推理需要同时关注知识获取和推理能力，提出了SciLit01作为8B参数的科学推理基线模型。

Abstract: Scientific problem solving poses unique challenges for LLMs, requiring both
deep domain knowledge and the ability to apply such knowledge through complex
reasoning. While automated scientific reasoners hold great promise for
assisting human scientists, there is currently no widely adopted holistic
benchmark for evaluating scientific reasoning, and few approaches
systematically disentangle the distinct roles of knowledge and reasoning in
these tasks. To address these gaps, we introduce SciReas, a diverse suite of
existing benchmarks for scientific reasoning tasks, and SciReas-Pro, a
selective subset that requires more complex reasoning. Our holistic evaluation
surfaces insights about scientific reasoning performance that remain hidden
when relying on individual benchmarks alone. We then propose KRUX, a probing
framework for studying the distinct roles of reasoning and knowledge in
scientific tasks. Combining the two, we conduct an in-depth analysis that
yields several key findings: (1) Retrieving task-relevant knowledge from model
parameters is a critical bottleneck for LLMs in scientific reasoning; (2)
Reasoning models consistently benefit from external knowledge added in-context
on top of the reasoning enhancement; (3) Enhancing verbalized reasoning
improves LLMs' ability to surface task-relevant knowledge. Finally, we conduct
a lightweight analysis, comparing our science-focused data composition with
concurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline
for scientific reasoning.

</details>


### [50] [Evaluating the Evaluators: Are readability metrics good measures of readability?](https://arxiv.org/abs/2508.19221)
*Isabel Cachola,Daniel Khashabi,Mark Dredze*

Main category: cs.CL

TL;DR: 本文研究发现传统可读性指标在普通语言摘要评估中与人类判断相关性差，而语言模型能更好地评估可读性，特别是在背景知识需求等深层维度上。


<details>
  <summary>Details</summary>
Motivation: 当前普通语言摘要领域使用传统可读性指标（如FKGL）作为标准评估方法，但这些指标尚未与人类可读性判断进行对比验证。

Method: 评估8种传统可读性指标与人类判断的相关性，并测试语言模型作为可读性评估工具的性能，分析其在普通语言摘要数据集上的表现。

Result: 大多数传统可读性指标与人类判断相关性差，最佳语言模型达到0.56的皮尔逊相关系数。语言模型能更好地捕捉背景知识需求等深层可读性维度。

Conclusion: 建议在普通语言摘要评估中使用语言模型替代传统可读性指标，并发布了分析代码和调查数据以供后续研究使用。

Abstract: Plain Language Summarization (PLS) aims to distill complex documents into
accessible summaries for non-expert audiences. In this paper, we conduct a
thorough survey of PLS literature, and identify that the current standard
practice for readability evaluation is to use traditional readability metrics,
such as Flesch-Kincaid Grade Level (FKGL). However, despite proven utility in
other fields, these metrics have not been compared to human readability
judgments in PLS. We evaluate 8 readability metrics and show that most
correlate poorly with human judgments, including the most popular metric, FKGL.
We then show that Language Models (LMs) are better judges of readability, with
the best-performing model achieving a Pearson correlation of 0.56 with human
judgments. Extending our analysis to PLS datasets, which contain summaries
aimed at non-expert audiences, we find that LMs better capture deeper measures
of readability, such as required background knowledge, and lead to different
conclusions than the traditional metrics. Based on these findings, we offer
recommendations for best practices in the evaluation of plain language
summaries. We release our analysis code and survey data.

</details>


### [51] [Generative Interfaces for Language Models](https://arxiv.org/abs/2508.19227)
*Jiaqi Chen,Yanzhe Zhang,Yutong Zhang,Yijia Shao,Diyi Yang*

Main category: cs.CL

TL;DR: 论文提出生成式界面范式，让大语言模型通过主动生成用户界面来响应查询，相比传统对话式界面在多项任务中表现更优，人类用户偏好度超过70%。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型系统受限于线性的请求-响应格式，在多轮、信息密集和探索性任务中效率低下，需要更自适应的交互方式。

Method: 提出生成式语言模型界面框架，利用结构化界面特定表示和迭代优化，将用户查询转换为任务特定的用户界面。

Result: 生成式界面在多样化任务、交互模式和查询类型中持续优于对话式界面，人类用户偏好度超过70%。

Conclusion: 研究阐明了用户偏好生成式界面的时机和原因，为未来人机交互的进步铺平了道路。

Abstract: Large language models (LLMs) are increasingly seen as assistants, copilots,
and consultants, capable of supporting a wide range of tasks through natural
conversation. However, most systems remain constrained by a linear
request-response format that often makes interactions inefficient in
multi-turn, information-dense, and exploratory tasks. To address these
limitations, we propose Generative Interfaces for Language Models, a paradigm
in which LLMs respond to user queries by proactively generating user interfaces
(UIs) that enable more adaptive and interactive engagement. Our framework
leverages structured interface-specific representations and iterative
refinements to translate user queries into task-specific UIs. For systematic
evaluation, we introduce a multidimensional assessment framework that compares
generative interfaces with traditional chat-based ones across diverse tasks,
interaction patterns, and query types, capturing functional, interactive, and
emotional aspects of user experience. Results show that generative interfaces
consistently outperform conversational ones, with humans preferring them in
over 70% of cases. These findings clarify when and why users favor generative
interfaces, paving the way for future advancements in human-AI interaction.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [52] [H-PRM: A Pluggable Hotword Pre-Retrieval Module for Various Speech Recognition Systems](https://arxiv.org/abs/2508.18295)
*Huangyu Dai,Lingtao Mao,Ben Chen,Zihan Wang,Zihan Liang,Ying Han,Chenyi Lei,Han Li*

Main category: cs.SD

TL;DR: 通过热词预检索模块(H-PRM)提升ASR中域特定热词识别准确度，解决大规模热词识别率下降问题


<details>
  <summary>Details</summary>
Motivation: 现有ASR模型在处理大规模热词时识别率显著下降，需要提升域特定术语的识别准确性

Method: 设计热词预检索模块(H-PRM)，通过声学相似性评估选出最相关热词候选，支持插入式集成到传统模型和通过提示方式集成到音频大语言模型

Result: 经过大量测试验证，H-PRM能够超越现有方法，显著提高热词后召回率(PRR)

Conclusion: H-PRM为ASR热词定制化提供了新方向，具有插入式便捷性和良好的扩展性

Abstract: Hotword customization is crucial in ASR to enhance the accuracy of
domain-specific terms. It has been primarily driven by the advancements in
traditional models and Audio large language models (LLMs). However, existing
models often struggle with large-scale hotwords, as the recognition rate drops
dramatically with the number of hotwords increasing. In this paper, we
introduce a novel hotword customization system that utilizes a hotword
pre-retrieval module (H-PRM) to identify the most relevant hotword candidate by
measuring the acoustic similarity between the hotwords and the speech segment.
This plug-and-play solution can be easily integrated into traditional models
such as SeACo-Paraformer, significantly enhancing hotwords post-recall rate
(PRR). Additionally, we incorporate H-PRM into Audio LLMs through a
prompt-based approach, enabling seamless customization of hotwords. Extensive
testing validates that H-PRM can outperform existing methods, showing a new
direction for hotword customization in ASR.

</details>


### [53] [SwiftF0: Fast and Accurate Monophonic Pitch Detection](https://arxiv.org/abs/2508.18440)
*Lars Nieradzik*

Main category: cs.SD

TL;DR: SwiftF0是一个轻量级神经网络模型，在嘈杂环境下实现实时单音高音估计，性能优于现有方法且计算效率高


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限设备上嘈杂环境下实时单音高音估计的挑战，以及现有语音语料库缺乏精确真实音高标注的问题

Method: 使用多样化语音、音乐和合成数据集进行训练，采用广泛数据增强，并引入SpeechSynth合成语音数据集提供精确真实音高曲线

Result: 在10dB SNR下达到91.80%的谐波平均值，比CREPE提升12个百分点，参数量仅95,842个，CPU运行速度比CREPE快42倍

Conclusion: SwiftF0在保持计算效率的同时实现了最先进的单音高音估计性能，适合实时部署，同时提供了开源基准测试套件和合成数据集

Abstract: Accurate and real-time monophonic pitch estimation in noisy conditions,
particularly on resource-constrained devices, remains an open challenge in
audio processing. We present \emph{SwiftF0}, a novel, lightweight neural model
that sets a new state-of-the-art for monophonic pitch estimation. Through
training on diverse speech, music, and synthetic datasets with extensive data
augmentation, SwiftF0 achieves robust generalization across acoustic domains
while maintaining computational efficiency. SwiftF0 achieves a 91.80\% harmonic
mean (HM) at 10 dB SNR, outperforming baselines like CREPE by over 12
percentage points and degrading by only 2.3 points from clean audio. SwiftF0
requires only 95,842 parameters and runs approximately 42x faster than CREPE on
CPU, making it ideal for efficient, real-time deployment. To address the
critical lack of perfectly accurate ground truth pitch in speech corpora (which
typically rely on algorithmic estimators or laryngograph signals), we introduce
\emph{SpeechSynth}. This synthetic speech dataset, generated by a phoneme-level
TTS model, provides exact, on-demand ground-truth pitch curves, enabling more
robust model training and evaluation. Furthermore, we propose a unified metric,
combining six complementary performance measures for comprehensive and reliable
pitch evaluation, and release an open-source pitch benchmark suite. A live demo
of SwiftF0 is available at https://swift-f0.github.io/, the source code at
https://github.com/lars76/swift-f0, and the benchmark framework at
https://github.com/lars76/pitch-benchmark.

</details>


### [54] [Cross-Learning Fine-Tuning Strategy for Dysarthric Speech Recognition Via CDSD database](https://arxiv.org/abs/2508.18732)
*Qing Xiao,Yingshan Peng,PeiPei Zhang*

Main category: cs.SD

TL;DR: 多语者精调比单语者精调更有效，能提升语音障碍语音识别的准确性


<details>
  <summary>Details</summary>
Motivation: 解决语音障碍语音识别中的严重程度变化和与正常语音的差异问题

Method: 采用多语者同时精调策略，在多个语音障碍语者上同时进行精细调整

Result: 实验结果显示多语者精调能够获得更低的词误率（WER），最高可降低13.15%

Conclusion: 多语者精调策略通过广泛的病理特征学习提升了模型的沿用性，减轻了语者特定过拟合，降低了对单个病人数据的依赖，并提高了目标语者的识别准确性

Abstract: Dysarthric speech recognition faces challenges from severity variations and
disparities relative to normal speech. Conventional approaches individually
fine-tune ASR models pre-trained on normal speech per patient to prevent
feature conflicts. Counter-intuitively, experiments reveal that multi-speaker
fine-tuning (simultaneously on multiple dysarthric speakers) improves
recognition of individual speech patterns. This strategy enhances
generalization via broader pathological feature learning, mitigates
speaker-specific overfitting, reduces per-patient data dependence, and improves
target-speaker accuracy - achieving up to 13.15% lower WER versus
single-speaker fine-tuning.

</details>


### [55] [SegReConcat: A Data Augmentation Method for Voice Anonymization Attack](https://arxiv.org/abs/2508.18907)
*Ridwan Arefeen,Xiaoxiao Miao,Rong Tong,Aik Beng Ng,Simon See*

Main category: cs.SD

TL;DR: SegReConcat是一种数据增强方法，通过在单词级别分割匿名化语音并重新排列片段，来增强说话人验证系统的攻击能力，提高去匿名化效果。


<details>
  <summary>Details</summary>
Motivation: 语音匿名化旨在隐藏说话人身份，但往往仍残留说话人线索，存在隐私风险。需要开发攻击者侧的方法来增强自动说话人验证系统的去匿名化能力。

Method: 提出SegReConcat方法：1）在单词级别分割匿名化语音；2）使用随机或基于相似性的策略重新排列片段以破坏长期上下文线索；3）将重新排列的片段与原始话语拼接，让攻击者能从多角度学习源说话人特征。

Result: 在VoicePrivacy Attacker Challenge 2024框架下评估了7个匿名化系统，SegReConcat在5个系统上显著提高了去匿名化性能。

Conclusion: SegReConcat是一种有效的攻击者侧数据增强方法，能够通过破坏语音的长期上下文结构来增强说话人验证系统的去匿名化能力，在多数匿名化系统上都表现出良好的效果。

Abstract: Anonymization of voice seeks to conceal the identity of the speaker while
maintaining the utility of speech data. However, residual speaker cues often
persist, which pose privacy risks. We propose SegReConcat, a data augmentation
method for attacker-side enhancement of automatic speaker verification systems.
SegReConcat segments anonymized speech at the word level, rearranges segments
using random or similarity-based strategies to disrupt long-term contextual
cues, and concatenates them with the original utterance, allowing an attacker
to learn source speaker traits from multiple perspectives. The proposed method
has been evaluated in the VoicePrivacy Attacker Challenge 2024 framework across
seven anonymization systems, SegReConcat improves de-anonymization on five out
of seven systems.

</details>
