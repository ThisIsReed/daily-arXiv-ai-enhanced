{"id": "2509.00053", "categories": ["cs.MM", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00053", "abs": "https://arxiv.org/abs/2509.00053", "authors": ["Shuo Liu", "Di Yao", "Yan Lin", "Gao Cong", "Jingping Bi"], "title": "Traj-MLLM: Can Multimodal Large Language Models Reform Trajectory Data Mining?", "comment": "20 pages, 10 figures", "summary": "Building a general model capable of analyzing human trajectories across\ndifferent geographic regions and different tasks becomes an emergent yet\nimportant problem for various applications. However, existing works suffer from\nthe generalization problem, \\ie, they are either restricted to train for\nspecific regions or only suitable for a few tasks. Given the recent advances of\nmultimodal large language models (MLLMs), we raise the question: can MLLMs\nreform current trajectory data mining and solve the problem? Nevertheless, due\nto the modality gap of trajectory, how to generate task-independent multimodal\ntrajectory representations and how to adapt flexibly to different tasks remain\nthe foundational challenges. In this paper, we propose \\texttt{Traj-MLLM}},\nwhich is the first general framework using MLLMs for trajectory data mining. By\nintegrating multiview contexts, \\texttt{Traj-MLLM}} transforms raw trajectories\ninto interleaved image-text sequences while preserving key spatial-temporal\ncharacteristics, and directly utilizes the reasoning ability of MLLMs for\ntrajectory analysis. Additionally, a prompt optimization method is proposed to\nfinalize data-invariant prompts for task adaptation. Extensive experiments on\nfour publicly available datasets show that \\texttt{Traj-MLLM}} outperforms\nstate-of-the-art baselines by $48.05\\%$, $15.52\\%$, $51.52\\%$, $1.83\\%$ on\ntravel time estimation, mobility prediction, anomaly detection and\ntransportation mode identification, respectively. \\texttt{Traj-MLLM}} achieves\nthese superior performances without requiring any training data or fine-tuning\nthe MLLM backbones.", "AI": {"tldr": "Traj-MLLM\u662f\u9996\u4e2a\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8f68\u8ff9\u6570\u636e\u6316\u6398\u7684\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89c6\u56fe\u4e0a\u4e0b\u6587\u5c06\u539f\u59cb\u8f68\u8ff9\u8f6c\u6362\u4e3a\u56fe\u50cf-\u6587\u672c\u5e8f\u5217\uff0c\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u6216\u5fae\u8c03\u5373\u53ef\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8f68\u8ff9\u5206\u6790\u65b9\u6cd5\u5b58\u5728\u6cdb\u5316\u6027\u95ee\u9898\uff0c\u8981\u4e48\u5c40\u9650\u4e8e\u7279\u5b9a\u533a\u57df\u8bad\u7ec3\uff0c\u8981\u4e48\u53ea\u9002\u7528\u4e8e\u5c11\u6570\u4efb\u52a1\u3002\u672c\u6587\u63a2\u7d22MLLMs\u662f\u5426\u80fd\u6539\u9769\u5f53\u524d\u8f68\u8ff9\u6570\u636e\u6316\u6398\u5e76\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faTraj-MLLM\u6846\u67b6\uff1a1\uff09\u96c6\u6210\u591a\u89c6\u56fe\u4e0a\u4e0b\u6587\u5c06\u539f\u59cb\u8f68\u8ff9\u8f6c\u6362\u4e3a\u4fdd\u6301\u65f6\u7a7a\u7279\u5f81\u7684\u56fe\u50cf-\u6587\u672c\u5e8f\u5217\uff1b2\uff09\u76f4\u63a5\u5229\u7528MLLMs\u7684\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u8f68\u8ff9\u5206\u6790\uff1b3\uff09\u63d0\u51fa\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u5b9e\u73b0\u4efb\u52a1\u81ea\u9002\u5e94\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cTraj-MLLM\u5728\u65c5\u884c\u65f6\u95f4\u4f30\u8ba1\u3001\u79fb\u52a8\u6027\u9884\u6d4b\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u4ea4\u901a\u65b9\u5f0f\u8bc6\u522b\u4efb\u52a1\u4e0a\u5206\u522b\u8d85\u8d8a\u6700\u5148\u8fdb\u57fa\u7ebf48.05%\u300115.52%\u300151.52%\u548c1.83%\uff0c\u4e14\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u6216\u5fae\u8c03MLLM\u9aa8\u5e72\u7f51\u7edc\u3002", "conclusion": "Traj-MLLM\u6210\u529f\u8bc1\u660e\u4e86MLLMs\u5728\u8f68\u8ff9\u6570\u636e\u6316\u6398\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8868\u793a\u548c\u63d0\u793a\u4f18\u5316\u5b9e\u73b0\u4e86\u8de8\u533a\u57df\u548c\u8de8\u4efb\u52a1\u7684\u901a\u7528\u8f68\u8ff9\u5206\u6790\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u8f68\u8ff9\u5206\u6790\u7684\u6cdb\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.01337", "categories": ["cs.MM", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01337", "abs": "https://arxiv.org/abs/2509.01337", "authors": ["Qianrui Zhou", "Hua Xu", "Yifan Wang", "Xinzhi Dong", "Hanlei Zhang"], "title": "LLM-Guided Semantic Relational Reasoning for Multimodal Intent Recognition", "comment": "Accepted by EMNLP 2025 (Main Track, Long Paper)", "summary": "Understanding human intents from multimodal signals is critical for analyzing\nhuman behaviors and enhancing human-machine interactions in real-world\nscenarios. However, existing methods exhibit limitations in their\nmodality-level reliance, constraining relational reasoning over fine-grained\nsemantics for complex intent understanding. This paper proposes a novel\nLLM-Guided Semantic Relational Reasoning (LGSRR) method, which harnesses the\nexpansive knowledge of large language models (LLMs) to establish semantic\nfoundations that boost smaller models' relational reasoning performance.\nSpecifically, an LLM-based strategy is proposed to extract fine-grained\nsemantics as guidance for subsequent reasoning, driven by a shallow-to-deep\nChain-of-Thought (CoT) that autonomously uncovers, describes, and ranks\nsemantic cues by their importance without relying on manually defined priors.\nBesides, we formally model three fundamental types of semantic relations\ngrounded in logical principles and analyze their nuanced interplay to enable\nmore effective relational reasoning. Extensive experiments on multimodal intent\nand dialogue act recognition tasks demonstrate LGSRR's superiority over\nstate-of-the-art methods, with consistent performance gains across diverse\nsemantic understanding scenarios. The complete data and code are available at\nhttps://github.com/thuiar/LGSRR.", "AI": {"tldr": "\u63d0\u51faLGSRR\u65b9\u6cd5\uff0c\u5229\u7528LLM\u7684\u77e5\u8bc6\u589e\u5f3a\u5c0f\u6a21\u578b\u7684\u5173\u7cfb\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u6d45\u5230\u6df1\u7684\u601d\u7ef4\u94fe\u81ea\u52a8\u63d0\u53d6\u7ec6\u7c92\u5ea6\u8bed\u4e49\uff0c\u5728\u591a\u6a21\u6001\u610f\u56fe\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6a21\u6001\u5c42\u9762\u4f9d\u8d56\u6027\u5f3a\uff0c\u9650\u5236\u4e86\u590d\u6742\u610f\u56fe\u7406\u89e3\u4e2d\u7684\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5173\u7cfb\u63a8\u7406\u80fd\u529b", "method": "\u4f7f\u7528LLM\u57fa\u4e8e\u6d45\u5230\u6df1\u601d\u7ef4\u94fe\u7b56\u7565\u81ea\u52a8\u63d0\u53d6\u7ec6\u7c92\u5ea6\u8bed\u4e49\u4f5c\u4e3a\u6307\u5bfc\uff0c\u5f62\u5f0f\u5316\u5efa\u6a21\u4e09\u79cd\u57fa\u672c\u8bed\u4e49\u5173\u7cfb\u7c7b\u578b\u5e76\u5206\u6790\u5176\u76f8\u4e92\u4f5c\u7528", "result": "\u5728\u591a\u6a21\u6001\u610f\u56fe\u548c\u5bf9\u8bdd\u884c\u4e3a\u8bc6\u522b\u4efb\u52a1\u4e0a\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u8bed\u4e49\u7406\u89e3\u573a\u666f\u4e2d\u5747\u83b7\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347", "conclusion": "LGSRR\u65b9\u6cd5\u901a\u8fc7LLM\u5f15\u5bfc\u7684\u8bed\u4e49\u5173\u7cfb\u63a8\u7406\u6709\u6548\u63d0\u5347\u4e86\u590d\u6742\u610f\u56fe\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u884c\u4e3a\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2509.02232", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2509.02232", "abs": "https://arxiv.org/abs/2509.02232", "authors": ["Liang Xie", "Yanting Li", "Luyang Tang", "Wei Gao"], "title": "Efficient Geometry Compression and Communication for 3D Gaussian Splatting Point Clouds", "comment": "8 pages,5 figures", "summary": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video.", "AI": {"tldr": "\u4f7f\u7528AVS PCRM\u538b\u7f29\u6280\u672f\u4f18\u53163D\u9ad8\u65af\u573a\u666f\u7684\u50a8\u5b58\u548c\u4f20\u8f93\u6548\u7387\uff0c\u572840Mbps\u5e26\u5bbd\u4e0b\u5b9e\u73b0\u9ad8\u4fdd\u771f\u4f20\u8f93\uff0c\u8282\u770110%-25%\u7801\u7387", "motivation": "\u89e3\u51b3\u52a8\u60013D\u573a\u666f\u8868\u793a\u4e2d\u9ad8\u65af\u6570\u636e\u91cf\u7206\u53d1\u5f0f\u589e\u957f\u5bfc\u81f4\u7684\u50a8\u5b58\u548c\u4f20\u8f93\u6311\u6218", "method": "\u5728i3DV\u5e73\u53f0\u4e2d\u96c6\u6210AVS PCRM\u53c2\u8003\u8f6f\u4ef6\uff0c\u7ed3\u5408\u4e8c\u8fdb\u5236\u54c8\u5e0c\u8868\u7684\u7387\u5934\u4f18\u5316\u673a\u5236\uff0c\u5bf9\u9ad8\u65af\u70b9\u4e91\u51e0\u4f55\u6570\u636e\u8fdb\u884c\u9ad8\u6548\u538b\u7f29", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u8054\u5408\u6846\u67b6\u5728\u4fdd\u6301\u5feb\u901f\u6e32\u67d3\u548c\u9ad8\u8d28\u91cf\u5408\u6210\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u5728\u901a\u7528\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e8610%-25%\u7684\u7801\u7387\u8282\u7701", "conclusion": "\u8be5\u65b9\u6848\u4e3a3D\u4f53\u79ef\u89c6\u9891\u7684\u50a8\u5b58\u3001\u4f20\u8f93\u548c\u4ea4\u4e92\u63d0\u4f9b\u4e86\u4f18\u79c0\u7684\u7387\u5934\u5938\u635f\u6062\u590d\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.00029", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00029", "abs": "https://arxiv.org/abs/2509.00029", "authors": ["Leo Vitasovic", "Stella Gra\u00dfhof", "Agnes Mercedes Kloft", "Ville V. Lehtola", "Martin Cunneen", "Justyna Starostka", "Glenn McGarry", "Kun Li", "Sami S. Brandt"], "title": "From Sound to Sight: Towards AI-authored Music Videos", "comment": "1st Workshop on Generative AI for Storytelling (AISTORY), 2025", "summary": "Conventional music visualisation systems rely on handcrafted ad hoc\ntransformations of shapes and colours that offer only limited expressiveness.\nWe propose two novel pipelines for automatically generating music videos from\nany user-specified, vocal or instrumental song using off-the-shelf deep\nlearning models. Inspired by the manual workflows of music video producers, we\nexperiment on how well latent feature-based techniques can analyse audio to\ndetect musical qualities, such as emotional cues and instrumental patterns, and\ndistil them into textual scene descriptions using a language model. Next, we\nemploy a generative model to produce the corresponding video clips. To assess\nthe generated videos, we identify several critical aspects and design and\nconduct a preliminary user evaluation that demonstrates storytelling potential,\nvisual coherency and emotional alignment with the music. Our findings\nunderscore the potential of latent feature techniques and deep generative\nmodels to expand music visualisation beyond traditional approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u81ea\u52a8\u97f3\u4e50\u89c6\u9891\u751f\u6210\u7ba1\u9053\uff0c\u901a\u8fc7\u97f3\u9891\u7279\u5f81\u5206\u6790\u548c\u6587\u672c\u573a\u666f\u63cf\u8ff0\u751f\u6210\uff0c\u5b9e\u73b0\u97f3\u4e50\u60c5\u611f\u548c\u8282\u594f\u7684\u53ef\u89c6\u5316\u8868\u8fbe", "motivation": "\u4f20\u7edf\u97f3\u4e50\u53ef\u89c6\u5316\u7cfb\u7edf\u4f9d\u8d56\u624b\u5de5\u5236\u4f5c\u7684\u5f62\u72b6\u548c\u989c\u8272\u53d8\u6362\uff0c\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u81ea\u52a8\u751f\u6210\u65b9\u6cd5\u6765\u63d0\u5347\u97f3\u4e50\u89c6\u9891\u7684\u521b\u4f5c\u6548\u7387\u548c\u8d28\u91cf", "method": "\u4f7f\u7528\u73b0\u6210\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1a1\uff09\u57fa\u4e8e\u6f5c\u5728\u7279\u5f81\u6280\u672f\u5206\u6790\u97f3\u9891\uff0c\u68c0\u6d4b\u97f3\u4e50\u54c1\u8d28\uff08\u60c5\u611f\u7ebf\u7d22\u3001\u4e50\u5668\u6a21\u5f0f\uff09\uff1b2\uff09\u7528\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u573a\u666f\u63cf\u8ff0\uff1b3\uff09\u7528\u751f\u6210\u6a21\u578b\u751f\u6210\u5bf9\u5e94\u89c6\u9891\u7247\u6bb5", "result": "\u901a\u8fc7\u521d\u6b65\u7528\u6237\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u751f\u6210\u89c6\u9891\u5728\u53d9\u4e8b\u6f5c\u529b\u3001\u89c6\u89c9\u8fde\u8d2f\u6027\u548c\u60c5\u611f\u5bf9\u9f50\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5c55\u793a\u4e86\u826f\u597d\u7684\u6548\u679c", "conclusion": "\u6f5c\u5728\u7279\u5f81\u6280\u672f\u548c\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u6709\u6f5c\u529b\u5c06\u97f3\u4e50\u53ef\u89c6\u5316\u6269\u5c55\u5230\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\u7684\u65b0\u9886\u57df\uff0c\u4e3a\u81ea\u52a8\u97f3\u4e50\u89c6\u9891\u521b\u4f5c\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2509.00030", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.00030", "abs": "https://arxiv.org/abs/2509.00030", "authors": ["Marshall Thomas", "Edward Fish", "Richard Bowden"], "title": "MultiStream-LLM: Bridging Modalities for Robust Sign Language Translation", "comment": null, "summary": "Despite progress in gloss-free Sign Language Translation (SLT), monolithic\nend-to-end models consistently fail on two critical components of natural\nsigning: the precise recognition of high-speed fingerspelling and the\nintegration of asynchronous non-manual cues from the face. Recent progress in\nAutomated Sign Language Translation with Large Language Models has side stepped\nthis challenge, forcing a single network to learn these simultaneously\nresulting in poor performance when tasked with translating crucial information\nsuch as names,places, and technical terms. We introduce MultiStream-LLM, a\nmodular framework designed to overcome these limitations. Our approach employs\nseparate, specialized predictors for continuous signing, fingerspelling, and\nlipreading. Each expert network first decodes its specific modality into a\nsequence of tokens. These parallel streams are then fused by a lightweight\ntransformer that resolves temporal misalignments before passing the combined\nrepresentation to a Large Language Model (LLM) for final sentence generation.\nOur method establishes a new state-of-the-art on the How2Sign benchmark with a\nBLEU-4 score of 23.5 and achieves 73.2% letter accuracy on the challenging\nChicagoFSWildPlus fingerspelling dataset. These results validate our core\nhypothesis: by isolating and solving distinct recogni tion tasks before fusion,\nour multi-expert approach provides a more powerful and effective pathway to\nrobust, high-fidelity sign language translation.", "AI": {"tldr": "\u63d0\u51fa\u4e86MultiStream-LLM\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u4e13\u95e8\u7684\u624b\u6307\u62fc\u5199\u3001\u5507\u8bfb\u548c\u8fde\u7eed\u624b\u8bed\u8bc6\u522b\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7aef\u5230\u7aef\u624b\u8bed\u7ffb\u8bd1\u6a21\u578b\u5728\u9ad8\u901f\u624b\u6307\u62fc\u5199\u548c\u975e\u624b\u52a8\u7ebf\u7d22\u6574\u5408\u4e0a\u7684\u6027\u80fd\u74f6\u9888\u3002", "motivation": "\u4f20\u7edf\u7aef\u5230\u7aef\u624b\u8bed\u7ffb\u8bd1\u6a21\u578b\u5728\u5904\u7406\u9ad8\u901f\u624b\u6307\u62fc\u5199\u548c\u9762\u90e8\u975e\u624b\u52a8\u7ebf\u7d22\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u7ffb\u8bd1\u59d3\u540d\u3001\u5730\u70b9\u548c\u6280\u672f\u672f\u8bed\u7b49\u5173\u952e\u4fe1\u606f\u65f6\u6548\u679c\u8f83\u5dee\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u4e13\u95e8\u7684\u9884\u6d4b\u5668\u5206\u522b\u5904\u7406\u8fde\u7eed\u624b\u8bed\u3001\u624b\u6307\u62fc\u5199\u548c\u5507\u8bfb\uff0c\u7136\u540e\u5c06\u5e76\u884c\u6d41\u901a\u8fc7\u8f7b\u91cf\u7ea7transformer\u8fdb\u884c\u65f6\u95f4\u5bf9\u9f50\u548c\u878d\u5408\uff0c\u6700\u540e\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u53e5\u5b50\u3002", "result": "\u5728How2Sign\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230BLEU-4\u5206\u657023.5\u7684\u65b0SOTA\uff0c\u5728ChicagoFSWildPlus\u624b\u6307\u62fc\u5199\u6570\u636e\u96c6\u4e0a\u8fbe\u523073.2%\u7684\u5b57\u6bcd\u51c6\u786e\u7387\u3002", "conclusion": "\u901a\u8fc7\u5206\u79bb\u548c\u4e13\u95e8\u89e3\u51b3\u4e0d\u540c\u7684\u8bc6\u522b\u4efb\u52a1\u518d\u8fdb\u884c\u878d\u5408\u7684\u591a\u4e13\u5bb6\u65b9\u6cd5\uff0c\u4e3a\u9c81\u68d2\u3001\u9ad8\u4fdd\u771f\u7684\u624b\u8bed\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u6709\u6548\u7684\u9014\u5f84\u3002"}}
{"id": "2509.00051", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00051", "abs": "https://arxiv.org/abs/2509.00051", "authors": ["Faria Binte Kader", "Santu Karmaker"], "title": "A Survey on Evaluation Metrics for Music Generation", "comment": "19 pages, 2 figures", "summary": "Despite significant advancements in music generation systems, the\nmethodologies for evaluating generated music have not progressed as expected\ndue to the complex nature of music, with aspects such as structure, coherence,\ncreativity, and emotional expressiveness. In this paper, we shed light on this\nresearch gap, introducing a detailed taxonomy for evaluation metrics for both\naudio and symbolic music representations. We include a critical review\nidentifying major limitations in current evaluation methodologies which\nincludes poor correlation between objective metrics and human perception,\ncross-cultural bias, and lack of standardization that hinders cross-model\ncomparisons. Addressing these gaps, we further propose future research\ndirections towards building a comprehensive evaluation framework for music\ngeneration evaluation.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u97f3\u4e50\u751f\u6210\u8bc4\u4f30\u65b9\u6cd5\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u6307\u51fa\u4e86\u5f53\u524d\u8bc4\u4f30\u4f53\u7cfb\u5728\u5ba2\u89c2\u6307\u6807\u4e0e\u4eba\u7c7b\u611f\u77e5\u76f8\u5173\u6027\u3001\u8de8\u6587\u5316\u504f\u89c1\u548c\u6807\u51c6\u5316\u7f3a\u5931\u7b49\u65b9\u9762\u7684\u4e3b\u8981\u5c40\u9650\uff0c\u5e76\u63d0\u51fa\u4e86\u6784\u5efa\u5168\u9762\u8bc4\u4f30\u6846\u67b6\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1\u97f3\u4e50\u751f\u6210\u7cfb\u7edf\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7531\u4e8e\u97f3\u4e50\u672c\u8eab\u7684\u590d\u6742\u6027\uff08\u5982\u7ed3\u6784\u3001\u8fde\u8d2f\u6027\u3001\u521b\u9020\u6027\u548c\u60c5\u611f\u8868\u8fbe\u7b49\u65b9\u9762\uff09\uff0c\u8bc4\u4f30\u751f\u6210\u97f3\u4e50\u7684\u65b9\u6cd5\u5b66\u53d1\u5c55\u6ede\u540e\uff0c\u5b58\u5728\u660e\u663e\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u9488\u5bf9\u97f3\u9891\u548c\u7b26\u53f7\u97f3\u4e50\u8868\u793a\u7684\u8be6\u7ec6\u8bc4\u4f30\u6307\u6807\u5206\u7c7b\u6cd5\uff0c\u5e76\u8fdb\u884c\u6279\u5224\u6027\u6587\u732e\u7efc\u8ff0\uff0c\u8bc6\u522b\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u5b66\u7684\u4e3b\u8981\u5c40\u9650\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u5ba2\u89c2\u6307\u6807\u4e0e\u4eba\u7c7b\u611f\u77e5\u76f8\u5173\u6027\u5dee\u3001\u5b58\u5728\u8de8\u6587\u5316\u504f\u89c1\u3001\u7f3a\u4e4f\u6807\u51c6\u5316\u5bfc\u81f4\u8de8\u6a21\u578b\u6bd4\u8f83\u56f0\u96be\u3002", "conclusion": "\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u5168\u9762\u7684\u97f3\u4e50\u751f\u6210\u8bc4\u4f30\u6846\u67b6\u6765\u89e3\u51b3\u73b0\u6709\u5c40\u9650\u6027\uff0c\u672c\u6587\u4e3a\u6b64\u63d0\u51fa\u4e86\u5177\u4f53\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2509.00038", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00038", "abs": "https://arxiv.org/abs/2509.00038", "authors": ["Teo Susnjak"], "title": "Compiling Prompts, Not Crafting Them: A Reproducible Workflow for AI-Assisted Evidence Synthesis", "comment": null, "summary": "Large language models (LLMs) offer significant potential to accelerate\nsystematic literature reviews (SLRs), yet current approaches often rely on\nbrittle, manually crafted prompts that compromise reliability and\nreproducibility. This fragility undermines scientific confidence in\nLLM-assisted evidence synthesis. In response, this work adapts recent advances\nin declarative prompt optimisation, developed for general-purpose LLM\napplications, and demonstrates their applicability to the domain of SLR\nautomation. This research proposes a structured, domain-specific framework that\nembeds task declarations, test suites, and automated prompt tuning into a\nreproducible SLR workflow. These emerging methods are translated into a\nconcrete blueprint with working code examples, enabling researchers to\nconstruct verifiable LLM pipelines that align with established principles of\ntransparency and rigour in evidence synthesis. This is a novel application of\nsuch approaches to SLR pipelines.", "AI": {"tldr": "\u672c\u6587\u63a8\u51fa\u4e86\u4e00\u79cd\u58f0\u660e\u5f0f\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u5584\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u4e2d\u7684\u53ef\u9760\u6027\u548c\u53ef\u590d\u73b0\u6027", "motivation": "\u5f53\u524d\u4f7f\u7528LLM\u8fdb\u884c\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u7684\u65b9\u6cd5\u4f9d\u8d56\u5bb9\u6613\u5931\u6548\u7684\u624b\u52a8\u5236\u4f5c\u63d0\u793a\uff0c\u5f71\u54cd\u4e86\u79d1\u5b66\u7814\u7a76\u7684\u53ef\u9760\u6027\u548c\u53ef\u590d\u73b0\u6027", "method": "\u91c7\u7528\u58f0\u660e\u5f0f\u63d0\u793a\u4f18\u5316\u6280\u672f\uff0c\u6784\u5efa\u5305\u542b\u4efb\u52a1\u58f0\u660e\u3001\u6d4b\u8bd5\u5957\u4ef6\u548c\u81ea\u52a8\u63d0\u793a\u8c03\u6574\u7684\u9886\u57df\u7279\u5b9a\u6846\u67b6", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5177\u4f53\u7684\u84dd\u56fe\u548c\u5de5\u4f5c\u4ee3\u7801\u793a\u4f8b\uff0c\u652f\u6301\u5efa\u7acb\u7b26\u5408\u900f\u660e\u6027\u548c\u4e25\u8c28\u6027\u539f\u5219\u7684\u53ef\u9a8c\u8bc1LLM\u6d41\u6c34\u7ebf", "conclusion": "\u8fd9\u662f\u58f0\u660e\u5f0f\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u5728SLR\u81ea\u52a8\u5316\u9886\u57df\u7684\u65b0\u9896\u5e94\u7528\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8LLM\u8f85\u52a9\u8bc1\u636e\u7efc\u5408\u7684\u79d1\u5b66\u4fe1\u5fc3"}}
{"id": "2509.00132", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00132", "abs": "https://arxiv.org/abs/2509.00132", "authors": ["Peiwen Xing", "Aske Plaat", "Niki van Stein"], "title": "CoComposer: LLM Multi-agent Collaborative Music Composition", "comment": null, "summary": "Existing AI Music composition tools are limited in generation duration,\nmusical quality, and controllability. We introduce CoComposer, a multi-agent\nsystem that consists of five collaborating agents, each with a task based on\nthe traditional music composition workflow. Using the AudioBox-Aesthetics\nsystem, we experimentally evaluate CoComposer on four compositional criteria.\nWe test with three LLMs (GPT-4o, DeepSeek-V3-0324, Gemini-2.5-Flash), and find\n(1) that CoComposer outperforms existing multi-agent LLM-based systems in music\nquality, and (2) compared to a single-agent system, in production complexity.\nCompared to non- LLM MusicLM, CoComposer has better interpretability and\neditability, although MusicLM still produces better music.", "AI": {"tldr": "CoComposer\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u97f3\u4e50\u521b\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e94\u4e2a\u534f\u4f5c\u667a\u80fd\u4f53\u6a21\u62df\u4f20\u7edf\u4f5c\u66f2\u6d41\u7a0b\uff0c\u5728\u97f3\u4e50\u8d28\u91cf\u548c\u5236\u4f5c\u590d\u6742\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u76f8\u6bd4\u975eLLM\u7684MusicLM\u5177\u6709\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u7f16\u8f91\u6027\u3002", "motivation": "\u73b0\u6709AI\u97f3\u4e50\u521b\u4f5c\u5de5\u5177\u5728\u751f\u6210\u65f6\u957f\u3001\u97f3\u4e50\u8d28\u91cf\u548c\u53ef\u63a7\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u597d\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7cfb\u7edf\u6765\u63d0\u5347\u97f3\u4e50\u521b\u4f5c\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b\u4e94\u4e2a\u534f\u4f5c\u667a\u80fd\u4f53\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u57fa\u4e8e\u4f20\u7edf\u97f3\u4e50\u521b\u4f5c\u6d41\u7a0b\u6267\u884c\u7279\u5b9a\u4efb\u52a1\uff0c\u4f7f\u7528AudioBox-Aesthetics\u7cfb\u7edf\u5728\u56db\u4e2a\u4f5c\u66f2\u6807\u51c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u6d4b\u8bd5\u4e86GPT-4o\u3001DeepSeek-V3-0324\u548cGemini-2.5-Flash\u4e09\u79cdLLM\u3002", "result": "CoComposer\u5728\u97f3\u4e50\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5728\u5236\u4f5c\u590d\u6742\u5ea6\u4e0a\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u76f8\u6bd4\u975eLLM\u7684MusicLM\u5177\u6709\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u7f16\u8f91\u6027\uff0c\u4f46MusicLM\u4ecd\u80fd\u4ea7\u751f\u66f4\u597d\u7684\u97f3\u4e50\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347AI\u97f3\u4e50\u521b\u4f5c\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u6027\u65b9\u9762\uff0c\u4f46\u5728\u7eaf\u97f3\u4e50\u8d28\u91cf\u65b9\u9762\u4ecd\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u4ee5\u5339\u654c\u4e13\u4e1a\u97f3\u4e50\u751f\u6210\u7cfb\u7edf\u3002"}}
{"id": "2509.00120", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00120", "abs": "https://arxiv.org/abs/2509.00120", "authors": ["Eyal Briman", "Eyal Leizerovich", "Nimrod Talmon"], "title": "Algorithms for Collaborative Harmonization", "comment": "Presented at the 15th Multidisciplinary Workshop on Advances in\n  Preference Handling M-PREF 2024, Santiago de Compostela, Oct 20, 2024", "summary": "We consider a specific scenario of text aggregation, in the realm of musical\nharmonization. Musical harmonization shares similarities with text aggregation,\nhowever the language of harmony is more structured than general text.\nConcretely, given a set of harmonization suggestions for a given musical\nmelody, our interest lies in devising aggregation algorithms that yield an\nharmonization sequence that satisfies the following two key criteria: (1) an\neffective representation of the collective suggestions; and (2) an\nharmonization that is musically coherent. We present different algorithms for\nthe aggregation of harmonies given by a group of agents and analyze their\ncomplexities. The results indicate that the Kemeny and plurality-based\nalgorithms are most effective in assessing representation and maintaining\nmusical coherence.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u97f3\u4e50\u548c\u58f0\u805a\u5408\u95ee\u9898\uff0c\u63d0\u51fa\u591a\u79cd\u7b97\u6cd5\u6765\u805a\u5408\u591a\u4e2a\u548c\u58f0\u5efa\u8bae\uff0c\u65e8\u5728\u5b9e\u73b0\u96c6\u4f53\u610f\u89c1\u7684\u6709\u6548\u4ee3\u8868\u548c\u97f3\u4e50\u8fde\u8d2f\u6027\uff0c\u53d1\u73b0Kemeny\u548c\u57fa\u4e8e\u591a\u6570\u7684\u7b97\u6cd5\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u97f3\u4e50\u548c\u58f0\u4e0e\u6587\u672c\u805a\u5408\u6709\u76f8\u4f3c\u4e4b\u5904\uff0c\u4f46\u548c\u58f0\u8bed\u8a00\u66f4\u5177\u7ed3\u6784\u6027\u3002\u9700\u8981\u5f00\u53d1\u805a\u5408\u7b97\u6cd5\u6765\u5904\u7406\u591a\u4e2a\u548c\u58f0\u5efa\u8bae\uff0c\u65e2\u8981\u6709\u6548\u4ee3\u8868\u96c6\u4f53\u610f\u89c1\uff0c\u53c8\u8981\u4fdd\u6301\u97f3\u4e50\u8fde\u8d2f\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e0d\u540c\u7684\u548c\u58f0\u805a\u5408\u7b97\u6cd5\uff0c\u5305\u62ecKemeny\u7b97\u6cd5\u548c\u57fa\u4e8e\u591a\u6570\u7684\u7b97\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u8fd9\u4e9b\u7b97\u6cd5\u7684\u590d\u6742\u5ea6\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cKemeny\u7b97\u6cd5\u548c\u57fa\u4e8e\u591a\u6570\u7684\u7b97\u6cd5\u5728\u8bc4\u4f30\u4ee3\u8868\u6027\u548c\u4fdd\u6301\u97f3\u4e50\u8fde\u8d2f\u6027\u65b9\u9762\u6700\u4e3a\u6709\u6548\u3002", "conclusion": "\u5728\u97f3\u4e50\u548c\u58f0\u805a\u5408\u573a\u666f\u4e2d\uff0cKemeny\u548c\u57fa\u4e8e\u591a\u6570\u7684\u7b97\u6cd5\u80fd\u591f\u5f88\u597d\u5730\u5e73\u8861\u96c6\u4f53\u610f\u89c1\u4ee3\u8868\u6027\u548c\u97f3\u4e50\u8d28\u91cf\u7684\u8981\u6c42\u3002"}}
{"id": "2509.00185", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00185", "abs": "https://arxiv.org/abs/2509.00185", "authors": ["Jian Wu", "Sarah Rajtmajer"], "title": "What Are Research Hypotheses?", "comment": "6 pages, accepted by Sci-K'25: International Workshop on Scientific\n  Knowledge", "summary": "Over the past decades, alongside advancements in natural language processing,\nsignificant attention has been paid to training models to automatically\nextract, understand, test, and generate hypotheses in open and scientific\ndomains. However, interpretations of the term \\emph{hypothesis} for various\nnatural language understanding (NLU) tasks have migrated from traditional\ndefinitions in the natural, social, and formal sciences. Even within NLU, we\nobserve differences defining hypotheses across literature. In this paper, we\noverview and delineate various definitions of hypothesis. Especially, we\ndiscern the nuances of definitions across recently published NLU tasks. We\nhighlight the importance of well-structured and well-defined hypotheses,\nparticularly as we move toward a machine-interpretable scholarly record.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\"\u5047\u8bbe\"\u6982\u5ff5\u7684\u4e0d\u540c\u5b9a\u4e49\uff0c\u7279\u522b\u5173\u6ce8\u4e86\u8fd1\u671fNLU\u4efb\u52a1\u4e2d\u7684\u5b9a\u4e49\u5dee\u5f02\uff0c\u5f3a\u8c03\u4e86\u5728\u673a\u5668\u53ef\u89e3\u91ca\u5b66\u672f\u8bb0\u5f55\u80cc\u666f\u4e0b\u660e\u786e\u5b9a\u4e49\u5047\u8bbe\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5404\u79cd\u4efb\u52a1\u5bf9\"\u5047\u8bbe\"\u7684\u5b9a\u4e49\u504f\u79bb\u4e86\u4f20\u7edf\u79d1\u5b66\u5b9a\u4e49\uff0c\u751a\u81f3\u5728NLU\u9886\u57df\u5185\u90e8\u4e5f\u5b58\u5728\u5b9a\u4e49\u5dee\u5f02\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u548c\u660e\u786e\u754c\u5b9a\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5bf9\u8fd1\u671f\u53d1\u8868\u7684NLU\u4efb\u52a1\u4e2d\u4f7f\u7528\u7684\u5047\u8bbe\u5b9a\u4e49\u8fdb\u884c\u7cfb\u7edf\u6027\u7684\u6982\u8ff0\u548c\u8fa8\u6790\uff0c\u7279\u522b\u5173\u6ce8\u5b9a\u4e49\u95f4\u7684\u7ec6\u5fae\u5dee\u522b\u3002", "result": "\u8bc6\u522b\u51faNLU\u9886\u57df\u4e2d\u5047\u8bbe\u6982\u5ff5\u5b58\u5728\u591a\u91cd\u5b9a\u4e49\u548c\u89e3\u91ca\uff0c\u8fd9\u4e9b\u5b9a\u4e49\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u5e94\u7528\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u5728\u5411\u673a\u5668\u53ef\u89e3\u91ca\u5b66\u672f\u8bb0\u5f55\u53d1\u5c55\u7684\u8fc7\u7a0b\u4e2d\uff0c\u9700\u8981\u5efa\u7acb\u7ed3\u6784\u826f\u597d\u3001\u5b9a\u4e49\u660e\u786e\u7684\u5047\u8bbe\u6982\u5ff5\u6846\u67b6\uff0c\u4ee5\u786e\u4fdd\u5b66\u672f\u4ea4\u6d41\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.00654", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00654", "abs": "https://arxiv.org/abs/2509.00654", "authors": ["Ashwin Nagarajan", "Hao-Wen Dong"], "title": "The Name-Free Gap: Policy-Aware Stylistic Control in Music Generation", "comment": "10 pages, 2 figures", "summary": "Text-to-music models capture broad attributes such as instrumentation or\nmood, but fine-grained stylistic control remains an open challenge. Existing\nstylization methods typically require retraining or specialized conditioning,\nwhich complicates reproducibility and limits policy compliance when artist\nnames are restricted. We study whether lightweight, human-readable modifiers\nsampled from a large language model can provide a policy-robust alternative for\nstylistic control. Using MusicGen-small, we evaluate two artists: Billie Eilish\n(vocal pop) and Ludovico Einaudi (instrumental piano). For each artist, we use\nfifteen reference excerpts and evaluate matched seeds under three conditions:\nbaseline prompts, artist-name prompts, and five descriptor sets. All prompts\nare generated using a large language model. Evaluation uses both VGGish and\nCLAP embeddings with distributional and per-clip similarity measures, including\na new min-distance attribution metric. Results show that artist names are the\nstrongest control signal across both artists, while name-free descriptors\nrecover much of this effect. This highlights that existing safeguards such as\nthe restriction of artist names in music generation prompts may not fully\nprevent style imitation. Cross-artist transfers reduce alignment, showing that\ndescriptors encode targeted stylistic cues. We also present a descriptor table\nacross ten contemporary artists to illustrate the breadth of the tokens.\nTogether these findings define the name-free gap, the controllability\ndifference between artist-name prompts and policy-compliant descriptors, shown\nthrough a reproducible evaluation protocol for prompt-level controllability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u4eba\u53ef\u8bfb\u63cf\u8ff0\u7b26\u4f5c\u4e3a\u827a\u672f\u5bb6\u540d\u79f0\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7528\u4e8e\u97f3\u4e50\u751f\u6210\u7684\u98ce\u683c\u63a7\u5236\uff0c\u53d1\u73b0\u540d\u79f0\u9650\u5236\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u9632\u6b62\u98ce\u683c\u6a21\u4eff\u3002", "motivation": "\u73b0\u6709\u97f3\u4e50\u98ce\u683c\u5316\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6216\u7279\u6b8a\u6761\u4ef6\u8bbe\u7f6e\uff0c\u9650\u5236\u4e86\u53ef\u590d\u73b0\u6027\u548c\u653f\u7b56\u5408\u89c4\u6027\u3002\u7814\u7a76\u65e8\u5728\u5bfb\u627e\u8f7b\u91cf\u7ea7\u3001\u653f\u7b56\u9c81\u68d2\u7684\u98ce\u683c\u63a7\u5236\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528MusicGen-small\u6a21\u578b\uff0c\u8bc4\u4f30Billie Eilish\u548cLudovico Einaudi\u4e24\u4f4d\u827a\u672f\u5bb6\u7684\u98ce\u683c\u63a7\u5236\u3002\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e09\u79cd\u63d0\u793a\u6761\u4ef6\uff1a\u57fa\u7ebf\u63d0\u793a\u3001\u827a\u672f\u5bb6\u540d\u79f0\u63d0\u793a\u548c\u4e94\u7ec4\u63cf\u8ff0\u7b26\u96c6\uff0c\u4f7f\u7528VGGish\u548cCLAP\u5d4c\u5165\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u827a\u672f\u5bb6\u540d\u79f0\u662f\u6700\u5f3a\u7684\u63a7\u5236\u4fe1\u53f7\uff0c\u4f46\u65e0\u540d\u79f0\u63cf\u8ff0\u7b26\u80fd\u6062\u590d\u5927\u90e8\u5206\u6548\u679c\u3002\u8de8\u827a\u672f\u5bb6\u8f6c\u79fb\u4f1a\u964d\u4f4e\u5bf9\u9f50\u5ea6\uff0c\u8868\u660e\u63cf\u8ff0\u7b26\u7f16\u7801\u4e86\u76ee\u6807\u98ce\u683c\u7ebf\u7d22\u3002", "conclusion": "\u7814\u7a76\u5b9a\u4e49\u4e86\"\u65e0\u540d\u79f0\u5dee\u8ddd\"\u6982\u5ff5\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u53ef\u590d\u73b0\u8bc4\u4f30\u534f\u8bae\u9a8c\u8bc1\u63d0\u793a\u7ea7\u53ef\u63a7\u6027\u7684\u65b9\u6cd5\uff0c\u8868\u660e\u73b0\u6709\u827a\u672f\u5bb6\u540d\u79f0\u9650\u5236\u63aa\u65bd\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u9632\u6b62\u98ce\u683c\u6a21\u4eff\u3002"}}
{"id": "2509.00190", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00190", "abs": "https://arxiv.org/abs/2509.00190", "authors": ["Sheldon Yu", "Yuxin Xiong", "Junda Wu", "Xintong Li", "Tong Yu", "Xiang Chen", "Ritwik Sinha", "Jingbo Shang", "Julian McAuley"], "title": "Explainable Chain-of-Thought Reasoning: An Empirical Analysis on State-Aware Reasoning Dynamics", "comment": "5 pages, 4 figures", "summary": "Recent advances in chain-of-thought (CoT) prompting have enabled large\nlanguage models (LLMs) to perform multi-step reasoning. However, the\nexplainability of such reasoning remains limited, with prior work primarily\nfocusing on local token-level attribution, such that the high-level semantic\nroles of reasoning steps and their transitions remain underexplored. In this\npaper, we introduce a state-aware transition framework that abstracts CoT\ntrajectories into structured latent dynamics. Specifically, to capture the\nevolving semantics of CoT reasoning, each reasoning step is represented via\nspectral analysis of token-level embeddings and clustered into semantically\ncoherent latent states. To characterize the global structure of reasoning, we\nmodel their progression as a Markov chain, yielding a structured and\ninterpretable view of the reasoning process. This abstraction supports a range\nof analyses, including semantic role identification, temporal pattern\nvisualization, and consistency evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u72b6\u6001\u611f\u77e5\u8f6c\u6362\u6846\u67b6\uff0c\u5c06\u601d\u7ef4\u94fe\u8f68\u8ff9\u62bd\u8c61\u4e3a\u7ed3\u6784\u5316\u6f5c\u5728\u52a8\u6001\uff0c\u901a\u8fc7\u8c31\u5206\u6790\u548c\u9a6c\u5c14\u53ef\u592b\u94fe\u5efa\u6a21\u6765\u63d0\u5347\u63a8\u7406\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u601d\u7ef4\u94fe\u63d0\u793a\u7684\u63a8\u7406\u53ef\u89e3\u91ca\u6027\u6709\u9650\uff0c\u4e3b\u8981\u5173\u6ce8\u5c40\u90e8\u8bcd\u7ea7\u5f52\u56e0\uff0c\u800c\u63a8\u7406\u6b65\u9aa4\u7684\u9ad8\u7ea7\u8bed\u4e49\u89d2\u8272\u548c\u8f6c\u6362\u5173\u7cfb\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u8c31\u5206\u6790\u5bf9\u8bcd\u7ea7\u5d4c\u5165\u8fdb\u884c\u805a\u7c7b\uff0c\u5c06\u63a8\u7406\u6b65\u9aa4\u8868\u793a\u4e3a\u8bed\u4e49\u4e00\u81f4\u7684\u6f5c\u5728\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u94fe\u5efa\u6a21\u63a8\u7406\u8fc7\u7a0b\u7684\u5168\u5c40\u7ed3\u6784\u3002", "result": "\u8be5\u6846\u67b6\u652f\u6301\u8bed\u4e49\u89d2\u8272\u8bc6\u522b\u3001\u65f6\u95f4\u6a21\u5f0f\u53ef\u89c6\u5316\u548c\u4e00\u81f4\u6027\u8bc4\u4f30\u7b49\u591a\u79cd\u5206\u6790\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u548c\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fc7\u7a0b\u89c6\u56fe\u3002", "conclusion": "\u63d0\u51fa\u7684\u72b6\u6001\u611f\u77e5\u8f6c\u6362\u6846\u67b6\u80fd\u591f\u6709\u6548\u6355\u6349\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u6f14\u5316\u8bed\u4e49\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u66f4\u6df1\u5165\u7684\u53ef\u89e3\u91ca\u6027\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2509.01588", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01588", "abs": "https://arxiv.org/abs/2509.01588", "authors": ["Andrea Poltronieri", "Xavier Serra", "Mart\u00edn Rocamora"], "title": "From Discord to Harmony: Decomposed Consonance-based Training for Improved Audio Chord Estimation", "comment": "9 pages, 3 figures, 3 tables", "summary": "Audio Chord Estimation (ACE) holds a pivotal role in music information\nresearch, having garnered attention for over two decades due to its relevance\nfor music transcription and analysis. Despite notable advancements, challenges\npersist in the task, particularly concerning unique characteristics of harmonic\ncontent, which have resulted in existing systems' performances reaching a glass\nceiling. These challenges include annotator subjectivity, where varying\ninterpretations among annotators lead to inconsistencies, and class imbalance\nwithin chord datasets, where certain chord classes are over-represented\ncompared to others, posing difficulties in model training and evaluation. As a\nfirst contribution, this paper presents an evaluation of inter-annotator\nagreement in chord annotations, using metrics that extend beyond traditional\nbinary measures. In addition, we propose a consonance-informed distance metric\nthat reflects the perceptual similarity between harmonic annotations. Our\nanalysis suggests that consonance-based distance metrics more effectively\ncapture musically meaningful agreement between annotations. Expanding on these\nfindings, we introduce a novel ACE conformer-based model that integrates\nconsonance concepts into the model through consonance-based label smoothing.\nThe proposed model also addresses class imbalance by separately estimating\nroot, bass, and all note activations, enabling the reconstruction of chord\nlabels from decomposed outputs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u534f\u548c\u5ea6\u6982\u5ff5\u7684\u97f3\u9891\u548c\u5f26\u4f30\u8ba1\u65b0\u65b9\u6cd5\uff0c\u5305\u62ec\u534f\u548c\u5ea6\u611f\u77e5\u7684\u8ddd\u79bb\u5ea6\u91cf\u6807\u51c6\u548c\u6574\u5408\u534f\u548c\u5ea6\u6807\u7b7e\u5e73\u6ed1\u7684conformer\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u6807\u6ce8\u8005\u4e3b\u89c2\u6027\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u97f3\u9891\u548c\u5f26\u4f30\u8ba1\u5b58\u5728\u6807\u6ce8\u8005\u4e3b\u89c2\u6027\u5bfc\u81f4\u7684\u6807\u6ce8\u4e0d\u4e00\u81f4\u548c\u6570\u636e\u96c6\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u73b0\u6709\u7cfb\u7edf\u6027\u80fd\u9047\u5230\u74f6\u9888\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e9b\u6839\u672c\u6311\u6218\u3002", "method": "1) \u63d0\u51fa\u8d85\u8d8a\u4f20\u7edf\u4e8c\u5143\u5ea6\u91cf\u7684\u6807\u6ce8\u8005\u4e00\u81f4\u6027\u8bc4\u4f30\u65b9\u6cd5\uff1b2) \u8bbe\u8ba1\u57fa\u4e8e\u534f\u548c\u5ea6\u7684\u611f\u77e5\u76f8\u4f3c\u6027\u8ddd\u79bb\u5ea6\u91cf\uff1b3) \u5f00\u53d1\u6574\u5408\u534f\u548c\u5ea6\u6807\u7b7e\u5e73\u6ed1\u7684conformer\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u522b\u4f30\u8ba1\u6839\u97f3\u3001\u4f4e\u97f3\u548c\u6240\u6709\u97f3\u7b26\u6fc0\u6d3b\u6765\u91cd\u6784\u548c\u5f26\u6807\u7b7e\u3002", "result": "\u5206\u6790\u8868\u660e\u57fa\u4e8e\u534f\u548c\u5ea6\u7684\u8ddd\u79bb\u5ea6\u91cf\u80fd\u66f4\u6709\u6548\u5730\u6355\u6349\u6807\u6ce8\u4e4b\u95f4\u7684\u97f3\u4e50\u610f\u4e49\u4e00\u81f4\u6027\uff0c\u65b0\u6a21\u578b\u901a\u8fc7\u5206\u89e3\u8f93\u51fa\u91cd\u6784\u548c\u5f26\u6807\u7b7e\u7684\u65b9\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "conclusion": "\u534f\u548c\u5ea6\u6982\u5ff5\u4e3a\u97f3\u9891\u548c\u5f26\u4f30\u8ba1\u63d0\u4f9b\u4e86\u66f4\u97f3\u4e50\u5316\u7684\u8bc4\u4f30\u548c\u5efa\u6a21\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u6807\u6ce8\u4e3b\u89c2\u6027\u548c\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7a81\u7834\u73b0\u6709\u6027\u80fd\u74f6\u9888\u3002"}}
{"id": "2509.00186", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00186", "abs": "https://arxiv.org/abs/2509.00186", "authors": ["Arnab Das", "Yassine El Kheir", "Carlos Franzreb", "Tim Herzig", "Tim Polzehl", "Sebastian M\u00f6ller"], "title": "Generalizable Audio Spoofing Detection using Non-Semantic Representations", "comment": null, "summary": "Rapid advancements in generative modeling have made synthetic audio\ngeneration easy, making speech-based services vulnerable to spoofing attacks.\nConsequently, there is a dire need for robust countermeasures more than ever.\nExisting solutions for deepfake detection are often criticized for lacking\ngeneralizability and fail drastically when applied to real-world data. This\nstudy proposes a novel method for generalizable spoofing detection leveraging\nnon-semantic universal audio representations. Extensive experiments have been\nperformed to find suitable non-semantic features using TRILL and TRILLsson\nmodels. The results indicate that the proposed method achieves comparable\nperformance on the in-domain test set while significantly outperforming\nstate-of-the-art approaches on out-of-domain test sets. Notably, it\ndemonstrates superior generalization on public-domain data, surpassing methods\nbased on hand-crafted features, semantic embeddings, and end-to-end\narchitectures.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u975e\u8bed\u4e49\u666e\u904d\u97f3\u9891\u8868\u5f81\u8fdb\u884c\u6df1\u5ea6\u4f2a\u9020\u97f3\u9891\u68c0\u6d4b\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u8de8\u57df\u6d4b\u8bd5\u96c6\u4e0a\u663e\u8457\u8d85\u8fc7\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5408\u6210\u97f3\u9891\u751f\u6210\u53d8\u5f97\u5bb9\u6613\uff0c\u4f7f\u8bed\u97f3\u670d\u52a1\u66f4\u6613\u53d7\u5230\u4f2a\u9020\u653b\u51fb\u3002\u73b0\u6709\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6848\u7f3a\u4e4f\u666e\u904d\u6027\uff0c\u5728\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u6570\u636e\u65f6\u6548\u679c\u5dee\u5f3a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9898\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u975e\u8bed\u4e49\u7684\u666e\u904d\u97f3\u9891\u8868\u5f81\u6765\u8fdb\u884c\u4f2a\u9020\u68c0\u6d4b\u3002\u901a\u8fc7\u4f7f\u7528TRILL\u548cTRILLsson\u6a21\u578b\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u627e\u5230\u9002\u5408\u7684\u975e\u8bed\u4e49\u7279\u5f81\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u57df\u5185\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e86\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u800c\u5728\u8de8\u57df\u6d4b\u8bd5\u96c6\u4e0a\u663e\u8457\u8d85\u8fc7\u4e86\u6700\u65b0\u7684\u72b6\u6001\u4e4b\u4e00\u65b9\u6cd5\u3002\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u666e\u904d\u6027\uff0c\u8d85\u8fc7\u4e86\u57fa\u4e8e\u624b\u5de5\u7279\u5f81\u3001\u8bed\u4e49\u5d4c\u5165\u548c\u7aef\u5230\u7aef\u67b6\u6784\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8e\u975e\u8bed\u4e49\u666e\u904d\u97f3\u9891\u8868\u5f81\u7684\u65b9\u6cd5\u4e3a\u6df1\u5ea6\u4f2a\u9020\u97f3\u9891\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5065\u58c1\u548c\u5177\u6709\u66f4\u597d\u666e\u904d\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.00245", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00245", "abs": "https://arxiv.org/abs/2509.00245", "authors": ["Seiji Maekawa", "Hayate Iso", "Nikita Bhutani"], "title": "The Rarity Blind Spot: A Framework for Evaluating Statistical Reasoning in LLMs", "comment": null, "summary": "Effective decision-making often relies on identifying what makes each\ncandidate distinctive. While existing benchmarks for LLMs emphasize retrieving\nor summarizing information relevant to a given query, they do not evaluate a\nmodel's ability to identify globally distinctive features across a set of\ndocuments. We introduce Distinctive Feature Mining (DFM), a new task that\nchallenges models to analyze a small-to-medium collection (10-40 documents) and\nsurface features that are rare in the global context (e.g., appearing in less\nthan 10% of documents). This setting mirrors real-world scenarios such as\ncandidate selection or product differentiation, where statistical reasoning,\nnot retrieval, is key. To enable systematic evaluation of this capability, we\npresent DiFBench, a configurable benchmark creation framework with controllable\nparameters such as document set size and distinctiveness thresholds. Using\nDiFBench, we perform a large-scale assessment of distinctive feature mining\nacross ten state-of-the-art LLMs. Our findings reveal a significant performance\ngap between general-purpose and reasoning-enhanced models. All models, however,\nsubstantially degrade as the task complexity and document count increase. We\nalso find that a common failure mode is misidentifying frequent features as\ndistinctive. These insights reveal core limitations in contemporary LLMs'\nabilities to perform fine-grained, statistical reasoning and rarity detection.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u533a\u5206\u6027\u7279\u5f81\u6316\u6398(DFM)\u65b0\u4efb\u52a1\uff0c\u8bc4\u4f30LLM\u5728\u5168\u5c40\u6587\u6863\u96c6\u4e2d\u8bc6\u522b\u7a00\u6709\u7279\u5f81\u7684\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u7edf\u8ba1\u63a8\u7406\u548c\u7a00\u6709\u6027\u68c0\u6d4b\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u91cd\u70b9\u5728\u4fe1\u606f\u68c0\u7d22\u548c\u6458\u8981\uff0c\u800c\u5ffd\u89c6\u4e86LLM\u8bc6\u522b\u5168\u5c40\u8303\u56f4\u5185\u7a00\u6709\u7279\u5f81\u7684\u80fd\u529b\uff0c\u8fd9\u5728\u5019\u9009\u4eba\u9009\u62e9\u3001\u4ea7\u54c1\u5dee\u5f02\u5316\u7b49\u5b9e\u9645\u573a\u666f\u4e2d\u5f88\u91cd\u8981\u3002", "method": "\u63d0\u51faDiFBench\u914d\u7f6e\u6027\u6d4b\u8bd5\u6846\u67b6\uff0c\u53ef\u63a7\u5236\u6587\u6863\u96c6\u5927\u5c0f\u548c\u533a\u5206\u6027\u9608\u503c\u7b49\u53c2\u6570\uff0c\u5e76\u5bf9\u5341\u4e2a\u6700\u65b0LLM\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0\u901a\u7528\u6a21\u578b\u4e0e\u63a8\u7406\u589e\u5f3a\u6a21\u578b\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff1b\u6240\u6709\u6a21\u578b\u5728\u4efb\u52a1\u590d\u6742\u5ea6\u548c\u6587\u6863\u6570\u91cf\u589e\u52a0\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1b\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\u662f\u5c06\u5e38\u89c1\u7279\u5f81\u8bef\u5224\u4e3a\u533a\u5206\u6027\u7279\u5f81\u3002", "conclusion": "\u73b0\u4ee3LLM\u5728\u7ec6\u7c92\u5ea6\u7edf\u8ba1\u63a8\u7406\u548c\u7a00\u6709\u6027\u68c0\u6d4b\u65b9\u9762\u5b58\u5728\u6838\u5fc3\u9650\u5236\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u6a21\u578b\u7684\u7edf\u8ba1\u5206\u6790\u80fd\u529b\u3002"}}
{"id": "2509.00230", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00230", "abs": "https://arxiv.org/abs/2509.00230", "authors": ["Linus Stuhlmann", "Michael Alexander Saxer"], "title": "Evaluating the Effectiveness of Transformer Layers in Wav2Vec 2.0, XLS-R, and Whisper for Speaker Identification Tasks", "comment": null, "summary": "This study evaluates the performance of three advanced speech encoder models,\nWav2Vec 2.0, XLS-R, and Whisper, in speaker identification tasks. By\nfine-tuning these models and analyzing their layer-wise representations using\nSVCCA, k-means clustering, and t-SNE visualizations, we found that Wav2Vec 2.0\nand XLS-R capture speaker-specific features effectively in their early layers,\nwith fine-tuning improving stability and performance. Whisper showed better\nperformance in deeper layers. Additionally, we determined the optimal number of\ntransformer layers for each model when fine-tuned for speaker identification\ntasks.", "AI": {"tldr": "\u8bc4\u4f30Wav2Vec 2.0\u3001XLS-R\u548cWhisper\u4e09\u79cd\u8bed\u97f3\u7f16\u7801\u5668\u5728\u8bf4\u8bdd\u4eba\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5206\u6790\u5404\u6a21\u578b\u5c42\u7ea7\u7684\u7279\u5f81\u8868\u793a\u80fd\u529b", "motivation": "\u7814\u7a76\u5148\u8fdb\u8bed\u97f3\u7f16\u7801\u6a21\u578b\u5728\u8bf4\u8bdd\u4eba\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u63a2\u7d22\u4e0d\u540c\u6a21\u578b\u5c42\u7ea7\u5bf9\u8bf4\u8bdd\u4eba\u7279\u5f81\u7684\u5b66\u4e60\u80fd\u529b", "method": "\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u5e76\u4f7f\u7528SVCCA\u3001k-means\u805a\u7c7b\u548ct-SNE\u53ef\u89c6\u5316\u5206\u6790\u5c42\u7ea7\u8868\u793a\uff0c\u786e\u5b9a\u5404\u6a21\u578b\u5728\u8bf4\u8bdd\u4eba\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6700\u4f18\u5c42\u6570", "result": "Wav2Vec 2.0\u548cXLS-R\u5728\u65e9\u671f\u5c42\u5c31\u80fd\u6709\u6548\u6355\u83b7\u8bf4\u8bdd\u4eba\u7279\u5f81\uff0c\u5fae\u8c03\u540e\u6027\u80fd\u66f4\u7a33\u5b9a\uff1bWhisper\u5728\u6df1\u5c42\u8868\u73b0\u66f4\u597d\uff1b\u786e\u5b9a\u4e86\u5404\u6a21\u578b\u5fae\u8c03\u540e\u7684\u6700\u4f18transformer\u5c42\u6570", "conclusion": "\u4e0d\u540c\u8bed\u97f3\u7f16\u7801\u6a21\u578b\u5728\u8bf4\u8bdd\u4eba\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5c42\u7ea7\u7279\u5f81\u6355\u83b7\u7684\u5dee\u5f02\u6027\uff0c\u4e3a\u6a21\u578b\u9009\u62e9\u548c\u5fae\u8c03\u7b56\u7565\u63d0\u4f9b\u4e86\u6307\u5bfc"}}
{"id": "2509.00248", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00248", "abs": "https://arxiv.org/abs/2509.00248", "authors": ["Zachary K. Stine", "James E. Deitrick"], "title": "The Differential Meaning of Models: A Framework for Analyzing the Structural Consequences of Semantic Modeling Decisions", "comment": null, "summary": "The proliferation of methods for modeling of human meaning-making constitutes\na powerful class of instruments for the analysis of complex semiotic systems.\nHowever, the field lacks a general theoretical framework for describing these\nmodeling practices across various model types in an apples-to-apples way. In\nthis paper, we propose such a framework grounded in the semiotic theory of C.\nS. Peirce. We argue that such models measure latent symbol geometries, which\ncan be understood as hypotheses about the complex of semiotic agencies\nunderlying a symbolic dataset. Further, we argue that in contexts where a\nmodel's value cannot be straightforwardly captured by proxy measures of\nperformance, models can instead be understood relationally, so that the\nparticular interpretive lens of a model becomes visible through its contrast\nwith other models. This forms the basis of a theory of model semantics in which\nmodels, and the modeling decisions that constitute them, are themselves treated\nas signs. In addition to proposing the framework, we illustrate its empirical\nuse with a few brief examples and consider foundational questions and future\ndirections enabled by the framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u76ae\u5c14\u58eb\u7b26\u53f7\u5b66\u7406\u8bba\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u7edf\u4e00\u63cf\u8ff0\u548c\u5206\u6790\u5404\u79cd\u4eba\u7c7b\u610f\u4e49\u5efa\u6784\u5efa\u6a21\u5b9e\u8df5\uff0c\u5c06\u6a21\u578b\u89c6\u4e3a\u7b26\u53f7\u5e76\u5173\u6ce8\u5176\u8bed\u4e49\u5173\u7cfb\u3002", "motivation": "\u5f53\u524d\u4eba\u7c7b\u610f\u4e49\u5efa\u6784\u5efa\u6a21\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u8de8\u6a21\u578b\u7c7b\u578b\u8fdb\u884c\u6807\u51c6\u5316\u63cf\u8ff0\u548c\u5206\u6790\uff0c\u9700\u8981\u5efa\u7acb\u901a\u7528\u7684\u7406\u8bba\u4f53\u7cfb\u3002", "method": "\u57fa\u4e8eC.S. Peirce\u7684\u7b26\u53f7\u5b66\u7406\u8bba\u6784\u5efa\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u6a21\u578b\u89c6\u4e3a\u6d4b\u91cf\u6f5c\u5728\u7b26\u53f7\u51e0\u4f55\u7684\u5047\u8bbe\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u95f4\u5bf9\u6bd4\u5173\u7cfb\u6765\u7406\u89e3\u6a21\u578b\u8bed\u4e49\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u80fd\u591f\u7edf\u4e00\u63cf\u8ff0\u5404\u79cd\u5efa\u6a21\u5b9e\u8df5\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u7b80\u8981\u5b9e\u4f8b\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u7684\u5b9e\u8bc1\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5efa\u6a21\u5b9e\u8df5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5c06\u6a21\u578b\u672c\u8eab\u89c6\u4e3a\u7b26\u53f7\u8fdb\u884c\u5904\u7406\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.00318", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00318", "abs": "https://arxiv.org/abs/2509.00318", "authors": ["Tianyu Song", "Ton Viet Ta"], "title": "Towards High-Fidelity and Controllable Bioacoustic Generation via Enhanced Diffusion Learning", "comment": null, "summary": "Generative modeling offers new opportunities for bioacoustics, enabling the\nsynthesis of realistic animal vocalizations that could support biomonitoring\nefforts and supplement scarce data for endangered species. However, directly\ngenerating bird call waveforms from noisy field recordings remains a major\nchallenge.\n  We propose BirdDiff, a generative framework designed to synthesize bird calls\nfrom a noisy dataset of 12 wild bird species. The model incorporates a \"zeroth\nlayer\" stage for multi-scale adaptive bird-call enhancement, followed by a\ndiffusion-based generator conditioned on three modalities: Mel-frequency\ncepstral coefficients, species labels, and textual descriptions. The\nenhancement stage improves signal-to-noise ratio (SNR) while minimizing\nspectral distortion, achieving the highest SNR gain (+10.45 dB) and lowest\nItakura-Saito Distance (0.54) compared to three widely used non-training\nenhancement methods.\n  We evaluate BirdDiff against a baseline generative model, DiffWave. Our\nmethod yields substantial improvements in generative quality metrics: Fr\\'echet\nAudio Distance (0.590 to 0.213), Jensen-Shannon Divergence (0.259 to 0.226),\nand Number of Statistically-Different Bins (7.33 to 5.58). To assess\nspecies-specific detail preservation, we use a ResNet50 classifier trained on\nthe original dataset to identify generated samples. Classification accuracy\nimproves from 35.9% (DiffWave) to 70.1% (BirdDiff), with 8 of 12 species\nexceeding 70% accuracy.\n  These results demonstrate that BirdDiff enables high-fidelity, controllable\nbird call generation directly from noisy field recordings.", "AI": {"tldr": "BirdDiff\u662f\u4e00\u4e2a\u751f\u6210\u5f0f\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u9e1f\u9e23\u589e\u5f3a\u548c\u6269\u6563\u6a21\u578b\uff0c\u76f4\u63a5\u4ece\u5608\u6742\u7684\u91ce\u5916\u5f55\u97f3\u4e2d\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u9e1f\u7c7b\u53eb\u58f0\uff0c\u5728\u4fe1\u566a\u6bd4\u548c\u5206\u7c7b\u51c6\u786e\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u751f\u6210\u5f0f\u5efa\u6a21\u4e3a\u751f\u7269\u58f0\u5b66\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\uff0c\u53ef\u4ee5\u5408\u6210\u771f\u5b9e\u7684\u52a8\u7269\u53eb\u58f0\u6765\u652f\u6301\u751f\u7269\u76d1\u6d4b\u5de5\u4f5c\u5e76\u4e3a\u6fd2\u5371\u7269\u79cd\u8865\u5145\u7a00\u7f3a\u6570\u636e\u3002\u4f46\u76f4\u63a5\u4ece\u5608\u6742\u7684\u91ce\u5916\u5f55\u97f3\u751f\u6210\u9e1f\u7c7b\u53eb\u58f0\u6ce2\u5f62\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "BirdDiff\u5305\u542b\u4e00\u4e2a\u7528\u4e8e\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u9e1f\u9e23\u589e\u5f3a\u7684\"\u96f6\u5c42\"\u9636\u6bb5\uff0c\u7136\u540e\u662f\u57fa\u4e8e\u4e09\u79cd\u6a21\u6001\uff08\u6885\u5c14\u9891\u7387\u5012\u8c31\u7cfb\u6570\u3001\u7269\u79cd\u6807\u7b7e\u548c\u6587\u672c\u63cf\u8ff0\uff09\u7684\u6269\u6563\u751f\u6210\u5668\u3002\u589e\u5f3a\u9636\u6bb5\u5728\u6700\u5c0f\u5316\u9891\u8c31\u5931\u771f\u7684\u540c\u65f6\u63d0\u9ad8\u4fe1\u566a\u6bd4\u3002", "result": "\u4e0e\u4e09\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u975e\u8bad\u7ec3\u589e\u5f3a\u65b9\u6cd5\u76f8\u6bd4\uff0c\u83b7\u5f97\u4e86\u6700\u9ad8\u7684SNR\u589e\u76ca\uff08+10.45 dB\uff09\u548c\u6700\u4f4e\u7684Itakura-Saito\u8ddd\u79bb\uff080.54\uff09\u3002\u76f8\u6bd4DiffWave\u57fa\u7ebf\uff0c\u5728FAD\uff080.590\u21920.213\uff09\u3001JSD\uff080.259\u21920.226\uff09\u7b49\u751f\u6210\u8d28\u91cf\u6307\u6807\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u4ece35.9%\u63d0\u9ad8\u523070.1%\u3002", "conclusion": "BirdDiff\u80fd\u591f\u76f4\u63a5\u4ece\u5608\u6742\u7684\u91ce\u5916\u5f55\u97f3\u4e2d\u5b9e\u73b0\u9ad8\u4fdd\u771f\u3001\u53ef\u63a7\u7684\u9e1f\u7c7b\u53eb\u58f0\u751f\u6210\uff0c\u4e3a\u751f\u7269\u58f0\u5b66\u76d1\u6d4b\u548c\u6fd2\u5371\u7269\u79cd\u4fdd\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00250", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00250", "abs": "https://arxiv.org/abs/2509.00250", "authors": ["Hugo Sousa", "Ricardo Campos", "Al\u00edpio Jorge"], "title": "The Temporal Game: A New Perspective on Temporal Relation Extraction", "comment": null, "summary": "In this paper we demo the Temporal Game, a novel approach to temporal\nrelation extraction that casts the task as an interactive game. Instead of\ndirectly annotating interval-level relations, our approach decomposes them into\npoint-wise comparisons between the start and end points of temporal entities.\nAt each step, players classify a single point relation, and the system applies\ntemporal closure to infer additional relations and enforce consistency. This\npoint-based strategy naturally supports both interval and instant entities,\nenabling more fine-grained and flexible annotation than any previous approach.\nThe Temporal Game also lays the groundwork for training reinforcement learning\nagents, by treating temporal annotation as a sequential decision-making task.\nTo showcase this potential, the demo presented in this paper includes a Game\nmode, in which users annotate texts from the TempEval-3 dataset and receive\nfeedback based on a scoring system, and an Annotation mode, that allows custom\ndocuments to be annotated and resulting timeline to be exported. Therefore,\nthis demo serves both as a research tool and an annotation interface. The demo\nis publicly available at https://temporal-game.inesctec.pt, and the source code\nis open-sourced to foster further research and community-driven development in\ntemporal reasoning and annotation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Temporal Game\uff0c\u4e00\u79cd\u5c06\u65f6\u95f4\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u8f6c\u5316\u4e3a\u4ea4\u4e92\u5f0f\u6e38\u620f\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u70b9\u7ea7\u6bd4\u8f83\u548c\u65f6\u5e8f\u95ed\u5305\u63a8\u7406\u6765\u5b9e\u73b0\u66f4\u7ec6\u7c92\u5ea6\u7684\u65f6\u95f4\u6807\u6ce8\u3002", "motivation": "\u4f20\u7edf\u7684\u65f6\u95f4\u5173\u7cfb\u6807\u6ce8\u65b9\u6cd5\u76f4\u63a5\u6807\u6ce8\u533a\u95f4\u7ea7\u5173\u7cfb\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7ec6\u7c92\u5ea6\u548c\u7075\u6d3b\u7684\u65b9\u6cd5\u6765\u540c\u65f6\u652f\u6301\u533a\u95f4\u548c\u77ac\u65f6\u5b9e\u4f53\u7684\u6807\u6ce8\u3002", "method": "\u5c06\u65f6\u95f4\u5173\u7cfb\u5206\u89e3\u4e3a\u65f6\u95f4\u5b9e\u4f53\u8d77\u70b9\u548c\u7ec8\u70b9\u7684\u70b9\u7ea7\u6bd4\u8f83\uff0c\u73a9\u5bb6\u5728\u6bcf\u4e00\u6b65\u5206\u7c7b\u5355\u4e2a\u70b9\u5173\u7cfb\uff0c\u7cfb\u7edf\u5e94\u7528\u65f6\u5e8f\u95ed\u5305\u63a8\u7406\u6765\u63a8\u65ad\u989d\u5916\u5173\u7cfb\u5e76\u786e\u4fdd\u4e00\u81f4\u6027\u3002", "result": "\u5f00\u53d1\u4e86\u5305\u542b\u6e38\u620f\u6a21\u5f0f\u548c\u6807\u6ce8\u6a21\u5f0f\u7684\u6f14\u793a\u7cfb\u7edf\uff0c\u652f\u6301TempEval-3\u6570\u636e\u96c6\u7684\u6807\u6ce8\u548c\u81ea\u5b9a\u4e49\u6587\u6863\u6807\u6ce8\uff0c\u7cfb\u7edf\u5df2\u516c\u5f00\u53ef\u7528\u5e76\u5f00\u6e90\u3002", "conclusion": "Temporal Game\u4e0d\u4ec5\u4f5c\u4e3a\u7814\u7a76\u5de5\u5177\u548c\u6807\u6ce8\u754c\u9762\uff0c\u8fd8\u4e3a\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c06\u65f6\u95f4\u6807\u6ce8\u89c6\u4e3a\u987a\u5e8f\u51b3\u7b56\u4efb\u52a1\uff0c\u63a8\u52a8\u4e86\u65f6\u95f4\u63a8\u7406\u548c\u6807\u6ce8\u7814\u7a76\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2509.00405", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00405", "abs": "https://arxiv.org/abs/2509.00405", "authors": ["Xihao Yuan", "Siqi Liu", "Yan Chen", "Hang Zhou", "Chang Liu", "Hanting Chen", "Jie Hu"], "title": "SaD: A Scenario-Aware Discriminator for Speech Enhancement", "comment": "5 pages, 2 figures.Accepted by InterSpeech2025", "summary": "Generative adversarial network-based models have shown remarkable performance\nin the field of speech enhancement. However, the current optimization\nstrategies for these models predominantly focus on refining the architecture of\nthe generator or enhancing the quality evaluation metrics of the discriminator.\nThis approach often overlooks the rich contextual information inherent in\ndiverse scenarios. In this paper, we propose a scenario-aware discriminator\nthat captures scene-specific features and performs frequency-domain division,\nthereby enabling a more accurate quality assessment of the enhanced speech\ngenerated by the generator. We conducted comprehensive experiments on three\nrepresentative models using two publicly available datasets. The results\ndemonstrate that our method can effectively adapt to various generator\narchitectures without altering their structure, thereby unlocking further\nperformance gains in speech enhancement across different scenarios.", "AI": {"tldr": "\u901a\u8fc7\u63d0\u51fa\u573a\u666f\u611f\u77e5\u8fa8\u522b\u5668\uff0c\u5728\u4e0d\u6539\u53d8\u751f\u6210\u5668\u7ed3\u6784\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u4e86\u8bed\u97f3\u589e\u5f3a\u6027\u80fd\u7684\u8fdb\u4e00\u6b65\u63d0\u5347", "motivation": "\u5f53\u524dGAN\u6a21\u578b\u7684\u4f18\u5316\u7b56\u7565\u4e3b\u8981\u96c6\u4e2d\u5728\u751f\u6210\u5668\u7ed3\u6784\u6216\u8bc4\u4f30\u6307\u6807\u7684\u6539\u8fdb\uff0c\u5ffd\u89c6\u4e86\u591a\u6837\u5316\u573a\u666f\u4e2d\u7684\u4e30\u5bcc\u4e0a\u4e0b\u6587\u4fe1\u606f", "method": "\u63d0\u51fa\u573a\u666f\u611f\u77e5\u8fa8\u522b\u5668\uff0c\u6293\u53d6\u573a\u666f\u7279\u5f81\u5e76\u8fdb\u884c\u9891\u57df\u5206\u5272\uff0c\u4ece\u800c\u66f4\u51c6\u786e\u8bc4\u4f30\u751f\u6210\u5668\u4ea7\u51fa\u7684\u589e\u5f3a\u8bed\u97f3\u8d28\u91cf", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5bf9\u4e09\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\u8fdb\u884c\u5b8c\u6574\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u65b9\u6cd5\u80fd\u6709\u6548\u9002\u5e94\u5404\u79cd\u751f\u6210\u5668\u7ed3\u6784", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5728\u4e0d\u6539\u53d8\u751f\u6210\u5668\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u8bed\u97f3\u589e\u5f3a\u5e94\u7528\u5e26\u6765\u8fdb\u4e00\u6b65\u7684\u6027\u80fd\u63d0\u5347"}}
{"id": "2509.00276", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00276", "abs": "https://arxiv.org/abs/2509.00276", "authors": ["Yuxiang Liu", "Tian Wang", "Gourab Kundu", "Tianyu Cao", "Guang Cheng", "Zhen Ge", "Jianshu Chen", "Qingjun Cui", "Trishul Chilimbi"], "title": "Exploring Reasoning-Infused Text Embedding with Large Language Models for Zero-Shot Dense Retrieval", "comment": "CIKM 2025", "summary": "Transformer-based models such as BERT and E5 have significantly advanced text\nembedding by capturing rich contextual representations. However, many complex\nreal-world queries require sophisticated reasoning to retrieve relevant\ndocuments beyond surface-level lexical matching, where encoder-only retrievers\noften fall short. Decoder-only large language models (LLMs), known for their\nstrong reasoning capabilities, offer a promising alternative. Despite this\npotential, existing LLM-based embedding methods primarily focus on contextual\nrepresentation and do not fully exploit the reasoning strength of LLMs. To\nbridge this gap, we propose Reasoning-Infused Text Embedding (RITE), a simple\nbut effective approach that integrates logical reasoning into the text\nembedding process using generative LLMs. RITE builds upon existing language\nmodel embedding techniques by generating intermediate reasoning texts in the\ntoken space before computing embeddings, thereby enriching representations with\ninferential depth. Experimental results on BRIGHT, a reasoning-intensive\nretrieval benchmark, demonstrate that RITE significantly enhances zero-shot\nretrieval performance across diverse domains, underscoring the effectiveness of\nincorporating reasoning into the embedding process.", "AI": {"tldr": "RITE\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u63a8\u7406\u6587\u672c\u6765\u589e\u5f3a\u6587\u672c\u5d4c\u5165\uff0c\u5728BRIGHT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u68c0\u7d22\u6027\u80fd", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eLLM\u7684\u5d4c\u5165\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4e0a\u4e0b\u6587\u8868\u793a\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528LLM\u7684\u63a8\u7406\u80fd\u529b\u6765\u5904\u7406\u9700\u8981\u590d\u6742\u63a8\u7406\u7684\u68c0\u7d22\u4efb\u52a1", "method": "\u63d0\u51faRITE\u65b9\u6cd5\uff0c\u5728\u8ba1\u7b97\u5d4c\u5165\u4e4b\u524d\u751f\u6210\u4e2d\u95f4\u63a8\u7406\u6587\u672c\uff0c\u5c06\u903b\u8f91\u63a8\u7406\u878d\u5165\u6587\u672c\u5d4c\u5165\u8fc7\u7a0b\uff0c\u4e30\u5bcc\u8868\u793a\u5c42\u7684\u63a8\u7406\u6df1\u5ea6", "result": "\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u68c0\u7d22\u57fa\u51c6BRIGHT\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cRITE\u663e\u8457\u63d0\u5347\u4e86\u8de8\u9886\u57df\u7684\u96f6\u6837\u672c\u68c0\u7d22\u6027\u80fd", "conclusion": "\u5c06\u63a8\u7406\u878d\u5165\u5d4c\u5165\u8fc7\u7a0b\u662f\u6709\u6548\u7684\uff0cRITE\u65b9\u6cd5\u6210\u529f\u5229\u7528\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\u6765\u589e\u5f3a\u6587\u672c\u5d4c\u5165\u8d28\u91cf"}}
{"id": "2509.00285", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.00285", "abs": "https://arxiv.org/abs/2509.00285", "authors": ["Mir Tafseer Nayeem", "Davood Rafiei"], "title": "OpinioRAG: Towards Generating User-Centric Opinion Highlights from Large-scale Online Reviews", "comment": "COLM 2025", "summary": "We study the problem of opinion highlights generation from large volumes of\nuser reviews, often exceeding thousands per entity, where existing methods\neither fail to scale or produce generic, one-size-fits-all summaries that\noverlook personalized needs. To tackle this, we introduce OpinioRAG, a\nscalable, training-free framework that combines RAG-based evidence retrieval\nwith LLMs to efficiently produce tailored summaries. Additionally, we propose\nnovel reference-free verification metrics designed for sentiment-rich domains,\nwhere accurately capturing opinions and sentiment alignment is essential. These\nmetrics offer a fine-grained, context-sensitive assessment of factual\nconsistency. To facilitate evaluation, we contribute the first large-scale\ndataset of long-form user reviews, comprising entities with over a thousand\nreviews each, paired with unbiased expert summaries and manually annotated\nqueries. Through extensive experiments, we identify key challenges, provide\nactionable insights into improving systems, pave the way for future research,\nand position OpinioRAG as a robust framework for generating accurate, relevant,\nand structured summaries at scale.", "AI": {"tldr": "\u63d0\u51faOpinioRAG\u6846\u67b6\uff0c\u57fa\u4e8eRAG\u548cLLM\u4ece\u6d77\u91cf\u7528\u6237\u8bc4\u8bba\u4e2d\u751f\u6210\u4e2a\u6027\u5316\u6458\u8981\uff0c\u65e0\u9700\u8bad\u7ec3\uff0c\u5e76\u8bbe\u8ba1\u4e86\u65b0\u7684\u65e0\u53c2\u8003\u9a8c\u8bc1\u6307\u6807\u7528\u4e8e\u60c5\u611f\u4e30\u5bcc\u9886\u57df\u7684\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u6570\u5343\u6761\u8bc4\u8bba\u7684\u89c4\u6a21\uff0c\u4e14\u751f\u6210\u7684\u6458\u8981\u8fc7\u4e8e\u901a\u7528\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4e2a\u6027\u5316\u9700\u6c42\uff0c\u9700\u8981\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u4e2a\u6027\u5316\u6458\u8981\u751f\u6210\u65b9\u6848\u3002", "method": "\u7ed3\u5408RAG\u8bc1\u636e\u68c0\u7d22\u548cLLM\uff0c\u6784\u5efa\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6OpinioRAG\uff0c\u540c\u65f6\u63d0\u51fa\u9488\u5bf9\u60c5\u611f\u4e30\u5bcc\u9886\u57df\u7684\u65b0\u578b\u65e0\u53c2\u8003\u9a8c\u8bc1\u6307\u6807\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u957f\u8bc4\u8bba\u6570\u636e\u96c6\uff0c\u5305\u542b\u6bcf\u4e2a\u5b9e\u4f53\u4e0a\u5343\u6761\u8bc4\u8bba\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u63d0\u4f9b\u4e86\u6539\u8fdb\u7cfb\u7edf\u7684\u53ef\u884c\u89c1\u89e3\u3002", "conclusion": "OpinioRAG\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5927\u89c4\u6a21\u751f\u6210\u51c6\u786e\u3001\u76f8\u5173\u4e14\u7ed3\u6784\u5316\u7684\u6458\u8981\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.00683", "categories": ["cs.SD", "eess.AS", "68Txx", "I.2"], "pdf": "https://arxiv.org/pdf/2509.00683", "abs": "https://arxiv.org/abs/2509.00683", "authors": ["Zihao Zheng", "Zeyu Xie", "Xuenan Xu", "Wen Wu", "Chao Zhang", "Mengyue Wu"], "title": "PicoAudio2: Temporal Controllable Text-to-Audio Generation with Natural Language Description", "comment": "Demo page: https://HiRookie9.github.io/PicoAudio2-Page", "summary": "Controllable text-to-audio generation (TTA) has attracted much attention\nrecently. Although existing works can achieve fine-grained controllability\nbased on timestamp information, sound event categories are limited to a fixed\nset. Moreover, since only simulated data is used for training, the generated\naudio quality and generalization performance on real data are limited. To\ntackle this issue, we propose PicoAudio2, improving temporal-controllable TTA\nvia a new data processing pipeline and model architecture. Specifically, we use\na grounding model to annotate event timestamps of real audio-text datasets to\ncurate temporally-strong real data, in addition to simulation data from\nexisting works. The model is trained on the combination of real and simulation\ndata. Moreover, following PicoAudio, we encode timestamp information into a\ntimestamp matrix to provide extra fine-grained time-aligned information to the\nmodel, on top of the coarse-grained textual description. Experiments show that\nPicoAudio2 exhibits superior performance in terms of temporal controllability\nand audio quality.", "AI": {"tldr": "PicoAudio2\u901a\u8fc7\u65b0\u7684\u6570\u636e\u5904\u7406\u6d41\u7a0b\u548c\u6a21\u578b\u67b6\u6784\u6539\u8fdb\u65f6\u5e8f\u53ef\u63a7\u7684\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\uff0c\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u548c\u6a21\u62df\u6570\u636e\u7ed3\u5408\u8bad\u7ec3\uff0c\u63d0\u5347\u751f\u6210\u97f3\u9891\u8d28\u91cf\u548c\u65f6\u5e8f\u63a7\u5236\u80fd\u529b", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\u65b9\u6cd5\u867d\u7136\u80fd\u57fa\u4e8e\u65f6\u95f4\u6233\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u4f46\u58f0\u97f3\u4e8b\u4ef6\u7c7b\u522b\u53d7\u9650\u4e14\u4ec5\u4f7f\u7528\u6a21\u62df\u6570\u636e\u8bad\u7ec3\uff0c\u5bfc\u81f4\u751f\u6210\u97f3\u9891\u8d28\u91cf\u548c\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u6027\u80fd\u6709\u9650", "method": "\u4f7f\u7528grounding\u6a21\u578b\u6807\u6ce8\u771f\u5b9e\u97f3\u9891-\u6587\u672c\u6570\u636e\u96c6\u7684\u65f6\u95f4\u6233\u6765\u6784\u5efa\u65f6\u5e8f\u5f3a\u5316\u7684\u771f\u5b9e\u6570\u636e\uff0c\u7ed3\u5408\u73b0\u6709\u5de5\u4f5c\u7684\u6a21\u62df\u6570\u636e\u8bad\u7ec3\u6a21\u578b\uff1b\u91c7\u7528\u65f6\u95f4\u6233\u77e9\u9635\u7f16\u7801\u65f6\u95f4\u4fe1\u606f\uff0c\u5728\u7c97\u7c92\u5ea6\u6587\u672c\u63cf\u8ff0\u57fa\u7840\u4e0a\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u65f6\u95f4\u5bf9\u9f50\u4fe1\u606f", "result": "\u5b9e\u9a8c\u8868\u660ePicoAudio2\u5728\u65f6\u5e8f\u53ef\u63a7\u6027\u548c\u97f3\u9891\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd", "conclusion": "\u901a\u8fc7\u771f\u5b9e\u6570\u636e\u548c\u6a21\u62df\u6570\u636e\u7ed3\u5408\u8bad\u7ec3\u4ee5\u53ca\u65f6\u95f4\u6233\u77e9\u9635\u7f16\u7801\uff0cPicoAudio2\u663e\u8457\u63d0\u5347\u4e86\u65f6\u5e8f\u53ef\u63a7\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\u7684\u6027\u80fd"}}
{"id": "2509.00290", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00290", "abs": "https://arxiv.org/abs/2509.00290", "authors": ["Taihei Sone"], "title": "Wage Sentiment Indices Derived from Survey Comments via Large Language Models", "comment": "Submitted to IEEE Big Data 2025. 10 pages, 2 tables, 16 figures", "summary": "The emergence of generative Artificial Intelligence (AI) has created new\nopportunities for economic text analysis. This study proposes a Wage Sentiment\nIndex (WSI) constructed with Large Language Models (LLMs) to forecast wage\ndynamics in Japan. The analysis is based on the Economy Watchers Survey (EWS),\na monthly survey conducted by the Cabinet Office of Japan that captures\nreal-time economic assessments from workers in industries highly sensitive to\nbusiness conditions. The WSI extends the framework of the Price Sentiment Index\n(PSI) used in prior studies, adapting it specifically to wage related\nsentiment. To ensure scalability and adaptability, a data architecture is also\ndeveloped that enables integration of additional sources such as newspapers and\nsocial media. Experimental results demonstrate that WSI models based on LLMs\nsignificantly outperform both baseline approaches and pretrained models. These\nfindings highlight the potential of LLM-driven sentiment indices to enhance the\ntimeliness and effectiveness of economic policy design by governments and\ncentral banks.", "AI": {"tldr": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u65b0\u85aa\u8d44\u60c5\u611f\u6307\u6570(WSI)\u9884\u6d4b\u65e5\u672c\u85aa\u8d44\u52a8\u6001\uff0c\u663e\u8457\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5", "motivation": "\u5229\u7528\u751f\u6210\u5f0fAI\u4e3a\u7ecf\u6d4e\u6587\u672c\u5206\u6790\u5e26\u6765\u65b0\u673a\u9047\uff0c\u901a\u8fc7\u6784\u5efa\u85aa\u8d44\u60c5\u611f\u6307\u6570\u63d0\u9ad8\u85aa\u8d44\u9884\u6d4b\u7684\u53ca\u65f6\u6027\u548c\u6709\u6548\u6027", "method": "\u57fa\u4e8e\u65e5\u672c\u5185\u95a1\u5e9c\u7ecf\u6d4e\u76d1\u6d4b\u8c03\u67e5(EWS)\u6570\u636e\uff0c\u6269\u5c55\u4ef7\u683c\u60c5\u611f\u6307\u6570(PSI)\u6846\u67b6\u6784\u5efaWSI\uff0c\u5e76\u53d1\u5c55\u652f\u6301\u591a\u6e90\u6570\u636e\u96c6\u6210\u7684\u6570\u636e\u67b6\u6784", "result": "LLM\u57fa\u7840\u7684WSI\u6a21\u578b\u663e\u8457\u8d85\u8d8a\u4e86\u57fa\u51c6\u65b9\u6cd5\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8868\u73b0", "conclusion": "LLM\u9a71\u52a8\u7684\u60c5\u611f\u6307\u6570\u6709\u529b\u63d0\u5347\u653f\u5e9c\u548c\u4e2d\u592e\u94f6\u884c\u7ecf\u6d4e\u653f\u7b56\u8bbe\u8ba1\u7684\u53ca\u65f6\u6027\u548c\u6709\u6548\u6027"}}
{"id": "2509.00813", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00813", "abs": "https://arxiv.org/abs/2509.00813", "authors": ["Gyehun Go", "Satbyul Han", "Ahyeon Choi", "Eunjin Choi", "Juhan Nam", "Jeong Mi Park"], "title": "AImoclips: A Benchmark for Evaluating Emotion Conveyance in Text-to-Music Generation", "comment": "to be published in HCMIR25: 3rd Workshop on Human-Centric Music\n  Information Research", "summary": "Recent advances in text-to-music (TTM) generation have enabled controllable\nand expressive music creation using natural language prompts. However, the\nemotional fidelity of TTM systems remains largely underexplored compared to\nhuman preference or text alignment. In this study, we introduce AImoclips, a\nbenchmark for evaluating how well TTM systems convey intended emotions to human\nlisteners, covering both open-source and commercial models. We selected 12\nemotion intents spanning four quadrants of the valence-arousal space, and used\nsix state-of-the-art TTM systems to generate over 1,000 music clips. A total of\n111 participants rated the perceived valence and arousal of each clip on a\n9-point Likert scale. Our results show that commercial systems tend to produce\nmusic perceived as more pleasant than intended, while open-source systems tend\nto perform the opposite. Emotions are more accurately conveyed under\nhigh-arousal conditions across all models. Additionally, all systems exhibit a\nbias toward emotional neutrality, highlighting a key limitation in affective\ncontrollability. This benchmark offers valuable insights into model-specific\nemotion rendering characteristics and supports future development of\nemotionally aligned TTM systems.", "AI": {"tldr": "AImoclips\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u6587\u672c\u5230\u97f3\u4e50\u751f\u6210\u7cfb\u7edf\u5728\u60c5\u611f\u4f20\u8fbe\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5546\u4e1a\u7cfb\u7edf\u503e\u5411\u4e8e\u751f\u6210\u6bd4\u9884\u671f\u66f4\u6109\u60a6\u7684\u97f3\u4e50\uff0c\u800c\u5f00\u6e90\u7cfb\u7edf\u5219\u76f8\u53cd\uff0c\u6240\u6709\u7cfb\u7edf\u90fd\u5b58\u5728\u60c5\u611f\u4e2d\u6027\u5316\u7684\u504f\u5dee\u3002", "motivation": "\u6587\u672c\u5230\u97f3\u4e50\u751f\u6210\u7cfb\u7edf\u5728\u60c5\u611f\u4fdd\u771f\u5ea6\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u8bc4\u4f30\u8fd9\u4e9b\u7cfb\u7edf\u5982\u4f55\u5411\u4eba\u7c7b\u542c\u4f17\u4f20\u8fbe\u9884\u671f\u60c5\u611f\u3002", "method": "\u521b\u5efaAImoclips\u57fa\u51c6\uff0c\u9009\u62e912\u79cd\u60c5\u611f\u610f\u56fe\u8986\u76d6\u6548\u4ef7-\u5524\u9192\u7a7a\u95f4\u7684\u56db\u4e2a\u8c61\u9650\uff0c\u4f7f\u75286\u4e2a\u6700\u5148\u8fdb\u7684TTM\u7cfb\u7edf\u751f\u62101000\u591a\u4e2a\u97f3\u4e50\u7247\u6bb5\uff0c111\u540d\u53c2\u4e0e\u8005\u57289\u70b9\u674e\u514b\u7279\u91cf\u8868\u4e0a\u8bc4\u5206\u3002", "result": "\u5546\u4e1a\u7cfb\u7edf\u4ea7\u751f\u6bd4\u9884\u671f\u66f4\u6109\u60a6\u7684\u97f3\u4e50\uff0c\u5f00\u6e90\u7cfb\u7edf\u8868\u73b0\u76f8\u53cd\uff1b\u9ad8\u5524\u9192\u6761\u4ef6\u4e0b\u60c5\u611f\u4f20\u8fbe\u66f4\u51c6\u786e\uff1b\u6240\u6709\u7cfb\u7edf\u90fd\u5b58\u5728\u60c5\u611f\u4e2d\u6027\u5316\u504f\u5dee\u3002", "conclusion": "\u8be5\u57fa\u51c6\u63ed\u793a\u4e86\u6a21\u578b\u7279\u5b9a\u7684\u60c5\u611f\u6e32\u67d3\u7279\u5f81\uff0c\u4e3a\u5f00\u53d1\u60c5\u611f\u5bf9\u9f50\u7684\u6587\u672c\u5230\u97f3\u4e50\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2509.00309", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00309", "abs": "https://arxiv.org/abs/2509.00309", "authors": ["Chen Zheng", "Yiyuan Ma", "Yuan Yang", "Deyi Liu", "Jing Liu", "Zuquan Song", "Yuxin Song", "Cheng Ren", "Hang Zhu", "Xin Liu", "Yiyuan Ma", "Siyuan Qiao", "Xun Zhou", "Liang Xiang", "Yonghui Wu"], "title": "Balanced Actor Initialization: Stable RLHF Training of Distillation-Based Reasoning Models", "comment": null, "summary": "The development of alignment and reasoning capabilities in large language\nmodels has seen remarkable progress through two paradigms: instruction tuning\nand reinforcement learning from human feedback (RLHF) alignment paradigm, and\ndistillation-based reasoning fine-tuning paradigm. While both approaches prove\neffective independently, the third paradigm of applying RLHF to\ndistillation-trained models presents significant challenges. Our investigation\nreveals two critical phenomena that emerge in this paradigm: Sequence Length\nCollapse, where language generation dramatically reduces during early RLHF\ntraining, and the Reward Hockey Stick Curve, featuring severe reward score\ndrops followed by gradual recovery. These instabilities fundamentally\ncompromise the model's alignment and reasoning capabilities. To address these\nchallenges, we propose Balanced Actor Initialization (BAI), a two-stage\nweighted model merging approach. BAI first merges instruction-following and\ndistillation-based reasoning fine-tuned models, then further combines this\nintermediate model with the pretrained model to preserve foundational\nknowledge. Through comprehensive experiments across diverse benchmarks and\ndetailed analysis of training experiments, we demonstrate that BAI resolves\nSequence Length Collapse, mitigates the Reward Hockey Stick Curve, and enables\ncontinuous sequence length improvement during training. Additionally, our\nanalysis reveals that balanced merging ratios achieve optimal trade-offs\nbetween training stability and reasoning capability preservation. Our work\nprovides the effective solution for stable training in this third paradigm,\nenabling more capable reasoning models that combine distillation efficiency\nwith RLHF alignment.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u84b8\u998f\u8bad\u7ec3\u6a21\u578b\u4e0a\u5e94\u7528RLHF\u65f6\u51fa\u73b0\u7684\u5e8f\u5217\u957f\u5ea6\u5d29\u6e83\u548c\u5956\u52b1\u66f2\u68cd\u7403\u68d2\u66f2\u7ebf\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5e73\u8861\u6f14\u5458\u521d\u59cb\u5316(BAI)\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u867d\u7136\u6307\u4ee4\u8c03\u4f18\u548cRLHF\u5bf9\u9f50\u4ee5\u53ca\u84b8\u998f\u63a8\u7406\u5fae\u8c03\u5404\u81ea\u6709\u6548\uff0c\u4f46\u5728\u84b8\u998f\u8bad\u7ec3\u6a21\u578b\u4e0a\u5e94\u7528RLHF\u7684\u7b2c\u4e09\u79cd\u8303\u5f0f\u5b58\u5728\u4e25\u91cd\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5f71\u54cd\u6a21\u578b\u7684\u5bf9\u9f50\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u5e73\u8861\u6f14\u5458\u521d\u59cb\u5316(BAI)\u65b9\u6cd5\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u52a0\u6743\u6a21\u578b\u5408\u5e76\uff1a\u9996\u5148\u5408\u5e76\u6307\u4ee4\u8ddf\u968f\u548c\u84b8\u998f\u63a8\u7406\u5fae\u8c03\u6a21\u578b\uff0c\u7136\u540e\u5c06\u4e2d\u95f4\u6a21\u578b\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u4e00\u6b65\u5408\u5e76\u4ee5\u4fdd\u7559\u57fa\u7840\u77e5\u8bc6\u3002", "result": "BAI\u89e3\u51b3\u4e86\u5e8f\u5217\u957f\u5ea6\u5d29\u6e83\u95ee\u9898\uff0c\u7f13\u89e3\u4e86\u5956\u52b1\u66f2\u68cd\u7403\u68d2\u66f2\u7ebf\u73b0\u8c61\uff0c\u5b9e\u73b0\u4e86\u8bad\u7ec3\u671f\u95f4\u5e8f\u5217\u957f\u5ea6\u7684\u6301\u7eed\u6539\u5584\uff0c\u5e76\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "BAI\u4e3a\u7b2c\u4e09\u79cd\u8303\u5f0f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7a33\u5b9a\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u63a8\u7406\u6a21\u578b\u80fd\u591f\u7ed3\u5408\u84b8\u998f\u6548\u7387\u548cRLHF\u5bf9\u9f50\uff0c\u5e73\u8861\u7684\u5408\u5e76\u6bd4\u4f8b\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u63a8\u7406\u80fd\u529b\u4fdd\u6301\u4e4b\u95f4\u8fbe\u5230\u6700\u4f18\u6743\u8861\u3002"}}
{"id": "2509.00839", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00839", "abs": "https://arxiv.org/abs/2509.00839", "authors": ["Yuli Zhang", "Pengfei Fan", "Ruiyuan Jiang", "Hankang Gu", "Dongyao Jia", "Xinheng Wang"], "title": "Adaptive Vehicle Speed Classification via BMCNN with Reinforcement Learning-Enhanced Acoustic Processing", "comment": null, "summary": "Traffic congestion remains a pressing urban challenge, requiring intelligent\ntransportation systems for real-time management. We present a hybrid framework\nthat combines deep learning and reinforcement learning for acoustic vehicle\nspeed classification. A dual-branch BMCNN processes MFCC and wavelet features\nto capture complementary frequency patterns. An attention-enhanced DQN\nadaptively selects the minimal number of audio frames and triggers early\ndecisions once confidence thresholds are reached. Evaluations on IDMT-Traffic\nand our SZUR-Acoustic (Suzhou) datasets show 95.99% and 92.3% accuracy, with up\nto 1.63x faster average processing via early termination. Compared with A3C,\nDDDQN, SA2C, PPO, and TD3, the method provides a superior accuracy-efficiency\ntrade-off and is suitable for real-time ITS deployment in heterogeneous urban\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u58f0\u5b66\u8f66\u8f86\u901f\u5ea6\u5206\u7c7b\uff0c\u901a\u8fc7\u53cc\u5206\u652fCNN\u5904\u7406\u97f3\u9891\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u6ce8\u610f\u529b\u589e\u5f3a\u7684DQN\u5b9e\u73b0\u65e9\u671f\u51b3\u7b56\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u8fbe\u5230\u4f18\u8d8a\u8868\u73b0\u3002", "motivation": "\u4ea4\u901a\u62e5\u5835\u662f\u57ce\u5e02\u9762\u4e34\u7684\u7d27\u8feb\u6311\u6218\uff0c\u9700\u8981\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u8fdb\u884c\u5b9e\u65f6\u7ba1\u7406\u3002\u73b0\u6709\u7684\u8f66\u8f86\u901f\u5ea6\u5206\u7c7b\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u5f02\u6784\u57ce\u5e02\u73af\u5883\u4e2d\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6846\u67b6\uff1a1\uff09\u53cc\u5206\u652fBMCNN\u5904\u7406MFCC\u548c\u5c0f\u6ce2\u7279\u5f81\u4ee5\u6355\u83b7\u4e92\u8865\u9891\u7387\u6a21\u5f0f\uff1b2\uff09\u6ce8\u610f\u529b\u589e\u5f3a\u7684DQN\u81ea\u9002\u5e94\u9009\u62e9\u6700\u5c11\u97f3\u9891\u5e27\u6570\uff0c\u5e76\u5728\u8fbe\u5230\u7f6e\u4fe1\u5ea6\u9608\u503c\u65f6\u89e6\u53d1\u65e9\u671f\u51b3\u7b56\u3002", "result": "\u5728IDMT-Traffic\u548cSZUR-Acoustic\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523095.99%\u548c92.3%\u7684\u51c6\u786e\u7387\uff0c\u901a\u8fc7\u65e9\u671f\u7ec8\u6b62\u5b9e\u73b0\u5e73\u5747\u5904\u7406\u901f\u5ea6\u63d0\u53471.63\u500d\u3002\u76f8\u6bd4A3C\u3001DDDQN\u3001SA2C\u3001PPO\u548cTD3\u7b49\u65b9\u6cd5\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u6743\u8861\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f02\u6784\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5904\u7406\u6548\u7387\u3002"}}
{"id": "2509.00325", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.00325", "abs": "https://arxiv.org/abs/2509.00325", "authors": ["Rinku Dewri"], "title": "GIER: Gap-Driven Self-Refinement for Large Language Models", "comment": null, "summary": "We introduce GIER (Gap-driven Iterative Enhancement of Responses), a general\nframework for improving large language model (LLM) outputs through\nself-reflection and revision based on conceptual quality criteria. Unlike\nprompting strategies that rely on demonstrations, examples, or chain-of-thought\ntemplates, GIER utilizes natural language descriptions of reasoning gaps, and\nprompts a model to iteratively critique and refine its own outputs to better\nsatisfy these criteria. Across three reasoning-intensive tasks (SciFact,\nPrivacyQA, and e-SNLI) and four LLMs (GPT-4.1, GPT-4o Mini, Gemini 1.5 Pro, and\nLlama 3.3 70B), GIER improves rationale quality, grounding, and reasoning\nalignment without degrading task accuracy. Our analysis demonstrates that\nmodels can not only interpret abstract conceptual gaps but also translate them\ninto concrete reasoning improvements.", "AI": {"tldr": "GIER\u662f\u4e00\u4e2a\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u548c\u57fa\u4e8e\u6982\u5ff5\u8d28\u91cf\u6807\u51c6\u7684\u8fed\u4ee3\u4fee\u8ba2\u6765\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u6846\u67b6\uff0c\u65e0\u9700\u4f9d\u8d56\u793a\u4f8b\u6216\u601d\u7ef4\u94fe\u6a21\u677f\uff0c\u76f4\u63a5\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u63a8\u7406\u5dee\u8ddd\u6765\u6307\u5bfc\u6a21\u578b\u4f18\u5316\u8f93\u51fa\u3002", "motivation": "\u73b0\u6709\u7684\u63d0\u793a\u7b56\u7565\u901a\u5e38\u9700\u8981\u4f9d\u8d56\u6f14\u793a\u3001\u793a\u4f8b\u6216\u601d\u7ef4\u94fe\u6a21\u677f\uff0c\u800cGIER\u65e8\u5728\u901a\u8fc7\u66f4\u62bd\u8c61\u7684\u6982\u5ff5\u8d28\u91cf\u63cf\u8ff0\u6765\u6307\u5bfc\u6a21\u578b\u81ea\u6211\u6539\u8fdb\uff0c\u63d0\u5347\u63a8\u7406\u8d28\u91cf\u548c\u51c6\u786e\u6027\u3002", "method": "GIER\u5229\u7528\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u63a8\u7406\u5dee\u8ddd\uff0c\u8ba9\u6a21\u578b\u8fed\u4ee3\u5f0f\u5730\u6279\u5224\u548c\u7cbe\u70bc\u81ea\u8eab\u8f93\u51fa\uff0c\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u6765\u6ee1\u8db3\u6982\u5ff5\u8d28\u91cf\u6807\u51c6\u3002", "result": "\u5728\u4e09\u4e2a\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\uff08SciFact\u3001PrivacyQA\u548ce-SNLI\uff09\u548c\u56db\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGIER\u63d0\u9ad8\u4e86\u7406\u7531\u8d28\u91cf\u3001\u57fa\u7840\u6027\u548c\u63a8\u7406\u4e00\u81f4\u6027\uff0c\u4e14\u4e0d\u964d\u4f4e\u4efb\u52a1\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u4e0d\u4ec5\u80fd\u591f\u89e3\u91ca\u62bd\u8c61\u7684\u6982\u5ff5\u5dee\u8ddd\uff0c\u8fd8\u80fd\u5c06\u5176\u8f6c\u5316\u4e3a\u5177\u4f53\u7684\u63a8\u7406\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86\u81ea\u6211\u53cd\u601d\u548c\u8fed\u4ee3\u4fee\u8ba2\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.00862", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00862", "abs": "https://arxiv.org/abs/2509.00862", "authors": ["Yuriy Izotov", "Andrei Velichko"], "title": "Speech Command Recognition Using LogNNet Reservoir Computing for Embedded Systems", "comment": "20 pages, 6 figures", "summary": "This paper presents a low-resource speech-command recognizer combining\nenergy-based voice activity detection (VAD), an optimized Mel-Frequency\nCepstral Coefficients (MFCC) pipeline, and the LogNNet reservoir-computing\nclassifier. Using four commands from the Speech Commands da-taset downsampled\nto 8 kHz, we evaluate four MFCC aggregation schemes and find that adaptive\nbinning (64-dimensional feature vector) offers the best accuracy-to-compactness\ntrade-off. The LogNNet classifier with architecture 64:33:9:4 reaches 92.04%\naccuracy under speaker-independent evaluation, while requiring significantly\nfewer parameters than conventional deep learn-ing models. Hardware\nimplementation on Arduino Nano 33 IoT (ARM Cor-tex-M0+, 48 MHz, 32 KB RAM)\nvalidates the practical feasibility, achieving ~90% real-time recognition\naccuracy while consuming only 18 KB RAM (55% utilization). The complete\npipeline (VAD -> MFCC -> LogNNet) thus enables reliable on-device\nspeech-command recognition under strict memory and compute limits, making it\nsuitable for battery-powered IoT nodes, wire-less sensor networks, and\nhands-free control interfaces.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u8d44\u6e90\u8bed\u97f3\u547d\u4ee4\u8bc6\u522b\u7cfb\u7edf\uff0c\u7ed3\u5408\u80fd\u91cfVAD\u3001\u4f18\u5316MFCC\u548cLogNNet\u5206\u7c7b\u5668\uff0c\u5728Arduino\u4e0a\u5b9e\u73b0\u9ad8\u6548\u5b9e\u65f6\u8bc6\u522b", "motivation": "\u89e3\u51b3\u5728\u4e25\u683c\u5185\u5b58\u548c\u8ba1\u7b97\u9650\u5236\u4e0b\uff08\u5982IoT\u8bbe\u5907\uff09\u5b9e\u73b0\u53ef\u9760\u8bed\u97f3\u547d\u4ee4\u8bc6\u522b\u7684\u9700\u6c42", "method": "\u4f7f\u7528\u80fd\u91cfVAD\u68c0\u6d4b\u8bed\u97f3\u6d3b\u52a8\uff0c\u4f18\u5316MFCC\u7279\u5f81\u63d0\u53d6\uff08\u81ea\u9002\u5e94\u5206\u7bb164\u7ef4\u7279\u5f81\uff09\uff0c\u91c7\u7528LogNNet\u50a8\u5c42\u8ba1\u7b97\u5206\u7c7b\u5668\uff0864:33:9:4\u67b6\u6784\uff09", "result": "\u8fbe\u523092.04%\u7684\u8bf4\u8bdd\u4eba\u65e0\u5173\u8bc6\u522b\u51c6\u786e\u7387\uff0cArduino\u5b9e\u73b0\u7ea690%\u5b9e\u65f6\u51c6\u786e\u7387\uff0c\u4ec5\u6d88\u801718KB RAM\uff0855%\u5229\u7528\u7387\uff09", "conclusion": "\u8be5\u5b8c\u6574\u6d41\u6c34\u7ebf\u53ef\u5728\u4e25\u683c\u8d44\u6e90\u9650\u5236\u4e0b\u5b9e\u73b0\u53ef\u9760\u7684\u8bbe\u5907\u7aef\u8bed\u97f3\u8bc6\u522b\uff0c\u9002\u7528\u4e8e\u7535\u6c60\u4f9b\u7535IoT\u8282\u70b9\u548c\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc"}}
{"id": "2509.00375", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00375", "abs": "https://arxiv.org/abs/2509.00375", "authors": ["Ziyi Xia", "Kun Luo", "Hongjin Qian", "Zheng Liu"], "title": "Open Data Synthesis For Deep Research", "comment": null, "summary": "Large language models (LLMs) are increasingly expected to go beyond simple\nfactual queries toward Deep Research-tasks that require decomposing questions\ninto sub-problems, coordinating multi-step reasoning, and synthesizing evidence\nfrom diverse sources. We formalize Deep Research tasks with verifiable answers\nas Hierarchical Constraint Satisfaction Problems (HCSPs), which are\nfundamentally different from single-constraint, multi-hop, or flat CSP\nformulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA)\nfail to capture this complexity, while recent synthetic datasets often\nintroduce shortcut reasoning, knowledge leakage, or lack sufficient structural\ndepth. To address this gap, we introduce InfoSeek, a scalable framework for\nsynthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to\nrecursively build a Research Tree from large-scale webpages, blurring\nintermediate nodes into valid sub-problems, and converting these trees into\nnatural language questions that require traversing the full hierarchy. It also\nenables rapid scaling, yielding over 50K training examples, a curated test set,\nand reasoning trajectories generated via reject sampling. Experiments show that\nmodels trained on InfoSeek consistently outperform strong baselines. On a\nchallenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass\nmuch larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash),\nwhile achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro).\nBy preserving meta-information such as intermediate steps and retrieval labels,\nInfoSeek further supports advanced optimization strategies, including compound\nreward design and trajectory-level exploration. We provide our codes and\ndatasets in \\href{https://github.com/VectorSpaceLab/InfoSeek}{this repository}.", "AI": {"tldr": "InfoSeek\u662f\u4e00\u4e2a\u7528\u4e8e\u5408\u6210\u590d\u6742\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u4ee3\u7406\u7cfb\u7edf\u6784\u5efa\u7814\u7a76\u6811\u5e76\u751f\u6210\u9700\u8981\u5c42\u6b21\u904d\u5386\u7684\u81ea\u7136\u8bed\u8a00\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u6355\u6349\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u7684\u590d\u6742\u6027\uff0c\u800c\u5408\u6210\u6570\u636e\u96c6\u5b58\u5728\u63a8\u7406\u6377\u5f84\u3001\u77e5\u8bc6\u6cc4\u9732\u6216\u7ed3\u6784\u6df1\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u6846\u67b6\u6765\u652f\u6301LLMs\u8fdb\u884c\u591a\u6b65\u63a8\u7406\u548c\u8bc1\u636e\u5408\u6210\u3002", "method": "\u4f7f\u7528\u53cc\u4ee3\u7406\u7cfb\u7edf\u9012\u5f52\u5730\u4ece\u5927\u89c4\u6a21\u7f51\u9875\u6784\u5efa\u7814\u7a76\u6811\uff0c\u6a21\u7cca\u4e2d\u95f4\u8282\u70b9\u4e3a\u6709\u6548\u5b50\u95ee\u9898\uff0c\u5e76\u5c06\u8fd9\u4e9b\u6811\u8f6c\u6362\u4e3a\u9700\u8981\u5b8c\u6574\u5c42\u6b21\u904d\u5386\u7684\u81ea\u7136\u8bed\u8a00\u95ee\u9898\uff0c\u652f\u6301\u5feb\u901f\u6269\u5c55\u751f\u6210\u8d85\u8fc75\u4e07\u4e2a\u8bad\u7ec3\u6837\u672c\u3002", "result": "\u5728BrowseComp-Plus\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u4f7f\u7528InfoSeek\u4f18\u5316\u76843B LLMs\u8d85\u8d8a\u4e8632B\u5927\u6a21\u578b\u548c\u8f7b\u91cf\u7ea7\u5546\u4e1aAPI\uff0c\u6027\u80fd\u4e0e\u66f4\u5f3a\u7684API\u76f8\u5f53\u3002", "conclusion": "InfoSeek\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\u6765\u5408\u6210\u590d\u6742\u7684\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u652f\u6301\u9ad8\u7ea7\u4f18\u5316\u7b56\u7565\u5982\u590d\u5408\u5956\u52b1\u8bbe\u8ba1\u548c\u8f68\u8ff9\u7ea7\u63a2\u7d22\u3002"}}
{"id": "2509.00914", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00914", "abs": "https://arxiv.org/abs/2509.00914", "authors": ["Hainan Wang", "Mehdi Hosseinzadeh", "Reza Rawassizadeh"], "title": "TinyMusician: On-Device Music Generation with Knowledge Distillation and Mixed Precision Quantization", "comment": "12 pages for main context, 5 figures", "summary": "The success of the generative model has gained unprecedented attention in the\nmusic generation area. Transformer-based architectures have set new benchmarks\nfor model performance. However, their practical adoption is hindered by some\ncritical challenges: the demand for massive computational resources and\ninference time, due to their large number of parameters. These obstacles make\nthem infeasible to deploy on edge devices, such as smartphones and wearables,\nwith limited computational resources. In this work, we present TinyMusician, a\nlightweight music generation model distilled from MusicGen (a State-of-the-art\nmusic generation model). TinyMusician integrates two innovations: (i)\nStage-mixed Bidirectional and Skewed KL-Divergence and (ii) Adaptive\nMixed-Precision Quantization. The experimental results demonstrate that\nTinyMusician retains 93% of the MusicGen-Small performance with 55% less model\nsize. TinyMusician is the first mobile-deployable music generation model that\neliminates cloud dependency while maintaining high audio fidelity and efficient\nresource usage", "AI": {"tldr": "TinyMusician\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u97f3\u4e50\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u91cf\u5316\u6280\u672f\uff0c\u5728\u4fdd\u630193%\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c1155%\u6a21\u578b\u5927\u5c0f\uff0c\u5b9e\u73b0\u79fb\u52a8\u7aef\u90e8\u7f72\u3002", "motivation": "Transformer\u67b6\u6784\u7684\u97f3\u4e50\u751f\u6210\u6a21\u578b\u867d\u7136\u6027\u80fd\u4f18\u79c0\uff0c\u4f46\u53c2\u6570\u91cf\u5927\u3001\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u96be\u4ee5\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\uff08\u5982\u667a\u80fd\u624b\u673a\uff09\u4e0a\u90e8\u7f72\u3002", "method": "\u4eceMusicGen\u6a21\u578b\u84b8\u998f\u5f97\u5230TinyMusician\uff0c\u91c7\u7528\u4e24\u79cd\u521b\u65b0\u6280\u672f\uff1a(i) \u9636\u6bb5\u6df7\u5408\u53cc\u5411\u548c\u504f\u659cKL\u6563\u5ea6 (ii) \u81ea\u9002\u5e94\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cTinyMusician\u5728\u6a21\u578b\u5927\u5c0f\u51cf\u5c1155%\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u80fd\u4fdd\u6301MusicGen-Small\u6a21\u578b93%\u7684\u6027\u80fd\u3002", "conclusion": "TinyMusician\u662f\u9996\u4e2a\u53ef\u79fb\u52a8\u90e8\u7f72\u7684\u97f3\u4e50\u751f\u6210\u6a21\u578b\uff0c\u6d88\u9664\u4e86\u5bf9\u4e91\u7aef\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u97f3\u9891\u4fdd\u771f\u5ea6\u548c\u9ad8\u6548\u7684\u8d44\u6e90\u4f7f\u7528\u3002"}}
{"id": "2509.00388", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00388", "abs": "https://arxiv.org/abs/2509.00388", "authors": ["Xuelin Li", "Xiangqi Jin", "Linfeng Zhang"], "title": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV Cache Eviction", "comment": null, "summary": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github.", "AI": {"tldr": "GraphKV\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7684KV\u7f13\u5b58\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4f20\u64ad\u673a\u5236\u81ea\u9002\u5e94\u4fdd\u7559\u91cd\u8981token\uff0c\u63d0\u5347\u957f\u6587\u672c\u5904\u7406\u6548\u7387", "motivation": "\u4f20\u7edfKV\u7f13\u5b58\u6dd8\u6c70\u7b56\u7565\u4f9d\u8d56\u9759\u6001\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u65e0\u6cd5\u6355\u6349\u63a8\u7406\u8fc7\u7a0b\u4e2dtoken\u95f4\u52a8\u6001\u7684\u9690\u5f0f\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u5185\u5b58\u53d7\u9650\u4e0b\u7684\u6027\u80fd\u4e0b\u964d", "method": "\u5c06token\u5efa\u6a21\u4e3a\u5e26\u91cd\u8981\u6027\u5206\u6570\u7684\u8282\u70b9\uff0c\u8fb9\u8868\u793a\u76f8\u4f3c\u5173\u7cfb\uff0c\u901a\u8fc7\u8870\u51cf\u4fe1\u53f7\u4f20\u64ad\u673a\u5236\u5728\u56fe\u4e0a\u52a8\u6001\u66f4\u65b0token\u91cd\u8981\u6027", "result": "GraphKV\u53ef\u4ee5\u5373\u63d2\u5373\u7528\u5730\u5e94\u7528\u4e8e\u73b0\u6709KV\u7f13\u5b58\u6dd8\u6c70\u65b9\u6cd5\uff08\u5982SnapKV\u3001PyramidKV\uff09\uff0c\u4ee3\u7801\u5c06\u5728Github\u53d1\u5e03", "conclusion": "GraphKV\u901a\u8fc7\u56fe\u7ed3\u6784\u52a8\u6001\u5efa\u6a21token\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e3aKV\u7f13\u5b58\u538b\u7f29\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684token\u9009\u62e9\u673a\u5236"}}
{"id": "2509.00988", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00988", "abs": "https://arxiv.org/abs/2509.00988", "authors": ["Swadhin Biswas", "Imran", "Tuhin Sheikh"], "title": "A Unified Denoising and Adaptation Framework for Self-Supervised Bengali Dialectal ASR", "comment": null, "summary": "Automatic Speech Recognition (ASR) for Bengali, the world's fifth most spoken\nlanguage, remains a significant challenge, critically hindering technological\naccessibility for its over 270 million speakers. This challenge is compounded\nby two persistent and intertwined factors: the language's vast dialectal\ndiversity and the prevalence of acoustic noise in real-world environments.\nWhile state-of-the-art self-supervised learning (SSL) models have advanced ASR\nfor low-resource languages, they often lack explicit mechanisms to handle\nenvironmental noise during pre-training or specialized adaptation strategies\nfor the complex phonetic and lexical variations across Bengali dialects. This\npaper introduces a novel, unified framework designed to address these dual\nchallenges simultaneously. Our approach is founded on the WavLM model, which is\nuniquely pre-trained with a masked speech denoising objective, making it\ninherently robust to acoustic distortions. We propose a specialized multi-stage\nfine-tuning strategy that first adapts the model to general-domain standard\nBengali to establish a strong linguistic foundation and subsequently\nspecializes it for noise-robust dialectal recognition through targeted data\naugmentation. The framework is rigorously evaluated on a comprehensive\nbenchmark comprising multiple Bengali dialects under a wide range of simulated\nnoisy conditions, from clean audio to low Signal-to-Noise Ratio (SNR) levels.\n  Experimental results demonstrate that the proposed framework significantly\noutperforms strong baselines, including standard fine-tuned wav2vec 2.0 and the\nlarge-scale multilingual Whisper model. This work establishes a new\nstate-of-the-art for this task and provides a scalable, effective blueprint for\ndeveloping practical ASR systems for other low-resource, high-variation\nlanguages globally.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWavLM\u7684\u65b0\u578b\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\u548c\u9488\u5bf9\u6027\u6570\u636e\u589e\u5f3a\uff0c\u540c\u65f6\u89e3\u51b3\u5b5f\u52a0\u62c9\u8bedASR\u4e2d\u7684\u65b9\u8a00\u591a\u6837\u6027\u548c\u73af\u5883\u566a\u58f0\u6311\u6218\uff0c\u5728\u591a\u79cd\u566a\u58f0\u6761\u4ef6\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u4f5c\u4e3a\u4e16\u754c\u7b2c\u4e94\u5927\u8bed\u8a00\uff0c\u5176ASR\u7cfb\u7edf\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u65b9\u8a00\u591a\u6837\u6027\u5de8\u5927\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u58f0\u5b66\u566a\u58f0\u3002\u73b0\u6709SSL\u6a21\u578b\u7f3a\u4e4f\u5904\u7406\u73af\u5883\u566a\u58f0\u7684\u663e\u5f0f\u673a\u5236\u548c\u9488\u5bf9\u65b9\u8a00\u53d8\u5316\u7684\u4e13\u95e8\u9002\u5e94\u7b56\u7565\u3002", "method": "\u57fa\u4e8e\u5177\u6709\u63a9\u7801\u8bed\u97f3\u53bb\u566a\u76ee\u6807\u7684WavLM\u6a21\u578b\uff0c\u63d0\u51fa\u591a\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\uff1a\u9996\u5148\u9002\u5e94\u901a\u7528\u6807\u51c6\u5b5f\u52a0\u62c9\u8bed\u5efa\u7acb\u8bed\u8a00\u57fa\u7840\uff0c\u7136\u540e\u901a\u8fc7\u9488\u5bf9\u6027\u6570\u636e\u589e\u5f3a\u4e13\u95e8\u5316\u7528\u4e8e\u566a\u58f0\u9c81\u68d2\u7684\u65b9\u8a00\u8bc6\u522b\u3002", "result": "\u5728\u5305\u542b\u591a\u79cd\u5b5f\u52a0\u62c9\u8bed\u65b9\u8a00\u548c\u5e7f\u6cdb\u6a21\u62df\u566a\u58f0\u6761\u4ef6\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u5305\u62ec\u6807\u51c6\u5fae\u8c03wav2vec 2.0\u548c\u5927\u89c4\u6a21\u591a\u8bed\u8a00Whisper\u6a21\u578b\u5728\u5185\u7684\u5f3a\u57fa\u7ebf\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8be5\u4efb\u52a1\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u4e3a\u5168\u7403\u5176\u4ed6\u4f4e\u8d44\u6e90\u3001\u9ad8\u53d8\u5f02\u8bed\u8a00\u7684\u5b9e\u7528ASR\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u6709\u6548\u7684\u84dd\u56fe\u3002"}}
{"id": "2509.00391", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00391", "abs": "https://arxiv.org/abs/2509.00391", "authors": ["Yuting Tan", "Xuying Li", "Zhuo Li", "Huizhen Shu", "Peikang Hu"], "title": "The Resurgence of GCG Adversarial Attacks on Large Language Models", "comment": "12 pages, 5 figures", "summary": "Gradient-based adversarial prompting, such as the Greedy Coordinate Gradient\n(GCG) algorithm, has emerged as a powerful method for jailbreaking large\nlanguage models (LLMs). In this paper, we present a systematic appraisal of GCG\nand its annealing-augmented variant, T-GCG, across open-source LLMs of varying\nscales. Using Qwen2.5-0.5B, LLaMA-3.2-1B, and GPT-OSS-20B, we evaluate attack\neffectiveness on both safety-oriented prompts (AdvBench) and\nreasoning-intensive coding prompts. Our study reveals three key findings: (1)\nattack success rates (ASR) decrease with model size, reflecting the increasing\ncomplexity and non-convexity of larger models' loss landscapes; (2)\nprefix-based heuristics substantially overestimate attack effectiveness\ncompared to GPT-4o semantic judgments, which provide a stricter and more\nrealistic evaluation; and (3) coding-related prompts are significantly more\nvulnerable than adversarial safety prompts, suggesting that reasoning itself\ncan be exploited as an attack vector. In addition, preliminary results with\nT-GCG show that simulated annealing can diversify adversarial search and\nachieve competitive ASR under prefix evaluation, though its benefits under\nsemantic judgment remain limited. Together, these findings highlight the\nscalability limits of GCG, expose overlooked vulnerabilities in reasoning\ntasks, and motivate further development of annealing-inspired strategies for\nmore robust adversarial evaluation.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86GCG\u548cT-GCG\u5bf9\u6297\u6027\u63d0\u793a\u653b\u51fb\u5728\u4e0d\u540c\u89c4\u6a21\u5f00\u6e90LLM\u4e0a\u7684\u6548\u679c\uff0c\u53d1\u73b0\u653b\u51fb\u6210\u529f\u7387\u968f\u6a21\u578b\u89c4\u6a21\u589e\u5927\u800c\u964d\u4f4e\uff0c\u8bed\u4e49\u8bc4\u4f30\u6bd4\u524d\u7f00\u542f\u53d1\u5f0f\u66f4\u4e25\u683c\uff0c\u4e14\u63a8\u7406\u5bc6\u96c6\u578b\u7f16\u7801\u63d0\u793a\u6bd4\u5b89\u5168\u63d0\u793a\u66f4\u6613\u53d7\u653b\u51fb\u3002", "motivation": "\u8bc4\u4f30\u68af\u5ea6\u57fa\u5bf9\u6297\u6027\u63d0\u793a\u653b\u51fb\uff08\u5982GCG\u7b97\u6cd5\uff09\u5728\u4e0d\u540c\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u6709\u6548\u6027\u548c\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5bf9\u5b89\u5168\u63d0\u793a\u548c\u63a8\u7406\u5bc6\u96c6\u578b\u7f16\u7801\u63d0\u793a\u7684\u653b\u51fb\u6548\u679c\u3002", "method": "\u4f7f\u7528Qwen2.5-0.5B\u3001LLaMA-3.2-1B\u548cGPT-OSS-20B\u4e09\u4e2a\u4e0d\u540c\u89c4\u6a21\u7684\u5f00\u6e90LLM\uff0c\u5728\u5b89\u5168\u5bfc\u5411\u63d0\u793a\uff08AdvBench\uff09\u548c\u63a8\u7406\u5bc6\u96c6\u578b\u7f16\u7801\u63d0\u793a\u4e0a\u8bc4\u4f30GCG\u53ca\u5176\u9000\u706b\u589e\u5f3a\u53d8\u4f53T-GCG\u7684\u653b\u51fb\u6548\u679c\uff0c\u91c7\u7528\u524d\u7f00\u542f\u53d1\u5f0f\u548cGPT-4o\u8bed\u4e49\u5224\u65ad\u4e24\u79cd\u8bc4\u4f30\u65b9\u5f0f\u3002", "result": "1) \u653b\u51fb\u6210\u529f\u7387\u968f\u6a21\u578b\u89c4\u6a21\u589e\u5927\u800c\u964d\u4f4e\uff1b2) \u524d\u7f00\u542f\u53d1\u5f0f\u663e\u8457\u9ad8\u4f30\u653b\u51fb\u6548\u679c\uff0c\u8bed\u4e49\u8bc4\u4f30\u66f4\u4e25\u683c\u73b0\u5b9e\uff1b3) \u7f16\u7801\u76f8\u5173\u63d0\u793a\u6bd4\u5bf9\u6297\u6027\u5b89\u5168\u63d0\u793a\u66f4\u6613\u53d7\u653b\u51fb\uff1b4) T-GCG\u80fd\u591a\u6837\u5316\u5bf9\u6297\u641c\u7d22\u4f46\u5728\u8bed\u4e49\u8bc4\u4f30\u4e0b\u6536\u76ca\u6709\u9650\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86GCG\u7684\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u66b4\u9732\u4e86\u63a8\u7406\u4efb\u52a1\u4e2d\u88ab\u5ffd\u89c6\u7684\u8106\u5f31\u6027\uff0c\u5e76\u63a8\u52a8\u5f00\u53d1\u9000\u706b\u542f\u53d1\u7b56\u7565\u4ee5\u8fdb\u884c\u66f4\u9c81\u68d2\u7684\u5bf9\u6297\u6027\u8bc4\u4f30\u3002"}}
{"id": "2509.01153", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01153", "abs": "https://arxiv.org/abs/2509.01153", "authors": ["Yun Chu", "Qiuhao Wang", "Enze Zhou", "Qian Liu", "Gang Zheng"], "title": "EZhouNet:A framework based on graph neural network and anchor interval for the respiratory sound event detection", "comment": null, "summary": "Auscultation is a key method for early diagnosis of respiratory and pulmonary\ndiseases, relying on skilled healthcare professionals. However, the process is\noften subjective, with variability between experts. As a result, numerous deep\nlearning-based automatic classification methods have emerged, most of which\nfocus on respiratory sound classification. In contrast, research on respiratory\nsound event detection remains limited. Existing sound event detection methods\ntypically rely on frame-level predictions followed by post-processing to\ngenerate event-level outputs, making interval boundaries challenging to learn\ndirectly. Furthermore, many approaches can only handle fixed-length audio, lim-\niting their applicability to variable-length respiratory sounds. Additionally,\nthe impact of respiratory sound location information on detection performance\nhas not been extensively explored. To address these issues, we propose a graph\nneural network-based framework with anchor intervals, capable of handling\nvariable-length audio and providing more precise temporal localization for\nabnormal respi- ratory sound events. Our method improves both the flexibility\nand applicability of respiratory sound detection. Experiments on the SPRSound\n2024 and HF Lung V1 datasets demonstrate the effec- tiveness of the proposed\napproach, and incorporating respiratory position information enhances the\ndiscrimination between abnormal sounds.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u951a\u70b9\u533a\u95f4\u7684\u547c\u5438\u97f3\u4e8b\u4ef6\u68c0\u6d4b\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u53d8\u957f\u97f3\u9891\u5e76\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u65f6\u95f4\u5b9a\u4f4d\uff0c\u7ed3\u5408\u547c\u5438\u4f4d\u7f6e\u4fe1\u606f\u63d0\u5347\u5f02\u5e38\u58f0\u97f3\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u547c\u5438\u97f3\u4e8b\u4ef6\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5e27\u7ea7\u9884\u6d4b\u548c\u540e\u5904\u7406\uff0c\u96be\u4ee5\u76f4\u63a5\u5b66\u4e60\u533a\u95f4\u8fb9\u754c\uff0c\u4e14\u5927\u591a\u53ea\u80fd\u5904\u7406\u56fa\u5b9a\u957f\u5ea6\u97f3\u9891\uff0c\u9650\u5236\u4e86\u5728\u53d8\u957f\u547c\u5438\u97f3\u4e2d\u7684\u5e94\u7528\u3002\u547c\u5438\u97f3\u4f4d\u7f6e\u4fe1\u606f\u5bf9\u68c0\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u4e5f\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u7ed3\u5408\u951a\u70b9\u533a\u95f4\uff0c\u80fd\u591f\u5904\u7406\u53d8\u957f\u97f3\u9891\u8f93\u5165\uff0c\u901a\u8fc7\u951a\u70b9\u673a\u5236\u76f4\u63a5\u5b66\u4e60\u4e8b\u4ef6\u7684\u65f6\u95f4\u8fb9\u754c\uff0c\u907f\u514d\u590d\u6742\u7684\u540e\u5904\u7406\u8fc7\u7a0b\uff0c\u5e76\u6574\u5408\u547c\u5438\u4f4d\u7f6e\u4fe1\u606f\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u3002", "result": "\u5728SPRSound 2024\u548cHF Lung V1\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u7ed3\u5408\u547c\u5438\u4f4d\u7f6e\u4fe1\u606f\u540e\u80fd\u66f4\u597d\u5730\u533a\u5206\u5f02\u5e38\u58f0\u97f3\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7684\u7075\u6d3b\u6027\u548c\u9002\u7528\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u89e3\u51b3\u4e86\u4f20\u7edf\u547c\u5438\u97f3\u4e8b\u4ef6\u68c0\u6d4b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\uff0c\u547c\u5438\u4f4d\u7f6e\u4fe1\u606f\u7684\u6574\u5408\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u5f02\u5e38\u58f0\u97f3\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u547c\u5438\u7cfb\u7edf\u75be\u75c5\u7684\u65e9\u671f\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2509.00414", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.00414", "abs": "https://arxiv.org/abs/2509.00414", "authors": ["Juraj Vladika", "Florian Matthes"], "title": "MedSEBA: Synthesizing Evidence-Based Answers Grounded in Evolving Medical Literature", "comment": "Accepted to CIKM 2025", "summary": "In the digital age, people often turn to the Internet in search of medical\nadvice and recommendations. With the increasing volume of online content, it\nhas become difficult to distinguish reliable sources from misleading\ninformation. Similarly, millions of medical studies are published every year,\nmaking it challenging for researchers to keep track of the latest scientific\nfindings. These evolving studies can reach differing conclusions, which is not\nreflected in traditional search tools. To address these challenges, we\nintroduce MedSEBA, an interactive AI-powered system for synthesizing\nevidence-based answers to medical questions. It utilizes the power of Large\nLanguage Models to generate coherent and expressive answers, but grounds them\nin trustworthy medical studies dynamically retrieved from the research database\nPubMed. The answers consist of key points and arguments, which can be traced\nback to respective studies. Notably, the platform also provides an overview of\nthe extent to which the most relevant studies support or refute the given\nmedical claim, and a visualization of how the research consensus evolved\nthrough time. Our user study revealed that medical experts and lay users find\nthe system usable and helpful, and the provided answers trustworthy and\ninformative. This makes the system well-suited for both everyday health\nquestions and advanced research insights.", "AI": {"tldr": "MedSEBA\u662f\u4e00\u4e2a\u57fa\u4e8eAI\u7684\u533b\u7597\u95ee\u7b54\u7cfb\u7edf\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7b54\u6848\uff0c\u5e76\u901a\u8fc7PubMed\u6570\u636e\u5e93\u52a8\u6001\u68c0\u7d22\u53ef\u4fe1\u533b\u5b66\u7814\u7a76\u6765\u652f\u6491\u56de\u7b54\uff0c\u63d0\u4f9b\u7814\u7a76\u5171\u8bc6\u6f14\u53d8\u7684\u53ef\u89c6\u5316\u3002", "motivation": "\u89e3\u51b3\u4e92\u8054\u7f51\u533b\u7597\u4fe1\u606f\u53ef\u9760\u6027\u95ee\u9898\u548c\u533b\u5b66\u7814\u7a76\u6570\u91cf\u5e9e\u5927\u96be\u4ee5\u8ffd\u8e2a\u7684\u6311\u6218\uff0c\u5e2e\u52a9\u7528\u6237\u533a\u5206\u53ef\u9760\u4fe1\u606f\u4e0e\u8bef\u5bfc\u6027\u5185\u5bb9\u3002", "method": "\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8fde\u8d2f\u8868\u8fbe\u7684\u56de\u7b54\uff0c\u540c\u65f6\u4ecePubMed\u6570\u636e\u5e93\u52a8\u6001\u68c0\u7d22\u53ef\u4fe1\u533b\u5b66\u7814\u7a76\u4f5c\u4e3a\u652f\u6491\uff0c\u63d0\u4f9b\u5173\u952e\u70b9\u548c\u8bba\u8bc1\uff0c\u5e76\u53ef\u89c6\u5316\u7814\u7a76\u5171\u8bc6\u7684\u65f6\u95f4\u6f14\u53d8\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u533b\u5b66\u4e13\u5bb6\u548c\u666e\u901a\u7528\u6237\u90fd\u8ba4\u4e3a\u7cfb\u7edf\u53ef\u7528\u4e14\u6709\u5e2e\u52a9\uff0c\u63d0\u4f9b\u7684\u7b54\u6848\u53ef\u4fe1\u4e14\u4fe1\u606f\u4e30\u5bcc\u3002", "conclusion": "MedSEBA\u7cfb\u7edf\u65e2\u9002\u7528\u4e8e\u65e5\u5e38\u5065\u5eb7\u95ee\u9898\uff0c\u4e5f\u9002\u5408\u9ad8\u7ea7\u7814\u7a76\u6d1e\u5bdf\uff0c\u80fd\u6709\u6548\u5408\u6210\u57fa\u4e8e\u8bc1\u636e\u7684\u533b\u7597\u7b54\u6848\u3002"}}
{"id": "2509.01336", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01336", "abs": "https://arxiv.org/abs/2509.01336", "authors": ["Wen-Chin Huang", "Hui Wang", "Cheng Liu", "Yi-Chiao Wu", "Andros Tjandra", "Wei-Ning Hsu", "Erica Cooper", "Yong Qin", "Tomoki Toda"], "title": "The AudioMOS Challenge 2025", "comment": "IEEE ASRU 2025", "summary": "This is the summary paper for the AudioMOS Challenge 2025, the very first\nchallenge for automatic subjective quality prediction for synthetic audio. The\nchallenge consists of three tracks. The first track aims to assess\ntext-to-music samples in terms of overall quality and textual alignment. The\nsecond track is based on the four evaluation dimensions of Meta Audiobox\nAesthetics, and the test set consists of text-to-speech, text-to-audio, and\ntext-to-music samples. The third track focuses on synthetic speech quality\nassessment in different sampling rates. The challenge attracted 24 unique teams\nfrom both academia and industry, and improvements over the baselines were\nconfirmed. The outcome of this challenge is expected to facilitate development\nand progress in the field of automatic evaluation for audio generation systems.", "AI": {"tldr": "AudioMOS Challenge 2025\u662f\u9996\u4e2a\u9488\u5bf9\u5408\u6210\u97f3\u9891\u4e3b\u89c2\u8d28\u91cf\u9884\u6d4b\u7684\u6311\u6218\u8d5b\uff0c\u5305\u542b\u4e09\u4e2a\u8d5b\u9053\uff1a\u6587\u672c\u5230\u97f3\u4e50\u8d28\u91cf\u8bc4\u4f30\u3001\u591a\u7ef4\u5ea6\u97f3\u9891\u7f8e\u5b66\u8bc4\u4f30\u548c\u4e0d\u540c\u91c7\u6837\u7387\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30\uff0c\u5438\u5f15\u4e8624\u4e2a\u56e2\u961f\u53c2\u4e0e\u5e76\u53d6\u5f97\u4e86\u8d85\u8d8a\u57fa\u7ebf\u7684\u6210\u679c\u3002", "motivation": "\u968f\u7740\u97f3\u9891\u751f\u6210\u7cfb\u7edf\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u5efa\u7acb\u81ea\u52a8\u5316\u7684\u4e3b\u89c2\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u6765\u66ff\u4ee3\u4eba\u5de5\u8bc4\u4f30\uff0c\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u6807\u51c6\u5316\u53d1\u5c55\u3002", "method": "\u6311\u6218\u8d5b\u8bbe\u7f6e\u4e86\u4e09\u4e2a\u8d5b\u9053\uff1a1\uff09\u6587\u672c\u5230\u97f3\u4e50\u6837\u672c\u7684\u6574\u4f53\u8d28\u91cf\u548c\u6587\u672c\u5bf9\u9f50\u5ea6\u8bc4\u4f30\uff1b2\uff09\u57fa\u4e8eMeta Audiobox\u7f8e\u5b66\u56db\u4e2a\u7ef4\u5ea6\u7684\u591a\u6a21\u6001\u97f3\u9891\u8bc4\u4f30\uff1b3\uff09\u4e0d\u540c\u91c7\u6837\u7387\u4e0b\u5408\u6210\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30\u3002", "result": "\u5438\u5f15\u4e86\u6765\u81ea\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u768424\u4e2a\u72ec\u7279\u56e2\u961f\u53c2\u4e0e\uff0c\u6240\u6709\u56e2\u961f\u7684\u8868\u73b0\u90fd\u8d85\u8d8a\u4e86\u57fa\u7ebf\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6311\u6218\u8d5b\u7684\u6210\u529f\u4e3e\u529e\u5c06\u63a8\u52a8\u97f3\u9891\u751f\u6210\u7cfb\u7edf\u81ea\u52a8\u8bc4\u4f30\u9886\u57df\u7684\u53d1\u5c55\u548c\u8fdb\u6b65\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u91cd\u8981\u57fa\u51c6\u548c\u65b9\u5411\u3002"}}
{"id": "2509.00425", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00425", "abs": "https://arxiv.org/abs/2509.00425", "authors": ["Fenghua Liu", "Yulong Chen", "Yixuan Liu", "Zhujun Jin", "Solomon Tsai", "Ming Zhong"], "title": "The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in LLMs with Camlang", "comment": "Working in progress", "summary": "Large Language Models (LLMs) achieve gold-medal performance across many\nbenchmarks, yet it remains unclear whether such success reflects genuine\nreasoning or pattern matching. From a cognitive science perspective, an\ninformative test is whether models can master an unfamiliar language through\nexplicit metalinguistic deductive learning, a paradigm where human learners can\nreliably internalise grammatical systems through metalinguistic reasoning. We\naddress this question with Camlang, a novel constructed language that exhibits\nnaturalistic yet unattested feature combinations. Camlang consists of two\nexplicit resources, a grammar book and a bilingual dictionary, which mirror\nadult second-language learning via explicit grammar rules and lexical lookup,\nand enable us to disentangle errors in morpho-syntax, lexical semantics, and\nsentence-level reasoning. Human experiments show that these resources are\nsufficient for participants to acquire Camlang and successfully solve Camlang\ntasks. To operationalise evaluation, we adapt CommonsenseQA into Camlang,\ncreating Camlang-CSQA-v0, the first task in a broader suite where solving\nquestions requires applying grammar rules and lexical mappings. Experimental\nresults show that GPT-5 achieves 98\\% EM accuracy in English but only 47\\% in\nCamlang, far below human performance at 87\\%, while other state-of-the-art\nreasoning LLMs perform even worse. Human verification further reveals that most\nmodel successes stem from shallow lexical alignment while GPT-5 shows emerging\nmetalinguistic awareness to a limited extent but not systematic grammatical\nmastery as humans. Camlang establishes a cognitively grounded evaluation\nparadigm that exposes fundamental gaps between current models and human\nmetalinguistic competence.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51faCamlang\u6784\u9020\u8bed\u8a00\u6848\u4f8b\uff0c\u901a\u8fc7\u660e\u786e\u7684\u8bed\u8a00\u5b66\u4e60\u6d4b\u8bd5LLM\u7684\u771f\u5b9e\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u8bed\u8a00\u5b66\u4e60\u65b9\u9762\u8fdc\u8f83\u4eba\u7c7b\u5dee\u5f02\u663e\u8457", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6210\u529f\u662f\u5426\u6765\u81ea\u771f\u6b63\u7684\u63a8\u7406\u80fd\u529b\u8fd8\u662f\u7b80\u5355\u7684\u6a21\u5f0f\u5339\u914d\uff0c\u901a\u8fc7\u8bed\u8a00\u5b66\u4e60\u8fd9\u4e2a\u8ba4\u77e5\u79d1\u5b66\u89c6\u89d2\u8fdb\u884c\u6d4b\u8bd5", "method": "\u521b\u5efaCamlang\u6784\u9020\u8bed\u8a00\uff0c\u5305\u542b\u8bed\u6cd5\u4e66\u548c\u53cc\u8bed\u8bcd\u5178\u4e24\u79cd\u660e\u786e\u8d44\u6e90\uff0c\u5e76\u5c06CommonsenseQA\u9002\u914d\u4e3aCamlang-CSQA-v0\u4efb\u52a1\uff0c\u6d4b\u8bd5\u6a21\u578b\u901a\u8fc7\u660e\u786e\u8bed\u6cd5\u89c4\u5219\u548c\u8bcd\u6c47\u6620\u5c04\u5b66\u4e60\u65b0\u8bed\u8a00\u7684\u80fd\u529b", "result": "GPT-5\u5728\u82f1\u8bed\u4e2d\u8fbe\u523098%\u51c6\u786e\u7387\uff0c\u4f46\u5728Camlang\u4e2d\u4ec547%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u768487%\u3002\u5176\u4ed6\u9886\u5148LLM\u8868\u73b0\u66f4\u5dee\u3002\u6a21\u578b\u6210\u529f\u4e3b\u8981\u6765\u81ea\u6d45\u5c42\u8bcd\u6c47\u5bf9\u9f50\uff0c\u800c\u975e\u7cfb\u7edf\u6027\u8bed\u6cd5\u638c\u63e1", "conclusion": "Camlang\u5efa\u7acb\u4e86\u4e00\u4e2a\u8ba4\u77e5\u57fa\u7840\u7684\u8bc4\u4f30\u8303\u5f0f\uff0c\u66dd\u9732\u4e86\u5f53\u524d\u6a21\u578b\u4e0e\u4eba\u7c7b\u8bed\u8a00\u5b66\u4e60\u80fd\u529b\u4e4b\u95f4\u7684\u6839\u672c\u5dee\u8ddd"}}
{"id": "2509.01399", "categories": ["cs.SD", "cs.AI", "cs.HC", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01399", "abs": "https://arxiv.org/abs/2509.01399", "authors": ["Runduo Han", "Yanxin Hu", "Yihui Fu", "Zihan Zhang", "Yukai Jv", "Li Chen", "Lei Xie"], "title": "CabinSep: IR-Augmented Mask-Based MVDR for Real-Time In-Car Speech Separation with Distributed Heterogeneous Arrays", "comment": "Accepted by Interspeech 2025", "summary": "Separating overlapping speech from multiple speakers is crucial for effective\nhuman-vehicle interaction. This paper proposes CabinSep, a lightweight neural\nmask-based minimum variance distortionless response (MVDR) speech separation\napproach, to reduce speech recognition errors in back-end automatic speech\nrecognition (ASR) models. Our contributions are threefold: First, we utilize\nchannel information to extract spatial features, which improves the estimation\nof speech and noise masks. Second, we employ MVDR during inference, reducing\nspeech distortion to make it more ASR-friendly. Third, we introduce a data\naugmentation method combining simulated and real-recorded impulse responses\n(IRs), improving speaker localization at zone boundaries and further reducing\nspeech recognition errors. With a computational complexity of only 0.4 GMACs,\nCabinSep achieves a 17.5% relative reduction in speech recognition error rate\nin a real-recorded dataset compared to the state-of-the-art DualSep model.\nDemos are available at: https://cabinsep.github.io/cabinsep/.", "AI": {"tldr": "CabinSep\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u8bed\u97f3\u5206\u79bb\u65b9\u6cd5\uff0c\u901a\u8fc7MVDR\u6280\u672f\u548c\u6570\u636e\u589e\u5f3a\uff0c\u5728\u8f66\u8f7d\u73af\u5883\u4e2d\u6709\u6548\u5206\u79bb\u91cd\u53e0\u8bed\u97f3\uff0c\u964d\u4f4e\u8bed\u97f3\u8bc6\u522b\u9519\u8bef\u738717.5%", "motivation": "\u89e3\u51b3\u8f66\u8f7d\u73af\u5883\u4e2d\u591a\u4eba\u91cd\u53e0\u8bed\u97f3\u5206\u79bb\u95ee\u9898\uff0c\u63d0\u9ad8\u540e\u7aef\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u7684\u6027\u80fd", "method": "\u4f7f\u7528\u901a\u9053\u4fe1\u606f\u63d0\u53d6\u7a7a\u95f4\u7279\u5f81\uff0c\u91c7\u7528MVDR\u51cf\u5c11\u8bed\u97f3\u5931\u771f\uff0c\u7ed3\u5408\u6a21\u62df\u548c\u771f\u5b9e\u8109\u51b2\u54cd\u5e94\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5", "result": "\u8ba1\u7b97\u590d\u6742\u5ea6\u4ec50.4 GMACs\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4DualSep\u6a21\u578b\u76f8\u5bf9\u964d\u4f4e17.5%\u7684\u8bed\u97f3\u8bc6\u522b\u9519\u8bef\u7387", "conclusion": "CabinSep\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u8f66\u8f7d\u8bed\u97f3\u5206\u79bb\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8bed\u97f3\u8bc6\u522b\u6027\u80fd"}}
{"id": "2509.00449", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00449", "abs": "https://arxiv.org/abs/2509.00449", "authors": ["Xuecheng Zou", "Ke Liu", "Bingbing Wang", "Huafei Deng", "Li Zhang", "Yu Tang"], "title": "GOSU: Retrieval-Augmented Generation with Global-Level Optimized Semantic Unit-Centric Framework", "comment": null, "summary": "Building upon the standard graph-based Retrieval-Augmented Generation (RAG),\nthe introduction of heterogeneous graphs and hypergraphs aims to enrich\nretrieval and generation by leveraging the relationships between multiple\nentities through the concept of semantic units (SUs). But this also raises a\nkey issue: The extraction of high-level SUs limited to local text chunks is\nprone to ambiguity, complex coupling, and increased retrieval overhead due to\nthe lack of global knowledge or the neglect of fine-grained relationships. To\naddress these issues, we propose GOSU, a semantic unit-centric RAG framework\nthat efficiently performs global disambiguation and utilizes SUs to capture\ninterconnections between different nodes across the global context. In the\ngraph construction phase, GOSU performs global merging on the pre-extracted SUs\nfrom local text chunks and guides entity and relationship extraction, reducing\nthe difficulty of coreference resolution while uncovering global semantic\nobjects across text chunks. In the retrieval and generation phase, we introduce\nhierarchical keyword extraction and semantic unit completion. The former\nuncovers the fine-grained binary relationships overlooked by the latter, while\nthe latter compensates for the coarse-grained n-ary relationships missing from\nthe former. Evaluation across multiple tasks demonstrates that GOSU outperforms\nthe baseline RAG methods in terms of generation quality.", "AI": {"tldr": "GOSU\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bed\u4e49\u5355\u5143\u7684RAG\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u6d88\u6b67\u548c\u8bed\u4e49\u5355\u5143\u6574\u5408\u6765\u89e3\u51b3\u4f20\u7edf\u56feRAG\u65b9\u6cd5\u4e2d\u5c40\u90e8\u8bed\u4e49\u5355\u5143\u63d0\u53d6\u7684\u6a21\u7cca\u6027\u548c\u590d\u6742\u8026\u5408\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u56fe\u7684RAG\u65b9\u6cd5\u4f7f\u7528\u5f02\u6784\u56fe\u548c\u8d85\u56fe\u6765\u4e30\u5bcc\u68c0\u7d22\u548c\u751f\u6210\uff0c\u4f46\u5c40\u90e8\u6587\u672c\u5757\u4e2d\u63d0\u53d6\u7684\u9ad8\u5c42\u8bed\u4e49\u5355\u5143\u5bb9\u6613\u4ea7\u751f\u6b67\u4e49\u3001\u590d\u6742\u8026\u5408\uff0c\u5e76\u56e0\u7f3a\u4e4f\u5168\u5c40\u77e5\u8bc6\u6216\u5ffd\u7565\u7ec6\u7c92\u5ea6\u5173\u7cfb\u800c\u589e\u52a0\u68c0\u7d22\u5f00\u9500\u3002", "method": "GOSU\u5728\u56fe\u5f62\u6784\u5efa\u9636\u6bb5\u5bf9\u5c40\u90e8\u6587\u672c\u5757\u9884\u63d0\u53d6\u7684\u8bed\u4e49\u5355\u5143\u8fdb\u884c\u5168\u5c40\u5408\u5e76\uff0c\u6307\u5bfc\u5b9e\u4f53\u548c\u5173\u7cfb\u63d0\u53d6\uff1b\u5728\u68c0\u7d22\u751f\u6210\u9636\u6bb5\u5f15\u5165\u5206\u5c42\u5173\u952e\u8bcd\u63d0\u53d6\u548c\u8bed\u4e49\u5355\u5143\u8865\u5168\u6280\u672f\uff0c\u5206\u522b\u6355\u6349\u7ec6\u7c92\u5ea6\u4e8c\u5143\u5173\u7cfb\u548c\u7c97\u7c92\u5ea6n\u5143\u5173\u7cfb\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cGOSU\u5728\u751f\u6210\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebfRAG\u65b9\u6cd5\u3002", "conclusion": "GOSU\u6846\u67b6\u901a\u8fc7\u5168\u5c40\u8bed\u4e49\u5355\u5143\u6574\u5408\u548c\u5206\u5c42\u5173\u7cfb\u6355\u6349\uff0c\u6709\u6548\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u7684\u68c0\u7d22\u548c\u751f\u6210\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u5c40\u90e8\u8bed\u4e49\u5355\u5143\u63d0\u53d6\u7684\u5c40\u9650\u6027\u95ee\u9898\u3002"}}
{"id": "2509.01401", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01401", "abs": "https://arxiv.org/abs/2509.01401", "authors": ["Ali Abouzeid", "Bilal Elbouardi", "Mohamed Maged", "Shady Shehata"], "title": "ArabEmoNet: A Lightweight Hybrid 2D CNN-BiLSTM Model with Attention for Robust Arabic Speech Emotion Recognition", "comment": "Accepted (The Third Arabic Natural Language Processing Conference)", "summary": "Speech emotion recognition is vital for human-computer interaction,\nparticularly for low-resource languages like Arabic, which face challenges due\nto limited data and research. We introduce ArabEmoNet, a lightweight\narchitecture designed to overcome these limitations and deliver\nstate-of-the-art performance. Unlike previous systems relying on discrete MFCC\nfeatures and 1D convolutions, which miss nuanced spectro-temporal patterns,\nArabEmoNet uses Mel spectrograms processed through 2D convolutions, preserving\ncritical emotional cues often lost in traditional methods.\n  While recent models favor large-scale architectures with millions of\nparameters, ArabEmoNet achieves superior results with just 1 million\nparameters, 90 times smaller than HuBERT base and 74 times smaller than\nWhisper. This efficiency makes it ideal for resource-constrained environments.\nArabEmoNet advances Arabic speech emotion recognition, offering exceptional\nperformance and accessibility for real-world applications.", "AI": {"tldr": "ArabEmoNet\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u67b6\u6784\uff0c\u4ec5\u4f7f\u7528100\u4e07\u53c2\u6570\u5c31\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6bd4\u73b0\u6709\u6a21\u578b\u5c0f90\u500d", "motivation": "\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4e2d\u9762\u4e34\u7684\u6570\u636e\u6709\u9650\u548c\u7814\u7a76\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u7528Mel\u9891\u8c31\u56fe\u901a\u8fc72D\u5377\u79ef\u5904\u7406\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u79bb\u6563MFCC\u7279\u5f81\u548c1D\u5377\u79ef\u65b9\u6cd5\uff0c\u66f4\u597d\u5730\u4fdd\u7559\u60c5\u611f\u7ebf\u7d22\u548c\u9891\u8c31-\u65f6\u95f4\u6a21\u5f0f", "result": "ArabEmoNet\u5728\u53c2\u6570\u89c4\u6a21\u4ec5\u4e3aHuBERT base\u76841/90\u548cWhisper\u76841/74\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u4f18\u4e8e\u5927\u578b\u6a21\u578b\u7684\u6027\u80fd", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u9645\u5e94\u7528\u73af\u5883"}}
{"id": "2509.00457", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00457", "abs": "https://arxiv.org/abs/2509.00457", "authors": ["Salah Eddine Bekhouche", "Abdellah Zakaria Sellam", "Hichem Telli", "Cosimo Distante", "Abdenour Hadid"], "title": "CVPD at QIAS 2025 Shared Task: An Efficient Encoder-Based Approach for Islamic Inheritance Reasoning", "comment": null, "summary": "Islamic inheritance law (Ilm al-Mawarith) requires precise identification of\nheirs and calculation of shares, which poses a challenge for AI. In this paper,\nwe present a lightweight framework for solving multiple-choice inheritance\nquestions using a specialised Arabic text encoder and Attentive Relevance\nScoring (ARS). The system ranks answer options according to semantic relevance,\nand enables fast, on-device inference without generative reasoning. We evaluate\nArabic encoders (MARBERT, ArabicBERT, AraBERT) and compare them with API-based\nLLMs (Gemini, DeepSeek) on the QIAS 2025 dataset. While large models achieve an\naccuracy of up to 87.6%, they require more resources and are context-dependent.\nOur MARBERT-based approach achieves 69.87% accuracy, presenting a compelling\ncase for efficiency, on-device deployability, and privacy. While this is lower\nthan the 87.6% achieved by the best-performing LLM, our work quantifies a\ncritical trade-off between the peak performance of large models and the\npractical advantages of smaller, specialized systems in high-stakes domains.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eMARBERT\u7f16\u7801\u5668\u548c\u6ce8\u610f\u529b\u76f8\u5173\u6027\u8bc4\u5206(ARS)\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4f0a\u65af\u5170\u7ee7\u627f\u6cd5\u9009\u62e9\u9898\uff0c\u5728\u6548\u7387\u3001\u8bbe\u5907\u90e8\u7f72\u548c\u9690\u79c1\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u51c6\u786e\u7387\u8fbe\u523069.87%", "motivation": "\u4f0a\u65af\u5170\u7ee7\u627f\u6cd5\u9700\u8981\u7cbe\u786e\u8bc6\u522b\u7ee7\u627f\u4eba\u548c\u8ba1\u7b97\u4efd\u989d\uff0c\u8fd9\u5bf9AI\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u51c6\u786e\u53c8\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u7528\u4e13\u95e8\u7684\u963f\u62c9\u4f2f\u8bed\u6587\u672c\u7f16\u7801\u5668(MARBERT\u3001ArabicBERT\u3001AraBERT)\u548c\u6ce8\u610f\u529b\u76f8\u5173\u6027\u8bc4\u5206(ARS)\u6765\u5bf9\u7b54\u6848\u9009\u9879\u8fdb\u884c\u8bed\u4e49\u76f8\u5173\u6027\u6392\u5e8f\uff0c\u5b9e\u73b0\u5feb\u901f\u8bbe\u5907\u7aef\u63a8\u7406", "result": "MARBERT\u65b9\u6cd5\u8fbe\u523069.87%\u51c6\u786e\u7387\uff0c\u867d\u7136\u4f4e\u4e8e\u6700\u4f73LLM\u768487.6%\uff0c\u4f46\u5728\u8d44\u6e90\u9700\u6c42\u3001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u5177\u6709\u4f18\u52bf", "conclusion": "\u91cf\u5316\u4e86\u5927\u6a21\u578b\u5cf0\u503c\u6027\u80fd\u4e0e\u5c0f\u578b\u4e13\u4e1a\u5316\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5173\u952e\u6743\u8861\uff0c\u8bc1\u660e\u4e86\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2509.00461", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00461", "abs": "https://arxiv.org/abs/2509.00461", "authors": ["Beining Xu"], "title": "TECP: Token-Entropy Conformal Prediction for LLMs", "comment": null, "summary": "Uncertainty quantification (UQ) for open-ended language generation remains a\ncritical yet underexplored challenge, especially under black-box constraints\nwhere internal model signals are inaccessible. In this paper, we introduce\nToken-Entropy Conformal Prediction (TECP), a novel framework that leverages\ntoken-level entropy as a logit-free, reference-free uncertainty measure and\nintegrates it into a split conformal prediction (CP) pipeline to construct\nprediction sets with formal coverage guarantees. Unlike existing approaches\nthat rely on semantic consistency heuristics or white-box features, TECP\ndirectly estimates epistemic uncertainty from the token entropy structure of\nsampled generations and calibrates uncertainty thresholds via CP quantiles to\nensure provable error control. Empirical evaluations across six large language\nmodels and two benchmarks (CoQA and TriviaQA) demonstrate that TECP\nconsistently achieves reliable coverage and compact prediction sets,\noutperforming prior self-consistency-based UQ methods. Our method provides a\nprincipled and efficient solution for trustworthy generation in black-box LLM\nsettings.", "AI": {"tldr": "\u63d0\u51fa\u4e86Token-Entropy Conformal Prediction (TECP)\u6846\u67b6\uff0c\u5229\u7528token-level\u71b5\u4f5c\u4e3a\u65e0\u9700logit\u548c\u53c2\u8003\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\uff0c\u901a\u8fc7conformal prediction\u4e3a\u9ed1\u76d2\u8bed\u8a00\u6a21\u578b\u751f\u6210\u63d0\u4f9b\u5f62\u5f0f\u5316\u8986\u76d6\u4fdd\u8bc1\u3002", "motivation": "\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u7684\u5f00\u653e\u6587\u672c\u751f\u6210\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8bed\u4e49\u4e00\u81f4\u6027\u542f\u53d1\u5f0f\u6216\u767d\u76d2\u7279\u5f81\uff0c\u7f3a\u4e4f\u5f62\u5f0f\u5316\u4fdd\u8bc1\u3002", "method": "\u4f7f\u7528token-level\u71b5\u4f5c\u4e3a\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\uff0c\u96c6\u6210\u5230split conformal prediction\u6d41\u7a0b\u4e2d\uff0c\u901a\u8fc7CP\u5206\u4f4d\u6570\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u9608\u503c\uff0c\u786e\u4fdd\u53ef\u8bc1\u660e\u7684\u9519\u8bef\u63a7\u5236\u3002", "result": "\u57286\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u548c2\u4e2a\u57fa\u51c6\u6d4b\u8bd5(CoQA\u548cTriviaQA)\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTECP\u59cb\u7ec8\u5b9e\u73b0\u53ef\u9760\u8986\u76d6\u548c\u7d27\u51d1\u9884\u6d4b\u96c6\uff0c\u4f18\u4e8e\u5148\u524d\u7684\u81ea\u4e00\u81f4\u6027UQ\u65b9\u6cd5\u3002", "conclusion": "TECP\u4e3a\u9ed1\u76d2LLM\u8bbe\u7f6e\u4e2d\u7684\u53ef\u4fe1\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01762", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.01762", "abs": "https://arxiv.org/abs/2509.01762", "authors": ["Alokit Mishra", "Ryyan Akhtar"], "title": "Music Genre Classification Using Machine Learning Techniques", "comment": "10 pages, 20 figures. Submitted in partial fulfillment of the\n  requirements for the Bachelor of Technology (B.Tech) degree in Artificial\n  Intelligence and Data Science", "summary": "This paper presents a comparative analysis of machine learning methodologies\nfor automatic music genre classification. We evaluate the performance of\nclassical classifiers, including Support Vector Machines (SVM) and ensemble\nmethods, trained on a comprehensive set of hand-crafted audio features, against\na Convolutional Neural Network (CNN) operating on Mel spectrograms. The study\nis conducted on the widely-used GTZAN dataset. Our findings demonstrate a\nnoteworthy result: the SVM, leveraging domain-specific feature engineering,\nachieves superior classification accuracy compared to the end-to-end CNN model.\nWe attribute this outcome to the data-constrained nature of the benchmark\ndataset, where the strong inductive bias of engineered features provides a\nregularization effect that mitigates the risk of overfitting inherent in\nhigh-capacity deep learning models. This work underscores the enduring\nrelevance of traditional feature extraction in practical audio processing tasks\nand provides a critical perspective on the universal applicability of deep\nlearning, especially for moderately sized datasets.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6bd4\u5206\u6790\u4e86\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u97f3\u4e50\u7c7b\u578b\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u57fa\u4e8e\u9886\u57df\u77e5\u8bc6\u7684SVM\u5206\u7c7b\u5668\u5728GTZAN\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u8d85\u8fc7\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bCNN\uff0c\u8bf4\u660e\u5728\u4e2d\u7b49\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u4f20\u7edf\u7279\u5f81\u5de5\u7a0b\u4ecd\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "motivation": "\u8bc4\u4f30\u4e0d\u540c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u97f3\u4e50\u7c7b\u578b\u81ea\u52a8\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5bf9\u6bd4\u4f20\u7edf\u7684\u7279\u5f81\u5de5\u7a0b\u65b9\u6cd5\u4e0e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u6548\u679c\uff0c\u4ee5\u63a2\u7d22\u6df1\u5ea6\u5b66\u4e60\u5728\u4e2d\u7b49\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u666e\u904d\u9002\u7528\u6027\u3002", "method": "\u4f7f\u7528GTZAN\u6570\u636e\u96c6\uff0c\u5bf9\u6bd4\u4e86\u652f\u6301\u5411\u91cf\u673a(SVM)\u548c\u96c6\u6210\u65b9\u6cd5\u7b49\u4f20\u7edf\u5206\u7c7b\u5668(\u57fa\u4e8e\u624b\u5de5\u7279\u5f81\u63d0\u53d6)\u4e0e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(CNN)\u6a21\u578b(\u57fa\u4e8eMel\u8c31\u56fe)\u7684\u6027\u80fd\u3002", "result": "SVM\u5728\u5206\u7c7b\u51c6\u786e\u7387\u4e0a\u8868\u73b0\u66f4\u4f18\u4e8eCNN\u3002\u8fd9\u662f\u56e0\u4e3a\u5728\u6570\u636e\u89c4\u6a21\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u9886\u57df\u7279\u5b9a\u7684\u7279\u5f81\u5de5\u7a0b\u63d0\u4f9b\u4e86\u5f3a\u70c8\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u80fd\u591f\u51cf\u5c11\u8fc7\u62df\u5408\u98ce\u9669\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u7a81\u51fa\u4e86\u4f20\u7edf\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u5728\u5b9e\u9645\u97f3\u9891\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u6301\u4e45\u4ef7\u503c\uff0c\u5e76\u4e3a\u6df1\u5ea6\u5b66\u4e60\u5728\u4e2d\u7b49\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u666e\u904d\u9002\u7528\u6027\u63d0\u4f9b\u4e86\u6279\u5224\u6027\u89c6\u89d2\u3002"}}
{"id": "2509.00482", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.00482", "abs": "https://arxiv.org/abs/2509.00482", "authors": ["Saksorn Ruangtanusak", "Pittawat Taveekitworachai", "Kunat Pipatanakul"], "title": "Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting", "comment": "17 pages, 2 figures", "summary": "This report investigates approaches for prompting a tool-augmented large\nlanguage model (LLM) to act as a role-playing dialogue agent in the API track\nof the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this\nsetting, dialogue agents often produce overly long in-character responses\n(over-speaking) while failing to use tools effectively according to the persona\n(under-acting), such as generating function calls that do not exist or making\nunnecessary tool calls before answering. We explore four prompting approaches\nto address these issues: 1) basic role prompting, 2) human-crafted role\nprompting, 3) automatic prompt optimization (APO), and 4) rule-based role\nprompting. The rule-based role prompting (RRP) approach achieved the best\nperformance through two novel techniques--character-card/scene-contract design\nand strict enforcement of function calling--which led to an overall score of\n0.571, improving on the zero-shot baseline score of 0.519. These findings\ndemonstrate that RRP design can substantially improve the effectiveness and\nreliability of role-playing dialogue agents compared with more elaborate\nmethods such as APO. To support future efforts in developing persona prompts,\nwe are open-sourcing all of our best-performing prompts and the APO tool.\nSource code is available at https://github.com/scb-10x/apo.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u56db\u79cd\u63d0\u793a\u65b9\u6cd5\u6765\u89e3\u51b3\u89d2\u8272\u626e\u6f14\u5bf9\u8bdd\u4ee3\u7406\u4e2d\u7684\u8fc7\u5ea6\u8bf4\u8bdd\u548c\u5de5\u5177\u4f7f\u7528\u4e0d\u8db3\u95ee\u9898\uff0c\u5176\u4e2d\u57fa\u4e8e\u89c4\u5219\u7684\u63d0\u793a\u65b9\u6cd5(RRP)\u8868\u73b0\u6700\u4f73\uff0c\u901a\u8fc7\u89d2\u8272\u5361/\u573a\u666f\u5951\u7ea6\u8bbe\u8ba1\u548c\u4e25\u683c\u51fd\u6570\u8c03\u7528\u6267\u884c\uff0c\u5c06\u5f97\u5206\u4ece0.519\u63d0\u5347\u52300.571\u3002", "motivation": "\u89e3\u51b3\u5de5\u5177\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89d2\u8272\u626e\u6f14\u5bf9\u8bdd\u4e2d\u4ea7\u751f\u7684\u8fc7\u5ea6\u8bf4\u8bdd(over-speaking)\u548c\u5de5\u5177\u4f7f\u7528\u4e0d\u8db3(under-acting)\u95ee\u9898\uff0c\u5982\u751f\u6210\u4e0d\u5b58\u5728\u7684\u51fd\u6570\u8c03\u7528\u6216\u5728\u56de\u7b54\u524d\u8fdb\u884c\u4e0d\u5fc5\u8981\u7684\u5de5\u5177\u8c03\u7528\u3002", "method": "\u63a2\u7d22\u4e86\u56db\u79cd\u63d0\u793a\u65b9\u6cd5\uff1a1)\u57fa\u7840\u89d2\u8272\u63d0\u793a 2)\u4eba\u5de5\u5236\u4f5c\u89d2\u8272\u63d0\u793a 3)\u81ea\u52a8\u63d0\u793a\u4f18\u5316(APO) 4)\u57fa\u4e8e\u89c4\u5219\u7684\u89d2\u8272\u63d0\u793a(RRP)\uff0c\u5176\u4e2dRRP\u91c7\u7528\u89d2\u8272\u5361/\u573a\u666f\u5951\u7ea6\u8bbe\u8ba1\u548c\u4e25\u683c\u51fd\u6570\u8c03\u7528\u6267\u884c\u6280\u672f\u3002", "result": "\u57fa\u4e8e\u89c4\u5219\u7684\u89d2\u8272\u63d0\u793a(RRP)\u65b9\u6cd5\u83b7\u5f97\u4e860.571\u7684\u6574\u4f53\u5f97\u5206\uff0c\u76f8\u6bd4\u96f6\u6837\u672c\u57fa\u7ebf\u5f97\u52060.519\u6709\u663e\u8457\u63d0\u5347\uff0c\u8868\u73b0\u4f18\u4e8e\u66f4\u590d\u6742\u7684APO\u65b9\u6cd5\u3002", "conclusion": "RRP\u8bbe\u8ba1\u80fd\u663e\u8457\u63d0\u9ad8\u89d2\u8272\u626e\u6f14\u5bf9\u8bdd\u4ee3\u7406\u7684\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\uff0c\u7814\u7a76\u56e2\u961f\u5f00\u6e90\u4e86\u6700\u4f73\u63d0\u793a\u548cAPO\u5de5\u5177\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2509.02020", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.02020", "abs": "https://arxiv.org/abs/2509.02020", "authors": ["Kun Xie", "Feiyu Shen", "Junjie Li", "Fenglong Xie", "Xu Tang", "Yao Hu"], "title": "FireRedTTS-2: Towards Long Conversational Speech Generation for Podcast and Chatbot", "comment": null, "summary": "Current dialogue generation approaches typically require the complete\ndialogue text before synthesis and produce a single, inseparable speech\ncontaining all voices, making them unsuitable for interactive chat; moreover,\nthey suffer from unstable synthesis, inaccurate speaker transitions, and\nincoherent prosody. In this work, we present FireRedTTS-2, a long-form\nstreaming TTS system for multi-speaker dialogue generation, delivering stable,\nnatural speech with reliable speaker switching and context-aware prosody. A new\n12.5Hz streaming speech tokenizer accelerates training and inference, extends\nmaximum dialogue length, encodes richer semantics to stabilize text-to-token\nmodeling and supports high-fidelity streaming generation for real-time\napplications. We adopt a text-speech interleaved format, concatenating\nspeaker-labeled text with aligned speech tokens in chronological order, and\nmodel it with a dual-transformer: a large decoder-only transformer predicts\ntokens at the first layer, and a smaller one completes subsequent layers.\nExperimental results show that FireRedTTS-2 integrates seamlessly with chat\nframeworks and, with minimal fine-tuning, produces emotionally expressive\nspeech guided by implicit contextual cues. In podcast generation, it surpasses\nexisting systems including MoonCast, Zipvoice-Dialogue, and MOSS-TTSD in\nobjective intelligibility, speaker-turn reliability, and perceived naturalness\nwith context-consistent prosody. Our demos are available at\nhttps://fireredteam.github.io/demos/firered_tts_2.", "AI": {"tldr": "FireRedTTS-2\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u8bf4\u8bdd\u4eba\u5bf9\u8bdd\u751f\u6210\u7684\u957f\u683c\u5f0f\u6d41\u5f0fTTS\u7cfb\u7edf\uff0c\u901a\u8fc7\u65b0\u768412.5Hz\u6d41\u5f0f\u8bed\u97f3\u5206\u8bcd\u5668\u548c\u53cc\u53d8\u6362\u5668\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u81ea\u7136\u8bed\u97f3\u3001\u53ef\u9760\u7684\u8bf4\u8bdd\u4eba\u5207\u6362\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u97f5\u5f8b\u3002", "motivation": "\u5f53\u524d\u5bf9\u8bdd\u751f\u6210\u65b9\u6cd5\u9700\u8981\u5b8c\u6574\u5bf9\u8bdd\u6587\u672c\u624d\u80fd\u5408\u6210\uff0c\u4ea7\u751f\u5305\u542b\u6240\u6709\u58f0\u97f3\u7684\u4e0d\u53ef\u5206\u5272\u8bed\u97f3\uff0c\u4e0d\u9002\u5408\u4ea4\u4e92\u5f0f\u804a\u5929\uff0c\u4e14\u5b58\u5728\u5408\u6210\u4e0d\u7a33\u5b9a\u3001\u8bf4\u8bdd\u4eba\u8f6c\u6362\u4e0d\u51c6\u786e\u548c\u97f5\u5f8b\u4e0d\u8fde\u8d2f\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u65b0\u768412.5Hz\u6d41\u5f0f\u8bed\u97f3\u5206\u8bcd\u5668\u52a0\u901f\u8bad\u7ec3\u548c\u63a8\u7406\uff1b\u4f7f\u7528\u6587\u672c-\u8bed\u97f3\u4ea4\u9519\u683c\u5f0f\uff0c\u6309\u65f6\u95f4\u987a\u5e8f\u8fde\u63a5\u8bf4\u8bdd\u4eba\u6807\u8bb0\u6587\u672c\u548c\u5bf9\u9f50\u7684\u8bed\u97f3\u6807\u8bb0\uff1b\u91c7\u7528\u53cc\u53d8\u6362\u5668\u67b6\u6784\uff1a\u5927\u578b\u4ec5\u89e3\u7801\u5668\u53d8\u6362\u5668\u5728\u7b2c\u4e00\u5c42\u9884\u6d4b\u6807\u8bb0\uff0c\u8f83\u5c0f\u7684\u53d8\u6362\u5668\u5b8c\u6210\u540e\u7eed\u5c42\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFireRedTTS-2\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u5230\u804a\u5929\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u6700\u5c0f\u5fae\u8c03\u5373\u53ef\u4ea7\u751f\u7531\u9690\u5f0f\u4e0a\u4e0b\u6587\u7ebf\u7d22\u5f15\u5bfc\u7684\u60c5\u611f\u8868\u8fbe\u8bed\u97f3\u3002\u5728\u64ad\u5ba2\u751f\u6210\u4e2d\uff0c\u5728\u5ba2\u89c2\u53ef\u61c2\u5ea6\u3001\u8bf4\u8bdd\u4eba\u8f6e\u6362\u53ef\u9760\u6027\u548c\u611f\u77e5\u81ea\u7136\u5ea6\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u7cfb\u7edf\u3002", "conclusion": "FireRedTTS-2\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6d41\u5f0f\u591a\u8bf4\u8bdd\u4eba\u5bf9\u8bdd\u751f\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u4ea4\u4e92\u5f0f\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.00496", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00496", "abs": "https://arxiv.org/abs/2509.00496", "authors": ["Li S. Yifei", "Allen Chang", "Chaitanya Malaviya", "Mark Yatskar"], "title": "ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields with Survey-Mined Questions and Rubrics", "comment": "11 pages main, 40 pages total, 16 figures", "summary": "Evaluating long-form responses to research queries heavily relies on expert\nannotators, restricting attention to areas like AI where researchers can\nconveniently enlist colleagues. Yet, research expertise is widespread: survey\narticles synthesize knowledge distributed across the literature. We introduce\nResearchQA, a resource for evaluating LLM systems by distilling survey articles\nfrom 75 research fields into 21K queries and 160K rubric items. Each rubric,\nderived jointly with queries from survey sections, lists query-specific answer\nevaluation criteria, i.e., citing papers, making explanations, and describing\nlimitations. Assessments by 31 Ph.D. annotators in 8 fields indicate 96% of\nqueries support Ph.D. information needs and 87% of rubric items should be\naddressed in system responses by a sentence or more. Using our rubrics, we are\nable to construct an automatic pairwise judge obtaining 74% agreement with\nexpert judgments. We leverage ResearchQA to analyze competency gaps in 18\nsystems in over 7.6K pairwise evaluations. No parametric or retrieval-augmented\nsystem we evaluate exceeds 70% on covering rubric items, and the\nhighest-ranking agentic system shows 75% coverage. Error analysis reveals that\nthe highest-ranking system fully addresses less than 11% of citation rubric\nitems, 48% of limitation items, and 49% of comparison items. We release our\ndata to facilitate more comprehensive multi-field evaluations.", "AI": {"tldr": "ResearchQA\u662f\u4e00\u4e2a\u4ece75\u4e2a\u7814\u7a76\u9886\u57df\u7684\u7efc\u8ff0\u6587\u7ae0\u4e2d\u63d0\u53d621K\u67e5\u8be2\u548c160K\u8bc4\u5206\u6807\u51c6\u9879\u7684\u8d44\u6e90\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u7cfb\u7edf\u5728\u56de\u7b54\u7814\u7a76\u95ee\u9898\u65f6\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u7cfb\u7edf\u5728\u5f15\u7528\u6587\u732e\u3001\u89e3\u91ca\u548c\u63cf\u8ff0\u5c40\u9650\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u80fd\u529b\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u7814\u7a76\u67e5\u8be2\u7684\u957f\u7bc7\u56de\u7b54\u4e3b\u8981\u4f9d\u8d56\u4e13\u5bb6\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u8bc4\u4f30\u8303\u56f4\u3002\u7814\u7a76\u4e13\u4e1a\u77e5\u8bc6\u5e7f\u6cdb\u5b58\u5728\u4e8e\u7efc\u8ff0\u6587\u7ae0\u4e2d\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u8d44\u6e90\u6765\u652f\u6301\u591a\u9886\u57df\u7814\u7a76\u3002", "method": "\u4ece75\u4e2a\u7814\u7a76\u9886\u57df\u7684\u7efc\u8ff0\u6587\u7ae0\u4e2d\u63d0\u53d621K\u4e2a\u67e5\u8be2\u548c160K\u4e2a\u8bc4\u5206\u6807\u51c6\u9879\uff0c\u6bcf\u4e2a\u8bc4\u5206\u6807\u51c6\u5305\u542b\u67e5\u8be2\u7279\u5b9a\u7684\u7b54\u6848\u8bc4\u4f30\u6807\u51c6\uff08\u5f15\u7528\u8bba\u6587\u3001\u89e3\u91ca\u8bf4\u660e\u3001\u63cf\u8ff0\u5c40\u9650\u6027\uff09\u3002\u4f7f\u752831\u4f4d\u535a\u58eb\u6807\u6ce8\u8005\u57288\u4e2a\u9886\u57df\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "96%\u7684\u67e5\u8be2\u652f\u6301\u535a\u58eb\u4fe1\u606f\u9700\u6c42\uff0c87%\u7684\u8bc4\u5206\u6807\u51c6\u9879\u9700\u8981\u5728\u7cfb\u7edf\u56de\u7b54\u4e2d\u7528\u81f3\u5c11\u4e00\u4e2a\u53e5\u5b50\u6765\u6ee1\u8db3\u3002\u81ea\u52a8\u914d\u5bf9\u8bc4\u4f30\u4e0e\u4e13\u5bb6\u5224\u65ad\u8fbe\u523074%\u4e00\u81f4\u7387\u300218\u4e2a\u7cfb\u7edf\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u6ca1\u6709\u53c2\u6570\u5316\u6216\u68c0\u7d22\u589e\u5f3a\u7cfb\u7edf\u8d85\u8fc770%\u7684\u8bc4\u5206\u6807\u51c6\u8986\u76d6\u7387\uff0c\u6700\u9ad8\u6392\u540d\u4ee3\u7406\u7cfb\u7edf\u8fbe\u523075%\u8986\u76d6\u7387\u3002", "conclusion": "ResearchQA\u63ed\u793a\u4e86LLM\u7cfb\u7edf\u5728\u7814\u7a76\u67e5\u8be2\u56de\u7b54\u4e2d\u7684\u663e\u8457\u80fd\u529b\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u5f15\u7528\u6587\u732e\u65b9\u9762\uff08\u5b8c\u5168\u6ee1\u8db3\u7387\u4f4e\u4e8e11%\uff09\u3002\u8be5\u8d44\u6e90\u4e3a\u66f4\u5168\u9762\u7684\u591a\u9886\u57df\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.02167", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.02167", "abs": "https://arxiv.org/abs/2509.02167", "authors": ["Jiayu Xiong", "Jun Xue", "Jianlong Kwan", "Jing Wang"], "title": "AudioRWKV: Efficient and Stable Bidirectional RWKV for Audio Pattern Recognition", "comment": "6 pages, 3 figures", "summary": "Recently, Transformers (e.g., Audio Spectrogram Transformers, AST) and\nstate-space models (e.g., Audio Mamba, AuM) have achieved remarkable progress\nin audio modeling. However, the O(L^2) computational complexity of the\nTransformer architecture hinders efficient long-sequence processing, while the\nMamba architecture tends to become unstable when scaling parameters and data.\nTo address these challenges, this paper proposes AudioRWKV (A-RWKV), a highly\nefficient and stable architecture for audio modeling. Specifically, we inherit\nthe stable and efficient recurrent formulation of RWKV7 and replace its 1D\ntoken-shift operation with a 2D depthwise separable convolution to better\ncapture local spectro-temporal patterns. Furthermore, we adapt the original\ncausal WKV kernel into a bidirectional WKV kernel (Bi-WKV), enabling global\ncontext modeling over the entire audio sequence while maintaining linear\ncomputational complexity. Benefiting from the inherent stability of the RWKV7\nfoundation, A-RWKV scales seamlessly to larger model sizes. Experimental\nresults demonstrate that, under the same linear-model regime, A-RWKV-S (22M)\nachieves performance parity with AuM-B (92M) while exhibiting more stable\nthroughput than AST; for long-form audio (~5 minutes 28 seconds), WKV7 achieves\nup to a 13.3X speedup in processing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AudioRWKV (A-RWKV)\uff0c\u4e00\u79cd\u9ad8\u6548\u7a33\u5b9a\u7684\u97f3\u9891\u5efa\u6a21\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u5408RWKV7\u7684\u5faa\u73af\u7ed3\u6784\u548c\u53cc\u5411WKV\u6838\uff0c\u5728\u4fdd\u6301\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u3002", "motivation": "Transformer\u67b6\u6784\u7684O(L^2)\u8ba1\u7b97\u590d\u6742\u5ea6\u9650\u5236\u4e86\u957f\u5e8f\u5217\u5904\u7406\u6548\u7387\uff0c\u800cMamba\u67b6\u6784\u5728\u6269\u5c55\u53c2\u6570\u548c\u6570\u636e\u65f6\u5bb9\u6613\u53d8\u5f97\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u4e00\u79cd\u65e2\u9ad8\u6548\u53c8\u7a33\u5b9a\u7684\u97f3\u9891\u5efa\u6a21\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ee7\u627fRWKV7\u7684\u7a33\u5b9a\u9ad8\u6548\u5faa\u73af\u516c\u5f0f\uff0c\u5c061D token-shift\u64cd\u4f5c\u66ff\u6362\u4e3a2D\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u4ee5\u6355\u6349\u5c40\u90e8\u9891\u8c31-\u65f6\u95f4\u6a21\u5f0f\uff0c\u5e76\u5c06\u539f\u59cb\u56e0\u679cWKV\u6838\u9002\u914d\u4e3a\u53cc\u5411WKV\u6838(Bi-WKV)\u3002", "result": "A-RWKV-S (22M)\u5728\u76f8\u540c\u7ebf\u6027\u6a21\u578b\u4f53\u7cfb\u4e0b\u8fbe\u5230\u4e0eAuM-B (92M)\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6bd4AST\u5177\u6709\u66f4\u7a33\u5b9a\u7684\u541e\u5410\u91cf\uff1b\u5bf9\u4e8e\u957f\u97f3\u9891(~5\u520628\u79d2)\uff0c\u5904\u7406\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe13.3\u500d\u3002", "conclusion": "A-RWKV\u67b6\u6784\u6210\u529f\u89e3\u51b3\u4e86\u97f3\u9891\u5efa\u6a21\u4e2d\u7684\u6548\u7387\u548c\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u80fd\u591f\u65e0\u7f1d\u6269\u5c55\u5230\u66f4\u5927\u6a21\u578b\u89c4\u6a21\uff0c\u4e3a\u957f\u5e8f\u5217\u97f3\u9891\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00503", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00503", "abs": "https://arxiv.org/abs/2509.00503", "authors": ["Jialong Zuo", "Guangyan Zhang", "Minghui Fang", "Shengpeng Ji", "Xiaoqi Jiao", "Jingyu Li", "Yiwen Guo", "Zhou Zhao"], "title": "Entropy-based Coarse and Compressed Semantic Speech Representation Learning", "comment": null, "summary": "Discrete speech representation learning has recently attracted increasing\ninterest in both acoustic and semantic modeling. Existing approaches typically\nencode 16 kHz waveforms into discrete tokens at a rate of 25 or 50 tokens per\nsecond. However, given that speech generally conveys only 2 to 5 words per\nsecond, such fine-grained tokenization introduces redundancy and hinders\nefficiency in downstream training and inference. Moreover, semantic speech\nrepresentations at this frequency primarily capture phonetic-level information,\nwhile semantic understanding may not require such detailed token-level\nresolution. To address these limitations, we propose an entropy-based dynamic\naggregation framework for learning compressed semantic speech representations.\nA speech language model is first pre-trained via next-token prediction on\nlarge-scale unlabeled data to capture frequent token patterns. Predictive\nentropy is then used to adaptively determine aggregation boundaries, followed\nby a cross-attention module that fuses information within each segment. By\nadjusting the entropy threshold, the granularity and compression ratio of the\nrepresentations can be flexibly controlled. Experiments on ASR, speech-to-text\ntranslation, and voice conversion tasks demonstrate that the compressed\nrepresentations perform on par with or better than dense token sequences,\ndemonstrating the effectiveness of the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u71b5\u7684\u52a8\u6001\u805a\u5408\u6846\u67b6\uff0c\u5c06\u7ec6\u7c92\u5ea6\u8bed\u97f3token\u538b\u7f29\u4e3a\u66f4\u7c97\u7c92\u5ea6\u7684\u8bed\u4e49\u8868\u793a\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u6548\u7387", "motivation": "\u73b0\u6709\u8bed\u97f3\u79bb\u6563\u8868\u793a\u65b9\u6cd5\u751f\u621025-50 tokens/\u79d2\uff0c\u4f46\u8bed\u97f3\u901a\u5e38\u53ea\u67092-5\u8bcd/\u79d2\uff0c\u5b58\u5728\u5197\u4f59\u4e14\u4e3b\u8981\u6355\u83b7\u97f3\u7d20\u7ea7\u4fe1\u606f\uff0c\u4e0d\u5229\u4e8e\u4e0b\u6e38\u4efb\u52a1\u6548\u7387", "method": "\u5148\u901a\u8fc7\u4e0b\u4e00token\u9884\u6d4b\u9884\u8bad\u7ec3\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff0c\u7136\u540e\u4f7f\u7528\u9884\u6d4b\u71b5\u81ea\u9002\u5e94\u786e\u5b9a\u805a\u5408\u8fb9\u754c\uff0c\u518d\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u878d\u5408\u6bb5\u5185\u4fe1\u606f", "result": "\u5728ASR\u3001\u8bed\u97f3\u8f6c\u6587\u672c\u7ffb\u8bd1\u548c\u8bed\u97f3\u8f6c\u6362\u4efb\u52a1\u4e0a\uff0c\u538b\u7f29\u540e\u7684\u8868\u793a\u6027\u80fd\u4e0e\u5bc6\u96c6token\u5e8f\u5217\u76f8\u5f53\u6216\u66f4\u597d", "conclusion": "\u63d0\u51fa\u7684\u71b5\u57fa\u52a8\u6001\u805a\u5408\u6846\u67b6\u80fd\u6709\u6548\u5b66\u4e60\u538b\u7f29\u7684\u8bed\u4e49\u8bed\u97f3\u8868\u793a\uff0c\u901a\u8fc7\u8c03\u6574\u71b5\u9608\u503c\u53ef\u7075\u6d3b\u63a7\u5236\u8868\u793a\u7c92\u5ea6\u548c\u538b\u7f29\u6bd4"}}
{"id": "2509.02244", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.02244", "abs": "https://arxiv.org/abs/2509.02244", "authors": ["Luis Felipe Chary", "Miguel Arjona Ramirez"], "title": "Spectrogram Patch Codec: A 2D Block-Quantized VQ-VAE and HiFi-GAN for Neural Speech Coding", "comment": null, "summary": "We present a neural speech codec that challenges the need for complex\nresidual vector quantization (RVQ) stacks by introducing a simpler,\nsingle-stage quantization approach. Our method operates directly on the\nmel-spectrogram, treating it as a 2D data and quantizing non-overlapping 4x4\npatches into a single, shared codebook. This patchwise design simplifies the\narchitecture, enables low-latency streaming, and yields a discrete latent grid.\nTo ensure high-fidelity synthesis, we employ a late-stage adversarial\nfine-tuning for the VQ-VAE and train a HiFi-GAN vocoder from scratch on the\ncodec's reconstructed spectrograms. Operating at approximately 7.5 kbits/s for\n16 kHz speech, our system was evaluated against several state-of-the-art neural\ncodecs using objective metrics such as STOI, PESQ, MCD, and ViSQOL. The results\ndemonstrate that our simplified, non-residual architecture achieves competitive\nperceptual quality and intelligibility, validating it as an effective and open\nfoundation for future low-latency codec designs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5316\u7684\u795e\u7ecf\u8bed\u97f3\u7f16\u89e3\u7801\u5668\uff0c\u4f7f\u7528\u5355\u9636\u6bb5\u91cf\u5316\u65b9\u6cd5\u66ff\u4ee3\u590d\u6742\u7684\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u5806\u6808\uff0c\u57287.5kbps\u7801\u7387\u4e0b\u5b9e\u73b0\u7ade\u4e89\u6027\u7684\u611f\u77e5\u8d28\u91cf\u548c\u53ef\u61c2\u5ea6\u3002", "motivation": "\u6311\u6218\u4f20\u7edf\u795e\u7ecf\u8bed\u97f3\u7f16\u89e3\u7801\u5668\u4e2d\u590d\u6742\u7684\u6b8b\u5dee\u5411\u91cf\u91cf\u5316(RVQ)\u5806\u6808\u7ed3\u6784\uff0c\u5bfb\u6c42\u66f4\u7b80\u5355\u3001\u4f4e\u5ef6\u8fdf\u7684\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u76f4\u63a5\u5728mel\u9891\u8c31\u56fe\u4e0a\u64cd\u4f5c\uff0c\u5c06\u5176\u89c6\u4e3a2D\u6570\u636e\uff0c\u5c06\u975e\u91cd\u53e0\u76844x4\u5757\u91cf\u5316\u4e3a\u5355\u4e2a\u5171\u4eab\u7801\u672c\uff1b\u91c7\u7528\u540e\u671f\u5bf9\u6297\u5fae\u8c03VQ-VAE\uff0c\u5e76\u4ece\u5934\u8bad\u7ec3HiFi-GAN\u58f0\u7801\u5668\u3002", "result": "\u572816kHz\u8bed\u97f3\u7ea67.5kbits/s\u7801\u7387\u4e0b\uff0c\u901a\u8fc7STOI\u3001PESQ\u3001MCD\u548cViSQOL\u7b49\u5ba2\u89c2\u6307\u6807\u8bc4\u4f30\uff0c\u663e\u793a\u8be5\u7b80\u5316\u67b6\u6784\u8fbe\u5230\u4e86\u7ade\u4e89\u6027\u7684\u611f\u77e5\u8d28\u91cf\u548c\u53ef\u61c2\u5ea6\u3002", "conclusion": "\u8fd9\u79cd\u7b80\u5316\u7684\u975e\u6b8b\u5dee\u67b6\u6784\u4e3a\u672a\u6765\u4f4e\u5ef6\u8fdf\u7f16\u89e3\u7801\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u5f00\u653e\u7684\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u7b80\u5355\u65b9\u6cd5\u4e5f\u80fd\u8fbe\u5230\u826f\u597d\u6027\u80fd\u3002"}}
{"id": "2509.00529", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.00529", "abs": "https://arxiv.org/abs/2509.00529", "authors": ["Eunjung Cho", "Alexander Hoyle", "Yoan Hermstr\u00fcwer"], "title": "Modeling Motivated Reasoning in Law: Evaluating Strategic Role Conditioning in LLM Summarization", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to generate user-tailored\nsummaries, adapting outputs to specific stakeholders. In legal contexts, this\nraises important questions about motivated reasoning -- how models\nstrategically frame information to align with a stakeholder's position within\nthe legal system. Building on theories of legal realism and recent trends in\nlegal practice, we investigate how LLMs respond to prompts conditioned on\ndifferent legal roles (e.g., judges, prosecutors, attorneys) when summarizing\njudicial decisions. We introduce an evaluation framework grounded in legal fact\nand reasoning inclusion, also considering favorability towards stakeholders.\nOur results show that even when prompts include balancing instructions, models\nexhibit selective inclusion patterns that reflect role-consistent perspectives.\nThese findings raise broader concerns about how similar alignment may emerge as\nLLMs begin to infer user roles from prior interactions or context, even without\nexplicit role instructions. Our results underscore the need for role-aware\nevaluation of LLM summarization behavior in high-stakes legal settings.", "AI": {"tldr": "LLMs\u5728\u751f\u6210\u6cd5\u5f8b\u6587\u4ef6\u6458\u8981\u65f6\u4f1a\u6839\u636e\u7528\u6237\u89d2\u8272\uff08\u6cd5\u5b98\u3001\u68c0\u5bdf\u5b98\u3001\u5f8b\u5e08\u7b49\uff09\u8fdb\u884c\u52a8\u673a\u6027\u63a8\u7406\uff0c\u9009\u62e9\u6027\u5305\u542b\u4fe1\u606f\u4ee5\u7b26\u5408\u89d2\u8272\u7acb\u573a\uff0c\u5373\u4f7f\u6709\u5e73\u8861\u6307\u4ee4\u4e5f\u96be\u4ee5\u907f\u514d\u8fd9\u79cd\u504f\u89c1\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u6cd5\u5f8b\u8bed\u5883\u4e2d\u5982\u4f55\u6839\u636e\u4e0d\u540c\u7684\u6cd5\u5f8b\u89d2\u8272\u8fdb\u884c\u52a8\u673a\u6027\u63a8\u7406\uff0c\u5373\u6a21\u578b\u5982\u4f55\u7b56\u7565\u6027\u5730\u6784\u5efa\u4fe1\u606f\u4ee5\u7b26\u5408\u5229\u76ca\u76f8\u5173\u8005\u5728\u6cd5\u5f8b\u4f53\u7cfb\u4e2d\u7684\u7acb\u573a\u3002", "method": "\u57fa\u4e8e\u6cd5\u5f8b\u73b0\u5b9e\u4e3b\u4e49\u548c\u8fd1\u671f\u6cd5\u5f8b\u5b9e\u8df5\u8d8b\u52bf\uff0c\u5f15\u5165\u57fa\u4e8e\u6cd5\u5f8b\u4e8b\u5b9e\u548c\u63a8\u7406\u5305\u542b\u5ea6\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5206\u6790LLMs\u5728\u4e0d\u540c\u6cd5\u5f8b\u89d2\u8272\u63d0\u793a\u4e0b\u603b\u7ed3\u53f8\u6cd5\u5224\u51b3\u7684\u884c\u4e3a\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5373\u4f7f\u63d0\u793a\u5305\u542b\u5e73\u8861\u6307\u4ee4\uff0c\u6a21\u578b\u4ecd\u8868\u73b0\u51fa\u9009\u62e9\u6027\u5305\u542b\u6a21\u5f0f\uff0c\u53cd\u6620\u51fa\u4e0e\u89d2\u8272\u4e00\u81f4\u7684\u89c6\u89d2\u504f\u597d\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u9ad8\u98ce\u9669\u6cd5\u5f8b\u73af\u5883\u4e2d\u5bf9LLM\u6458\u8981\u884c\u4e3a\u8fdb\u884c\u89d2\u8272\u611f\u77e5\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\uff0c\u56e0\u4e3a\u5373\u4f7f\u6ca1\u6709\u660e\u786e\u89d2\u8272\u6307\u4ee4\uff0cLLMs\u4e5f\u53ef\u80fd\u4ece\u5148\u524d\u4ea4\u4e92\u6216\u4e0a\u4e0b\u6587\u4e2d\u63a8\u65ad\u7528\u6237\u89d2\u8272\u800c\u4ea7\u751f\u7c7b\u4f3c\u7684\u5bf9\u9f50\u95ee\u9898\u3002"}}
{"id": "2509.02259", "categories": ["cs.SD", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.02259", "abs": "https://arxiv.org/abs/2509.02259", "authors": ["Guillem Bonafos", "J\u00e9remy Rouch", "L\u00e9ny Lego", "David Reby", "Hugues Patural", "Nicolas Mathevon", "R\u00e9my Emonet"], "title": "Speech transformer models for extracting information from baby cries", "comment": "Accepted to WOCCI2025 (interspeech2025 workshop)", "summary": "Transfer learning using latent representations from pre-trained speech models\nachieves outstanding performance in tasks where labeled data is scarce.\nHowever, their applicability to non-speech data and the specific acoustic\nproperties encoded in these representations remain largely unexplored. In this\nstudy, we investigate both aspects. We evaluate five pre-trained speech models\non eight baby cries datasets, encompassing 115 hours of audio from 960 babies.\nFor each dataset, we assess the latent representations of each model across all\navailable classification tasks. Our results demonstrate that the latent\nrepresentations of these models can effectively classify human baby cries and\nencode key information related to vocal source instability and identity of the\ncrying baby. In addition, a comparison of the architectures and training\nstrategies of these models offers valuable insights for the design of future\nmodels tailored to similar tasks, such as emotion detection.", "AI": {"tldr": "\u9884\u8bad\u7ec3\u8bed\u97f3\u6a21\u578b\u7684\u6f5c\u5728\u8868\u793a\u53ef\u4ee5\u6709\u6548\u5206\u7c7b\u5a74\u513f\u54ed\u58f0\uff0c\u7f16\u7801\u58f0\u97f3\u6e90\u4e0d\u7a33\u5b9a\u6027\u548c\u5a74\u513f\u8eab\u4efd\u4fe1\u606f\uff0c\u4e3a\u7c7b\u4f3c\u60c5\u611f\u68c0\u6d4b\u4efb\u52a1\u63d0\u4f9b\u8bbe\u8ba1\u53c2\u8003", "motivation": "\u63a2\u7d22\u9884\u8bad\u7ec3\u8bed\u97f3\u6a21\u578b\u5728\u975e\u8bed\u97f3\u6570\u636e\uff08\u5a74\u513f\u54ed\u58f0\uff09\u4e0a\u7684\u9002\u7528\u6027\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u6a21\u578b\u4e2d\u7f16\u7801\u7684\u5177\u4f53\u58f0\u5b66\u7279\u6027", "method": "\u8bc4\u4f305\u4e2a\u9884\u8bad\u7ec3\u8bed\u97f3\u6a21\u578b\u57288\u4e2a\u5a74\u513f\u54ed\u58f0\u6570\u636e\u96c6\uff08115\u5c0f\u65f6\u97f3\u9891\uff0c960\u540d\u5a74\u513f\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u5404\u6a21\u578b\u7684\u6f5c\u5728\u8868\u793a\u5728\u6240\u6709\u53ef\u7528\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6548\u679c", "result": "\u6a21\u578b\u6f5c\u5728\u8868\u793a\u80fd\u6709\u6548\u5206\u7c7b\u5a74\u513f\u54ed\u58f0\uff0c\u7f16\u7801\u4e86\u58f0\u97f3\u6e90\u4e0d\u7a33\u5b9a\u6027\u548c\u5a74\u513f\u8eab\u4efd\u7684\u5173\u952e\u4fe1\u606f", "conclusion": "\u9884\u8bad\u7ec3\u8bed\u97f3\u6a21\u578b\u9002\u7528\u4e8e\u5a74\u513f\u54ed\u58f0\u5206\u6790\uff0c\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\u7684\u6bd4\u8f83\u4e3a\u672a\u6765\u7c7b\u4f3c\u4efb\u52a1\uff08\u5982\u60c5\u611f\u68c0\u6d4b\uff09\u7684\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3"}}
{"id": "2509.00544", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00544", "abs": "https://arxiv.org/abs/2509.00544", "authors": ["Hanqi Yan", "Hainiu Xu", "Yulan He"], "title": "Thinking Hard, Going Misaligned: Emergent Misalignment in LLMs", "comment": null, "summary": "With Large Language Models (LLMs) becoming increasingly widely adopted,\nconcerns regarding their safety and alignment with human values have\nintensified. Previous studies have shown that fine-tuning LLMs on narrow and\nmalicious datasets induce misaligned behaviors. In this work, we report a more\nconcerning phenomenon, Reasoning-Induced Misalignment. Specifically, we observe\nthat LLMs become more responsive to malicious requests when reasoning is\nstrengthened, via switching to \"think-mode\" or fine-tuning on benign math\ndatasets, with dense models particularly vulnerable. Moreover, we analyze\ninternal model states and find that both attention shifts and specialized\nexperts in mixture-of-experts models help redirect excessive reasoning towards\nsafety guardrails. These findings provide new insights into the emerging\nreasoning-safety trade-off and underscore the urgency of advancing alignment\nfor advanced reasoning models.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u5728\u63a8\u7406\u80fd\u529b\u589e\u5f3a\u65f6\u66f4\u5bb9\u6613\u54cd\u5e94\u6076\u610f\u8bf7\u6c42\uff0c\u8fd9\u79cd\u73b0\u8c61\u88ab\u79f0\u4e3a\u63a8\u7406\u8bf1\u5bfc\u9519\u4f4d\uff0c\u7279\u522b\u662f\u5728\u5bc6\u96c6\u6a21\u578b\u4e2d\u66f4\u4e3a\u660e\u663e\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5e7f\u6cdb\u5e94\u7528\uff0c\u5bf9\u5176\u5b89\u5168\u6027\u548c\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u7684\u62c5\u5fe7\u65e5\u76ca\u589e\u52a0\u3002\u4e4b\u524d\u7814\u7a76\u8868\u660e\u5728\u6076\u610f\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u4f1a\u5bfc\u81f4\u9519\u4f4d\u884c\u4e3a\uff0c\u4f46\u672c\u7814\u7a76\u53d1\u73b0\u5373\u4f7f\u4f7f\u7528\u826f\u6027\u6570\u636e\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u4e5f\u4f1a\u5bfc\u81f4\u5b89\u5168\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5207\u6362\u5230\"\u601d\u8003\u6a21\u5f0f\"\u6216\u5728\u826f\u6027\u6570\u5b66\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u6765\u589e\u5f3aLLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u7136\u540e\u5206\u6790\u6a21\u578b\u5bf9\u6076\u610f\u8bf7\u6c42\u7684\u54cd\u5e94\u6027\uff0c\u5e76\u68c0\u67e5\u5185\u90e8\u6a21\u578b\u72b6\u6001\u5982\u6ce8\u610f\u529b\u8f6c\u79fb\u548c\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u4e2d\u7684\u4e13\u5bb6\u5206\u5de5\u3002", "result": "\u53d1\u73b0\u63a8\u7406\u80fd\u529b\u589e\u5f3a\u540eLLMs\u5bf9\u6076\u610f\u8bf7\u6c42\u7684\u54cd\u5e94\u6027\u663e\u8457\u63d0\u9ad8\uff0c\u5bc6\u96c6\u6a21\u578b\u7279\u522b\u8106\u5f31\u3002\u540c\u65f6\u53d1\u73b0\u6ce8\u610f\u529b\u8f6c\u79fb\u548c\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u4e2d\u7684\u4e13\u95e8\u4e13\u5bb6\u6709\u52a9\u4e8e\u5c06\u8fc7\u5ea6\u63a8\u7406\u91cd\u65b0\u5bfc\u5411\u5b89\u5168\u9632\u62a4\u673a\u5236\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u65b0\u5174\u7684\u63a8\u7406-\u5b89\u5168\u6743\u8861\u95ee\u9898\uff0c\u5f3a\u8c03\u4e86\u4e3a\u9ad8\u7ea7\u63a8\u7406\u6a21\u578b\u63a8\u8fdb\u5bf9\u9f50\u6280\u672f\u7684\u7d27\u8feb\u6027\uff0c\u4e3a\u7406\u89e3LLMs\u5b89\u5168\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2509.02349", "categories": ["cs.SD", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02349", "abs": "https://arxiv.org/abs/2509.02349", "authors": ["Lu Wang", "Hao Chen", "Siyu Wu", "Zhiyue Wu", "Hao Zhou", "Chengfeng Zhang", "Ting Wang", "Haodi Zhang"], "title": "AudioCodecBench: A Comprehensive Benchmark for Audio Codec Evaluation", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have been widely applied in speech\nand music. This tendency has led to a focus on audio tokenization for Large\nModels (LMs). Unlike semantic-only text tokens, audio tokens must both capture\nglobal semantic content and preserve fine-grained acoustic details. Moreover,\nthey provide a discrete method for speech and music that can be effectively\nintegrated into MLLMs. However, existing research is unsuitable in the\ndefinitions of semantic tokens and acoustic tokens. In addition, the evaluation\nof different codecs typically concentrates on specific domains or tasks, such\nas reconstruction or Automatic Speech Recognition (ASR) task, which prevents\nfair and comprehensive comparisons. To address these problems, this paper\nprovides suitable definitions for semantic and acoustic tokens and introduces a\nsystematic evaluation framework. This framework allows for a comprehensive\nassessment of codecs' capabilities which evaluate across four dimensions: audio\nreconstruction metric, codebook index (ID) stability, decoder-only transformer\nperplexity, and performance on downstream probe tasks. Our results show the\ncorrectness of the provided suitable definitions and the correlation among\nreconstruction metrics, codebook ID stability, downstream probe tasks and\nperplexity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u97f3\u9891token\u5316\u7684\u5408\u9002\u5b9a\u4e49\u548c\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u8bed\u4e49token\u548c\u58f0\u5b66token\u5b9a\u4e49\u4e0d\u660e\u786e\u4ee5\u53ca\u8bc4\u4f30\u4e0d\u5168\u9762\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u97f3\u548c\u97f3\u4e50\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u97f3\u9891token\u5316\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8bed\u4e49token\u548c\u58f0\u5b66token\u7684\u660e\u786e\u5b9a\u4e49\uff0c\u4e14\u8bc4\u4f30\u65b9\u6cd5\u5c40\u9650\u4e8e\u7279\u5b9a\u9886\u57df\u6216\u4efb\u52a1\uff0c\u65e0\u6cd5\u8fdb\u884c\u516c\u5e73\u5168\u9762\u7684\u6bd4\u8f83\u3002", "method": "\u63d0\u51fa\u4e86\u8bed\u4e49token\u548c\u58f0\u5b66token\u7684\u5408\u9002\u5b9a\u4e49\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u4ece\u56db\u4e2a\u7ef4\u5ea6\u5168\u9762\u8bc4\u4f30\u7f16\u89e3\u7801\u5668\u80fd\u529b\uff1a\u97f3\u9891\u91cd\u5efa\u6307\u6807\u3001\u7801\u672c\u7d22\u5f15\u7a33\u5b9a\u6027\u3001\u4ec5\u89e3\u7801\u5668transformer\u56f0\u60d1\u5ea6\u4ee5\u53ca\u4e0b\u6e38\u63a2\u6d4b\u4efb\u52a1\u6027\u80fd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u4f9b\u5408\u9002\u5b9a\u4e49\u7684\u6b63\u786e\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u91cd\u5efa\u6307\u6807\u3001\u7801\u672c\u7d22\u5f15\u7a33\u5b9a\u6027\u3001\u4e0b\u6e38\u63a2\u6d4b\u4efb\u52a1\u548c\u56f0\u60d1\u5ea6\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u97f3\u9891token\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u7cfb\u7edf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u66f4\u516c\u5e73\u5168\u9762\u5730\u6bd4\u8f83\u4e0d\u540c\u7f16\u89e3\u7801\u5668\u7684\u6027\u80fd\uff0c\u63a8\u52a8\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u97f3\u9891\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.00591", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00591", "abs": "https://arxiv.org/abs/2509.00591", "authors": ["Lang Xiong", "Nishant Bhargava", "Wesley Chang", "Jianhang Hong", "Haihao Liu", "Kevin Zhu"], "title": "StealthEval: A Probe-Rewrite-Evaluate Workflow for Reliable Benchmarks", "comment": null, "summary": "Large Language Models (LLMs) often exhibit significant behavioral shifts when\nthey perceive a change from a real-world deployment context to a controlled\nevaluation setting, a phenomenon known as \"evaluation awareness.\" This\ndiscrepancy poses a critical challenge for AI alignment, as benchmark\nperformance may not accurately reflect a model's true safety and honesty. In\nthis work, we systematically quantify these behavioral changes by manipulating\nthe perceived context of prompts. We introduce a methodology that uses a linear\nprobe to score prompts on a continuous scale from \"test-like\" to \"deploy-like\"\nand leverage an LLM rewriting strategy to shift these prompts towards a more\nnatural, deployment-style context while preserving the original task. Using\nthis method, we achieved a 30% increase in the average probe score across a\nstrategic role-playing dataset after rewriting. Evaluating a suite of\nstate-of-the-art models on these original and rewritten prompts, we find that\nrewritten \"deploy-like\" prompts induce a significant and consistent shift in\nbehavior. Across all models, we observed an average increase in honest\nresponses of 5.26% and a corresponding average decrease in deceptive responses\nof 12.40%. Furthermore, refusal rates increased by an average of 6.38%,\nindicating heightened safety compliance. Our findings demonstrate that\nevaluation awareness is a quantifiable and manipulable factor that directly\ninfluences LLM behavior, revealing that models are more prone to unsafe or\ndeceptive outputs in perceived test environments. This underscores the urgent\nneed for more realistic evaluation frameworks to accurately gauge true model\nalignment before deployment.", "AI": {"tldr": "LLMs\u5728\u4e0d\u540c\u611f\u77e5\u73af\u5883\u4e0b\u884c\u4e3a\u5dee\u5f02\u663e\u8457\uff0c\u8bc4\u4f30\u610f\u8bc6\u73b0\u8c61\u5bfc\u81f4\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u6a21\u578b\u771f\u5b9e\u5b89\u5168\u6027\u548c\u8bda\u5b9e\u5ea6\u3002\u901a\u8fc7\u7ebf\u6027\u63a2\u6d4b\u548c\u63d0\u793a\u91cd\u5199\u65b9\u6cd5\uff0c\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u5728\u6d4b\u8bd5\u73af\u5883\u4e0b\u66f4\u5bb9\u6613\u4ea7\u751f\u4e0d\u5b89\u5168\u6216\u6b3a\u9a97\u6027\u8f93\u51fa\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u611f\u77e5\u5230\u4ece\u5b9e\u9645\u90e8\u7f72\u73af\u5883\u5207\u6362\u5230\u53d7\u63a7\u8bc4\u4f30\u73af\u5883\u65f6\u4f1a\u51fa\u73b0\u663e\u8457\u884c\u4e3a\u53d8\u5316\uff0c\u8fd9\u79cd\u8bc4\u4f30\u610f\u8bc6\u73b0\u8c61\u5bf9AI\u5bf9\u9f50\u6784\u6210\u4e25\u5cfb\u6311\u6218\uff0c\u56e0\u4e3a\u57fa\u51c6\u6027\u80fd\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u6a21\u578b\u7684\u771f\u5b9e\u5b89\u5168\u6027\u548c\u8bda\u5b9e\u5ea6\u3002", "method": "\u5f15\u5165\u7ebf\u6027\u63a2\u6d4b\u65b9\u6cd5\u5bf9\u63d0\u793a\u8fdb\u884c\u4ece\"\u6d4b\u8bd5\u7c7b\"\u5230\"\u90e8\u7f72\u7c7b\"\u7684\u8fde\u7eed\u8bc4\u5206\uff0c\u5229\u7528LLM\u91cd\u5199\u7b56\u7565\u5c06\u63d0\u793a\u8f6c\u5411\u66f4\u81ea\u7136\u7684\u90e8\u7f72\u98ce\u683c\u4e0a\u4e0b\u6587\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u4efb\u52a1\u4e0d\u53d8\u3002", "result": "\u91cd\u5199\u540e\u63d0\u793a\u7684\u5e73\u5747\u63a2\u6d4b\u5206\u6570\u63d0\u9ad8\u4e8630%\uff0c\u6240\u6709\u6a21\u578b\u8bda\u5b9e\u56de\u7b54\u5e73\u5747\u589e\u52a05.26%\uff0c\u6b3a\u9a97\u6027\u56de\u7b54\u5e73\u5747\u51cf\u5c1112.40%\uff0c\u62d2\u7edd\u7387\u5e73\u5747\u589e\u52a06.38%\uff0c\u8868\u660e\u5b89\u5168\u5408\u89c4\u6027\u589e\u5f3a\u3002", "conclusion": "\u8bc4\u4f30\u610f\u8bc6\u662f\u53ef\u91cf\u5316\u548c\u53ef\u64cd\u7eb5\u7684\u56e0\u7d20\uff0c\u76f4\u63a5\u5f71\u54cdLLM\u884c\u4e3a\uff0c\u6a21\u578b\u5728\u611f\u77e5\u7684\u6d4b\u8bd5\u73af\u5883\u4e2d\u66f4\u5bb9\u6613\u4ea7\u751f\u4e0d\u5b89\u5168\u6216\u6b3a\u9a97\u6027\u8f93\u51fa\uff0c\u8feb\u5207\u9700\u8981\u66f4\u73b0\u5b9e\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u51c6\u786e\u8861\u91cf\u6a21\u578b\u5bf9\u9f50\u5ea6\u3002"}}
{"id": "2509.02398", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.02398", "abs": "https://arxiv.org/abs/2509.02398", "authors": ["Hui Wang", "Cheng Liu", "Junyang Chen", "Haoze Liu", "Yuhang Jia", "Shiwan Zhao", "Jiaming Zhou", "Haoqin Sun", "Hui Bu", "Yong Qin"], "title": "TTA-Bench: A Comprehensive Benchmark for Evaluating Text-to-Audio Models", "comment": null, "summary": "Text-to-Audio (TTA) generation has made rapid progress, but current\nevaluation methods remain narrow, focusing mainly on perceptual quality while\noverlooking robustness, generalization, and ethical concerns. We present\nTTA-Bench, a comprehensive benchmark for evaluating TTA models across\nfunctional performance, reliability, and social responsibility. It covers seven\ndimensions including accuracy, robustness, fairness, and toxicity, and includes\n2,999 diverse prompts generated through automated and manual methods. We\nintroduce a unified evaluation protocol that combines objective metrics with\nover 118,000 human annotations from both experts and general users. Ten\nstate-of-the-art models are benchmarked under this framework, offering detailed\ninsights into their strengths and limitations. TTA-Bench establishes a new\nstandard for holistic and responsible evaluation of TTA systems. The dataset\nand evaluation tools are open-sourced at https://nku-hlt.github.io/tta-bench/.", "AI": {"tldr": "TTA-Bench\u662f\u4e00\u4e2a\u5168\u9762\u7684\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6\uff0c\u6db5\u76d67\u4e2a\u7ef4\u5ea6\u30012999\u4e2a\u591a\u6837\u5316\u63d0\u793a\uff0c\u7ed3\u5408\u5ba2\u89c2\u6307\u6807\u548c\u8d85\u8fc711.8\u4e07\u4e2a\u4eba\u5de5\u6807\u6ce8\uff0c\u5bf910\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524dTTA\u751f\u6210\u6a21\u578b\u7684\u8bc4\u4f30\u65b9\u6cd5\u8fc7\u4e8e\u72ed\u7a84\uff0c\u4e3b\u8981\u5173\u6ce8\u611f\u77e5\u8d28\u91cf\uff0c\u800c\u5ffd\u89c6\u4e86\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u4f26\u7406\u95ee\u9898\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u5f00\u53d1TTA-Bench\u57fa\u51c6\uff0c\u5305\u542b7\u4e2a\u8bc4\u4f30\u7ef4\u5ea6\uff08\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u3001\u516c\u5e73\u6027\u3001\u6bd2\u6027\u7b49\uff09\uff0c\u4f7f\u7528\u81ea\u52a8\u548c\u624b\u52a8\u65b9\u6cd5\u751f\u62102999\u4e2a\u591a\u6837\u5316\u63d0\u793a\uff0c\u7ed3\u5408\u5ba2\u89c2\u6307\u6807\u548c\u4eba\u7c7b\u6807\u6ce8\u7684\u7edf\u4e00\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u5bf910\u4e2a\u6700\u5148\u8fdb\u7684TTA\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u6027\u80fd\u4f18\u52bf\u548c\u5c40\u9650\u6027\u5206\u6790\uff0c\u5efa\u7acb\u4e86TTA\u7cfb\u7edf\u6574\u4f53\u548c\u8d1f\u8d23\u4efb\u8bc4\u4f30\u7684\u65b0\u6807\u51c6\u3002", "conclusion": "TTA-Bench\u4e3a\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u5411\u66f4\u8d1f\u8d23\u4efb\u548c\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u5411\u53d1\u5c55\uff0c\u6240\u6709\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5de5\u5177\u5747\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.00605", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00605", "abs": "https://arxiv.org/abs/2509.00605", "authors": ["Rishiraj Acharya"], "title": "Gated Associative Memory: A Parallel O(N) Architecture for Efficient Sequence Modeling", "comment": "11 pages, 4 figures, 3 tables", "summary": "The Transformer architecture, underpinned by the self-attention mechanism,\nhas become the de facto standard for sequence modeling tasks. However, its core\ncomputational primitive scales quadratically with sequence length (O(N^2)),\ncreating a significant bottleneck for processing long contexts. In this paper,\nwe propose the Gated Associative Memory (GAM) network, a novel, fully parallel\narchitecture for sequence modeling that exhibits linear complexity (O(N)) with\nrespect to sequence length. The GAM block replaces the self-attention layer\nwith two parallel pathways: a causal convolution to efficiently capture local,\nposition-dependent context, and a parallel associative memory retrieval\nmechanism to model global, content-based patterns. These pathways are\ndynamically fused using a gating mechanism, allowing the model to flexibly\ncombine local and global information for each token. We implement GAM from\nscratch and conduct a rigorous comparative analysis against a standard\nTransformer model and a modern linear-time baseline (Mamba) on the WikiText-2\nbenchmark, as well as against the Transformer on the TinyStories dataset. Our\nexperiments demonstrate that GAM is consistently faster, outperforming both\nbaselines on training speed, and achieves a superior or competitive final\nvalidation perplexity across all datasets, establishing it as a promising and\nefficient alternative for sequence modeling.", "AI": {"tldr": "\u63d0\u51faGated Associative Memory (GAM)\u7f51\u7edc\uff0c\u4e00\u79cd\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u5e76\u884c\u67b6\u6784\uff0c\u66ff\u4ee3Transformer\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u901f\u5ea6\u3002", "motivation": "Transformer\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b58\u5728\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u6210\u4e3a\u663e\u8457\u74f6\u9888\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u5e8f\u5217\u5efa\u6a21\u67b6\u6784\u3002", "method": "\u4f7f\u7528\u5e76\u884c\u53cc\u901a\u8def\u7ed3\u6784\uff1a\u56e0\u679c\u5377\u79ef\u6355\u83b7\u5c40\u90e8\u4f4d\u7f6e\u4f9d\u8d56\u4e0a\u4e0b\u6587\uff0c\u5173\u8054\u8bb0\u5fc6\u68c0\u7d22\u673a\u5236\u5efa\u6a21\u5168\u5c40\u5185\u5bb9\u6a21\u5f0f\uff0c\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u52a8\u6001\u878d\u5408\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u3002", "result": "\u5728WikiText-2\u548cTinyStories\u6570\u636e\u96c6\u4e0a\uff0cGAM\u6bd4\u6807\u51c6Transformer\u548cMamba\u57fa\u7ebf\u8bad\u7ec3\u901f\u5ea6\u66f4\u5feb\uff0c\u9a8c\u8bc1\u56f0\u60d1\u5ea6\u8fbe\u5230\u76f8\u5f53\u6216\u66f4\u4f18\u6c34\u5e73\u3002", "conclusion": "GAM\u4f5c\u4e3a\u7ebf\u6027\u590d\u6742\u5ea6\u67b6\u6784\uff0c\u5728\u5e8f\u5217\u5efa\u6a21\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6709\u524d\u666f\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2509.02471", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02471", "abs": "https://arxiv.org/abs/2509.02471", "authors": ["Chengyuan Ma", "Peng Jia", "Hongyue Guo", "Wenming Yang"], "title": "ESTM: An Enhanced Dual-Branch Spectral-Temporal Mamba for Anomalous Sound Detection", "comment": "Accepted in IEEE Signal Processing Letters 2025", "summary": "The core challenge in industrial equipment anoma lous sound detection (ASD)\nlies in modeling the time-frequency coupling characteristics of acoustic\nfeatures. Existing modeling methods are limited by local receptive fields,\nmaking it difficult to capture long-range temporal patterns and cross-band\ndynamic coupling effects in machine acoustic features. In this paper, we\npropose a novel framework, ESTM, which is based on a dual-path Mamba\narchitecture with time-frequency decoupled modeling and utilizes Selective\nState-Space Models (SSM) for long-range sequence modeling. ESTM extracts rich\nfeature representations from different time segments and frequency bands by\nfusing enhanced Mel spectrograms and raw audio features, while further\nimproving sensitivity to anomalous patterns through the TriStat-Gating (TSG)\nmodule. Our experiments demonstrate that ESTM improves anomalous detection\nperformance on the DCASE 2020 Task 2 dataset, further validating the\neffectiveness of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53cc\u8def\u5f84Mamba\u67b6\u6784\u7684ESTM\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4-\u9891\u7387\u89e3\u8026\u5efa\u6a21\u548c\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u6765\u6355\u6349\u5de5\u4e1a\u8bbe\u5907\u5f02\u5e38\u58f0\u97f3\u68c0\u6d4b\u4e2d\u7684\u957f\u7a0b\u65f6\u5e8f\u6a21\u5f0f\u548c\u8de8\u9891\u5e26\u52a8\u6001\u8026\u5408\u7279\u5f81\u3002", "motivation": "\u5de5\u4e1a\u8bbe\u5907\u5f02\u5e38\u58f0\u97f3\u68c0\u6d4b\u7684\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u5efa\u6a21\u58f0\u5b66\u7279\u5f81\u7684\u65f6\u9891\u8026\u5408\u7279\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u5c40\u90e8\u611f\u53d7\u91ce\uff0c\u96be\u4ee5\u6355\u6349\u957f\u7a0b\u65f6\u5e8f\u6a21\u5f0f\u548c\u8de8\u9891\u5e26\u52a8\u6001\u8026\u5408\u6548\u5e94\u3002", "method": "ESTM\u6846\u67b6\u91c7\u7528\u53cc\u8def\u5f84Mamba\u67b6\u6784\uff0c\u7ed3\u5408\u65f6\u95f4-\u9891\u7387\u89e3\u8026\u5efa\u6a21\uff0c\u4f7f\u7528\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u8fdb\u884c\u957f\u7a0b\u5e8f\u5217\u5efa\u6a21\uff0c\u878d\u5408\u589e\u5f3a\u6885\u5c14\u9891\u8c31\u56fe\u548c\u539f\u59cb\u97f3\u9891\u7279\u5f81\uff0c\u5e76\u901a\u8fc7TriStat-Gating\u6a21\u5757\u63d0\u9ad8\u5bf9\u5f02\u5e38\u6a21\u5f0f\u7684\u654f\u611f\u6027\u3002", "result": "\u5728DCASE 2020 Task 2\u6570\u636e\u96c6\u4e0a\uff0cESTM\u63d0\u9ad8\u4e86\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684ESTM\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u8def\u5f84Mamba\u67b6\u6784\u548c\u65f6\u9891\u89e3\u8026\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u4e1a\u8bbe\u5907\u5f02\u5e38\u58f0\u97f3\u68c0\u6d4b\u4e2d\u7684\u957f\u7a0b\u65f6\u5e8f\u6a21\u5f0f\u6355\u6349\u95ee\u9898\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00623", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00623", "abs": "https://arxiv.org/abs/2509.00623", "authors": ["Ali Zain", "Sareem Farooqui", "Muhammad Rafi"], "title": "A Multi-Strategy Approach for AI-Generated Text Detection", "comment": null, "summary": "This paper presents presents three distinct systems developed for the M-DAIGT\nshared task on detecting AI generated content in news articles and academic\nabstracts. The systems includes: (1) A fine-tuned RoBERTa-base classifier, (2)\nA classical TF-IDF + Support Vector Machine (SVM) classifier , and (3) An\nInnovative ensemble model named Candace, leveraging probabilistic features\nextracted from multiple Llama-3.2 models processed by a customTransformer\nencoder.The RoBERTa-based system emerged as the most performant, achieving\nnear-perfect results on both development and test sets.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e09\u79cd\u7528\u4e8e\u68c0\u6d4b\u65b0\u95fb\u6587\u7ae0\u548c\u5b66\u672f\u6458\u8981\u4e2dAI\u751f\u6210\u5185\u5bb9\u7684\u7cfb\u7edf\uff0c\u5176\u4e2d\u57fa\u4e8eRoBERTa\u7684\u5206\u7c7b\u5668\u8868\u73b0\u6700\u4f73\uff0c\u5728\u5f00\u53d1\u548c\u6d4b\u8bd5\u96c6\u4e0a\u5747\u53d6\u5f97\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u7ed3\u679c\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u5185\u5bb9\u7684\u666e\u53ca\uff0c\u5f00\u53d1\u6709\u6548\u7684\u68c0\u6d4b\u7cfb\u7edf\u5bf9\u4e8e\u7ef4\u62a4\u5185\u5bb9\u771f\u5b9e\u6027\u548c\u5b66\u672f\u8bda\u4fe1\u81f3\u5173\u91cd\u8981\u3002M-DAIGT\u5171\u4eab\u4efb\u52a1\u65e8\u5728\u63a8\u52a8\u8fd9\u4e00\u9886\u57df\u7684\u6280\u672f\u53d1\u5c55\u3002", "method": "\u5f00\u53d1\u4e86\u4e09\u79cd\u7cfb\u7edf\uff1a(1) \u57fa\u4e8eRoBERTa-base\u7684\u5fae\u8c03\u5206\u7c7b\u5668\uff1b(2) TF-IDF + SVM\u4f20\u7edf\u5206\u7c7b\u5668\uff1b(3) \u521b\u65b0\u7684Candace\u96c6\u6210\u6a21\u578b\uff0c\u5229\u7528\u591a\u4e2aLlama-3.2\u6a21\u578b\u63d0\u53d6\u6982\u7387\u7279\u5f81\u5e76\u901a\u8fc7\u81ea\u5b9a\u4e49Transformer\u7f16\u7801\u5668\u5904\u7406\u3002", "result": "RoBERTa-based\u7cfb\u7edf\u8868\u73b0\u6700\u4f18\uff0c\u5728\u5f00\u53d1\u548c\u6d4b\u8bd5\u96c6\u4e0a\u90fd\u8fbe\u5230\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u68c0\u6d4b\u6548\u679c\u3002", "conclusion": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u5fae\u8c03\u65b9\u6cd5\u5728AI\u751f\u6210\u5185\u5bb9\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.02521", "categories": ["cs.SD", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.02521", "abs": "https://arxiv.org/abs/2509.02521", "authors": ["Yiqun Yao", "Xiang Li", "Xin Jiang", "Xuezhi Fang", "Naitong Yu", "Wenjia Ma", "Aixin Sun", "Yequan Wang"], "title": "FLM-Audio: Natural Monologues Improves Native Full-Duplex Chatbots via Dual Training", "comment": null, "summary": "Full-duplex dialog models are designed to listen and speak simultaneously\nwith rapid responses to fast-changing user input. Among existing approaches,\nnative full-duplex models merges different channels (e.g. listen and speak) in\na single time step, overcoming the high response latency inherent to\ntime-division multiplexing time-division multiplexing (TDM) alternatives. Yet,\na key challenge remains: aligning textual monologues with audio streams that\noperate at different bitrates. The prevailing solution relies on word-level\nalignment, but this can degrade the language ability of large pre-trained\nmodels. Moreover, it requires highly accurate timestamps for every token, which\nintroduces cascading errors and increases pre-processing costs. In this paper,\nwe propose textual monologues in continuous tokens sequence, namely \"natural\"\nmonologues, which mimics humanoid cognitive behavior in dialogs. For temporal\nalignment, we alternate the position of the natural monologue - leading or\ntrailing the audio - across different training stages. This \"dual\" training\nparadigm proves highly effective in building FLM-Audio, our 7B spoken dialog\nmodel that demonstrates superior responsiveness, duplexity, and chatting\nexperiences, as confirmed by experimental results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5168\u53cc\u5de5\u5bf9\u8bdd\u6a21\u578bFLM-Audio\uff0c\u901a\u8fc7\u4f7f\u7528\u81ea\u7136\u72ec\u767d\u548c\u53cc\u91cd\u8bad\u7ec3\u65b9\u6848\u89e3\u51b3\u4e86\u97f3\u9891\u6d41\u4e0e\u6587\u672c\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f4e\u5ef6\u8fdf\u548c\u66f4\u597d\u7684\u5bf9\u8bdd\u4f53\u9a8c\u3002", "motivation": "\u73b0\u6709\u7684\u5168\u53cc\u5de5\u5bf9\u8bdd\u6a21\u578b\u5b58\u5728\u6587\u672c\u72ec\u767d\u4e0e\u97f3\u9891\u6d41\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u5355\u8bcd\u7ea7\u5bf9\u9f50\u65b9\u6848\u4f1a\u964d\u4f4e\u5927\u6a21\u578b\u7684\u8bed\u8a00\u80fd\u529b\uff0c\u4e14\u9700\u8981\u7cbe\u786e\u7684\u65f6\u95f4\u6233\u6807\u7d22\u5f15\u5165\u7ea7\u8054\u9519\u8bef\u548c\u9ad8\u5904\u7406\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\"\u81ea\u7136\u72ec\u767d\"\uff08\u8fde\u7eed\u6807\u8bb0\u5e8f\u5217\uff09\u6a21\u4eff\u4eba\u7c7b\u8ba4\u77e5\u884c\u4e3a\uff0c\u5e76\u91c7\u7528\u53cc\u91cd\u8bad\u7ec3\u8303\u5f0f\uff1a\u5728\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u4ea4\u66ff\u5c06\u81ea\u7136\u72ec\u767d\u653e\u7f6e\u5728\u97f3\u9891\u7684\u524d\u7aef\u6216\u540e\u7aef\u3002", "result": "\u6784\u5efa\u4e86FLM-Audio 7B\u8bed\u97f3\u5bf9\u8bdd\u6a21\u578b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u5176\u5728\u54cd\u5e94\u901f\u5ea6\u3001\u5168\u53cc\u5de5\u80fd\u529b\u548c\u804a\u5929\u4f53\u9a8c\u65b9\u9762\u90fd\u663e\u793a\u51fa\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u97f3\u9891\u6d41\u4e0e\u6587\u672c\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u4e3a\u5efa\u7acb\u9ad8\u6027\u80fd\u5168\u53cc\u5de5\u5bf9\u8bdd\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u8bed\u8a00\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u7684\u76f4\u63a5\u5bf9\u8bdd\u3002"}}
{"id": "2509.00629", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00629", "abs": "https://arxiv.org/abs/2509.00629", "authors": ["Md Tanzib Hosain", "Md Kishor Morol"], "title": "Can Multi-turn Self-refined Single Agent LMs with Retrieval Solve Hard Coding Problems?", "comment": "Accepted in Proceedings of the 63rd Annual Meeting of the Association\n  for Computational Linguistics (Student Research Workshop), 2025", "summary": "Among the hardest tasks for humans are those found in competitive programming\nwhere problems require sophisticated algorithmic thinking, puzzle solving, and\nthe creation of effective code. As a domain to assess language models (LMs), it\nhas not received enough attention, though. This study presents the ICPC\nbenchmark, which consists of 254 international collegiate programming contest\n(ICPC) tasks. Each problem includes official analysis, reference code, and\nsample, high-quality unit, and hidden tests. We are able to develop and\nevaluate a variety of LM inference techniques for competitive programming with\nthese resources. With zero-shot chain-of-thought prompting, we find that o1\nonly achieves a 19.1\\% pass@1 solve rate. With our best inference technique,\nwhich combines multi-turn self-judge with reflection and retrieval over\nepisodic information, raises this to 42.2\\%. Furthermore, we conduct a new\nhuman-in-the-loop investigation to gain a deeper understanding of the remaining\ndifficulties. Surprisingly, we discover that o1 can solve 17 out of 18 problems\nthat were previously unsolvable by any model or technique with just a few\nspecific instructions. A footstep toward LMs with grounded, imaginative, and\nalgorithmic thinking is provided by our quantitative findings and qualitative\nresearch. We open-source our code and data at https://github.com/kraritt/zolve.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86ICPC\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b254\u4e2a\u56fd\u9645\u5927\u5b66\u751f\u7a0b\u5e8f\u8bbe\u8ba1\u7ade\u8d5b\u9898\u76ee\uff0c\u901a\u8fc7\u591a\u8f6e\u81ea\u5224\u65ad\u3001\u53cd\u601d\u548c\u68c0\u7d22\u6280\u672f\u5c06\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u9898\u7387\u4ece19.1%\u63d0\u5347\u523042.2%\uff0c\u5e76\u53d1\u73b0\u7279\u5b9a\u6307\u4ee4\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u7ade\u4e89\u6027\u7f16\u7a0b\u4efb\u52a1\u9700\u8981\u590d\u6742\u7684\u7b97\u6cd5\u601d\u7ef4\u548c\u4ee3\u7801\u521b\u5efa\u80fd\u529b\uff0c\u4f46\u4f5c\u4e3a\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u9886\u57df\u5c1a\u672a\u5f97\u5230\u8db3\u591f\u5173\u6ce8\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u7b97\u6cd5\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u4e0a\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u5305\u542b254\u4e2aICPC\u9898\u76ee\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u6bcf\u4e2a\u95ee\u9898\u5305\u542b\u5b98\u65b9\u5206\u6790\u3001\u53c2\u8003\u4ee3\u7801\u548c\u6d4b\u8bd5\u7528\u4f8b\u3002\u91c7\u7528\u96f6\u6837\u672c\u601d\u7ef4\u94fe\u63d0\u793a\u3001\u591a\u8f6e\u81ea\u5224\u65ad\u7ed3\u5408\u53cd\u601d\u548c\u60c5\u666f\u4fe1\u606f\u68c0\u7d22\u7b49\u6280\u672f\u8fdb\u884c\u6a21\u578b\u63a8\u7406\u3002", "result": "\u96f6\u6837\u672c\u601d\u7ef4\u94fe\u63d0\u793a\u4e0b\u6a21\u578b\u4ec5\u8fbe\u523019.1%\u7684\u901a\u8fc7\u7387\uff0c\u6700\u4f73\u63a8\u7406\u6280\u672f\uff08\u591a\u8f6e\u81ea\u5224\u65ad+\u53cd\u601d+\u68c0\u7d22\uff09\u5c06\u901a\u8fc7\u7387\u63d0\u5347\u81f342.2%\u3002\u4eba\u7c7b\u53c2\u4e0e\u7814\u7a76\u53d1\u73b0\u7279\u5b9a\u6307\u4ee4\u80fd\u8ba9\u6a21\u578b\u89e3\u51b3\u4e4b\u524d\u65e0\u6cd5\u89e3\u51b3\u768417/18\u4e2a\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u5177\u6709\u57fa\u7840\u6027\u3001\u60f3\u8c61\u529b\u548c\u7b97\u6cd5\u601d\u7ef4\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u901a\u8fc7\u5b9a\u91cf\u7ed3\u679c\u548c\u5b9a\u6027\u5206\u6790\u5c55\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u7ade\u4e89\u6027\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u548c\u5269\u4f59\u6311\u6218\u3002"}}
{"id": "2509.01200", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01200", "abs": "https://arxiv.org/abs/2509.01200", "authors": ["Chenyang Le", "Bing Han", "Jinshun Li", "Songyong Chen", "Yanmin Qian"], "title": "SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous Speech Translation", "comment": null, "summary": "Simultaneous Speech Translation (SimulST) enables real-time cross-lingual\ncommunication by jointly optimizing speech recognition and machine translation\nunder strict latency constraints. Existing systems struggle to balance\ntranslation quality, latency, and semantic coherence, particularly in\nmultilingual many-to-many scenarios where divergent read and write policies\nhinder unified strategy learning. In this paper, we present SimulMEGA\n(Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy\nlearning framework that combines prefix-based training with a\nMixture-of-Experts refiner to learn effective read and write decisions in an\nimplicit manner, without adding inference-time overhead. Our design requires\nonly minimal modifications to standard transformer architectures and\ngeneralizes across both speech-to-text and text-to-speech streaming tasks.\nThrough comprehensive evaluation on six language pairs, our 500M parameter\nspeech-to-text model outperforms the Seamless baseline, achieving under 7\npercent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3\nseconds. We further demonstrate the versatility of SimulMEGA by extending it to\nstreaming TTS with a unidirectional backbone, yielding superior latency quality\ntradeoffs.", "AI": {"tldr": "SimulMEGA\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u95e8\u63a7\u673a\u5236\u548c\u524d\u7f00\u8bad\u7ec3\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u540c\u6b65\u8bed\u97f3\u7ffb\u8bd1\u7684\u8d28\u91cf\u548c\u5ef6\u8fdf\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u540c\u6b65\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\u5728\u591a\u8bed\u8a00\u591a\u5bf9\u591a\u573a\u666f\u4e2d\u96be\u4ee5\u5e73\u8861\u7ffb\u8bd1\u8d28\u91cf\u3001\u5ef6\u8fdf\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\uff0c\u4e0d\u540c\u7684\u8bfb\u5199\u7b56\u7565\u963b\u788d\u4e86\u7edf\u4e00\u7b56\u7565\u5b66\u4e60\u3002", "method": "\u7ed3\u5408\u524d\u7f00\u8bad\u7ec3\u548c\u6df7\u5408\u4e13\u5bb6\u7cbe\u70bc\u5668\uff0c\u4ee5\u9690\u5f0f\u65b9\u5f0f\u5b66\u4e60\u6709\u6548\u7684\u8bfb\u5199\u51b3\u7b56\uff0c\u65e0\u9700\u589e\u52a0\u63a8\u7406\u65f6\u95f4\u5f00\u9500\uff0c\u4ec5\u9700\u5bf9\u6807\u51c6Transformer\u67b6\u6784\u8fdb\u884c\u6700\u5c0f\u4fee\u6539\u3002", "result": "\u57286\u4e2a\u8bed\u8a00\u5bf9\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c5\u4ebf\u53c2\u6570\u7684\u8bed\u97f3\u8f6c\u6587\u672c\u6a21\u578b\u4f18\u4e8eSeamless\u57fa\u7ebf\uff0c\u57281.5\u79d2\u5e73\u5747\u5ef6\u8fdf\u4e0bBLEU\u4e0b\u964d\u4f4e\u4e8e7%\uff0c\u57283\u79d2\u5ef6\u8fdf\u4e0b\u4f4e\u4e8e3%\u3002", "conclusion": "SimulMEGA\u6846\u67b6\u5728\u540c\u6b65\u8bed\u97f3\u7ffb\u8bd1\u548c\u6d41\u5f0f\u6587\u672c\u8f6c\u8bed\u97f3\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u5ef6\u8fdf-\u8d28\u91cf\u6743\u8861\uff0c\u5177\u6709\u5f88\u597d\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2509.00673", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7; I.6"], "pdf": "https://arxiv.org/pdf/2509.00673", "abs": "https://arxiv.org/abs/2509.00673", "authors": ["Sanjeeevan Selvaganapathy", "Mehwish Nasim"], "title": "Confident, Calibrated, or Complicit: Probing the Trade-offs between Safety Alignment and Ideological Bias in Language Models in Detecting Hate Speech", "comment": null, "summary": "We investigate the efficacy of Large Language Models (LLMs) in detecting\nimplicit and explicit hate speech, examining whether models with minimal safety\nalignment (uncensored) might provide more objective classification capabilities\ncompared to their heavily-aligned (censored) counterparts. While uncensored\nmodels theoretically offer a less constrained perspective free from moral\nguardrails that could bias classification decisions, our results reveal a\nsurprising trade-off: censored models significantly outperform their uncensored\ncounterparts in both accuracy and robustness, achieving 78.7% versus 64.1%\nstrict accuracy. However, this enhanced performance comes with its own\nlimitation -- the safety alignment acts as a strong ideological anchor, making\ncensored models resistant to persona-based influence, while uncensored models\nprove highly malleable to ideological framing. Furthermore, we identify\ncritical failures across all models in understanding nuanced language such as\nirony. We also find alarming fairness disparities in performance across\ndifferent targeted groups and systemic overconfidence that renders\nself-reported certainty unreliable. These findings challenge the notion of LLMs\nas objective arbiters and highlight the need for more sophisticated auditing\nframeworks that account for fairness, calibration, and ideological consistency.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u7ecf\u8fc7\u5b89\u5168\u5bf9\u9f50\u7684\u5ba1\u67e5\u6a21\u578b\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u672a\u5ba1\u67e5\u6a21\u578b\uff0878.7% vs 64.1%\uff09\uff0c\u4f46\u5b58\u5728\u610f\u8bc6\u5f62\u6001\u951a\u5b9a\u6548\u5e94\uff1b\u6240\u6709\u6a21\u578b\u5728\u7406\u89e3\u8bbd\u523a\u7b49\u5fae\u5999\u8bed\u8a00\u65b9\u9762\u90fd\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u4e14\u5b58\u5728\u516c\u5e73\u6027\u95ee\u9898\u548c\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u6d4b\u9690\u6027\u548c\u663e\u6027\u4ec7\u6068\u8a00\u8bba\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u6bd4\u8f83\u7ecf\u8fc7\u5b89\u5168\u5bf9\u9f50\uff08\u5ba1\u67e5\uff09\u548c\u672a\u5bf9\u9f50\uff08\u672a\u5ba1\u67e5\uff09\u6a21\u578b\u5728\u5ba2\u89c2\u5206\u7c7b\u80fd\u529b\u4e0a\u7684\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\u7ecf\u8fc7\u5b89\u5168\u5bf9\u9f50\u548c\u672a\u5b89\u5168\u5bf9\u9f50\u7684LLMs\u6a21\u578b\uff0c\u8bc4\u4f30\u5b83\u4eec\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u3001\u5bf9\u610f\u8bc6\u5f62\u6001\u6846\u67b6\u7684\u654f\u611f\u6027\u4ee5\u53ca\u5bf9\u5fae\u5999\u8bed\u8a00\u7684\u7406\u89e3\u80fd\u529b\u3002", "result": "\u5ba1\u67e5\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u672a\u5ba1\u67e5\u6a21\u578b\uff0878.7% vs 64.1%\uff09\uff0c\u4f46\u5177\u6709\u5f3a\u70c8\u7684\u610f\u8bc6\u5f62\u6001\u951a\u5b9a\u6548\u5e94\uff1b\u672a\u5ba1\u67e5\u6a21\u578b\u5bf9\u610f\u8bc6\u5f62\u6001\u6846\u67b6\u9ad8\u5ea6\u654f\u611f\uff1b\u6240\u6709\u6a21\u578b\u5728\u7406\u89e3\u8bbd\u523a\u8bed\u8a00\u65b9\u9762\u90fd\u5b58\u5728\u5173\u952e\u5931\u8d25\uff1b\u5b58\u5728\u9488\u5bf9\u4e0d\u540c\u76ee\u6807\u7fa4\u4f53\u7684\u516c\u5e73\u6027\u5dee\u5f02\u548c\u7cfb\u7edf\u6027\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u6311\u6218\u4e86LLMs\u4f5c\u4e3a\u5ba2\u89c2\u4ef2\u88c1\u8005\u7684\u89c2\u5ff5\uff0c\u5f3a\u8c03\u9700\u8981\u5f00\u53d1\u66f4\u590d\u6742\u7684\u5ba1\u8ba1\u6846\u67b6\u6765\u8003\u8651\u516c\u5e73\u6027\u3001\u6821\u51c6\u6027\u548c\u610f\u8bc6\u5f62\u6001\u4e00\u81f4\u6027\u3002"}}
{"id": "2509.02038", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.02038", "abs": "https://arxiv.org/abs/2509.02038", "authors": ["Bashar Talafha", "Hawau Olamide Toyin", "Peter Sullivan", "AbdelRahim Elmadany", "Abdurrahman Juma", "Amirbek Djanibekov", "Chiyu Zhang", "Hamad Alshehhi", "Hanan Aldarmaki", "Mustafa Jarrar", "Nizar Habash", "Muhammad Abdul-Mageed"], "title": "NADI 2025: The First Multidialectal Arabic Speech Processing Shared Task", "comment": null, "summary": "We present the findings of the sixth Nuanced Arabic Dialect Identification\n(NADI 2025) Shared Task, which focused on Arabic speech dialect processing\nacross three subtasks: spoken dialect identification (Subtask 1), speech\nrecognition (Subtask 2), and diacritic restoration for spoken dialects (Subtask\n3). A total of 44 teams registered, and during the testing phase, 100 valid\nsubmissions were received from eight unique teams. The distribution was as\nfollows: 34 submissions for Subtask 1 \"five teams{\\ae}, 47 submissions for\nSubtask 2 \"six teams\", and 19 submissions for Subtask 3 \"two teams\". The\nbest-performing systems achieved 79.8% accuracy on Subtask 1, 35.68/12.20\nWER/CER (overall average) on Subtask 2, and 55/13 WER/CER on Subtask 3. These\nresults highlight the ongoing challenges of Arabic dialect speech processing,\nparticularly in dialect identification, recognition, and diacritic restoration.\nWe also summarize the methods adopted by participating teams and briefly\noutline directions for future editions of NADI.", "AI": {"tldr": "NADI 2025\u5171\u4eab\u4efb\u52a1\u805a\u7126\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u8bed\u97f3\u5904\u7406\uff0c\u5305\u542b\u65b9\u8a00\u8bc6\u522b\u3001\u8bed\u97f3\u8bc6\u522b\u548c\u65b9\u8a00\u97f3\u6807\u6062\u590d\u4e09\u4e2a\u5b50\u4efb\u52a1\uff0c\u5171\u67098\u4e2a\u56e2\u961f\u63d0\u4ea4100\u4efd\u6709\u6548\u7ed3\u679c\uff0c\u6700\u4f73\u7cfb\u7edf\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u5206\u522b\u8fbe\u523079.8%\u51c6\u786e\u7387\u548c35.68/12.20\u300155/13\u7684WER/CER\u6307\u6807", "motivation": "\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u8bed\u97f3\u5904\u7406\u4e2d\u7684\u6311\u6218\uff0c\u5305\u62ec\u65b9\u8a00\u8bc6\u522b\u3001\u8bed\u97f3\u8bc6\u522b\u548c\u97f3\u6807\u6062\u590d\uff0c\u4fc3\u8fdb\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u8ba1\u7b97\u8bed\u8a00\u5b66\u7684\u53d1\u5c55", "method": "\u901a\u8fc7\u5171\u4eab\u4efb\u52a1\u5f62\u5f0f\u7ec4\u7ec7\u7814\u7a76\u793e\u533a\uff0c\u8bbe\u7f6e\u4e09\u4e2a\u5b50\u4efb\u52a1\uff1a\u53e3\u8bed\u65b9\u8a00\u8bc6\u522b\u3001\u8bed\u97f3\u8bc6\u522b\u3001\u65b9\u8a00\u97f3\u6807\u6062\u590d\uff0c\u6536\u96c6\u591a\u56e2\u961f\u63d0\u4ea4\u7684\u7cfb\u7edf\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30", "result": "44\u4e2a\u56e2\u961f\u6ce8\u518c\uff0c8\u4e2a\u56e2\u961f\u63d0\u4ea4100\u4efd\u6709\u6548\u7ed3\u679c\uff0c\u6700\u4f73\u7cfb\u7edf\u5728\u5b50\u4efb\u52a11\u8fbe\u523079.8%\u51c6\u786e\u7387\uff0c\u5b50\u4efb\u52a12\u8fbe\u523035.68/12.20 WER/CER\uff0c\u5b50\u4efb\u52a13\u8fbe\u523055/13 WER/CER", "conclusion": "\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u8bed\u97f3\u5904\u7406\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u65b9\u8a00\u8bc6\u522b\u3001\u8bc6\u522b\u51c6\u786e\u7387\u548c\u97f3\u6807\u6062\u590d\u65b9\u9762\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6539\u8fdb"}}
{"id": "2509.00679", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00679", "abs": "https://arxiv.org/abs/2509.00679", "authors": ["Junfeng Ran", "Guangxiang Zhao", "Yuhan Wu", "Dawei Zhu", "Longyun Wu", "Yikai Zhao", "Tong Yang", "Lin Sun", "Xiangzheng Zhang", "Sujian Li"], "title": "Router Upcycling: Leveraging Mixture-of-Routers in Mixture-of-Experts Upcycling", "comment": null, "summary": "The Mixture-of-Experts (MoE) models have gained significant attention in deep\nlearning due to their dynamic resource allocation and superior performance\nacross diverse tasks. However, efficiently training these models remains\nchallenging. The MoE upcycling technique has been proposed to reuse and improve\nexisting model components, thereby minimizing training overhead. Despite this,\nsimple routers, such as linear routers, often struggle with complex routing\ntasks within MoE upcycling. In response, we propose a novel routing technique\ncalled Router Upcycling to enhance the performance of MoE upcycling models. Our\napproach initializes multiple routers from the attention heads of preceding\nattention layers during upcycling. These routers collaboratively assign tokens\nto specialized experts in an attention-like manner. Each token is processed\ninto diverse queries and aligned with the experts' features (serving as keys).\nExperimental results demonstrate that our method achieves state-of-the-art\n(SOTA) performance, outperforming other upcycling baselines.", "AI": {"tldr": "\u63d0\u51faRouter Upcycling\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u6ce8\u610f\u529b\u5934\u521d\u59cb\u5316\u591a\u4e2a\u8def\u7531\u5668\u6765\u589e\u5f3aMoE\u4e0a\u5faa\u73af\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5b9e\u73b0SOTA\u6548\u679c", "motivation": "MoE\u6a21\u578b\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u83b7\u5f97\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u9ad8\u6548\u8bad\u7ec3\u4ecd\u5177\u6311\u6218\u6027\u3002\u73b0\u6709MoE\u4e0a\u5faa\u73af\u6280\u672f\u4e2d\u7684\u7b80\u5355\u8def\u7531\u5668\u96be\u4ee5\u5904\u7406\u590d\u6742\u8def\u7531\u4efb\u52a1", "method": "\u5728MoE\u4e0a\u5faa\u73af\u8fc7\u7a0b\u4e2d\uff0c\u4ece\u524d\u9762\u6ce8\u610f\u529b\u5c42\u7684\u6ce8\u610f\u529b\u5934\u521d\u59cb\u5316\u591a\u4e2a\u8def\u7531\u5668\uff0c\u8fd9\u4e9b\u8def\u7531\u5668\u4ee5\u7c7b\u4f3c\u6ce8\u610f\u529b\u7684\u65b9\u5f0f\u534f\u4f5c\u5206\u914dtoken\u7ed9\u4e13\u5bb6", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u5176\u4ed6\u4e0a\u5faa\u73af\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "Router Upcycling\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86MoE\u4e0a\u5faa\u73af\u4e2d\u7684\u8def\u7531\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd"}}
{"id": "2509.02523", "categories": ["cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.02523", "abs": "https://arxiv.org/abs/2509.02523", "authors": ["Evan King", "Adam Sabra", "Manjunath Kudlur", "James Wang", "Pete Warden"], "title": "Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices", "comment": null, "summary": "We present the Flavors of Moonshine, a suite of tiny automatic speech\nrecognition (ASR) models specialized for a range of underrepresented languages.\nPrevailing wisdom suggests that multilingual ASR models outperform monolingual\ncounterparts by exploiting cross-lingual phonetic similarities. We challenge\nthis assumption, showing that for sufficiently small models (27M parameters),\ntraining monolingual systems on a carefully balanced mix of high-quality\nhuman-labeled, pseudo-labeled, and synthetic data yields substantially superior\nperformance. On average, our models achieve error rates 48% lower than the\ncomparably sized Whisper Tiny model, outperform the 9x larger Whisper Small\nmodel, and in most cases match or outperform the 28x larger Whisper Medium\nmodel. These results advance the state of the art for models of this size,\nenabling accurate on-device ASR for languages that previously had limited\nsupport. We release Arabic, Chinese, Japanese, Korean, Ukrainian, and\nVietnamese Moonshine models under a permissive open-source license.", "AI": {"tldr": "Flavors of Moonshine\u662f\u4e00\u7cfb\u5217\u4e13\u95e8\u4e3a\u4ee3\u8868\u6027\u4e0d\u8db3\u8bed\u8a00\u8bbe\u8ba1\u7684\u5c0f\u578b\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\uff0c\u901a\u8fc7\u5355\u8bed\u8bad\u7ec3\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u6df7\u5408\u7b56\u7565\uff0c\u572827M\u53c2\u6570\u89c4\u6a21\u4e0b\u663e\u8457\u8d85\u8d8a\u591a\u8bed\u8a00\u6a21\u578b\u6027\u80fd", "motivation": "\u6311\u6218\u5f53\u524d\u591a\u8bed\u8a00ASR\u6a21\u578b\u4f18\u4e8e\u5355\u8bed\u6a21\u578b\u7684\u666e\u904d\u8ba4\u77e5\uff0c\u63a2\u7d22\u5728\u5c0f\u578b\u6a21\u578b\u89c4\u6a21\u4e0b\u901a\u8fc7\u9ad8\u8d28\u91cf\u6570\u636e\u6df7\u5408\u7b56\u7565\u4e3a\u4ee3\u8868\u6027\u4e0d\u8db3\u8bed\u8a00\u63d0\u4f9b\u66f4\u597d\u7684\u8bed\u97f3\u8bc6\u522b\u652f\u6301", "method": "\u4f7f\u7528\u7cbe\u5fc3\u5e73\u8861\u7684\u9ad8\u8d28\u91cf\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\u3001\u4f2a\u6807\u6ce8\u6570\u636e\u548c\u5408\u6210\u6570\u636e\u8bad\u7ec3\u5355\u8bed\u7cfb\u7edf\uff0c\u6a21\u578b\u53c2\u6570\u91cf\u4e3a27M", "result": "\u5e73\u5747\u9519\u8bef\u7387\u6bd4\u540c\u7b49\u89c4\u6a21\u7684Whisper Tiny\u6a21\u578b\u4f4e48%\uff0c\u6027\u80fd\u8d85\u8d8a9\u500d\u5927\u7684Whisper Small\u6a21\u578b\uff0c\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u5339\u914d\u6216\u8d85\u8d8a28\u500d\u5927\u7684Whisper Medium\u6a21\u578b", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5c0f\u578b\u6a21\u578b\u89c4\u6a21\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e3a\u4e4b\u524d\u652f\u6301\u6709\u9650\u7684\u8bed\u8a00\u63d0\u4f9b\u4e86\u51c6\u786e\u7684\u8bbe\u5907\u7aefASR\u80fd\u529b\uff0c\u5e76\u5f00\u6e90\u53d1\u5e03\u4e86\u963f\u62c9\u4f2f\u8bed\u3001\u4e2d\u6587\u3001\u65e5\u8bed\u3001\u97e9\u8bed\u3001\u4e4c\u514b\u5170\u8bed\u548c\u8d8a\u5357\u8bed\u6a21\u578b"}}
{"id": "2509.00680", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.00680", "abs": "https://arxiv.org/abs/2509.00680", "authors": ["Austin McCutcheon", "Chris Brogly"], "title": "Do small language models generate realistic variable-quality fake news headlines?", "comment": null, "summary": "Small language models (SLMs) have the capability for text generation and may\npotentially be used to generate falsified texts online. This study evaluates 14\nSLMs (1.7B-14B parameters) including LLaMA, Gemma, Phi, SmolLM, Mistral, and\nGranite families in generating perceived low and high quality fake news\nheadlines when explicitly prompted, and whether they appear to be similar to\nreal-world news headlines. Using controlled prompt engineering, 24,000\nheadlines were generated across low-quality and high-quality deceptive\ncategories. Existing machine learning and deep learning-based news headline\nquality detectors were then applied against these SLM-generated fake news\nheadlines. SLMs demonstrated high compliance rates with minimal ethical\nresistance, though there were some occasional exceptions. Headline quality\ndetection using established DistilBERT and bagging classifier models showed\nthat quality misclassification was common, with detection accuracies only\nranging from 35.2% to 63.5%. These findings suggest the following: tested SLMs\ngenerally are compliant in generating falsified headlines, although there are\nslight variations in ethical restraints, and the generated headlines did not\nclosely resemble existing primarily human-written content on the web, given the\nlow quality classification accuracy.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e8614\u4e2a\u5c0f\u8bed\u8a00\u6a21\u578b\u751f\u6210\u865a\u5047\u65b0\u95fb\u6807\u9898\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u5728\u660e\u786e\u63d0\u793a\u4e0b\u9ad8\u5ea6\u914d\u5408\u751f\u6210\u5047\u65b0\u95fb\uff0c\u4e14\u73b0\u6709\u68c0\u6d4b\u5668\u96be\u4ee5\u51c6\u786e\u8bc6\u522b\u8fd9\u4e9bAI\u751f\u6210\u7684\u5047\u65b0\u95fb\u6807\u9898\u3002", "motivation": "\u968f\u7740\u5c0f\u8bed\u8a00\u6a21\u578b(SLMs)\u7684\u666e\u53ca\uff0c\u9700\u8981\u8bc4\u4f30\u5b83\u4eec\u88ab\u6ee5\u7528\u4e8e\u751f\u6210\u865a\u5047\u65b0\u95fb\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u4ee5\u53ca\u73b0\u6709\u68c0\u6d4b\u7cfb\u7edf\u5bf9AI\u751f\u6210\u5047\u65b0\u95fb\u7684\u8bc6\u522b\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u53d7\u63a7\u63d0\u793a\u5de5\u7a0b\u5bf914\u4e2aSLM\u6a21\u578b(1.7B-14B\u53c2\u6570)\u751f\u621024,000\u4e2a\u4f4e\u8d28\u91cf\u548c\u9ad8\u8d28\u91cf\u865a\u5047\u65b0\u95fb\u6807\u9898\uff0c\u5e76\u5e94\u7528\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u68c0\u6d4b\u6a21\u578b\u8fdb\u884c\u5206\u6790\u3002", "result": "SLMs\u8868\u73b0\u51fa\u9ad8\u914d\u5408\u7387\uff0c\u4f26\u7406\u7ea6\u675f\u6709\u9650\uff1bDistilBERT\u548cbagging\u5206\u7c7b\u5668\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u4ec5\u4e3a35.2%-63.5%\uff0c\u8868\u660e\u8d28\u91cf\u8bef\u5206\u7c7b\u73b0\u8c61\u666e\u904d\u3002", "conclusion": "\u6d4b\u8bd5\u7684SLM\u6a21\u578b\u666e\u904d\u80fd\u591f\u751f\u6210\u865a\u5047\u6807\u9898\uff0c\u751f\u6210\u7684\u6807\u9898\u4e0e\u771f\u4eba\u64b0\u5199\u5185\u5bb9\u76f8\u4f3c\u5ea6\u4f4e\uff0c\u73b0\u6709\u68c0\u6d4b\u7cfb\u7edf\u5bf9AI\u751f\u6210\u5047\u65b0\u95fb\u7684\u8bc6\u522b\u6548\u679c\u4e0d\u4f73\u3002"}}
{"id": "2509.00687", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00687", "abs": "https://arxiv.org/abs/2509.00687", "authors": ["Chen Su", "Yuanhe Tian", "Yan Song", "Yongdong Zhang"], "title": "Text Reinforcement for Multimodal Time Series Forecasting", "comment": null, "summary": "Recent studies in time series forecasting (TSF) use multimodal inputs, such\nas text and historical time series data, to predict future values. These\nstudies mainly focus on developing advanced techniques to integrate textual\ninformation with time series data to perform the task and achieve promising\nresults. Meanwhile, these approaches rely on high-quality text and time series\ninputs, whereas in some cases, the text does not accurately or fully capture\nthe information carried by the historical time series, which leads to unstable\nperformance in multimodal TSF. Therefore, it is necessary to enhance the\ntextual content to improve the performance of multimodal TSF. In this paper, we\npropose improving multimodal TSF by reinforcing the text modalities. We propose\na text reinforcement model (TeR) to generate reinforced text that addresses\npotential weaknesses in the original text, then apply this reinforced text to\nsupport the multimodal TSF model's understanding of the time series, improving\nTSF performance. To guide the TeR toward producing higher-quality reinforced\ntext, we design a reinforcement learning approach that assigns rewards based on\nthe impact of each reinforced text on the performance of the multimodal TSF\nmodel and its relevance to the TSF task. We optimize the TeR accordingly, so as\nto improve the quality of the generated reinforced text and enhance TSF\nperformance. Extensive experiments on a real-world benchmark dataset covering\nvarious domains demonstrate the effectiveness of our approach, which\noutperforms strong baselines and existing studies on the dataset.", "AI": {"tldr": "\u63d0\u51fa\u6587\u672c\u5f3a\u5316\u6a21\u578b(TeR)\u6765\u589e\u5f3a\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6587\u672c\u8d28\u91cf\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u751f\u6210\u6587\u672c\uff0c\u63d0\u5347\u9884\u6d4b\u6027\u80fd", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6587\u672c\u8f93\u5165\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u6587\u672c\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u4fe1\u606f\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u9700\u8981\u589e\u5f3a\u6587\u672c\u5185\u5bb9", "method": "\u8bbe\u8ba1\u6587\u672c\u5f3a\u5316\u6a21\u578b(TeR)\u751f\u6210\u5f3a\u5316\u6587\u672c\uff0c\u91c7\u7528\u57fa\u4e8e\u591a\u6a21\u6001TSF\u6a21\u578b\u6027\u80fd\u548c\u4efb\u52a1\u76f8\u5173\u6027\u7684\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u673a\u5236\u6765\u4f18\u5316TeR", "result": "\u5728\u6db5\u76d6\u591a\u4e2a\u9886\u57df\u7684\u771f\u5b9e\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u65b9\u6cd5\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u548c\u73b0\u6709\u7814\u7a76", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u6587\u672c\u6a21\u6001\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\uff0c\u63d0\u51fa\u7684TeR\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5177\u6709\u663e\u8457\u6548\u679c"}}
{"id": "2509.00691", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00691", "abs": "https://arxiv.org/abs/2509.00691", "authors": ["Alex Gulko", "Yusen Peng", "Sachin Kumar"], "title": "CE-Bench: Towards a Reliable Contrastive Evaluation Benchmark of Interpretability of Sparse Autoencoders", "comment": null, "summary": "Probing with sparse autoencoders is a promising approach for uncovering\ninterpretable features in large language models (LLMs). However, the lack of\nautomated evaluation methods has hindered their broader adoption and\ndevelopment. In this work, we introduce CE-Bench, a novel and lightweight\ncontrastive evaluation benchmark for sparse autoencoders, built on a curated\ndataset of contrastive story pairs. We conduct comprehensive ablation studies\nto validate the effectiveness of our approach. Our results show that CE-Bench\nreliably measures the interpretability of sparse autoencoders and aligns well\nwith existing benchmarks, all without requiring an external LLM. The official\nimplementation and evaluation dataset are open-sourced under the MIT License.", "AI": {"tldr": "CE-Bench\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5bf9\u6bd4\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u57fa\u4e8e\u5bf9\u6bd4\u6545\u4e8b\u5bf9\u6570\u636e\u96c6\u6784\u5efa\uff0c\u65e0\u9700\u5916\u90e8LLM\u5373\u53ef\u53ef\u9760\u6d4b\u91cf\u6027\u80fd\u3002", "motivation": "\u7a00\u758f\u81ea\u7f16\u7801\u5668\u662f\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u53ef\u89e3\u91ca\u7279\u5f81\u7684\u6709\u524d\u666f\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\u963b\u788d\u4e86\u5176\u5e7f\u6cdb\u91c7\u7528\u548c\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86CE-Bench\u57fa\u51c6\uff0c\u57fa\u4e8e\u7cbe\u5fc3\u7b56\u5212\u7684\u5bf9\u6bd4\u6545\u4e8b\u5bf9\u6570\u636e\u96c6\u6784\u5efa\uff0c\u901a\u8fc7\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u3002", "result": "CE-Bench\u80fd\u591f\u53ef\u9760\u6d4b\u91cf\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e0e\u73b0\u6709\u57fa\u51c6\u826f\u597d\u5bf9\u9f50\uff0c\u4e14\u4e0d\u9700\u8981\u5916\u90e8LLM\u3002", "conclusion": "CE-Bench\u4e3a\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u5b98\u65b9\u5b9e\u73b0\u548c\u8bc4\u4f30\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.00698", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00698", "abs": "https://arxiv.org/abs/2509.00698", "authors": ["Kaiwen Wei", "Jinpeng Gao", "Jiang Zhong", "Yuming Yang", "Fengmao Lv", "Zhenyang Li"], "title": "Learning to Shop Like Humans: A Review-driven Retrieval-Augmented Recommendation Framework with LLMs", "comment": null, "summary": "Large language models (LLMs) have shown strong potential in recommendation\ntasks due to their strengths in language understanding, reasoning and knowledge\nintegration. These capabilities are especially beneficial for review-based\nrecommendation, which relies on semantically rich user-generated texts to\nreveal fine-grained user preferences and item attributes. However, effectively\nincorporating reviews into LLM-based recommendation remains challenging due to\n(1) inefficient to dynamically utilize user reviews under LLMs' constrained\ncontext windows, and (2) lacking effective mechanisms to prioritize reviews\nmost relevant to the user's current decision context. To address these\nchallenges, we propose RevBrowse, a review-driven recommendation framework\ninspired by the \"browse-then-decide\" decision process commonly observed in\nonline user behavior. RevBrowse integrates user reviews into the LLM-based\nreranking process to enhance its ability to distinguish between candidate\nitems. To improve the relevance and efficiency of review usage, we introduce\nPrefRAG, a retrieval-augmented module that disentangles user and item\nrepresentations into structured forms and adaptively retrieves\npreference-relevant content conditioned on the target item. Extensive\nexperiments on four Amazon review datasets demonstrate that RevBrowse achieves\nconsistent and significant improvements over strong baselines, highlighting its\ngeneralizability and effectiveness in modeling dynamic user preferences.\nFurthermore, since the retrieval-augmented process is transparent, RevBrowse\noffers a certain level of interpretability by making visible which reviews\ninfluence the final recommendation.", "AI": {"tldr": "RevBrowse\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u8bc4\u8bba\u9a71\u52a8\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7PrefRAG\u6a21\u5757\u81ea\u9002\u5e94\u68c0\u7d22\u76f8\u5173\u8bc4\u8bba\u6765\u89e3\u51b3LLM\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u548c\u8bc4\u8bba\u4f18\u5148\u7ea7\u95ee\u9898", "motivation": "LLM\u5728\u63a8\u8350\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5982\u4f55\u6709\u6548\u5229\u7528\u7528\u6237\u8bc4\u8bba\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1aLLM\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u65e0\u6cd5\u52a8\u6001\u4f7f\u7528\u5927\u91cf\u8bc4\u8bba\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u673a\u5236\u6765\u4f18\u5148\u5904\u7406\u4e0e\u5f53\u524d\u51b3\u7b56\u6700\u76f8\u5173\u7684\u8bc4\u8bba", "method": "\u63d0\u51faRevBrowse\u6846\u67b6\uff0c\u6a21\u62df\u7528\u6237\"\u6d4f\u89c8-\u51b3\u7b56\"\u8fc7\u7a0b\uff0c\u96c6\u6210\u7528\u6237\u8bc4\u8bba\u5230LLM\u91cd\u6392\u5e8f\u4e2d\u3002\u5f15\u5165PrefRAG\u6a21\u5757\uff0c\u5c06\u7528\u6237\u548c\u7269\u54c1\u8868\u793a\u89e3\u8026\u4e3a\u7ed3\u6784\u5316\u5f62\u5f0f\uff0c\u6839\u636e\u76ee\u6807\u7269\u54c1\u81ea\u9002\u5e94\u68c0\u7d22\u504f\u597d\u76f8\u5173\u5185\u5bb9", "result": "\u5728\u56db\u4e2aAmazon\u8bc4\u8bba\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRevBrowse\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u53d6\u5f97\u4e86\u4e00\u81f4\u4e14\u663e\u8457\u7684\u6539\u8fdb\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5efa\u6a21\u52a8\u6001\u7528\u6237\u504f\u597d\u65b9\u9762\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6709\u6548\u6027", "conclusion": "RevBrowse\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684\u900f\u660e\u8fc7\u7a0b\u4e0d\u4ec5\u63d0\u5347\u4e86\u63a8\u8350\u6027\u80fd\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u5b9a\u7a0b\u5ea6\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4f7f\u5f71\u54cd\u6700\u7ec8\u63a8\u8350\u7684\u8bc4\u8bba\u53ef\u89c1"}}
{"id": "2509.00707", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00707", "abs": "https://arxiv.org/abs/2509.00707", "authors": ["Daehoon Gwak", "Minseo Jung", "Junwoo Park", "Minho Park", "ChaeHun Park", "Junha Hyung", "Jaegul Choo"], "title": "Reward-Weighted Sampling: Enhancing Non-Autoregressive Characteristics in Masked Diffusion LLMs", "comment": "EMNLP 2025 Main Paper (Long)", "summary": "Masked diffusion models (MDMs) offer a promising non-autoregressive\nalternative for large language modeling. Standard decoding methods for MDMs,\nsuch as confidence-based sampling, select tokens independently based on\nindividual token confidences at each diffusion step. However, we observe that\nthis independent token selection often results in generation orders resembling\nsequential autoregressive processes, limiting the advantages of\nnon-autoregressive modeling. To mitigate this pheonomenon, we propose\nReward-Weighted Sampling (RWS), a novel decoding strategy that leverages an\nexternal reward model to provide a principled global signal during the\niterative diffusion process. Specifically, at each diffusion step, RWS\nevaluates the quality of the entire intermediate sequence and scales token\nlogits accordingly, guiding token selection by integrating global\nsequence-level coherence. This method selectively increases the confidence of\ntokens that initially have lower scores, thereby promoting a more\nnon-autoregressive generation order. Furthermore, we provide theoretical\njustification showing that reward-weighted logit scaling induces beneficial\nrank reversals in token selection and consistently improves expected reward.\nExperiments demonstrate that RWS significantly promotes non-autoregressive\ngeneration orders, leading to improvements across multiple evaluation metrics.\nThese results highlight the effectiveness of integrating global signals in\nenhancing both the non-autoregressive properties and overall performance of\nMDMs.", "AI": {"tldr": "\u63d0\u51faReward-Weighted Sampling (RWS)\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u5916\u90e8\u5956\u52b1\u6a21\u578b\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u5168\u5c40\u4fe1\u53f7\uff0c\u6539\u5584\u63a9\u7801\u6269\u6563\u6a21\u578b\u7684\u975e\u81ea\u56de\u5f52\u751f\u6210\u987a\u5e8f\u548c\u6027\u80fd", "motivation": "\u6807\u51c6\u7f6e\u4fe1\u5ea6\u91c7\u6837\u65b9\u6cd5\u5bfc\u81f4\u63a9\u7801\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u987a\u5e8f\u7c7b\u4f3c\u81ea\u56de\u5f52\u8fc7\u7a0b\uff0c\u9650\u5236\u4e86\u975e\u81ea\u56de\u5f52\u5efa\u6a21\u7684\u4f18\u52bf", "method": "\u5728\u6bcf\u4e00\u6b65\u6269\u6563\u8fc7\u7a0b\u4e2d\uff0cRWS\u8bc4\u4f30\u6574\u4e2a\u4e2d\u95f4\u5e8f\u5217\u8d28\u91cf\u5e76\u76f8\u5e94\u7f29\u653etoken\u5bf9\u6570\u6982\u7387\uff0c\u901a\u8fc7\u5168\u5c40\u5e8f\u5217\u4e00\u81f4\u6027\u6307\u5bfctoken\u9009\u62e9", "result": "RWS\u663e\u8457\u4fc3\u8fdb\u4e86\u975e\u81ea\u56de\u5f52\u751f\u6210\u987a\u5e8f\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u90fd\u6709\u6539\u8fdb", "conclusion": "\u6574\u5408\u5168\u5c40\u4fe1\u53f7\u80fd\u6709\u6548\u589e\u5f3a\u63a9\u7801\u6269\u6563\u6a21\u578b\u7684\u975e\u81ea\u56de\u5f52\u7279\u6027\u548c\u6574\u4f53\u6027\u80fd"}}
{"id": "2509.00709", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00709", "abs": "https://arxiv.org/abs/2509.00709", "authors": ["Elias Ra", "Seung Je Kim", "Eui-Yeong Seo", "Geunju So"], "title": "Designing LMS and Instructional Strategies for Integrating Generative-Conversational AI", "comment": null, "summary": "Higher education faces growing challenges in delivering personalized,\nscalable, and pedagogically coherent learning experiences. This study\nintroduces a structured framework for designing an AI-powered Learning\nManagement System (AI-LMS) that integrates generative and conversational AI to\nsupport adaptive, interactive, and learner-centered instruction. Using a\ndesign-based research (DBR) methodology, the framework unfolds through five\nphases: literature review, SWOT analysis, development of ethical-pedagogical\nprinciples, system design, and instructional strategy formulation. The\nresulting AI-LMS features modular components -- including configurable prompts,\nadaptive feedback loops, and multi-agent conversation flows -- aligned with\npedagogical paradigms such as behaviorist, constructivist, and connectivist\nlearning theories. By combining AI capabilities with human-centered design and\nethical safeguards, this study advances a practical model for AI integration in\neducation. Future research will validate and refine the system through\nreal-world implementation.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u8bbe\u8ba1\u57fa\u4e8eAI\u7684\u5b66\u4e60\u7ba1\u7406\u7cfb\u7edf(AI-LMS)\uff0c\u901a\u8fc7\u6574\u5408\u751f\u6210\u5f0fAI\u548c\u5bf9\u8bdd\u5f0fAI\u6765\u652f\u6301\u9002\u5e94\u6027\u3001\u4e92\u52a8\u6027\u548c\u4ee5\u5b66\u4e60\u8005\u4e3a\u4e2d\u5fc3\u7684\u6559\u5b66\u4f53\u9a8c\u3002", "motivation": "\u9ad8\u7b49\u6559\u80b2\u9762\u4e34\u7740\u63d0\u4f9b\u4e2a\u6027\u5316\u3001\u53ef\u6269\u5c55\u548c\u6559\u80b2\u5b66\u4e00\u81f4\u7684\u5b66\u4e60\u4f53\u9a8c\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u6574\u5408AI\u6280\u672f\u4ee5\u652f\u6301\u9002\u5e94\u6027\u6559\u5b66\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8bbe\u8ba1\u7684\u7814\u7a76(DBR)\u65b9\u6cd5\u8bba\uff0c\u5305\u62ec\u4e94\u4e2a\u9636\u6bb5\uff1a\u6587\u732e\u7efc\u8ff0\u3001SWOT\u5206\u6790\u3001\u4f26\u7406-\u6559\u80b2\u5b66\u539f\u5219\u5f00\u53d1\u3001\u7cfb\u7edf\u8bbe\u8ba1\u548c\u6559\u5b66\u7b56\u7565\u5236\u5b9a\u3002\u7cfb\u7edf\u5305\u542b\u53ef\u914d\u7f6e\u63d0\u793a\u3001\u9002\u5e94\u6027\u53cd\u9988\u5faa\u73af\u548c\u591a\u4ee3\u7406\u5bf9\u8bdd\u6d41\u7b49\u6a21\u5757\u7ec4\u4ef6\u3002", "result": "\u5f00\u53d1\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408AI\u80fd\u529b\u4e0e\u4eba\u7c7b\u4e2d\u5fc3\u8bbe\u8ba1\u53ca\u4f26\u7406\u4fdd\u969c\u7684AI-LMS\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4e0e\u884c\u4e3a\u4e3b\u4e49\u3001\u5efa\u6784\u4e3b\u4e49\u548c\u8fde\u63a5\u4e3b\u4e49\u7b49\u6559\u80b2\u5b66\u7406\u8bba\u76f8\u7b26\u5408\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u7528\u7684AI\u6574\u5408\u6a21\u578b\uff0c\u4e3a\u6559\u80b2\u9886\u57df\u7684AI\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002\u672a\u6765\u7814\u7a76\u5c06\u901a\u8fc7\u5b9e\u9645\u5e94\u7528\u6765\u9a8c\u8bc1\u548c\u4f18\u5316\u8be5\u7cfb\u7edf\u3002"}}
{"id": "2509.00731", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00731", "abs": "https://arxiv.org/abs/2509.00731", "authors": ["Houji Jin", "Negin Ashrafi", "Armin Abdollahi", "Wei Liu", "Jian Wang", "Ganyu Gui", "Maryam Pishgar", "Huanghao Feng"], "title": "LLM Encoder vs. Decoder: Robust Detection of Chinese AI-Generated Text with LoRA", "comment": null, "summary": "The rapid growth of large language models (LLMs) has heightened the demand\nfor accurate detection of AI-generated text, particularly in languages like\nChinese, where subtle linguistic nuances pose significant challenges to current\nmethods. In this study, we conduct a systematic comparison of encoder-based\nTransformers (Chinese BERT-large and RoBERTa-wwm-ext-large), a decoder-only LLM\n(Alibaba's Qwen2.5-7B/DeepSeek-R1-Distill-Qwen-7B fine-tuned via Low-Rank\nAdaptation, LoRA), and a FastText baseline using the publicly available dataset\nfrom the NLPCC 2025 Chinese AI-Generated Text Detection Task. Encoder models\nwere fine-tuned using a novel prompt-based masked language modeling approach,\nwhile Qwen2.5-7B was adapted for classification with an instruction-format\ninput and a lightweight classification head trained via LoRA. Experiments\nreveal that although encoder models nearly memorize training data, they suffer\nsignificant performance degradation under distribution shifts (RoBERTa: 76.3%\ntest accuracy; BERT: 79.3%). FastText demonstrates surprising lexical\nrobustness (83.5% accuracy) yet lacks deeper semantic understanding. In\ncontrast, the LoRA-adapted Qwen2.5-7B achieves 95.94% test accuracy with\nbalanced precision-recall metrics, indicating superior generalization and\nresilience to dataset-specific artifacts. These findings underscore the\nefficacy of decoder-based LLMs with parameter-efficient fine-tuning for robust\nChinese AI-generated text detection. Future work will explore next-generation\nQwen3 models, distilled variants, and ensemble strategies to enhance\ncross-domain robustness further.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u4e2d\u6587AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4f7f\u7528LoRA\u5fae\u8c03\u7684Qwen2.5-7B\u89e3\u7801\u5668\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u8fbe\u523095.94%\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u663e\u793a\u4e86\u66f4\u597d\u7684\u6f14\u5316\u80fd\u529b\u548c\u7a33\u5065\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u51c6\u786e\u68c0\u6d4bAI\u751f\u6210\u6587\u672c\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u7279\u522b\u662f\u5728\u4e2d\u6587\u8bed\u8a00\u4e2d\uff0c\u7ec6\u5fae\u7684\u8bed\u8a00\u7ec6\u8282\u7ed9\u73b0\u6709\u65b9\u6cd5\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u91c7\u7528NLPCC 2025\u4e2d\u6587AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e86\u7f16\u7801\u5668\u6a21\u578b\uff08BERT-large\u548cRoBERTa-wwm-ext-large\uff09\u3001\u89e3\u7801\u5668\u6a21\u578b\uff08Qwen2.5-7B\u901a\u8fc7LoRA\u5fae\u8c03\uff09\u4ee5\u53caFastText\u57fa\u7ebf\u3002\u7f16\u7801\u5668\u6a21\u578b\u91c7\u7528\u63d0\u793a\u57fa\u7840\u7684\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u65b9\u6cd5\uff0c\u89e3\u7801\u5668\u6a21\u578b\u901a\u8fc7\u6307\u4ee4\u683c\u5f0f\u8f93\u5165\u548c\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5934\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u7f16\u7801\u5668\u6a21\u578b\u867d\u7136\u51e0\u4e4e\u8bb0\u4f4f\u4e86\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u8868\u73b0\u5927\u5e45\u4e0b\u964d\uff08RoBERTa: 76.3%\uff0cBERT: 79.3%\uff09\u3002FastText\u663e\u793a\u51fa\u610f\u5916\u7684\u8bcd\u6c47\u7a33\u5065\u6027\uff0883.5%\uff09\u4f46\u7f3a\u4e4f\u6df1\u5c42\u8bed\u4e49\u7406\u89e3\u3002LoRA\u5fae\u8c03\u7684Qwen2.5-7B\u8fbe\u523095.94%\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u5177\u6709\u5e73\u8861\u7684\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u57fa\u4e8e\u89e3\u7801\u5668\u7684\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u53c2\u6570\u6548\u7387\u5fae\u8c03\u5728\u4e2d\u6587AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u4e2d\u7684\u6548\u679c\u4f18\u52bf\uff0c\u5177\u6709\u66f4\u597d\u7684\u6f14\u5316\u80fd\u529b\u548c\u5bf9\u6570\u636e\u96c6\u7279\u5b9a\u4eba\u5de5\u7269\u7684\u8010\u53d7\u6027\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u63a2\u7d22Qwen3\u6a21\u578b\u3001\u7cbe\u7b80\u7248\u672c\u548c\u96c6\u6210\u7b56\u7565\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u8de8\u57df\u7a33\u5065\u6027\u3002"}}
{"id": "2509.00765", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00765", "abs": "https://arxiv.org/abs/2509.00765", "authors": ["Zhichao Yan", "Jiaoyan Chen", "Jiapu Wang", "Xiaoli Li", "Ru Li", "Jeff Z. Pan"], "title": "Decomposing and Revising What Language Models Generate", "comment": null, "summary": "Attribution is crucial in question answering (QA) with Large Language Models\n(LLMs).SOTA question decomposition-based approaches use long form answers to\ngenerate questions for retrieving related documents. However, the generated\nquestions are often irrelevant and incomplete, resulting in a loss of facts in\nretrieval.These approaches also fail to aggregate evidence snippets from\ndifferent documents and paragraphs. To tackle these problems, we propose a new\nfact decomposition-based framework called FIDES (\\textit{faithful context\nenhanced fact decomposition and evidence aggregation}) for attributed QA. FIDES\nuses a contextually enhanced two-stage faithful decomposition method to\ndecompose long form answers into sub-facts, which are then used by a retriever\nto retrieve related evidence snippets. If the retrieved evidence snippets\nconflict with the related sub-facts, such sub-facts will be revised\naccordingly. Finally, the evidence snippets are aggregated according to the\noriginal sentences.Extensive evaluation has been conducted with six datasets,\nwith an additionally proposed new metric called $Attr_{auto-P}$ for evaluating\nthe evidence precision. FIDES outperforms the SOTA methods by over 14\\% in\naverage with GPT-3.5-turbo, Gemini and Llama 70B series.", "AI": {"tldr": "FIDES\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e8b\u5b9e\u5206\u89e3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u589e\u5f3a\u7684\u4e24\u9636\u6bb5\u5fe0\u5b9e\u5206\u89e3\u65b9\u6cd5\u5c06\u957f\u7b54\u6848\u5206\u89e3\u4e3a\u5b50\u4e8b\u5b9e\uff0c\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u8bc1\u636e\u7247\u6bb5\uff0c\u5e76\u5728\u8bc1\u636e\u51b2\u7a81\u65f6\u4fee\u8ba2\u5b50\u4e8b\u5b9e\uff0c\u6700\u540e\u6839\u636e\u539f\u59cb\u53e5\u5b50\u805a\u5408\u8bc1\u636e\uff0c\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u8d85\u8d8aSOTA\u65b9\u6cd514%\u4ee5\u4e0a\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u95ee\u9898\u5206\u89e3\u7684\u65b9\u6cd5\u751f\u6210\u7684\u95ee\u9898\u5f80\u5f80\u4e0d\u76f8\u5173\u4e14\u4e0d\u5b8c\u6574\uff0c\u5bfc\u81f4\u68c0\u7d22\u4e2d\u4e22\u5931\u4e8b\u5b9e\uff0c\u5e76\u4e14\u65e0\u6cd5\u805a\u5408\u6765\u81ea\u4e0d\u540c\u6587\u6863\u548c\u6bb5\u843d\u7684\u8bc1\u636e\u7247\u6bb5\u3002", "method": "\u4f7f\u7528\u4e0a\u4e0b\u6587\u589e\u5f3a\u7684\u4e24\u9636\u6bb5\u5fe0\u5b9e\u5206\u89e3\u65b9\u6cd5\u5206\u89e3\u957f\u7b54\u6848\u4e3a\u5b50\u4e8b\u5b9e\uff0c\u68c0\u7d22\u76f8\u5173\u8bc1\u636e\u7247\u6bb5\uff0c\u5728\u8bc1\u636e\u51b2\u7a81\u65f6\u4fee\u8ba2\u5b50\u4e8b\u5b9e\uff0c\u6700\u540e\u6839\u636e\u539f\u59cb\u53e5\u5b50\u805a\u5408\u8bc1\u636e\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528GPT-3.5-turbo\u3001Gemini\u548cLlama 70B\u7cfb\u5217\uff0cFIDES\u5e73\u5747\u8d85\u8d8aSOTA\u65b9\u6cd514%\u4ee5\u4e0a\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc1\u636e\u7cbe\u5ea6\u8bc4\u4f30\u6307\u6807$Attr_{auto-P}$\u3002", "conclusion": "FIDES\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u95ee\u9898\u751f\u6210\u548c\u8bc1\u636e\u805a\u5408\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f52\u56e0\u95ee\u7b54\u7684\u6027\u80fd\u548c\u8bc1\u636e\u7cbe\u5ea6\u3002"}}
{"id": "2509.00783", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00783", "abs": "https://arxiv.org/abs/2509.00783", "authors": ["Weizhe Shi", "Qiqi Wang", "Yihong Pan", "Qian Liu", "Kaiqi Zhao"], "title": "LegalChainReasoner: A Legal Chain-guided Framework for Criminal Judicial Opinion Generation", "comment": null, "summary": "A criminal judicial opinion represents the judge's disposition of a case,\nincluding the decision rationale and sentencing. Automatically generating such\nopinions can assist in analyzing sentencing consistency and provide judges with\nreferences to similar past cases. However, current research typically\napproaches this task by dividing it into two isolated subtasks: legal reasoning\nand sentencing prediction. This separation often leads to inconsistency between\nthe reasoning and predictions, failing to meet real-world judicial\nrequirements. Furthermore, prior studies rely on manually curated knowledge to\nenhance applicability, yet such methods remain limited in practical deployment.\nTo address these limitations and better align with legal practice, we propose a\nnew LegalAI task: Judicial Opinion Generation, which simultaneously produces\nboth legal reasoning and sentencing decisions. To achieve this, we introduce\nLegalChainReasoner, a framework that applies structured legal chains to guide\nthe model through comprehensive case assessments. By integrating factual\npremises, composite legal conditions, and sentencing conclusions, our approach\nensures flexible knowledge injection and end-to-end opinion generation.\nExperiments on two real-world and open-source Chinese legal case datasets\ndemonstrate that our method outperforms baseline models.", "AI": {"tldr": "\u63d0\u51faLegalChainReasoner\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6cd5\u5f8b\u94fe\u5b9e\u73b0\u6cd5\u5f8b\u63a8\u7406\u548c\u91cf\u5211\u5224\u51b3\u7684\u7aef\u5230\u7aef\u751f\u6210\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u4e24\u8005\u4e0d\u4e00\u81f4\u7684\u95ee\u9898", "motivation": "\u73b0\u6709\u7814\u7a76\u5c06\u53f8\u6cd5\u610f\u89c1\u751f\u6210\u5206\u4e3a\u6cd5\u5f8b\u63a8\u7406\u548c\u91cf\u5211\u9884\u6d4b\u4e24\u4e2a\u72ec\u7acb\u5b50\u4efb\u52a1\uff0c\u5bfc\u81f4\u63a8\u7406\u4e0e\u9884\u6d4b\u4e0d\u4e00\u81f4\uff0c\u4e14\u4f9d\u8d56\u4eba\u5de5\u77e5\u8bc6\u96be\u4ee5\u5b9e\u9645\u90e8\u7f72", "method": "\u5f15\u5165LegalChainReasoner\u6846\u67b6\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u6cd5\u5f8b\u94fe\u6574\u5408\u4e8b\u5b9e\u524d\u63d0\u3001\u590d\u5408\u6cd5\u5f8b\u6761\u4ef6\u548c\u91cf\u5211\u7ed3\u8bba\uff0c\u5b9e\u73b0\u7075\u6d3b\u77e5\u8bc6\u6ce8\u5165\u548c\u7aef\u5230\u7aef\u610f\u89c1\u751f\u6210", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e2d\u6587\u6cd5\u5f8b\u6848\u4f8b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b", "conclusion": "\u63d0\u51fa\u7684\u53f8\u6cd5\u610f\u89c1\u751f\u6210\u4efb\u52a1\u548cLegalChainReasoner\u6846\u67b6\u80fd\u66f4\u597d\u5730\u7b26\u5408\u6cd5\u5f8b\u5b9e\u8df5\u9700\u6c42\uff0c\u5b9e\u73b0\u4e00\u81f4\u7684\u6cd5\u5f8b\u63a8\u7406\u548c\u91cf\u5211\u51b3\u7b56"}}
{"id": "2509.00806", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00806", "abs": "https://arxiv.org/abs/2509.00806", "authors": ["Reem Abdel-Salam", "Mary Adewunmi", "Modinat A. Abayomi"], "title": "CaresAI at BioCreative IX Track 1 -- LLM for Biomedical QA", "comment": "Proceedings of the BioCreative IX Challenge and Workshop (BC9): Large\n  Language Models for Clinical and Biomedical NLP at the International Joint\n  Conference on Artificial Intelligence (IJCAI), Montreal, Canada, 2025", "summary": "Large language models (LLMs) are increasingly evident for accurate question\nanswering across various domains. However, rigorous evaluation of their\nperformance on complex question-answering (QA) capabilities is essential before\ndeployment in real-world biomedical and healthcare applications. This paper\npresents our approach to the MedHopQA track of the BioCreative IX shared task,\nwhich focuses on multi-hop biomedical question answering involving diseases,\ngenes, and chemicals. We adopt a supervised fine-tuning strategy leveraging\nLLaMA 3 8B, enhanced with a curated biomedical question-answer dataset compiled\nfrom external sources including BioASQ, MedQuAD, and TREC. Three experimental\nsetups are explored: fine-tuning on combined short and long answers, short\nanswers only, and long answers only. While our models demonstrate strong domain\nunderstanding, achieving concept-level accuracy scores of up to 0.8, their\nExact Match (EM) scores remain significantly lower, particularly in the test\nphase. We introduce a two-stage inference pipeline for precise short-answer\nextraction to mitigate verbosity and improve alignment with evaluation metrics.\nDespite partial improvements, challenges persist in generating strictly\nformatted outputs. Our findings highlight the gap between semantic\nunderstanding and exact answer evaluation in biomedical LLM applications,\nmotivating further research in output control and post-processing strategies.", "AI": {"tldr": "\u672c\u6587\u91c7\u7528LLaMA 3 8B\u6a21\u578b\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u5728\u751f\u7269\u533b\u5b66\u591a\u8df3\u95ee\u7b54\u4efb\u52a1MedHopQA\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u6982\u5ff5\u7406\u89e3\u4e0a\u8868\u73b0\u826f\u597d\u4f46\u7cbe\u786e\u5339\u914d\u5f97\u5206\u8f83\u4f4e\uff0c\u63d0\u51fa\u4e86\u4e24\u9636\u6bb5\u63a8\u7406\u7ba1\u9053\u6765\u6539\u8fdb\u7b54\u6848\u63d0\u53d6\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u5bf9\u5176\u5728\u590d\u6742\u751f\u7269\u533b\u5b66\u95ee\u7b54\u80fd\u529b\u4e0a\u8fdb\u884c\u4e25\u683c\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u5728\u771f\u5b9e\u533b\u7597\u5e94\u7528\u90e8\u7f72\u524d\u7684\u6027\u80fd\u9a8c\u8bc1\u3002", "method": "\u4f7f\u7528LLaMA 3 8B\u6a21\u578b\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u5229\u7528\u4eceBioASQ\u3001MedQuAD\u548cTREC\u7b49\u5916\u90e8\u6765\u6e90\u6574\u7406\u7684\u751f\u7269\u533b\u5b66\u95ee\u7b54\u6570\u636e\u96c6\u3002\u63a2\u7d22\u4e86\u4e09\u79cd\u5b9e\u9a8c\u8bbe\u7f6e\uff1a\u957f\u77ed\u7b54\u6848\u8054\u5408\u5fae\u8c03\u3001\u4ec5\u77ed\u7b54\u6848\u548c\u4ec5\u957f\u7b54\u6848\u5fae\u8c03\u3002", "result": "\u6a21\u578b\u5728\u6982\u5ff5\u7ea7\u522b\u51c6\u786e\u7387\u8fbe\u52300.8\uff0c\u4f46\u7cbe\u786e\u5339\u914d\u5f97\u5206\u663e\u8457\u8f83\u4f4e\uff0c\u7279\u522b\u662f\u5728\u6d4b\u8bd5\u9636\u6bb5\u3002\u4e24\u9636\u6bb5\u63a8\u7406\u7ba1\u9053\u90e8\u5206\u6539\u5584\u4e86\u7b54\u6848\u63d0\u53d6\uff0c\u4f46\u5728\u4e25\u683c\u683c\u5f0f\u5316\u8f93\u51fa\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u751f\u7269\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e2d\u8bed\u4e49\u7406\u89e3\u4e0e\u7cbe\u786e\u7b54\u6848\u8bc4\u4f30\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u8f93\u51fa\u63a7\u5236\u548c\u540e\u5904\u7406\u7b56\u7565\u3002"}}
{"id": "2509.00822", "categories": ["cs.CL", "cs.IR", "I.2.7; H.3.1"], "pdf": "https://arxiv.org/pdf/2509.00822", "abs": "https://arxiv.org/abs/2509.00822", "authors": ["Felix Engl", "Andreas Henrich"], "title": "TMT: A Simple Way to Translate Topic Models Using Dictionaries", "comment": "10 pages, 2 figures, 8 tables", "summary": "The training of topic models for a multilingual environment is a challenging\ntask, requiring the use of sophisticated algorithms, topic-aligned corpora, and\nmanual evaluation. These difficulties are further exacerbated when the\ndeveloper lacks knowledge of the target language or is working in an\nenvironment with limited data, where only small or unusable multilingual\ncorpora are available.\n  Considering these challenges, we introduce Topic Model Translation (TMT), a\nnovel, robust and transparent technique designed to transfer topic models\n(e.g., Latent Dirichlet Allocation (LDA) based topic models) from one language\nto another, without the need for metadata, embeddings, or aligned corpora. TMT\nenables the reuse of topic models across languages, making it especially\nsuitable for scenarios where large corpora in the target language are\nunavailable or manual translation is infeasible. Furthermore, we evaluate TMT\nextensively using both quantitative and qualitative methods, demonstrating that\nit produces semantically coherent and consistent topic translations.", "AI": {"tldr": "\u63d0\u51faTopic Model Translation (TMT)\u65b9\u6cd5\uff0c\u65e0\u9700\u5bf9\u9f50\u8bed\u6599\u5e93\u5373\u53ef\u5c06\u4e3b\u9898\u6a21\u578b\u4ece\u4e00\u79cd\u8bed\u8a00\u8fc1\u79fb\u5230\u53e6\u4e00\u79cd\u8bed\u8a00\uff0c\u7279\u522b\u9002\u7528\u4e8e\u76ee\u6807\u8bed\u8a00\u6570\u636e\u7a00\u7f3a\u7684\u573a\u666f\u3002", "motivation": "\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u8bad\u7ec3\u4e3b\u9898\u6a21\u578b\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u5f00\u53d1\u8005\u4e0d\u719f\u6089\u76ee\u6807\u8bed\u8a00\u6216\u53ef\u7528\u8bed\u6599\u5e93\u6709\u9650\u65f6\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u5bf9\u9f50\u8bed\u6599\u5e93\u5c31\u80fd\u8de8\u8bed\u8a00\u91cd\u7528\u4e3b\u9898\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "method": "TMT\u6280\u672f\u901a\u8fc7\u7ffb\u8bd1\u673a\u5236\u5c06\u6e90\u8bed\u8a00\u7684\u4e3b\u9898\u6a21\u578b\u8f6c\u79fb\u5230\u76ee\u6807\u8bed\u8a00\uff0c\u4e0d\u9700\u8981\u5143\u6570\u636e\u3001\u5d4c\u5165\u5411\u91cf\u6216\u5bf9\u9f50\u7684\u8bed\u6599\u5e93\u652f\u6301\u3002", "result": "\u901a\u8fc7\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u8868\u660e\uff0cTMT\u80fd\u591f\u4ea7\u751f\u8bed\u4e49\u4e00\u81f4\u4e14\u8fde\u8d2f\u7684\u4e3b\u9898\u7ffb\u8bd1\u7ed3\u679c\u3002", "conclusion": "TMT\u662f\u4e00\u79cd\u9c81\u68d2\u4e14\u900f\u660e\u7684\u4e3b\u9898\u6a21\u578b\u8de8\u8bed\u8a00\u8fc1\u79fb\u6280\u672f\uff0c\u5728\u76ee\u6807\u8bed\u8a00\u5927\u6570\u636e\u4e0d\u53ef\u5f97\u6216\u4eba\u5de5\u7ffb\u8bd1\u4e0d\u53ef\u884c\u7684\u60c5\u51b5\u4e0b\u7279\u522b\u6709\u6548\u3002"}}
{"id": "2509.00841", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00841", "abs": "https://arxiv.org/abs/2509.00841", "authors": ["Michelle Elizabeth", "Alicja Kasicka", "Natalia Krawczyk", "Magalie Ochs", "Gw\u00e9nol\u00e9 Lecorv\u00e9", "Justyna Gromada", "Lina M. Rojas-Barahona"], "title": "Neural Models and Language Model Prompting for the Multidimensional Evaluation of Open-Ended Conversations", "comment": null, "summary": "The growing number of generative AI-based dialogue systems has made their\nevaluation a crucial challenge. This paper presents our contribution to this\nimportant problem through the Dialogue System Technology Challenge (DSTC-12,\nTrack 1), where we developed models to predict dialogue-level,\ndimension-specific scores. Given the constraint of using relatively small\nmodels (i.e. fewer than 13 billion parameters) our work follows two main\nstrategies: employing Language Models (LMs) as evaluators through prompting,\nand training encoder-based classification and regression models.\n  Our results show that while LM prompting achieves only modest correlations\nwith human judgments, it still ranks second on the test set, outperformed only\nby the baseline. The regression and classification models, with significantly\nfewer parameters, demonstrate high correlation for some dimensions on the\nvalidation set. Although their performance decreases on the test set, it is\nimportant to note that the test set contains annotations with significantly\ndifferent score ranges for some of the dimensions with respect to the train and\nvalidation sets.", "AI": {"tldr": "\u672c\u6587\u5728DSTC-12 Track 1\u6311\u6218\u4e2d\u5f00\u53d1\u4e86\u76f8\u5bf9\u5c0f\u578b\u6a21\u578b\uff08\u5c11\u4e8e130\u4ebf\u53c2\u6570\uff09\u6765\u9884\u6d4b\u5bf9\u8bdd\u7ea7\u7ef4\u5ea6\u8bc4\u5206\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u63d0\u793a\u7684\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u548c\u7f16\u7801\u5668\u5206\u7c7b\u56de\u5f52\u6a21\u578b\u7684\u6548\u679c\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5bf9\u8bdd\u7cfb\u7edf\u7684\u5feb\u901f\u589e\u957f\uff0c\u5982\u4f55\u6709\u6548\u8bc4\u4f30\u8fd9\u4e9b\u7cfb\u7edf\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u76f8\u5bf9\u5c0f\u578b\u7684\u6a21\u578b\u6765\u89e3\u51b3\u5bf9\u8bdd\u7cfb\u7edf\u8bc4\u4f30\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u4e3b\u8981\u7b56\u7565\uff1a1\uff09\u901a\u8fc7\u63d0\u793a\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bc4\u4f30\u5668\uff1b2\uff09\u8bad\u7ec3\u57fa\u4e8e\u7f16\u7801\u5668\u7684\u5206\u7c7b\u548c\u56de\u5f52\u6a21\u578b\u3002", "result": "\u8bed\u8a00\u6a21\u578b\u63d0\u793a\u65b9\u6cd5\u4ec5\u83b7\u5f97\u4e2d\u7b49\u76f8\u5173\u6027\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u6392\u540d\u7b2c\u4e8c\uff1b\u7f16\u7801\u5668\u6a21\u578b\u5728\u9a8c\u8bc1\u96c6\u4e0a\u67d0\u4e9b\u7ef4\u5ea6\u8868\u73b0\u51fa\u9ad8\u76f8\u5173\u6027\uff0c\u4f46\u5728\u6d4b\u8bd5\u96c6\u4e0a\u6027\u80fd\u4e0b\u964d\uff0c\u4e3b\u8981\u56e0\u4e3a\u6d4b\u8bd5\u96c6\u8bc4\u5206\u8303\u56f4\u4e0e\u8bad\u7ec3\u9a8c\u8bc1\u96c6\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u76f8\u5bf9\u5c0f\u578b\u7684\u6a21\u578b\u5728\u5bf9\u8bdd\u8bc4\u4f30\u4efb\u52a1\u4e2d\u5177\u6709\u4e00\u5b9a\u6f5c\u529b\uff0c\u4f46\u6d4b\u8bd5\u96c6\u5206\u5e03\u5dee\u5f02\u5bf9\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u663e\u8457\uff0c\u9700\u8981\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.00842", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00842", "abs": "https://arxiv.org/abs/2509.00842", "authors": ["Tengyu Pan", "Zhichao Duan", "Zhenyu Li", "Bowen Dong", "Ning Liu", "Xiuxing Li", "Jianyong Wang"], "title": "Negative Matters: Multi-Granularity Hard-Negative Synthesis and Anchor-Token-Aware Pooling for Enhanced Text Embeddings", "comment": null, "summary": "Text embedding models are essential for various natural language processing\ntasks, enabling the effective encoding of semantic information into dense\nvector representations. These models are typically optimized using triplets of\n(query, positive, negative) data pairs for contrastive learning, where the\nnegative samples play a critical role in enhancing the model's ability to\ndiscern subtle semantic distinctions. In this work, we introduce a\nMulti-Granularity Hard-negative (MGH) synthesis framework that leverages large\nlanguage models (LLMs) to generate diverse negative samples with varying levels\nof similarity with the query. This approach facilitates a coarse-to-fine\ncurriculum learning strategy during supervised training, allowing the embedding\nmodel to progressively learn more nuanced semantic representations. Meanwhile,\nwe propose an Anchor Token Aware (ATA) pooling method that assigns higher\nweights to anchor tokens based on aggregation patterns observed in LLMs,\nimproving text embedding accuracy without increasing model complexity.\nComprehensive experiments on the MTEB benchmark demonstrate that our methods\nachieve state-of-the-art performance, surpassing existing synthesis strategies\nboth with synthetic data and when combined with public retrieval datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86MGH\u6846\u67b6\u548cATA\u6c60\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u8d1f\u6837\u672c\u5408\u6210\u548c\u951a\u70b9\u4ee4\u724c\u611f\u77e5\u673a\u5236\uff0c\u5728\u6587\u672c\u5d4c\u5165\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5d4c\u5165\u6a21\u578b\u7684\u8d1f\u6837\u672c\u751f\u6210\u7b56\u7565\u4e0d\u591f\u591a\u6837\u5316\uff0c\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49\u5dee\u5f02\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8d1f\u6837\u672c\u5408\u6210\u65b9\u6cd5\u548c\u7279\u5f81\u805a\u5408\u673a\u5236\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "1. MGH\u6846\u67b6\uff1a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e0d\u540c\u76f8\u4f3c\u5ea6\u7ea7\u522b\u7684\u8d1f\u6837\u672c\uff0c\u5b9e\u73b0\u4ece\u7c97\u5230\u7ec6\u7684\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff1b2. ATA\u6c60\u5316\uff1a\u57fa\u4e8eLLM\u805a\u5408\u6a21\u5f0f\u4e3a\u951a\u70b9\u4ee4\u724c\u5206\u914d\u66f4\u9ad8\u6743\u91cd\uff0c\u63d0\u5347\u5d4c\u5165\u51c6\u786e\u6027\u3002", "result": "\u5728MTEB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5408\u6210\u7b56\u7565\uff0c\u65e0\u8bba\u662f\u5728\u7eaf\u5408\u6210\u6570\u636e\u8fd8\u662f\u4e0e\u516c\u5171\u68c0\u7d22\u6570\u636e\u96c6\u7ed3\u5408\u7684\u60c5\u51b5\u4e0b\u90fd\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u591a\u7c92\u5ea6\u8d1f\u6837\u672c\u5408\u6210\u548c\u951a\u70b9\u4ee4\u724c\u611f\u77e5\u6c60\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6587\u672c\u5d4c\u5165\u6a21\u578b\u7684\u8bed\u4e49\u533a\u5206\u80fd\u529b\uff0c\u4e3a\u5bf9\u6bd4\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2509.00849", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00849", "abs": "https://arxiv.org/abs/2509.00849", "authors": ["Shaina Raza", "Maximus Powers", "Partha Pratim Saha", "Mahveen Raza", "Rizwan Qureshi"], "title": "Prompting Away Stereotypes? Evaluating Bias in Text-to-Image Models for Occupations", "comment": null, "summary": "Text-to-Image (TTI) models are powerful creative tools but risk amplifying\nharmful social biases. We frame representational societal bias assessment as an\nimage curation and evaluation task and introduce a pilot benchmark of\noccupational portrayals spanning five socially salient roles (CEO, Nurse,\nSoftware Engineer, Teacher, Athlete). Using five state-of-the-art models:\nclosed-source (DALLE 3, Gemini Imagen 4.0) and open-source (FLUX.1-dev, Stable\nDiffusion XL Turbo, Grok-2 Image), we compare neutral baseline prompts against\nfairness-aware controlled prompts designed to encourage demographic diversity.\nAll outputs are annotated for gender (male, female) and race (Asian, Black,\nWhite), enabling structured distributional analysis. Results show that\nprompting can substantially shift demographic representations, but with highly\nmodel-specific effects: some systems diversify effectively, others overcorrect\ninto unrealistic uniformity, and some show little responsiveness. These\nfindings highlight both the promise and the limitations of prompting as a\nfairness intervention, underscoring the need for complementary model-level\nstrategies. We release all code and data for transparency and reproducibility\nhttps://github.com/maximus-powers/img-gen-bias-analysis.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e865\u4e2aTTI\u6a21\u578b\u5728\u804c\u4e1a\u63cf\u7ed8\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u901a\u8fc7\u5bf9\u6bd4\u4e2d\u6027\u63d0\u793a\u8bcd\u548c\u516c\u5e73\u6027\u63d0\u793a\u8bcd\uff0c\u53d1\u73b0\u63d0\u793a\u8bcd\u5de5\u7a0b\u53ef\u4ee5\u663e\u8457\u6539\u53d8\u4eba\u53e3\u7edf\u8ba1\u8868\u5f81\uff0c\u4f46\u6548\u679c\u56e0\u6a21\u578b\u800c\u5f02\uff0c\u6709\u4e9b\u80fd\u6709\u6548\u591a\u6837\u5316\uff0c\u6709\u4e9b\u5219\u8fc7\u5ea6\u6821\u6b63\u6216\u54cd\u5e94\u6709\u9650\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4f5c\u4e3a\u5f3a\u5927\u7684\u521b\u610f\u5de5\u5177\uff0c\u5b58\u5728\u653e\u5927\u6709\u5bb3\u793e\u4f1a\u504f\u89c1\u98ce\u9669\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5176\u8868\u5f81\u504f\u89c1\u5e76\u63a2\u7d22\u5e72\u9884\u7b56\u7565\u3002", "method": "\u6784\u5efa\u804c\u4e1a\u63cf\u7ed8\u57fa\u51c6\u6570\u636e\u96c6\uff085\u4e2a\u793e\u4f1a\u91cd\u8981\u89d2\u8272\uff09\uff0c\u4f7f\u75285\u4e2aSOTA\u6a21\u578b\uff08\u95ed\u6e90\u548c\u5f00\u6e90\uff09\uff0c\u5bf9\u6bd4\u4e2d\u6027\u63d0\u793a\u8bcd\u4e0e\u516c\u5e73\u6027\u63d0\u793a\u8bcd\uff0c\u5bf9\u8f93\u51fa\u56fe\u50cf\u8fdb\u884c\u6027\u522b\u548c\u79cd\u65cf\u6807\u6ce8\u5e76\u8fdb\u884c\u5206\u5e03\u5206\u6790\u3002", "result": "\u63d0\u793a\u8bcd\u5de5\u7a0b\u80fd\u663e\u8457\u6539\u53d8\u4eba\u53e3\u7edf\u8ba1\u8868\u5f81\uff0c\u4f46\u6548\u679c\u9ad8\u5ea6\u6a21\u578b\u4f9d\u8d56\uff1a\u90e8\u5206\u7cfb\u7edf\u80fd\u6709\u6548\u591a\u6837\u5316\uff0c\u90e8\u5206\u8fc7\u5ea6\u6821\u6b63\u5bfc\u81f4\u4e0d\u73b0\u5b9e\u5747\u5300\u6027\uff0c\u90e8\u5206\u54cd\u5e94\u6709\u9650\u3002", "conclusion": "\u63d0\u793a\u8bcd\u5de5\u7a0b\u4f5c\u4e3a\u516c\u5e73\u6027\u5e72\u9884\u624b\u6bb5\u65e2\u6709\u6f5c\u529b\u4e5f\u6709\u5c40\u9650\uff0c\u9700\u8981\u7ed3\u5408\u6a21\u578b\u5c42\u9762\u7684\u8865\u5145\u7b56\u7565\uff0c\u7814\u7a76\u63d0\u4f9b\u4e86\u900f\u660e\u53ef\u590d\u73b0\u7684\u4ee3\u7801\u548c\u6570\u636e\u3002"}}
{"id": "2509.00869", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00869", "abs": "https://arxiv.org/abs/2509.00869", "authors": ["Zixuan Shangguan", "Yanjie Dong", "Lanjun Wang", "Xiaoyi Fan", "Victor C. M. Leung", "Xiping Hu"], "title": "Exploring and Mitigating Fawning Hallucinations in Large Language Models", "comment": null, "summary": "Large language models (LLMs) have demonstrated exceptional proficiency in\nlanguage understanding. However, when LLMs align their outputs with deceptive\nand/or misleading prompts, the generated responses could deviate from the de\nfacto information. Such observations are known as fawning hallucinations, where\nthe model prioritizes alignment with the input's implied perspective over\naccuracy and truthfulness. In this work, we analyze fawning hallucinations in\nvarious natural language processing tasks and tailor the so-termed contrastive\ndecoding method for fawning-hallucination mitigation. Specifically, we design\ntwo paradigms to generate corresponding deceptive and/or misleading inputs for\nthe consistent fawning hallucinations induction. Then, we propose the\ncollaborative contrastive decoding (CCD) to handle the fawning hallucinations\nacross different tasks in LLMs. By contrasting the deviation in output\ndistribution between induced and transformed neutral inputs, the proposed CCD\ncan reduce reliance on deceptive and/or misleading information without\nrequiring additional training. Extensive experiments demonstrate that the\nproposed CCD can effectively mitigate fawning hallucinations and improve the\nfactuality of the generated responses over various tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u534f\u4f5c\u5bf9\u6bd4\u89e3\u7801(CCD)\u65b9\u6cd5\u6765\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5949\u627f\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u5bf9\u6bd4\u8bf1\u5bfc\u6b3a\u9a97\u6027\u8f93\u5165\u548c\u4e2d\u6027\u8f93\u5165\u4e4b\u95f4\u7684\u8f93\u51fa\u5206\u5e03\u5dee\u5f02\uff0c\u5728\u4e0d\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u751f\u6210\u54cd\u5e94\u7684\u771f\u5b9e\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u6b3a\u9a97\u6027\u548c\u8bef\u5bfc\u6027\u63d0\u793a\u65f6\u4f1a\u4ea7\u751f\u5949\u627f\u5e7b\u89c9\uff0c\u5373\u6a21\u578b\u4f18\u5148\u8003\u8651\u4e0e\u8f93\u5165\u9690\u542b\u89c6\u89d2\u7684\u4e00\u81f4\u6027\u800c\u975e\u51c6\u786e\u6027\uff0c\u8fd9\u4f1a\u5f71\u54cd\u751f\u6210\u4fe1\u606f\u7684\u771f\u5b9e\u6027\u3002", "method": "\u8bbe\u8ba1\u4e24\u79cd\u8303\u5f0f\u751f\u6210\u6b3a\u9a97\u6027/\u8bef\u5bfc\u6027\u8f93\u5165\u6765\u8bf1\u5bfc\u5949\u627f\u5e7b\u89c9\uff0c\u7136\u540e\u63d0\u51fa\u534f\u4f5c\u5bf9\u6bd4\u89e3\u7801(CCD)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u8bf1\u5bfc\u8f93\u5165\u548c\u8f6c\u6362\u540e\u4e2d\u6027\u8f93\u5165\u4e4b\u95f4\u7684\u8f93\u51fa\u5206\u5e03\u504f\u5dee\u6765\u51cf\u5c11\u5bf9\u6b3a\u9a97\u4fe1\u606f\u7684\u4f9d\u8d56\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684CCD\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u5949\u627f\u5e7b\u89c9\uff0c\u5e76\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u63d0\u9ad8\u751f\u6210\u54cd\u5e94\u7684\u771f\u5b9e\u6027\u3002", "conclusion": "\u534f\u4f5c\u5bf9\u6bd4\u89e3\u7801\u662f\u4e00\u79cd\u6709\u6548\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5949\u627f\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u8f93\u51fa\u7684\u771f\u5b9e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.00877", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00877", "abs": "https://arxiv.org/abs/2509.00877", "authors": ["Yuqin Dai", "Guoqing Wang", "Yuan Wang", "Kairan Dou", "Kaichen Zhou", "Zhanwei Zhang", "Shuo Yang", "Fei Tang", "Jun Yin", "Pengyu Zeng", "Zhenzhe Ying", "Can Yi", "Changhua Meng", "Yuchen Zhou", "Yongliang Shen", "Shuai Lu"], "title": "EviNote-RAG: Enhancing RAG Models via Answer-Supportive Evidence Notes", "comment": null, "summary": "Large Language Models (LLMs) empowered with retrieval mechanisms have\nachieved strong progress in open-domain question answering (QA). Yet, the\nconventional retrieve--then--answer paradigm often suffers from two key\nlimitations: (1) low signal-to-noise ratio in retrieved evidence, where useful\ninformation is buried under irrelevant content, and (2) error accumulation in\nmulti-hop reasoning when incomplete or noisy passages are involved. To address\nthese challenges, we present EviNote-RAG, an agentic RAG framework that\nintroduces a structured retrieve--note--answer pipeline. Instead of directly\nreasoning over raw retrievals, the model is trained to compose\nSupportive-Evidence Notes (SENs), concise, human-like notes that preserve only\nanswer-relevant information, highlight uncertainty, and explicitly state when\nno useful evidence exists. This distillation process is further reinforced by\nthe Evidence Quality Reward (EQR), an entailment-based signal that evaluates\nwhether SENs logically support the final answer. Together, SENs and EQR guide\nthe model toward faithful and robust reasoning, while reducing the impact of\nnoise. Experiments on in-domain and out-of-domain QA benchmarks show that\nEviNote-RAG consistently outperforms strong baselines in accuracy,\ngeneralization, and training stability. In particular, it achieves\nstate-of-the-art results while enhancing robustness and efficiency, yielding\nrelative F1 gains of 20\\% on HotpotQA (+0.093), 40\\% on Bamboogle (+0.151), and\n91\\% on 2Wiki (+0.256) via denser rewards and reduced verbosity.", "AI": {"tldr": "EviNote-RAG\u662f\u4e00\u4e2a\u65b0\u7684RAG\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u68c0\u7d22-\u7b14\u8bb0-\u56de\u7b54\u6d41\u7a0b\uff0c\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u7b80\u6d01\u7684\u652f\u6301\u6027\u8bc1\u636e\u7b14\u8bb0(SENs)\u6765\u63d0\u5347\u5f00\u653e\u57df\u95ee\u7b54\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u68c0\u7d22-\u56de\u7b54\u8303\u5f0f\u4e2d\u566a\u58f0\u5e72\u6270\u548c\u591a\u8df3\u63a8\u7406\u9519\u8bef\u7d2f\u79ef\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u68c0\u7d22-\u56de\u7b54\u8303\u5f0f\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a(1)\u68c0\u7d22\u8bc1\u636e\u4e2d\u4fe1\u53f7\u566a\u58f0\u6bd4\u4f4e\uff0c\u6709\u7528\u4fe1\u606f\u88ab\u65e0\u5173\u5185\u5bb9\u6df9\u6ca1\uff1b(2)\u591a\u8df3\u63a8\u7406\u4e2d\u4e0d\u5b8c\u6574\u6216\u566a\u58f0\u6bb5\u843d\u5bfc\u81f4\u9519\u8bef\u7d2f\u79ef\u3002", "method": "\u63d0\u51faEviNote-RAG\u6846\u67b6\uff0c\u91c7\u7528\u7ed3\u6784\u5316\u68c0\u7d22-\u7b14\u8bb0-\u56de\u7b54\u6d41\u7a0b\uff0c\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u652f\u6301\u6027\u8bc1\u636e\u7b14\u8bb0(SENs)\u6765\u63d0\u70bc\u7b54\u6848\u76f8\u5173\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u8574\u542b\u7684\u8bc1\u636e\u8d28\u91cf\u5956\u52b1(EQR)\u6765\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5728\u9886\u57df\u5185\u548c\u9886\u57df\u5916QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEviNote-RAG\u5728\u51c6\u786e\u6027\u3001\u6cdb\u5316\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5728HotpotQA\u3001Bamboogle\u548c2Wiki\u6570\u636e\u96c6\u4e0a\u5206\u522b\u83b7\u5f9720%\u300140%\u548c91%\u7684\u76f8\u5bf9F1\u63d0\u5347\u3002", "conclusion": "EviNote-RAG\u901a\u8fc7SENs\u548cEQR\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u66f4\u5fe0\u5b9e\u548c\u9c81\u68d2\u7684\u63a8\u7406\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u566a\u58f0\u5f71\u54cd\uff0c\u5728\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u589e\u5f3a\u4e86\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2509.00893", "categories": ["cs.CL", "I.2.7; I.7"], "pdf": "https://arxiv.org/pdf/2509.00893", "abs": "https://arxiv.org/abs/2509.00893", "authors": ["R\u0103zvan-Alexandru Sm\u0103du", "Andreea Iuga", "Dumitru-Clementin Cercel", "Florin Pop"], "title": "SeLeRoSa: Sentence-Level Romanian Satire Detection Dataset", "comment": "12 pages, 2 Figures", "summary": "Satire, irony, and sarcasm are techniques typically used to express humor and\ncritique, rather than deceive; however, they can occasionally be mistaken for\nfactual reporting, akin to fake news. These techniques can be applied at a more\ngranular level, allowing satirical information to be incorporated into news\narticles. In this paper, we introduce the first sentence-level dataset for\nRomanian satire detection for news articles, called SeLeRoSa. The dataset\ncomprises 13,873 manually annotated sentences spanning various domains,\nincluding social issues, IT, science, and movies. With the rise and recent\nprogress of large language models (LLMs) in the natural language processing\nliterature, LLMs have demonstrated enhanced capabilities to tackle various\ntasks in zero-shot settings. We evaluate multiple baseline models based on LLMs\nin both zero-shot and fine-tuning settings, as well as baseline\ntransformer-based models. Our findings reveal the current limitations of these\nmodels in the sentence-level satire detection task, paving the way for new\nresearch directions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u53e5\u7ea7\u8bbd\u523a\u68c0\u6d4b\u6570\u636e\u96c6SeLeRoSa\uff0c\u5305\u542b13,873\u4e2a\u624b\u5de5\u6807\u6ce8\u7684\u53e5\u5b50\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cdLLM\u548ctransformer\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u8bbd\u523a\u3001\u53cd\u8bbd\u548c\u6316\u82e6\u901a\u5e38\u7528\u4e8e\u8868\u8fbe\u5e7d\u9ed8\u548c\u6279\u8bc4\uff0c\u4f46\u6709\u65f6\u4f1a\u88ab\u8bef\u8ba4\u4e3a\u662f\u4e8b\u5b9e\u62a5\u9053\uff0c\u7c7b\u4f3c\u4e8e\u5047\u65b0\u95fb\u3002\u8fd9\u4e9b\u6280\u672f\u53ef\u4ee5\u5728\u66f4\u7ec6\u7c92\u5ea6\u7684\u53e5\u5b50\u7ea7\u522b\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u521b\u5efa\u4e86\u5305\u542b13,873\u4e2a\u624b\u5de5\u6807\u6ce8\u53e5\u5b50\u7684\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u8bbd\u523a\u68c0\u6d4b\u6570\u636e\u96c6SeLeRoSa\uff0c\u6db5\u76d6\u591a\u4e2a\u9886\u57df\u3002\u8bc4\u4f30\u4e86\u591a\u79cd\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\uff0c\u4ee5\u53ca\u57fa\u4e8etransformer\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u8fd9\u4e9b\u6a21\u578b\u5728\u53e5\u5b50\u7ea7\u8bbd\u523a\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3a\u65b0\u7684\u7814\u7a76\u65b9\u5411\u94fa\u5e73\u4e86\u9053\u8def\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u53e5\u7ea7\u8bbd\u523a\u68c0\u6d4b\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.00921", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00921", "abs": "https://arxiv.org/abs/2509.00921", "authors": ["David Duki\u0107", "Goran Glava\u0161", "Jan \u0160najder"], "title": "Supervised In-Context Fine-Tuning for Generative Sequence Labeling", "comment": null, "summary": "Sequence labeling (SL) tasks, where labels are assigned to tokens, are\nabundant in NLP (e.g., named entity recognition and aspect-based sentiment\nanalysis). Owing to the intuition that they require bidirectional context, SL\ntasks are commonly tackled with encoder-only models. Recent work also shows\nthat removing the causal mask in fine-tuning enables decoder-based LLMs to\nbecome effective token classifiers. Less work, however, focused on (supervised)\ngenerative SL, a more natural setting for causal LLMs. Due to their rapid\nscaling, causal LLMs applied to SL are expected to outperform encoders, whose\nown development has stagnated. In this work, we propose supervised in-context\nfine-tuning (SIFT) for generative SL. SIFT casts SL tasks as constrained\nresponse generation, natural to LLMs, combining (1) in-context learning (ICL)\nfrom demonstrations with (2) supervised fine-tuning. SIFT considerably\noutperforms both ICL and decoder-as-encoder fine-tuning baselines on a range of\nstandard SL tasks. We further find that although long context hinders the\nperformance of generative SL in both ICL and SIFT, this deficiency can be\nmitigated by removing the instruction, as instructions are shown to be largely\nunnecessary for achieving strong SL performance with SIFT. Our findings\nhighlight strengths and limitations of SL with LLMs, underscoring the\nimportance of a response-based generative task formulation for effective SL\nperformance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u76d1\u7763\u4e0a\u4e0b\u6587\u5fae\u8c03(SIFT)\u65b9\u6cd5\uff0c\u5c06\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\u8f6c\u5316\u4e3a\u53d7\u9650\u54cd\u5e94\u751f\u6210\u95ee\u9898\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u76d1\u7763\u5fae\u8c03\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u56e0\u679c\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9884\u671f\u5176\u5728\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\u4e0a\u80fd\u8d85\u8d8a\u505c\u6ede\u53d1\u5c55\u7684\u7f16\u7801\u5668\u6a21\u578b\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u8f83\u5c11\u5173\u6ce8\u66f4\u9002\u5408\u56e0\u679cLLMs\u7684\u751f\u6210\u5f0f\u5e8f\u5217\u6807\u6ce8\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u76d1\u7763\u4e0a\u4e0b\u6587\u5fae\u8c03(SIFT)\uff0c\u5c06\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\u91cd\u65b0\u8868\u8ff0\u4e3a\u53d7\u9650\u54cd\u5e94\u751f\u6210\u95ee\u9898\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u548c\u76d1\u7763\u5fae\u8c03\uff0c\u65e0\u9700\u56e0\u679c\u63a9\u7801\u5373\u53ef\u6709\u6548\u5904\u7406\u5e8f\u5217\u6807\u6ce8\u3002", "result": "SIFT\u5728\u591a\u4e2a\u6807\u51c6\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8eICL\u548c\u89e3\u7801\u5668\u4f5c\u4e3a\u7f16\u7801\u5668\u7684\u5fae\u8c03\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5373\u4f7f\u53bb\u9664\u6307\u4ee4\u4e5f\u80fd\u4fdd\u6301\u5f3a\u52b2\u6027\u80fd\u3002", "conclusion": "\u751f\u6210\u5f0f\u4efb\u52a1\u8868\u8ff0\u5bf9\u5e8f\u5217\u6807\u6ce8\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0cSIFT\u65b9\u6cd5\u5c55\u793a\u4e86LLMs\u5728\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u6709\u6548\u5229\u7528LLMs\u8fdb\u884c\u5e8f\u5217\u6807\u6ce8\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2509.00934", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00934", "abs": "https://arxiv.org/abs/2509.00934", "authors": ["Md Shahidul Salim", "Lian Fu", "Arav Adikesh Ramakrishnan", "Zonghai Yao", "Hong Yu"], "title": "MedCOD: Enhancing English-to-Spanish Medical Translation of Large Language Models Using Enriched Chain-of-Dictionary Framework", "comment": "To appear in Findings of the Association for Computational\n  Linguistics: EMNLP 2025", "summary": "We present MedCOD (Medical Chain-of-Dictionary), a hybrid framework designed\nto improve English-to-Spanish medical translation by integrating\ndomain-specific structured knowledge into large language models (LLMs). MedCOD\nintegrates domain-specific knowledge from both the Unified Medical Language\nSystem (UMLS) and the LLM-as-Knowledge-Base (LLM-KB) paradigm to enhance\nstructured prompting and fine-tuning. We constructed a parallel corpus of 2,999\nEnglish-Spanish MedlinePlus articles and a 100-sentence test set annotated with\nstructured medical contexts. Four open-source LLMs (Phi-4, Qwen2.5-14B,\nQwen2.5-7B, and LLaMA-3.1-8B) were evaluated using structured prompts that\nincorporated multilingual variants, medical synonyms, and UMLS-derived\ndefinitions, combined with LoRA-based fine-tuning. Experimental results\ndemonstrate that MedCOD significantly improves translation quality across all\nmodels. For example, Phi-4 with MedCOD and fine-tuning achieved BLEU 44.23,\nchrF++ 28.91, and COMET 0.863, surpassing strong baseline models like GPT-4o\nand GPT-4o-mini. Ablation studies confirm that both MedCOD prompting and model\nadaptation independently contribute to performance gains, with their\ncombination yielding the highest improvements. These findings highlight the\npotential of structured knowledge integration to enhance LLMs for medical\ntranslation tasks.", "AI": {"tldr": "MedCOD\u662f\u4e00\u4e2a\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u533b\u5b66\u9886\u57df\u7ed3\u6784\u5316\u77e5\u8bc6\uff08UMLS\u548cLLM-KB\uff09\u6765\u63d0\u5347\u82f1\u897f\u533b\u5b66\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5728\u591a\u4e2a\u5f00\u6e90LLM\u4e0a\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u7ffb\u8bd1\u4e2d\u4e13\u4e1a\u672f\u8bed\u51c6\u786e\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u6574\u5408\u9886\u57df\u7279\u5b9a\u7ed3\u6784\u5316\u77e5\u8bc6\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u533b\u5b66\u7ffb\u8bd1\u80fd\u529b\u3002", "method": "\u6784\u5efa2999\u5bf9\u82f1\u897f\u533b\u5b66\u6587\u7ae0\u5e73\u884c\u8bed\u6599\u548c100\u53e5\u6d4b\u8bd5\u96c6\uff0c\u4f7f\u7528\u5305\u542b\u591a\u8bed\u8a00\u53d8\u4f53\u3001\u533b\u5b66\u540c\u4e49\u8bcd\u548cUMLS\u5b9a\u4e49\u7684\u7ed3\u6784\u5316\u63d0\u793a\uff0c\u7ed3\u5408LoRA\u5fae\u8c03\u65b9\u6cd5\u3002", "result": "\u6240\u6709\u6a21\u578b\u7ffb\u8bd1\u8d28\u91cf\u663e\u8457\u63d0\u5347\uff0cPhi-4\u6a21\u578b\u8fbe\u5230BLEU 44.23\u3001chrF++ 28.91\u3001COMET 0.863\uff0c\u8d85\u8d8aGPT-4o\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u7ed3\u6784\u5316\u77e5\u8bc6\u6574\u5408\u80fd\u6709\u6548\u63d0\u5347LLM\u5728\u533b\u5b66\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0cMedCOD\u63d0\u793a\u548c\u6a21\u578b\u9002\u914d\u5747\u72ec\u7acb\u8d21\u732e\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.00949", "categories": ["cs.CL", "cs.AI", "68T05, 68T30, 68T50", "I.2.6; I.2.7; H.3.3; K.8.0"], "pdf": "https://arxiv.org/pdf/2509.00949", "abs": "https://arxiv.org/abs/2509.00949", "authors": ["Yihong Chen"], "title": "Structure and Destructure: Dual Forces in the Making of Knowledge Engines", "comment": "PhD thesis. https://discovery.ucl.ac.uk/id/eprint/10211291/", "summary": "The making of knowledge engines in natural language processing has been\nshaped by two seemingly distinct paradigms: one grounded in structure, the\nother driven by massively available unstructured data. The structured paradigm\nleverages predefined symbolic interactions, such as knowledge graphs, as priors\nand designs models to capture them. In contrast, the unstructured paradigm\ncenters on scaling transformer architectures with increasingly vast data and\nmodel sizes, as seen in modern large language models. Despite their divergence,\nthis thesis seeks to establish conceptual connections bridging these paradigms.\nTwo complementary forces, structure and destructure, emerge across both\nparadigms: structure organizes seen symbolic interactions, while destructure,\nthrough periodic embedding resets, improves model plasticity and generalization\nto unseen scenarios. These connections form a new recipe for developing general\nknowledge engines that can support transparent, controllable, and adaptable\nintelligent systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86NLP\u4e2d\u7ed3\u6784\u5316\u4e0e\u975e\u7ed3\u6784\u5316\u77e5\u8bc6\u5f15\u64ce\u8303\u5f0f\u7684\u7edf\u4e00\uff0c\u63d0\u51fa\u4e86\u7ed3\u6784\u4e0e\u89e3\u6784\u7684\u4e92\u8865\u673a\u5236\u6765\u6784\u5efa\u900f\u660e\u53ef\u63a7\u7684\u901a\u7528\u77e5\u8bc6\u7cfb\u7edf", "motivation": "\u5f25\u5408NLP\u4e2d\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u5316\u8303\u5f0f\u4e0e\u57fa\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u7684\u975e\u7ed3\u6784\u5316\u8303\u5f0f\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u5efa\u7acb\u6982\u5ff5\u8054\u7cfb", "method": "\u901a\u8fc7\u7ed3\u6784\u673a\u5236\u7ec4\u7ec7\u5df2\u77e5\u7b26\u53f7\u4ea4\u4e92\uff0c\u901a\u8fc7\u89e3\u6784\u673a\u5236\uff08\u5468\u671f\u6027\u5d4c\u5165\u91cd\u7f6e\uff09\u63d0\u5347\u6a21\u578b\u53ef\u5851\u6027\u548c\u5bf9\u672a\u89c1\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b", "result": "\u5efa\u7acb\u4e86\u4e24\u79cd\u8303\u5f0f\u95f4\u7684\u6982\u5ff5\u8fde\u63a5\uff0c\u63d0\u51fa\u4e86\u5f00\u53d1\u901a\u7528\u77e5\u8bc6\u5f15\u64ce\u7684\u65b0\u65b9\u6cd5", "conclusion": "\u7ed3\u6784\u4e0e\u89e3\u6784\u7684\u4e92\u8865\u4f5c\u7528\u4e3a\u6784\u5efa\u900f\u660e\u3001\u53ef\u63a7\u3001\u81ea\u9002\u5e94\u7684\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u8df5\u8def\u5f84"}}
{"id": "2509.00974", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00974", "abs": "https://arxiv.org/abs/2509.00974", "authors": ["Chia-Hsuan Hsu", "Jun-En Ding", "Hsin-Ling Hsu", "Feng Liu", "Fang-Ming Hung"], "title": "RPRO:Ranked Preference Reinforcement Optimization for Enhancing Medical QA and Diagnostic Reasoning", "comment": null, "summary": "Medical question answering requires advanced reasoning that integrates domain\nknowledge with logical inference. However, existing large language models\n(LLMs) often generate reasoning chains that lack factual accuracy and clinical\nreliability. We propose Ranked Preference Reinforcement Optimization (RPRO), a\nnovel framework that uniquely combines reinforcement learning with\npreference-driven reasoning refinement to enhance clinical chain-of-thought\n(CoT) performance. RPRO differentiates itself from prior approaches by\nemploying task-adaptive reasoning templates and a probabilistic evaluation\nmechanism that aligns outputs with established clinical workflows, while\nautomatically identifying and correcting low-quality reasoning chains. Unlike\ntraditional pairwise preference methods, RPRO introduces a groupwise ranking\noptimization based on the Bradley-Terry model and incorporates KL-divergence\nregularization for stable training. Experiments on PubMedQA and MedQA-USMLE\nshow consistent improvements over strong baselines. Remarkably, our 1.1B\nparameter model outperforms much larger 7B-13B models, including\nmedical-specialized variants. These findings demonstrate that combining\npreference optimization with quality-driven refinement offers a scalable and\neffective approach to building more reliable, clinically grounded medical LLMs.", "AI": {"tldr": "RPRO\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u504f\u597d\u9a71\u52a8\u7684\u63a8\u7406\u4f18\u5316\uff0c\u63d0\u5347\u533b\u7597\u95ee\u7b54\u4e2d\u601d\u7ef4\u94fe\u7684\u4e34\u5e8a\u53ef\u9760\u6027\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u5728PubMedQA\u548cMedQA-USMLE\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u66f4\u5927\u7684\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u95ee\u7b54\u4e2d\u751f\u6210\u7684\u63a8\u7406\u94fe\u7f3a\u4e4f\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u53ef\u9760\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6574\u5408\u9886\u57df\u77e5\u8bc6\u548c\u903b\u8f91\u63a8\u7406\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRanked Preference Reinforcement Optimization (RPRO)\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u504f\u597d\u9a71\u52a8\u63a8\u7406\u4f18\u5316\uff0c\u4f7f\u7528\u4efb\u52a1\u81ea\u9002\u5e94\u63a8\u7406\u6a21\u677f\u548c\u6982\u7387\u8bc4\u4f30\u673a\u5236\uff0c\u57fa\u4e8eBradley-Terry\u6a21\u578b\u8fdb\u884c\u7ec4\u95f4\u6392\u5e8f\u4f18\u5316\uff0c\u5e76\u52a0\u5165KL\u6563\u5ea6\u6b63\u5219\u5316\u786e\u4fdd\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u5728PubMedQA\u548cMedQA-USMLE\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c1.1B\u53c2\u6570\u7684\u6a21\u578b\u6027\u80fd\u8d85\u8fc77B-13B\u7684\u5927\u578b\u6a21\u578b\uff0c\u5305\u62ec\u533b\u7597\u4e13\u7528\u53d8\u4f53\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5c06\u504f\u597d\u4f18\u5316\u4e0e\u8d28\u91cf\u9a71\u52a8\u7684\u7ec6\u5316\u76f8\u7ed3\u5408\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u3001\u4e34\u5e8a\u57fa\u7840\u66f4\u5f3a\u7684\u533b\u7597LLMs\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.00983", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00983", "abs": "https://arxiv.org/abs/2509.00983", "authors": ["Sadia Zaman Mishu", "S M Rafiuddin"], "title": "Performance Analysis of Supervised Machine Learning Algorithms for Text Classification", "comment": "8 pages, 2 figures, published in 2016 at the 19th International\n  Conference on Computer and Information Technology (ICCIT), Bangladesh,\n  proceedings pp. 409-413, IEEE", "summary": "The demand for text classification is growing significantly in web searching,\ndata mining, web ranking, recommendation systems, and so many other fields of\ninformation and technology. This paper illustrates the text classification\nprocess on different datasets using some standard supervised machine learning\ntechniques. Text documents can be classified through various kinds of\nclassifiers. Labeled text documents are used to classify the text in supervised\nclassifications. This paper applies these classifiers on different kinds of\nlabeled documents and measures the accuracy of the classifiers. An Artificial\nNeural Network (ANN) model using Back Propagation Network (BPN) is used with\nseveral other models to create an independent platform for labeled and\nsupervised text classification process. An existing benchmark approach is used\nto analyze the performance of classification using labeled documents.\nExperimental analysis on real data reveals which model works well in terms of\nclassification accuracy.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u591a\u79cd\u76d1\u7763\u673a\u5668\u5b66\u4e60\u6280\u672f\u5728\u6587\u672c\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\uff0c\u5305\u62ec\u4f7f\u7528\u53cd\u5411\u4f20\u64ad\u7f51\u7edc\u7684\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e0d\u540c\u5206\u7c7b\u5668\u5728\u6807\u6ce8\u6587\u6863\u4e0a\u7684\u51c6\u786e\u7387", "motivation": "\u968f\u7740\u7f51\u7edc\u641c\u7d22\u3001\u6570\u636e\u6316\u6398\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u9886\u57df\u5bf9\u6587\u672c\u5206\u7c7b\u9700\u6c42\u7684\u5feb\u901f\u589e\u957f\uff0c\u9700\u8981\u8bc4\u4f30\u4e0d\u540c\u5206\u7c7b\u5668\u5728\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0", "method": "\u4f7f\u7528\u6807\u51c6\u76d1\u7763\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u5305\u62ec\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\u548c\u53cd\u5411\u4f20\u64ad\u7f51\u7edc\uff08BPN\uff09\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u5e94\u7528\u4e0d\u540c\u5206\u7c7b\u5668\u8fdb\u884c\u6587\u672c\u5206\u7c7b", "result": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u63ed\u793a\u4e86\u54ea\u4e9b\u6a21\u578b\u5728\u5206\u7c7b\u51c6\u786e\u7387\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u4e3a\u6807\u6ce8\u548c\u76d1\u7763\u6587\u672c\u5206\u7c7b\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u72ec\u7acb\u5e73\u53f0", "conclusion": "\u7814\u7a76\u4e3a\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u6027\u80fd\u8bc4\u4f30\u6846\u67b6\uff0c\u5e2e\u52a9\u9009\u62e9\u6700\u9002\u5408\u7279\u5b9a\u5e94\u7528\u573a\u666f\u7684\u5206\u7c7b\u5668\u6a21\u578b"}}
{"id": "2509.01011", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01011", "abs": "https://arxiv.org/abs/2509.01011", "authors": ["S M Rafiuddin"], "title": "Ranking of Bangla Word Graph using Graph-based Ranking Algorithms", "comment": "8 pages, 2 figures, Publication date 2017-12-07, Conference 2017 3rd\n  International Conference on Electrical Information and Communication\n  Technology EICT, Pages 1-5, Publisher IEEE", "summary": "Ranking words is an important way to summarize a text or to retrieve\ninformation. A word graph is a way to represent the words of a sentence or a\ntext as the vertices of a graph and to show the relationship among the words.\nIt is also useful to determine the relative importance of a word among the\nwords in the word-graph. In this research, the ranking of Bangla words are\ncalculated, representing Bangla words from a text in a word graph using various\ngraph based ranking algorithms. There is a lack of a standard Bangla word\ndatabase. In this research, the Indian Language POS-tag Corpora is used, which\nhas a rich collection of Bangla words in the form of sentences with their parts\nof speech tags. For applying a word graph to various graph based ranking\nalgorithms, several standard procedures are applied. The preprocessing steps\nare done in every word graph and then applied to graph based ranking algorithms\nto make a comparison among these algorithms. This paper illustrate the entire\nprocedure of calculating the ranking of Bangla words, including the\nconstruction of the word graph from text. Experimental result analysis on real\ndata reveals the accuracy of each ranking algorithm in terms of F1 measure.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4f7f\u7528\u56fe\u6392\u5e8f\u7b97\u6cd5\u5bf9\u5b5f\u52a0\u62c9\u8bed\u5355\u8bcd\u8fdb\u884c\u6392\u540d\uff0c\u901a\u8fc7\u6784\u5efa\u8bcd\u56fe\u5e76\u5e94\u7528\u591a\u79cd\u56fe\u6392\u5e8f\u7b97\u6cd5\u6765\u8bc4\u4f30\u5355\u8bcd\u91cd\u8981\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4e0d\u540c\u7b97\u6cd5\u5728F1\u5206\u6570\u4e0a\u7684\u51c6\u786e\u5ea6\u5dee\u5f02\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u6807\u51c6\u7684\u5b5f\u52a0\u62c9\u8bed\u8bcd\u6c47\u6570\u636e\u5e93\uff0c\u4e14\u8bcd\u56fe\u80fd\u6709\u6548\u8868\u793a\u6587\u672c\u4e2d\u5355\u8bcd\u95f4\u7684\u5173\u7cfb\u5e76\u786e\u5b9a\u5355\u8bcd\u76f8\u5bf9\u91cd\u8981\u6027\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u5229\u7528\u56fe\u6392\u5e8f\u7b97\u6cd5\u5bf9\u5b5f\u52a0\u62c9\u8bed\u5355\u8bcd\u8fdb\u884c\u51c6\u786e\u6392\u540d\u3002", "method": "\u4f7f\u7528\u5370\u5ea6\u8bed\u8a00\u8bcd\u6027\u6807\u6ce8\u8bed\u6599\u5e93\u6784\u5efa\u5b5f\u52a0\u62c9\u8bed\u8bcd\u56fe\uff0c\u7ecf\u8fc7\u9884\u5904\u7406\u6b65\u9aa4\u540e\u5e94\u7528\u591a\u79cd\u56fe\u6392\u5e8f\u7b97\u6cd5\uff08\u5982PageRank\u7b49\uff09\u8fdb\u884c\u5355\u8bcd\u6392\u540d\u8ba1\u7b97\uff0c\u5e76\u901a\u8fc7F1\u5206\u6570\u8bc4\u4f30\u7b97\u6cd5\u51c6\u786e\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e0d\u540c\u56fe\u6392\u5e8f\u7b97\u6cd5\u5728\u5b5f\u52a0\u62c9\u8bed\u5355\u8bcd\u6392\u540d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u51c6\u786e\u5ea6\uff0c\u5177\u4f53F1\u5206\u6570\u663e\u793a\u4e86\u5404\u7b97\u6cd5\u7684\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5efa\u7acb\u4e86\u5b5f\u52a0\u62c9\u8bed\u5355\u8bcd\u6392\u540d\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u8bc1\u660e\u4e86\u56fe\u6392\u5e8f\u7b97\u6cd5\u5728\u8be5\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u7f3a\u4e4f\u6807\u51c6\u6570\u636e\u5e93\u7684\u5b5f\u52a0\u62c9\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2509.01035", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01035", "abs": "https://arxiv.org/abs/2509.01035", "authors": ["Nikta Gohari Sadr", "Sahar Heidariasl", "Karine Megerdoomian", "Laleh Seyyed-Kalantari", "Ali Emami"], "title": "We Politely Insist: Your LLM Must Learn the Persian Art of Taarof", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) struggle to navigate culturally specific\ncommunication norms, limiting their effectiveness in global contexts. We focus\non Persian taarof, a social norm in Iranian interactions, which is a\nsophisticated system of ritual politeness that emphasizes deference, modesty,\nand indirectness, yet remains absent from existing cultural benchmarks. We\nintroduce TaarofBench, the first benchmark for evaluating LLM understanding of\ntaarof, comprising 450 role-play scenarios covering 12 common social\ninteraction topics, validated by native speakers. Our evaluation of five\nfrontier LLMs reveals substantial gaps in cultural competence, with accuracy\nrates 40-48% below native speakers when taarof is culturally appropriate.\nPerformance varies between interaction topics, improves with Persian-language\nprompts, and exhibits gender-based asymmetries. We also show that responses\nrated \"polite\" by standard metrics often violate taarof norms, indicating the\nlimitations of Western politeness frameworks. Through supervised fine-tuning\nand Direct Preference Optimization, we achieve 21.8% and 42.3% improvement in\nmodel alignment with cultural expectations. Our human study with 33\nparticipants (11 native Persian, 11 heritage, and 11 non-Iranian speakers)\nforms baselines in varying degrees of familiarity with Persian norms. This work\nlays the foundation for developing diverse and culturally aware LLMs, enabling\napplications that better navigate complex social interactions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86TaarofBench\uff0c\u9996\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u6ce2\u65aftaarof\u6587\u5316\u793c\u4eea\u7406\u89e3\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u8de8\u6587\u5316\u6c9f\u901a\u4e2d\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6587\u5316\u5bf9\u9f50\u80fd\u529b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u7279\u5b9a\u6587\u5316\u6c9f\u901a\u89c4\u8303\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u6ce2\u65aftaarof\u8fd9\u79cd\u5f3a\u8c03\u8c26\u900a\u3001\u95f4\u63a5\u548c\u5c0a\u91cd\u7684\u590d\u6742\u793c\u4eea\u7cfb\u7edf\u65b9\u9762\uff0c\u73b0\u6709\u6587\u5316\u57fa\u51c6\u6d4b\u8bd5\u5b8c\u5168\u7f3a\u5931\u8fd9\u65b9\u9762\u7684\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u5305\u542b450\u4e2a\u89d2\u8272\u626e\u6f14\u573a\u666f\u7684TaarofBench\u57fa\u51c6\uff0c\u6db5\u76d612\u4e2a\u5e38\u89c1\u793e\u4ea4\u8bdd\u9898\uff0c\u7531\u6bcd\u8bed\u8005\u9a8c\u8bc1\u3002\u8bc4\u4f305\u4e2a\u524d\u6cbfLLM\uff0c\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff0c\u5e76\u5f00\u5c55\u5305\u542b33\u540d\u53c2\u4e0e\u8005\u7684\u4eba\u7c7b\u7814\u7a76\u5efa\u7acb\u57fa\u7ebf\u3002", "result": "\u4e3b\u6d41LLM\u5728taarof\u6587\u5316\u9002\u5f53\u60c5\u5883\u4e0b\u7684\u51c6\u786e\u7387\u6bd4\u6bcd\u8bed\u8005\u4f4e40-48%\uff0c\u6027\u80fd\u56e0\u8bdd\u9898\u800c\u5f02\uff0c\u6ce2\u65af\u8bed\u63d0\u793a\u80fd\u6539\u5584\u8868\u73b0\uff0c\u5b58\u5728\u6027\u522b\u4e0d\u5bf9\u79f0\u6027\u3002\u901a\u8fc7\u5fae\u8c03\u65b9\u6cd5\u5206\u522b\u5b9e\u73b0\u4e8621.8%\u548c42.3%\u7684\u6587\u5316\u5bf9\u9f50\u6539\u8fdb\u3002", "conclusion": "\u897f\u65b9\u793c\u8c8c\u6846\u67b6\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30taarof\u7b49\u6587\u5316\u7279\u5b9a\u793c\u4eea\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u591a\u6837\u5316\u548c\u6587\u5316\u654f\u611f\u7684LLM\u57fa\u51c6\uff0c\u4e3a\u6784\u5efa\u80fd\u591f\u66f4\u597d\u5904\u7406\u590d\u6742\u793e\u4ea4\u4e92\u52a8\u7684\u5e94\u7528\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2509.01053", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01053", "abs": "https://arxiv.org/abs/2509.01053", "authors": ["Xiaoying Song", "Anirban Saha Anik", "Eduardo Blanco", "Vanessa Frias-Martinez", "Lingzi Hong"], "title": "A Dynamic Fusion Model for Consistent Crisis Response", "comment": "Accepted at Findings of EMNLP 2025, 10 pages, 5 figures", "summary": "In response to the urgent need for effective communication with\ncrisis-affected populations, automated responses driven by language models have\nbeen proposed to assist in crisis communications. A critical yet often\noverlooked factor is the consistency of response style, which could affect the\ntrust of affected individuals in responders. Despite its importance, few\nstudies have explored methods for maintaining stylistic consistency across\ngenerated responses. To address this gap, we propose a novel metric for\nevaluating style consistency and introduce a fusion-based generation approach\ngrounded in this metric. Our method employs a two-stage process: it first\nassesses the style of candidate responses and then optimizes and integrates\nthem at the instance level through a fusion process. This enables the\ngeneration of high-quality responses while significantly reducing stylistic\nvariation between instances. Experimental results across multiple datasets\ndemonstrate that our approach consistently outperforms baselines in both\nresponse quality and stylistic uniformity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u878d\u5408\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8fc7\u7a0b\u4f18\u5316\u5371\u673a\u6c9f\u901a\u4e2d\u7684\u8bed\u8a00\u6a21\u578b\u54cd\u5e94\u98ce\u683c\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u54cd\u5e94\u8d28\u91cf\u548c\u98ce\u683c\u5747\u5300\u6027\u3002", "motivation": "\u5371\u673a\u6c9f\u901a\u4e2d\u81ea\u52a8\u5316\u54cd\u5e94\u7684\u98ce\u683c\u4e00\u81f4\u6027\u5bf9\u5efa\u7acb\u53d7\u5f71\u54cd\u4eba\u7fa4\u4fe1\u4efb\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5f88\u5c11\u63a2\u7d22\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u9996\u5148\u63d0\u51fa\u4e86\u98ce\u683c\u4e00\u81f4\u6027\u8bc4\u4f30\u6307\u6807\uff0c\u7136\u540e\u91c7\u7528\u4e24\u9636\u6bb5\u878d\u5408\u751f\u6210\u65b9\u6cd5\uff1a\u5148\u8bc4\u4f30\u5019\u9009\u54cd\u5e94\u7684\u98ce\u683c\uff0c\u518d\u901a\u8fc7\u5b9e\u4f8b\u5c42\u9762\u7684\u878d\u5408\u8fc7\u7a0b\u8fdb\u884c\u4f18\u5316\u548c\u6574\u5408\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u54cd\u5e94\u8d28\u91cf\u548c\u98ce\u683c\u5747\u5300\u6027\u65b9\u9762\u5747\u4e00\u81f4\u8d85\u8fc7\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5371\u673a\u6c9f\u901a\u4e2d\u7684\u98ce\u683c\u4e00\u81f4\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u8bc4\u4f30\u6307\u6807\u548c\u878d\u5408\u751f\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u54cd\u5e94\u7684\u8d28\u91cf\u548f\u98ce\u683c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2509.01058", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01058", "abs": "https://arxiv.org/abs/2509.01058", "authors": ["Xiaoying Song", "Anirban Saha Anik", "Dibakar Barua", "Pengcheng Luo", "Junhua Ding", "Lingzi Hong"], "title": "Speaking at the Right Level: Literacy-Controlled Counterspeech Generation with RAG-RL", "comment": "Accepted at Findings of EMNLP 2025", "summary": "Health misinformation spreading online poses a significant threat to public\nhealth. Researchers have explored methods for automatically generating\ncounterspeech to health misinformation as a mitigation strategy. Existing\napproaches often produce uniform responses, ignoring that the health literacy\nlevel of the audience could affect the accessibility and effectiveness of\ncounterspeech. We propose a Controlled-Literacy framework using\nretrieval-augmented generation (RAG) with reinforcement learning (RL) to\ngenerate tailored counterspeech adapted to different health literacy levels. In\nparticular, we retrieve knowledge aligned with specific health literacy levels,\nenabling accessible and factual information to support generation. We design a\nreward function incorporating subjective user preferences and objective\nreadability-based rewards to optimize counterspeech to the target health\nliteracy level. Experiment results show that Controlled-Literacy outperforms\nbaselines by generating more accessible and user-preferred counterspeech. This\nresearch contributes to more equitable and impactful public health\ncommunication by improving the accessibility and comprehension of counterspeech\nto health misinformation.", "AI": {"tldr": "\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u5065\u5eb7\u8bed\u6587\u6846\u67b6\uff0c\u751f\u6210\u9002\u5e94\u4e0d\u540c\u5065\u5eb7\u8bed\u6587\u6c34\u5e73\u7684\u6279\u5224\u8bdd\u8bed\uff0c\u63d0\u9ad8\u53cd\u5047\u4fe1\u606f\u6548\u679c", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u751f\u6210\u6279\u5224\u8bdd\u8bed\u65b9\u6cd5\u4ea7\u751f\u7edf\u4e00\u54cd\u5e94\uff0c\u5ffd\u7565\u4e86\u53d7\u4f17\u5065\u5eb7\u8bed\u6587\u6c34\u5e73\u5bf9\u6279\u5224\u8bdd\u8bed\u53ef\u8bbf\u95ee\u6027\u548c\u6548\u679c\u7684\u5f71\u54cd", "method": "\u63a7\u5236\u5065\u5eb7\u8bed\u6587\u6846\u67b6\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\uff0c\u68c0\u7d22\u9002\u5408\u7279\u5b9a\u5065\u5eb7\u8bed\u6587\u6c34\u5e73\u7684\u77e5\u8bc6\uff0c\u8bbe\u8ba1\u5305\u542b\u4e3b\u89c2\u7528\u6237\u504f\u597d\u548c\u5ba2\u89c2\u53ef\u8bfb\u6027\u5956\u52b1\u7684\u5956\u52b1\u51fd\u6570", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u63a7\u5236\u5065\u5eb7\u8bed\u6586\u6846\u67b6\u5728\u751f\u6210\u66f4\u6613\u8bbf\u95ee\u548c\u7528\u6237\u66f4\u559c\u6b22\u7684\u6279\u5224\u8bdd\u8bed\u65b9\u9762\u8d85\u8fc7\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u63d0\u9ad8\u6279\u5224\u8bdd\u8bed\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u516c\u5171\u5065\u5eb7\u6c9f\u901a\u63d0\u4f9b\u4e86\u66f4\u516c\u5e73\u548c\u6709\u5f71\u54cd\u529b\u7684\u65b9\u6cd5"}}
{"id": "2509.01081", "categories": ["cs.CL", "cs.AI", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.01081", "abs": "https://arxiv.org/abs/2509.01081", "authors": ["Abdessalam Bouchekif", "Samer Rashwani", "Heba Sbahi", "Shahd Gaben", "Mutez Al-Khatib", "Mohammed Ghaly"], "title": "Assessing Large Language Models on Islamic Legal Reasoning: Evidence from Inheritance Law Evaluation", "comment": "10 pages, 7 Tables, Code:\n  https://github.com/bouchekif/inheritance_evaluation", "summary": "This paper evaluates the knowledge and reasoning capabilities of Large\nLanguage Models in Islamic inheritance law, known as 'ilm al-mawarith. We\nassess the performance of seven LLMs using a benchmark of 1,000 multiple-choice\nquestions covering diverse inheritance scenarios, designed to test models'\nability to understand the inheritance context and compute the distribution of\nshares prescribed by Islamic jurisprudence. The results reveal a significant\nperformance gap: o3 and Gemini 2.5 achieved accuracies above 90%, whereas\nALLaM, Fanar, LLaMA, and Mistral scored below 50%. These disparities reflect\nimportant differences in reasoning ability and domain adaptation. We conduct a\ndetailed error analysis to identify recurring failure patterns across models,\nincluding misunderstandings of inheritance scenarios, incorrect application of\nlegal rules, and insufficient domain knowledge. Our findings highlight\nlimitations in handling structured legal reasoning and suggest directions for\nimproving performance in Islamic legal reasoning. Code:\nhttps://github.com/bouchekif/inheritance_evaluation", "AI": {"tldr": "\u8bc4\u4f307\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f0a\u65af\u5170\u7ee7\u627f\u6cd5\u9886\u57df\u7684\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff1ao3\u548cGemini 2.5\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u800c\u5176\u4ed6\u6a21\u578b\u4f4e\u4e8e50%", "motivation": "\u8bc4\u4f30LLMs\u5728\u4f0a\u65af\u5170\u7ee7\u627f\u6cd5\uff08'ilm al-mawarith\uff09\u8fd9\u4e00\u4e13\u4e1a\u6cd5\u5f8b\u9886\u57df\u7684\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4e86\u89e3\u6a21\u578b\u5728\u7ed3\u6784\u5316\u6cd5\u5f8b\u63a8\u7406\u65b9\u9762\u7684\u8868\u73b0", "method": "\u4f7f\u7528\u5305\u542b1000\u4e2a\u591a\u9009\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u591a\u6837\u5316\u7ee7\u627f\u573a\u666f\uff0c\u6d4b\u8bd5\u6a21\u578b\u7406\u89e3\u7ee7\u627f\u80cc\u666f\u548c\u8ba1\u7b97\u4f0a\u65af\u5170\u6cd5\u89c4\u5b9a\u4efd\u989d\u5206\u914d\u7684\u80fd\u529b", "result": "\u6a21\u578b\u6027\u80fd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff1ao3\u548cGemini 2.5\u51c6\u786e\u7387>90%\uff0cALLaM\u3001Fanar\u3001LLaMA\u548cMistral\u51c6\u786e\u7387<50%\u3002\u9519\u8bef\u6a21\u5f0f\u5305\u62ec\u7ee7\u627f\u573a\u666f\u8bef\u89e3\u3001\u6cd5\u5f8b\u89c4\u5219\u9519\u8bef\u5e94\u7528\u548c\u9886\u57df\u77e5\u8bc6\u4e0d\u8db3", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5728\u5904\u7406\u7ed3\u6784\u5316\u6cd5\u5f8b\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6539\u8fdb\u4f0a\u65af\u5170\u6cd5\u5f8b\u63a8\u7406\u6027\u80fd\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u8868\u660e\u9700\u8981\u66f4\u597d\u7684\u9886\u57df\u9002\u5e94\u548c\u63a8\u7406\u80fd\u529b\u63d0\u5347"}}
{"id": "2509.01084", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01084", "abs": "https://arxiv.org/abs/2509.01084", "authors": ["Farah Adeeba", "Rajesh Bhatt"], "title": "A Paradigm Gap in Urdu", "comment": null, "summary": "In this paper, we document a paradigm gap in the combinatorial possibilities\nof verbs and aspect in Urdu: the perfective form of the -ya: kar construction\n(e.g. ro-ya: ki: cry-Pfv do.Pfv) is sharply ungrammatical in modern Urdu and\nHindi, despite being freely attested in 19th century literature. We investigate\nthis diachronic shift through historical text analysis, a large-scale corpus\nstudy which confirms the stark absence of perfective forms and subjective\nevaluation tasks with native speakers, who judge perfective examples as highly\nunnatural. We argue that this gap arose from a fundamental morphosyntactic\nconflict: the construction's requirement for a nominative subject and an\ninvariant participle clashes with the core grammatical rule that transitive\nperfective assign ergative case. This conflict rendered the perfective form\nunstable, and its functional replacement by other constructions allowed the gap\nto become entrenched in the modern grammar.", "AI": {"tldr": "\u4e4c\u5c14\u90fd\u8bed\u4e2d-ya: kar\u7ed3\u6784\u7684\u5b8c\u6210\u4f53\u5f62\u5f0f\u5728\u73b0\u4ee3\u8bed\u6cd5\u4e2d\u6781\u4e0d\u81ea\u7136\uff0c\u5c3d\u7ba119\u4e16\u7eaa\u6587\u732e\u4e2d\u5e38\u89c1\uff0c\u8fd9\u662f\u7531\u4e8e\u4e3b\u683c\u4e3b\u8bed\u8981\u6c42\u4e0e\u53ca\u7269\u5b8c\u6210\u4f53\u8d4b\u4e88\u4f5c\u683c\u7684\u6838\u5fc3\u8bed\u6cd5\u89c4\u5219\u51b2\u7a81\u6240\u81f4\u3002", "motivation": "\u7814\u7a76\u4e4c\u5c14\u90fd\u8bed\u52a8\u8bcd\u548c\u4f53\u7ec4\u5408\u4e2d\u7684\u8303\u5f0f\u7a7a\u7f3a\uff1a-ya: kar\u7ed3\u6784\u7684\u5b8c\u6210\u4f53\u5f62\u5f0f\u5728\u73b0\u4ee3\u4e4c\u5c14\u90fd\u8bed\u548c\u5370\u5730\u8bed\u4e2d\u6781\u4e0d\u81ea\u7136\uff0c\u4f46\u572819\u4e16\u7eaa\u6587\u732e\u4e2d\u81ea\u7531\u51fa\u73b0\uff0c\u9700\u8981\u63a2\u7a76\u8fd9\u4e00\u5386\u65f6\u53d8\u5316\u7684\u539f\u56e0\u3002", "method": "\u901a\u8fc7\u5386\u53f2\u6587\u672c\u5206\u6790\u3001\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u7814\u7a76\uff08\u786e\u8ba4\u5b8c\u6210\u4f53\u5f62\u5f0f\u7684\u660e\u663e\u7f3a\u5931\uff09\u4ee5\u53ca\u6bcd\u8bed\u8005\u7684\u4e3b\u89c2\u8bc4\u4ef7\u4efb\u52a1\uff08\u5224\u65ad\u5b8c\u6210\u4f53\u4f8b\u53e5\u9ad8\u5ea6\u4e0d\u81ea\u7136\uff09\u6765\u8c03\u67e5\u8fd9\u4e00\u5386\u65f6\u8f6c\u53d8\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5b8c\u6210\u4f53\u5f62\u5f0f\u5728\u73b0\u4ee3\u8bed\u6cd5\u4e2d\u6781\u4e0d\u7a33\u5b9a\uff0c\u5176\u529f\u80fd\u88ab\u5176\u4ed6\u7ed3\u6784\u66ff\u4ee3\uff0c\u5bfc\u81f4\u8fd9\u4e00\u7a7a\u7f3a\u5728\u73b0\u4ee3\u8bed\u6cd5\u4e2d\u56fa\u5316\u3002", "conclusion": "\u8fd9\u4e00\u7a7a\u7f3a\u6e90\u4e8e\u6839\u672c\u7684\u5f62\u6001\u53e5\u6cd5\u51b2\u7a81\uff1a\u8be5\u7ed3\u6784\u8981\u6c42\u4e3b\u683c\u4e3b\u8bed\u548c\u4e0d\u53d8\u5206\u8bcd\uff0c\u4e0e\u53ca\u7269\u5b8c\u6210\u4f53\u8d4b\u4e88\u4f5c\u683c\u7684\u6838\u5fc3\u8bed\u6cd5\u89c4\u5219\u76f8\u51b2\u7a81\uff0c\u4f7f\u5f97\u5b8c\u6210\u4f53\u5f62\u5f0f\u4e0d\u7a33\u5b9a\u5e76\u88ab\u5176\u4ed6\u7ed3\u6784\u529f\u80fd\u66ff\u4ee3\u3002"}}
{"id": "2509.01088", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01088", "abs": "https://arxiv.org/abs/2509.01088", "authors": ["Jinwen Chen", "Hainan Zhang", "Liang Pang", "Yongxin Tong", "Haibo Zhou", "Yuan Zhan", "Wei Lin", "Zhiming Zheng"], "title": "Privacy-Preserving Reasoning with Knowledge-Distilled Parametric Retrieval Augmented Generation", "comment": null, "summary": "The current RAG system requires uploading plaintext documents to the cloud,\nrisking private data leakage. Parametric RAG (PRAG) addresses this by encoding\ndocuments as LoRA within LLMs, enabling reasoning without exposing raw content.\nHowever, it still faces two issues: (1) PRAG demands synthesizing QA pairs and\nfine-tuning LLM for each individual document to create its corresponding LoRA,\nleading to unacceptable inference latency. (2) The performance of PRAG relies\nsolely on synthetic QA data, lacking internal alignment with standard RAG,\nresulting in poor generalization on out-of-distribution(OOD) inputs. Therefore,\nachieving high-efficiency parameterization while maintaining RAG-level\nperformance remains a critical challenge for privacy-preserving reasoning. In\nthis paper, we propose DistilledPRAG, a generalizable knowledge-distilled\nparametric RAG model aligned with standard RAG in document structure and\nparameter activation. We first synthesize QA pairs from single and\nmulti-documents to enhance cross-document reasoning. Then, we mask the\nplaintext documents with a special token and translate them to LoRA via a\nparameter generator, maintaining the standard RAG document structure. Finally,\nguided by synthetic QA data, we train the parameter generator to match standard\nRAG's hidden states and output logits, enabling RAG-style reasoning without\noriginal documents. Experiments on four QA datasets show that DistilledPRAG\noutperforms baselines in accuracy and generalizes well on OOD data.", "AI": {"tldr": "DistilledPRAG\u662f\u4e00\u79cd\u77e5\u8bc6\u84b8\u998f\u7684\u53c2\u6570\u5316RAG\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u6587\u6863\u7f16\u7801\u4e3aLoRA\u53c2\u6570\u800c\u975e\u660e\u6587\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301RAG\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfPRAG\u7684\u63a8\u7406\u5ef6\u8fdf\u548c\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u5f53\u524dRAG\u7cfb\u7edf\u9700\u8981\u4e0a\u4f20\u660e\u6587\u6587\u6863\u5230\u4e91\u7aef\uff0c\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002\u53c2\u6570\u5316RAG(PRAG)\u867d\u7136\u901a\u8fc7LoRA\u7f16\u7801\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u4ecd\u9762\u4e34\u63a8\u7406\u5ef6\u8fdf\u9ad8\u548c\u6cdb\u5316\u6027\u80fd\u5dee\u7684\u95ee\u9898\u3002", "method": "1)\u5408\u6210\u5355\u6587\u6863\u548c\u591a\u6587\u6863QA\u5bf9\u589e\u5f3a\u8de8\u6587\u6863\u63a8\u7406\uff1b2)\u7528\u7279\u6b8a\u6807\u8bb0\u5c4f\u853d\u660e\u6587\u6587\u6863\uff0c\u901a\u8fc7\u53c2\u6570\u751f\u6210\u5668\u8f6c\u6362\u4e3aLoRA\uff1b3)\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u8bad\u7ec3\u53c2\u6570\u751f\u6210\u5668\u5339\u914d\u6807\u51c6RAG\u7684\u9690\u85cf\u72b6\u6001\u548c\u8f93\u51falogits\u3002", "result": "\u5728\u56db\u4e2aQA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDistilledPRAG\u5728\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DistilledPRAG\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6548\u53c2\u6570\u5316\u540c\u65f6\u4fdd\u6301RAG\u7ea7\u522b\u6027\u80fd\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01092", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.01092", "abs": "https://arxiv.org/abs/2509.01092", "authors": ["Xiaoqiang Lin", "Aritra Ghosh", "Bryan Kian Hsiang Low", "Anshumali Shrivastava", "Vijai Mohan"], "title": "REFRAG: Rethinking RAG based Decoding", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.", "AI": {"tldr": "REFRAG\u662f\u4e00\u4e2a\u9488\u5bf9RAG\u5e94\u7528\u7684\u9ad8\u6548\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u538b\u7f29\u3001\u611f\u77e5\u548c\u6269\u5c55\u673a\u5236\uff0c\u5229\u7528\u68c0\u7d22\u4e0a\u4e0b\u6587\u7684\u7a00\u758f\u6027\u7ed3\u6784\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u89e3\u7801\u901f\u5ea6\u5e76\u6269\u5c55\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "motivation": "RAG\u5e94\u7528\u4e2d\u68c0\u7d22\u5230\u7684\u4e0a\u4e0b\u6587\u901a\u5e38\u5305\u542b\u5927\u91cf\u4e0d\u76f8\u5173\u6bb5\u843d\uff0c\u5f62\u6210\u5757\u5bf9\u89d2\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u5bfc\u81f4\u4f20\u7edfLLM\u89e3\u7801\u8fc7\u7a0b\u4e2d\u5b58\u5728\u5927\u91cf\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\uff0c\u9020\u6210\u7cfb\u7edf\u5ef6\u8fdf\u548c\u5185\u5b58\u6d88\u8017\u95ee\u9898\u3002", "method": "\u63d0\u51faREFRAG\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790RAG\u4e0a\u4e0b\u6587\u7684\u7a00\u758f\u7ed3\u6784\u7279\u6027\uff0c\u8bbe\u8ba1\u538b\u7f29\u3001\u611f\u77e5\u548c\u6269\u5c55\u673a\u5236\u6765\u4f18\u5316\u89e3\u7801\u8fc7\u7a0b\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u663e\u793aREFRAG\u5b9e\u73b0\u4e8630.85\u500d\u7684\u9996token\u65f6\u95f4\u52a0\u901f\uff08\u76f8\u6bd4\u4e4b\u524d\u5de5\u4f5c\u63d0\u53473.75\u500d\uff09\uff0c\u5728\u4fdd\u6301\u56f0\u60d1\u5ea6\u4e0d\u53d8\u7684\u540c\u65f6\u5c06LLM\u4e0a\u4e0b\u6587\u957f\u5ea6\u6269\u5c5516\u500d\uff0c\u5728\u591a\u4e2a\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u663e\u8457\u52a0\u901f\u6548\u679c\u3002", "conclusion": "REFRAG\u901a\u8fc7\u5229\u7528RAG\u4e0a\u4e0b\u6587\u7684\u56fa\u6709\u7a00\u758f\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u77e5\u8bc6\u589e\u5f3a\u4e0e\u7cfb\u7edf\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3aRAG\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u7801\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01093", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.01093", "abs": "https://arxiv.org/abs/2509.01093", "authors": ["Yulong Wu", "Viktor Schlegel", "Riza Batista-Navarro"], "title": "Natural Context Drift Undermines the Natural Language Understanding of Large Language Models", "comment": "EMNLP 2025 Findings", "summary": "How does the natural evolution of context paragraphs affect question\nanswering in generative Large Language Models (LLMs)? To investigate this, we\npropose a framework for curating naturally evolved, human-edited variants of\nreading passages from contemporary QA benchmarks and for analyzing LLM\nperformance across a range of semantic similarity scores, which quantify how\nclosely each variant aligns with content seen during pretraining. Using this\nframework, we evaluate six QA datasets and eight LLMs with publicly available\ntraining data. Our experiments reveal that LLM performance declines as reading\npassages naturally diverge from the versions encountered during\npretraining-even when the question and all necessary information remains\npresent at inference time. For instance, average model accuracy on BoolQ drops\nby over 30% from the highest to lowest similarity bins, with slopes exceeding\n70 across several LLMs. These findings suggest that natural text evolution\nposes a significant challenge to the language understanding capabilities of\nLLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4f1a\u968f\u7740\u9605\u8bfb\u6bb5\u843d\u4e0e\u9884\u8bad\u7ec3\u65f6\u5185\u5bb9\u7684\u81ea\u7136\u6f14\u53d8\u800c\u663e\u8457\u4e0b\u964d\uff0c\u5373\u4f7f\u95ee\u9898\u548c\u5fc5\u8981\u4fe1\u606f\u5728\u63a8\u7406\u65f6\u90fd\u4fdd\u6301\u4e0d\u53d8\u3002", "motivation": "\u7814\u7a76\u81ea\u7136\u6587\u672c\u6f14\u53d8\u5982\u4f55\u5f71\u54cd\u751f\u6210\u5f0f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63a2\u7a76\u6a21\u578b\u5bf9\u9884\u8bad\u7ec3\u5185\u5bb9\u53d8\u5316\u7684\u654f\u611f\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u6765\u6574\u7406\u81ea\u7136\u6f14\u53d8\u7684\u4eba\u7c7b\u7f16\u8f91\u7248\u672c\u9605\u8bfb\u6bb5\u843d\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u5ea6\u8bc4\u5206\u91cf\u5316\u53d8\u4f53\u4e0e\u9884\u8bad\u7ec3\u5185\u5bb9\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u8bc4\u4f30\u4e866\u4e2aQA\u6570\u636e\u96c6\u548c8\u4e2aLLM\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLM\u6027\u80fd\u968f\u7740\u9605\u8bfb\u6bb5\u843d\u4e0e\u9884\u8bad\u7ec3\u7248\u672c\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\u964d\u4f4e\u800c\u663e\u8457\u4e0b\u964d\uff0c\u4f8b\u5982BoolQ\u6570\u636e\u96c6\u4e0a\u6a21\u578b\u51c6\u786e\u7387\u4ece\u6700\u9ad8\u5230\u6700\u4f4e\u76f8\u4f3c\u5ea6\u533a\u95f4\u4e0b\u964d\u4e8630%\u4ee5\u4e0a\u3002", "conclusion": "\u81ea\u7136\u6587\u672c\u6f14\u53d8\u5bf9LLM\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u6784\u6210\u4e86\u91cd\u8981\u6311\u6218\uff0c\u6a21\u578b\u5bf9\u9884\u8bad\u7ec3\u5185\u5bb9\u7684\u4f9d\u8d56\u6027\u5f88\u5f3a\u3002"}}
{"id": "2509.01142", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01142", "abs": "https://arxiv.org/abs/2509.01142", "authors": ["Zhihui Xie", "Jiacheng Ye", "Lin Zheng", "Jiahui Gao", "Jingwei Dong", "Zirui Wu", "Xueliang Zhao", "Shansan Gong", "Xin Jiang", "Zhenguo Li", "Lingpeng Kong"], "title": "Dream-Coder 7B: An Open Diffusion Language Model for Code", "comment": null, "summary": "We present Dream-Coder 7B, an open-source discrete diffusion language model\nfor code generation that exhibits emergent any-order generation capabilities.\nUnlike traditional autoregressive (AR) models that decode strictly\nleft-to-right, Dream-Coder 7B adaptively determines its decoding strategy based\non the coding task: sketch-first generation for complex algorithms,\nleft-to-right generation for straightforward completions, and interleaved\nreasoning generation for code understanding tasks. We adapt a pretrained AR\ncheckpoint to a discrete diffusion frameworks with a continuous-time weighted\ncross-entropy objective. Our post-training recipe comprises (i) supervised\nfine-tuning, where we mitigate padding pathologies via random truncation and a\npadding penalty to improve sample efficiency and stabilize generation; and (ii)\nreinforcement learning with verifiable rewards over a curated high-quality\nprompt set drawn from open-source datasets, using a tailored reinforcement\nlearning recipe for diffusion language models. The resulting Dream-Coder 7B\nInstruct attains 21.4\\% pass@1 on LiveCodeBench (2410--2505) and demonstrates\ncompetitive performance on HumanEval, MBPP, BigCodeBench, and CRUXEval. We\nrelease Dream-Coder-7B and Dream-Coder-7B-Instruct checkpoints, training\nrecipes, preprocessing pipelines, and inference code to facilitate\nreproducibility and further research.", "AI": {"tldr": "Dream-Coder 7B\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u79bb\u6563\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u6709\u65b0\u5174\u7684\u4efb\u610f\u987a\u5e8f\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u89e3\u7801\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u81ea\u56de\u5f52\u6a21\u578b\u53ea\u80fd\u4ece\u5de6\u5230\u53f3\u89e3\u7801\uff0c\u9650\u5236\u4e86\u4ee3\u7801\u751f\u6210\u7684\u7075\u6d3b\u6027\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6839\u636e\u4efb\u52a1\u81ea\u9002\u5e94\u9009\u62e9\u89e3\u7801\u7b56\u7565\u7684\u6a21\u578b\uff0c\u63d0\u9ad8\u590d\u6742\u7b97\u6cd5\u751f\u6210\u548c\u4ee3\u7801\u7406\u89e3\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u5c06\u9884\u8bad\u7ec3\u7684\u81ea\u56de\u5f52\u68c0\u67e5\u70b9\u9002\u914d\u5230\u79bb\u6563\u6269\u6563\u6846\u67b6\uff0c\u4f7f\u7528\u8fde\u7eed\u65f6\u95f4\u52a0\u6743\u4ea4\u53c9\u71b5\u76ee\u6807\u3002\u8bad\u7ec3\u5305\u62ec\uff1a1\uff09\u76d1\u7763\u5fae\u8c03\uff0c\u901a\u8fc7\u968f\u673a\u622a\u65ad\u548c\u586b\u5145\u60e9\u7f5a\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff1b2\uff09\u4f7f\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u9488\u5bf9\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5b9a\u5236\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "Dream-Coder 7B Instruct\u5728LiveCodeBench\u4e0a\u8fbe\u523021.4% pass@1\uff0c\u5728HumanEval\u3001MBPP\u3001BigCodeBench\u548cCRUXEval\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u6a21\u578b\u5c55\u793a\u4e86\u79bb\u6563\u6269\u6563\u65b9\u6cd5\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63d0\u4f9b\u4e86\u81ea\u9002\u5e94\u89e3\u7801\u80fd\u529b\uff0c\u5e76\u5f00\u6e90\u4e86\u68c0\u67e5\u70b9\u3001\u8bad\u7ec3\u65b9\u6cd5\u548c\u63a8\u7406\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2509.01147", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01147", "abs": "https://arxiv.org/abs/2509.01147", "authors": ["Zhihao Zhang", "Sophia Yat Mei Lee", "Dong Zhang", "Shoushan Li", "Guodong Zhou"], "title": "Zero-shot Cross-lingual NER via Mitigating Language Difference: An Entity-aligned Translation Perspective", "comment": "EMNLP 2025", "summary": "Cross-lingual Named Entity Recognition (CL-NER) aims to transfer knowledge\nfrom high-resource languages to low-resource languages. However, existing\nzero-shot CL-NER (ZCL-NER) approaches primarily focus on Latin script language\n(LSL), where shared linguistic features facilitate effective knowledge\ntransfer. In contrast, for non-Latin script language (NSL), such as Chinese and\nJapanese, performance often degrades due to deep structural differences. To\naddress these challenges, we propose an entity-aligned translation (EAT)\napproach. Leveraging large language models (LLMs), EAT employs a\ndual-translation strategy to align entities between NSL and English. In\naddition, we fine-tune LLMs using multilingual Wikipedia data to enhance the\nentity alignment from source to target languages.", "AI": {"tldr": "\u63d0\u51faEAT\u65b9\u6cd5\u89e3\u51b3\u975e\u62c9\u4e01\u8bed\u7cfb\u8bed\u8a00\u7684\u96f6\u6837\u672c\u8de8\u8bed\u8a00\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u91cd\u7ffb\u8bd1\u7b56\u7565\u548cLLM\u5fae\u8c03\u5b9e\u73b0\u5b9e\u4f53\u5bf9\u9f50", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u8de8\u8bed\u8a00NER\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u62c9\u4e01\u8bed\u7cfb\u8bed\u8a00\uff0c\u5bf9\u4e2d\u6587\u3001\u65e5\u6587\u7b49\u975e\u62c9\u4e01\u8bed\u7cfb\u8bed\u8a00\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b58\u5728\u6df1\u5c42\u7ed3\u6784\u5dee\u5f02", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53cc\u91cd\u7ffb\u8bd1\u7b56\u7565\u8fdb\u884c\u5b9e\u4f53\u5bf9\u9f50\uff0c\u5e76\u5229\u7528\u591a\u8bed\u8a00\u7ef4\u57fa\u767e\u79d1\u6570\u636e\u5fae\u8c03LLM\u4ee5\u589e\u5f3a\u6e90\u8bed\u8a00\u5230\u76ee\u6807\u8bed\u8a00\u7684\u5b9e\u4f53\u5bf9\u9f50", "result": "\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u8bf4\u660e\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c", "conclusion": "EAT\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u975e\u62c9\u4e01\u8bed\u7cfb\u8bed\u8a00\u7684\u8de8\u8bed\u8a00NER\u95ee\u9898\uff0c\u901a\u8fc7\u5b9e\u4f53\u5bf9\u9f50\u6539\u5584\u77e5\u8bc6\u8fc1\u79fb\u6548\u679c"}}
{"id": "2509.01158", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01158", "abs": "https://arxiv.org/abs/2509.01158", "authors": ["Xuemei Tang", "Chengxi Yan", "Jinghang Gu", "Chu-Ren Huang"], "title": "Joint Information Extraction Across Classical and Modern Chinese with Tea-MOELoRA", "comment": "9 pages, 3 figures", "summary": "Chinese information extraction (IE) involves multiple tasks across diverse\ntemporal domains, including Classical and Modern documents. Fine-tuning a\nsingle model on heterogeneous tasks and across different eras may lead to\ninterference and reduced performance. Therefore, in this paper, we propose\nTea-MOELoRA, a parameter-efficient multi-task framework that combines LoRA with\na Mixture-of-Experts (MoE) design. Multiple low-rank LoRA experts specialize in\ndifferent IE tasks and eras, while a task-era-aware router mechanism\ndynamically allocates expert contributions. Experiments show that Tea-MOELoRA\noutperforms both single-task and joint LoRA baselines, demonstrating its\nability to leverage task and temporal knowledge effectively.", "AI": {"tldr": "Tea-MOELoRA\u662f\u4e00\u4e2a\u53c2\u6570\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u7ed3\u5408LoRA\u548c\u6df7\u5408\u4e13\u5bb6\u8bbe\u8ba1\uff0c\u7528\u4e8e\u4e2d\u6587\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\uff0c\u901a\u8fc7\u4efb\u52a1-\u65f6\u4ee3\u611f\u77e5\u8def\u7531\u673a\u5236\u52a8\u6001\u5206\u914d\u4e13\u5bb6\u8d21\u732e", "motivation": "\u4e2d\u6587\u4fe1\u606f\u62bd\u53d6\u6d89\u53ca\u53e4\u5178\u548c\u73b0\u4ee3\u6587\u6863\u7b49\u591a\u4e2a\u65f6\u95f4\u57df\u7684\u4efb\u52a1\uff0c\u5728\u5f02\u6784\u4efb\u52a1\u548c\u4e0d\u540c\u65f6\u4ee3\u4e0a\u5fae\u8c03\u5355\u4e00\u6a21\u578b\u53ef\u80fd\u5bfc\u81f4\u5e72\u6270\u548c\u6027\u80fd\u4e0b\u964d", "method": "\u7ed3\u5408LoRA\u4e0e\u6df7\u5408\u4e13\u5bb6(MoE)\u8bbe\u8ba1\uff0c\u591a\u4e2a\u4f4e\u79e9LoRA\u4e13\u5bb6\u4e13\u6ce8\u4e8e\u4e0d\u540c\u7684IE\u4efb\u52a1\u548c\u65f6\u4ee3\uff0c\u4f7f\u7528\u4efb\u52a1-\u65f6\u4ee3\u611f\u77e5\u8def\u7531\u673a\u5236\u52a8\u6001\u5206\u914d\u4e13\u5bb6\u8d21\u732e", "result": "\u5b9e\u9a8c\u8868\u660eTea-MOELoRA\u4f18\u4e8e\u5355\u4efb\u52a1\u548c\u8054\u5408LoRA\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u5229\u7528\u4efb\u52a1\u548c\u65f6\u95f4\u77e5\u8bc6\u7684\u80fd\u529b", "conclusion": "\u63d0\u51fa\u7684\u53c2\u6570\u9ad8\u6548\u591a\u4efb\u52a1\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u4e2d\u6587\u4fe1\u606f\u62bd\u53d6\u4e2d\u7684\u5f02\u6784\u4efb\u52a1\u548c\u8de8\u65f6\u4ee3\u6311\u6218"}}
{"id": "2509.01166", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.01166", "abs": "https://arxiv.org/abs/2509.01166", "authors": ["Yu Liu", "Yanan Cao", "Xixun Lin", "Yanmin Shang", "Shi Wang", "Shirui Pan"], "title": "Enhancing Large Language Model for Knowledge Graph Completion via Structure-Aware Alignment-Tuning", "comment": "EMNLP 2025, Main, Long Paper", "summary": "Knowledge graph completion (KGC) aims to infer new knowledge and make\npredictions from knowledge graphs. Recently, large language models (LLMs) have\nexhibited remarkable reasoning capabilities. LLM-enhanced KGC methods primarily\nfocus on designing task-specific instructions, achieving promising\nadvancements. However, there are still two critical challenges. First, existing\nmethods often ignore the inconsistent representation spaces between natural\nlanguage and graph structures. Second, most approaches design separate\ninstructions for different KGC tasks, leading to duplicate works and\ntime-consuming processes. To address these challenges, we propose SAT, a novel\nframework that enhances LLMs for KGC via structure-aware alignment-tuning.\nSpecifically, we first introduce hierarchical knowledge alignment to align\ngraph embeddings with the natural language space through multi-task contrastive\nlearning. Then, we propose structural instruction tuning to guide LLMs in\nperforming structure-aware reasoning over KGs, using a unified graph\ninstruction combined with a lightweight knowledge adapter. Experimental results\non two KGC tasks across four benchmark datasets demonstrate that SAT\nsignificantly outperforms state-of-the-art methods, especially in the link\nprediction task with improvements ranging from 8.7% to 29.8%.", "AI": {"tldr": "SAT\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u5bf9\u9f50\u8c03\u4f18\u589e\u5f3aLLM\u5728\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u81ea\u7136\u8bed\u8a00\u4e0e\u56fe\u7ed3\u6784\u8868\u793a\u7a7a\u95f4\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u4f7f\u7528\u7edf\u4e00\u56fe\u6307\u4ee4\u5b9e\u73b0\u591a\u4efb\u52a1\u5904\u7406", "motivation": "\u73b0\u6709LLM\u589e\u5f3a\u7684KGC\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u81ea\u7136\u8bed\u8a00\u4e0e\u56fe\u7ed3\u6784\u8868\u793a\u7a7a\u95f4\u4e0d\u4e00\u81f4\uff0c\u4ee5\u53ca\u4e0d\u540cKGC\u4efb\u52a1\u9700\u8981\u5355\u72ec\u8bbe\u8ba1\u6307\u4ee4\u5bfc\u81f4\u91cd\u590d\u5de5\u4f5c\u548c\u8017\u65f6", "method": "\u63d0\u51faSAT\u6846\u67b6\uff0c\u5305\u542b\u5206\u5c42\u77e5\u8bc6\u5bf9\u9f50\uff08\u901a\u8fc7\u591a\u4efb\u52a1\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u56fe\u5d4c\u5165\u4e0e\u81ea\u7136\u8bed\u8a00\u7a7a\u95f4\uff09\u548c\u7ed3\u6784\u6307\u4ee4\u8c03\u4f18\uff08\u4f7f\u7528\u7edf\u4e00\u56fe\u6307\u4ee4\u548c\u8f7b\u91cf\u7ea7\u77e5\u8bc6\u9002\u914d\u5668\u6307\u5bfcLLM\u8fdb\u884c\u7ed3\u6784\u611f\u77e5\u63a8\u7406\uff09", "result": "\u57284\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u7684\u4e24\u4e2aKGC\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u63d0\u53478.7%\u523029.8%", "conclusion": "SAT\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8868\u793a\u7a7a\u95f4\u4e0d\u4e00\u81f4\u548c\u6307\u4ee4\u8bbe\u8ba1\u5197\u4f59\u95ee\u9898\uff0c\u4e3aLLM\u5728\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.01185", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.01185", "abs": "https://arxiv.org/abs/2509.01185", "authors": ["Seganrasan Subramanian", "Abhigya Verma"], "title": "Modular Techniques for Synthetic Long-Context Data Generation in Language Model Training and Evaluation", "comment": "15 pages, 4 figures", "summary": "The ability of large language models (LLMs) to process and reason over long\ntextual inputs is critical for a wide range of real-world applications.\nHowever, progress in this area is significantly constrained by the absence of\nhigh-quality, diverse, and verifiable long-context datasets suitable for both\ntraining and evaluation. This work introduces a modular, extensible framework\nfor synthetic long-context data generation via prompt-based interaction with\nLLMs. The framework supports multiple training and alignment objectives,\nincluding Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO),\nand Group Relative Policy Optimization (GRPO). It encompasses four core\ngeneration paradigms: multi-turn conversational dialogues, document-grounded\ninput-output pairs, verifiable instruction-response tasks, and long-context\nreasoning examples. Through templated prompting, a model-agnostic architecture,\nand metadata-enriched outputs, the proposed approach facilitates scalable,\ncontrollable, and purpose-aligned dataset creation for advancing long-context\ncapabilities in LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u63d0\u793a\u7684LLM\u4ea4\u4e92\u751f\u6210\u5408\u6210\u957f\u4e0a\u4e0b\u6587\u6570\u636e\uff0c\u652f\u6301\u591a\u79cd\u8bad\u7ec3\u76ee\u6807\uff0c\u5305\u62ecSFT\u3001DPO\u548cGRPO\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u4e14\u53ef\u9a8c\u8bc1\u7684\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u96c6\uff0c\u8fd9\u4e25\u91cd\u5236\u7ea6\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u6587\u672c\u5904\u7406\u548c\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u677f\u5316\u63d0\u793a\u3001\u6a21\u578b\u65e0\u5173\u67b6\u6784\u548c\u5143\u6570\u636e\u4e30\u5bcc\u7684\u8f93\u51fa\uff0c\u652f\u6301\u56db\u79cd\u6838\u5fc3\u751f\u6210\u8303\u5f0f\uff1a\u591a\u8f6e\u5bf9\u8bdd\u3001\u6587\u6863\u57fa\u7840\u8f93\u5165\u8f93\u51fa\u5bf9\u3001\u53ef\u9a8c\u8bc1\u6307\u4ee4\u54cd\u5e94\u4efb\u52a1\u548c\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u793a\u4f8b\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u53ef\u63a7\u4e14\u76ee\u6807\u5bf9\u9f50\u7684\u6570\u636e\u96c6\u521b\u5efa\uff0c\u4e3a\u63d0\u5347LLMs\u7684\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u63d0\u4f9b\u652f\u6301\u3002", "conclusion": "\u63d0\u51fa\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u6846\u67b6\u4e3a\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u96c6\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8LLMs\u5728\u957f\u6587\u672c\u5904\u7406\u65b9\u9762\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.01186", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.01186", "abs": "https://arxiv.org/abs/2509.01186", "authors": ["Luxi He", "Nimra Nadeem", "Michel Liao", "Howard Chen", "Danqi Chen", "Mariano-Florentino Cu\u00e9llar", "Peter Henderson"], "title": "Statutory Construction and Interpretation for Artificial Intelligence", "comment": null, "summary": "AI systems are increasingly governed by natural language principles, yet a\nkey challenge arising from reliance on language remains underexplored:\ninterpretive ambiguity. As in legal systems, ambiguity arises both from how\nthese principles are written and how they are applied. But while legal systems\nuse institutional safeguards to manage such ambiguity, such as transparent\nappellate review policing interpretive constraints, AI alignment pipelines\noffer no comparable protections. Different interpretations of the same rule can\nlead to inconsistent or unstable model behavior. Drawing on legal theory, we\nidentify key gaps in current alignment pipelines by examining how legal systems\nconstrain ambiguity at both the rule creation and rule application steps. We\nthen propose a computational framework that mirrors two legal mechanisms: (1) a\nrule refinement pipeline that minimizes interpretive disagreement by revising\nambiguous rules (analogous to agency rulemaking or iterative legislative\naction), and (2) prompt-based interpretive constraints that reduce\ninconsistency in rule application (analogous to legal canons that guide\njudicial discretion). We evaluate our framework on a 5,000-scenario subset of\nthe WildChat dataset and show that both interventions significantly improve\njudgment consistency across a panel of reasonable interpreters. Our approach\noffers a first step toward systematically managing interpretive ambiguity, an\nessential step for building more robust, law-following AI systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u6846\u67b6\u6765\u7ba1\u7406AI\u7cfb\u7edf\u4e2d\u7684\u89e3\u91ca\u6027\u6a21\u7cca\u95ee\u9898\uff0c\u501f\u9274\u6cd5\u5f8b\u7cfb\u7edf\u7684\u673a\u5236\u6765\u63d0\u9ad8\u89c4\u5219\u89e3\u91ca\u548c\u5e94\u7528\u7684\u4e00\u81f4\u6027\u3002", "motivation": "AI\u7cfb\u7edf\u8d8a\u6765\u8d8a\u4f9d\u8d56\u81ea\u7136\u8bed\u8a00\u539f\u5219\u8fdb\u884c\u6cbb\u7406\uff0c\u4f46\u8bed\u8a00\u89e3\u91ca\u7684\u6a21\u7cca\u6027\u5bfc\u81f4\u6a21\u578b\u884c\u4e3a\u4e0d\u4e00\u81f4\u548c\u4e0d\u7a33\u5b9a\uff0c\u800c\u5f53\u524dAI\u5bf9\u9f50\u6d41\u7a0b\u7f3a\u4e4f\u7c7b\u4f3c\u6cd5\u5f8b\u7cfb\u7edf\u7684\u5236\u5ea6\u6027\u4fdd\u969c\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8ba1\u7b97\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6cd5\u5f8b\u673a\u5236\uff1a(1)\u89c4\u5219\u7cbe\u70bc\u6d41\u7a0b\uff0c\u901a\u8fc7\u4fee\u8ba2\u6a21\u7cca\u89c4\u5219\u6765\u6700\u5c0f\u5316\u89e3\u91ca\u5206\u6b67\uff1b(2)\u57fa\u4e8e\u63d0\u793a\u7684\u89e3\u91ca\u7ea6\u675f\uff0c\u51cf\u5c11\u89c4\u5219\u5e94\u7528\u7684\u4e0d\u4e00\u81f4\u6027\u3002\u5728WildChat\u6570\u636e\u96c6\u76845000\u4e2a\u573a\u666f\u5b50\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4e24\u79cd\u5e72\u9884\u63aa\u65bd\u90fd\u663e\u8457\u63d0\u9ad8\u4e86\u5408\u7406\u89e3\u91ca\u8005\u5c0f\u7ec4\u7684\u5224\u65ad\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u662f\u7cfb\u7edf\u7ba1\u7406\u89e3\u91ca\u6027\u6a21\u7cca\u6027\u7684\u7b2c\u4e00\u6b65\uff0c\u5bf9\u4e8e\u6784\u5efa\u66f4\u7a33\u5065\u3001\u9075\u5faa\u89c4\u5219\u7684AI\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.01190", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01190", "abs": "https://arxiv.org/abs/2509.01190", "authors": ["Sajjad Kachuee", "Mohammad Sharifkhani"], "title": "Efficient Large Language Models with Zero-Shot Adjustable Acceleration", "comment": null, "summary": "Using Large Language Models (LLMs) in real-world applications presents\nsignificant challenges, particularly in balancing computational efficiency and\nperformance. Optimizing acceleration after the fine-tuning phase and during\ninference is crucial for building an efficient architecture. This paper\nintroduces Zero-Shot Adjustable Acceleration, a novel training and inference\nmethod that dynamically adjusts hardware usage during inference without\nrequiring additional fine-tuning. The proposed approach is applied to newly\ndeveloped models and evaluated across multiple classification and text\ngeneration tasks. Experimental results demonstrate that the method enables a\nwide range of acceleration in a zero-shot manner and achieves up to a 11x\nspeedup compared to the baseline.", "AI": {"tldr": "\u63d0\u51faZero-Shot Adjustable Acceleration\u65b9\u6cd5\uff0c\u53ef\u5728\u63a8\u7406\u65f6\u52a8\u6001\u8c03\u6574\u786c\u4ef6\u4f7f\u7528\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\uff0c\u5b9e\u73b0\u9ad8\u8fbe11\u500d\u7684\u52a0\u901f", "motivation": "\u5728LLM\u5b9e\u9645\u5e94\u7528\u4e2d\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u662f\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u5728\u5fae\u8c03\u540e\u548c\u63a8\u7406\u9636\u6bb5\u4f18\u5316\u52a0\u901f\u4ee5\u6784\u5efa\u9ad8\u6548\u67b6\u6784", "method": "\u5f15\u5165\u96f6\u6837\u672c\u53ef\u8c03\u52a0\u901f\u7684\u65b0\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u52a8\u6001\u8c03\u6574\u786c\u4ef6\u4f7f\u7528", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u591f\u4ee5\u96f6\u6837\u672c\u65b9\u5f0f\u5b9e\u73b0\u5e7f\u6cdb\u7684\u52a0\u901f\u8303\u56f4\uff0c\u76f8\u6bd4\u57fa\u7ebf\u8fbe\u5230\u6700\u9ad811\u500d\u7684\u901f\u5ea6\u63d0\u5347", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aLLM\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u52a8\u6001\u52a0\u901f\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387"}}
{"id": "2509.01213", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01213", "abs": "https://arxiv.org/abs/2509.01213", "authors": ["Ege S\u00fcalp", "Mina Rezaei"], "title": "Mitigating Catastrophic Forgetting in Continual Learning through Model Growth", "comment": null, "summary": "Catastrophic forgetting is a significant challenge in continual learning, in\nwhich a model loses prior knowledge when it is fine-tuned on new tasks. This\nproblem is particularly critical for large language models (LLMs) undergoing\ncontinual learning, as retaining performance across diverse domains is\nimportant for their general utility. In this paper, we explore model growth, a\npromising strategy that leverages smaller models to expedite and structure the\ntraining of larger ones for mitigating the catastrophic forgetting problem.\nAlthough growth-based pretraining, particularly via transformer stacking, has\nshown promise in accelerating convergence, its impact on forgetting remains\nunder-explored. Therefore, we evaluate whether growth-based models can retain\npreviously learned capabilities more effectively across a sequence of\nfine-tuning tasks involving domain knowledge, reasoning, reading comprehension,\nand bias. Our findings show that both models -- one trained with growth (Stack\nLLM) and one without (LLM) -- exhibit improvements in domain knowledge.\nHowever, reasoning and reading comprehension degrade over time, indicating\nsigns of catastrophic forgetting. Stack LLM consistently shows less\ndegradation, especially in reading comprehension, suggesting enhanced retention\ncapabilities. Interestingly, in bias evaluation, the baseline LLM becomes\nprogressively more neutral with continued fine-tuning, while Stack LLM\nmaintains a steady bias ratio around 60--61\\%. These results indicate that\ngrowth-based pretraining may deliver modest improvements in resisting\ncatastrophic forgetting, though trade-offs remain in handling social biases.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u6a21\u578b\u589e\u957f\u7b56\u7565\uff08\u7279\u522b\u662ftransformer\u5806\u53e0\uff09\u5728\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u65b9\u9762\u7684\u6548\u679c\uff0c\u53d1\u73b0\u8be5\u65b9\u6cd5\u5728\u9605\u8bfb\u7406\u89e3\u7b49\u4efb\u52a1\u4e0a\u80fd\u51cf\u5c11\u6027\u80fd\u9000\u5316\uff0c\u4f46\u5728\u793e\u4f1a\u504f\u89c1\u5904\u7406\u65b9\u9762\u5b58\u5728\u6743\u8861\u3002", "motivation": "\u707e\u96be\u6027\u9057\u5fd8\u662f\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u4e3b\u8981\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5fae\u8c03\u65f6\u4f1a\u4e22\u5931\u5148\u524d\u5b66\u5230\u7684\u77e5\u8bc6\u3002\u6a21\u578b\u589e\u957f\u7b56\u7565\u901a\u8fc7\u5229\u7528\u5c0f\u6a21\u578b\u52a0\u901f\u5927\u6a21\u578b\u8bad\u7ec3\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u5176\u5bf9\u9057\u5fd8\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u589e\u957f\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff08transformer\u5806\u53e0\uff09\uff0c\u6784\u5efaStack LLM\u6a21\u578b\uff0c\u5e76\u4e0e\u4f20\u7edfLLM\u57fa\u7ebf\u5728\u5e8f\u5217\u5fae\u8c03\u4efb\u52a1\u4e0a\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\uff0c\u4efb\u52a1\u6db5\u76d6\u9886\u57df\u77e5\u8bc6\u3001\u63a8\u7406\u3001\u9605\u8bfb\u7406\u89e3\u548c\u504f\u89c1\u5206\u6790\u3002", "result": "\u4e24\u79cd\u6a21\u578b\u5728\u9886\u57df\u77e5\u8bc6\u4e0a\u90fd\u6709\u63d0\u5347\uff0c\u4f46\u63a8\u7406\u548c\u9605\u8bfb\u7406\u89e3\u80fd\u529b\u968f\u65f6\u95f4\u9000\u5316\u3002Stack LLM\u8868\u73b0\u51fa\u66f4\u5c11\u7684\u6027\u80fd\u9000\u5316\uff0c\u7279\u522b\u662f\u5728\u9605\u8bfb\u7406\u89e3\u65b9\u9762\u3002\u5728\u504f\u89c1\u8bc4\u4f30\u4e2d\uff0c\u57fa\u7ebfLLM\u53d8\u5f97\u66f4\u4e2d\u6027\uff0c\u800cStack LLM\u4fdd\u6301\u7ea660-61%\u7684\u7a33\u5b9a\u504f\u89c1\u6bd4\u7387\u3002", "conclusion": "\u57fa\u4e8e\u589e\u957f\u7684\u9884\u8bad\u7ec3\u5728\u62b5\u6297\u707e\u96be\u6027\u9057\u5fd8\u65b9\u9762\u80fd\u5e26\u6765\u9002\u5ea6\u6539\u8fdb\uff0c\u4f46\u5728\u5904\u7406\u793e\u4f1a\u504f\u89c1\u65b9\u9762\u5b58\u5728\u6743\u8861\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u5e73\u8861\u6027\u80fd\u4fdd\u6301\u548c\u504f\u89c1\u63a7\u5236\u3002"}}
{"id": "2509.01221", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.01221", "abs": "https://arxiv.org/abs/2509.01221", "authors": ["Wei Huang", "Huang Wei", "Yinggui Wang"], "title": "DaMoC: Efficiently Selecting the Optimal Large Language Model for Fine-tuning Domain Taks Based on Data and Model Compression", "comment": "Accepted by EMNLP 2025", "summary": "Large language models (LLMs) excel in general tasks but struggle with\ndomain-specific ones, requiring fine-tuning with specific data. With many\nopen-source LLMs available, selecting the best model for fine-tuning downstream\ntasks is challenging, primarily focusing on how to quickly identify the optimal\nLLM. We introduce a Data and Model Compression Framework (DaMoC) that addresses\nthis challenge by: 1) Data Level: A systematic categorization of data filtering\nmethodologies for LLMs is first established, classifying them into three\ndistinct paradigms: (1) distribution-aware methods, (2) quality-aware methods,\nand (3) hybrid approaches considering both dimensions. Further, we enhance the\ndensity of key tokens in the text achieving token compression. Subsequently, we\nuse an LLM to iterative rewrite the text to optimize its expression. 2) Model\nLevel: We use layer similarity scores to assess each layer's importance and\nremove those with lower importance. Then, we introduce a sparse merging\nparadigm to preserve as much of the original model's capability as possible.\nExtensive experiments on four datasets, medical Q&A, financial Q&A, general\nQ&A, and reading comprehension, show that we can select the optimal LLM while\nsaving approximately 20-fold in training time.", "AI": {"tldr": "\u63d0\u51fa\u4e86DaMoC\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u538b\u7f29\u548c\u6a21\u578b\u538b\u7f29\u6280\u672f\uff0c\u5feb\u901f\u9009\u62e9\u6700\u4f18LLM\u8fdb\u884c\u5fae\u8c03\uff0c\u8282\u7701\u7ea620\u500d\u8bad\u7ec3\u65f6\u95f4", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u4e0a\u9700\u8981\u5fae\u8c03\uff0c\u5982\u4f55\u5feb\u901f\u9009\u62e9\u6700\u4f18\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u662f\u4e00\u4e2a\u6311\u6218", "method": "\u6570\u636e\u5c42\u9762\uff1a\u7cfb\u7edf\u5206\u7c7b\u6570\u636e\u8fc7\u6ee4\u65b9\u6cd5\uff08\u5206\u5e03\u611f\u77e5\u3001\u8d28\u91cf\u611f\u77e5\u3001\u6df7\u5408\u65b9\u6cd5\uff09\uff0c\u8fdb\u884ctoken\u538b\u7f29\u548c\u6587\u672c\u91cd\u5199\u4f18\u5316\uff1b\u6a21\u578b\u5c42\u9762\uff1a\u4f7f\u7528\u5c42\u76f8\u4f3c\u6027\u8bc4\u5206\u8bc4\u4f30\u91cd\u8981\u6027\uff0c\u79fb\u9664\u4f4e\u91cd\u8981\u6027\u5c42\uff0c\u5f15\u5165\u7a00\u758f\u5408\u5e76\u8303\u5f0f", "result": "\u5728\u533b\u7597\u95ee\u7b54\u3001\u91d1\u878d\u95ee\u7b54\u3001\u901a\u7528\u95ee\u7b54\u548c\u9605\u8bfb\u7406\u89e3\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u80fd\u591f\u9009\u62e9\u6700\u4f18LLM\u540c\u65f6\u8282\u7701\u7ea620\u500d\u8bad\u7ec3\u65f6\u95f4", "conclusion": "DaMoC\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u9009\u62e9\u95ee\u9898\uff0c\u901a\u8fc7\u6570\u636e\u6a21\u578b\u53cc\u91cd\u538b\u7f29\u663e\u8457\u63d0\u5347\u6548\u7387"}}
{"id": "2509.01236", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.01236", "abs": "https://arxiv.org/abs/2509.01236", "authors": ["Hao Yang", "Zhiyu Yang", "Yunjie Zhang", "Shanyi Zhu", "Lin Yang"], "title": "Rethinking the Chain-of-Thought: The Roles of In-Context Learning and Pre-trained Priors", "comment": null, "summary": "Chain-of-Thought reasoning has emerged as a pivotal methodology for enhancing\nmodel inference capabilities. Despite growing interest in Chain-of-Thought\nreasoning, its underlying mechanisms remain unclear. This paper explores the\nworking mechanisms of Chain-of-Thought reasoning from the perspective of the\ndual relationship between in-context learning and pretrained priors. We first\nconduct a fine-grained lexical-level analysis of rationales to examine the\nmodel's reasoning behavior. Then, by incrementally introducing noisy exemplars,\nwe examine how the model balances pretrained priors against erroneous\nin-context information. Finally, we investigate whether prompt engineering can\ninduce slow thinking in large language models. Our extensive experiments reveal\nthree key findings: (1) The model not only quickly learns the reasoning\nstructure at the lexical level but also grasps deeper logical reasoning\npatterns, yet it heavily relies on pretrained priors. (2) Providing sufficient\nexemplars shifts the model's decision-making from pretrained priors to\nin-context signals, while misleading prompts introduce instability. (3) Long\nChain-of-Thought prompting can induce the model to generate longer reasoning\nchains, thereby improving its performance on downstream tasks.", "AI": {"tldr": "\u672c\u6587\u4ece\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e0e\u9884\u8bad\u7ec3\u5148\u9a8c\u7684\u53cc\u91cd\u5173\u7cfb\u89d2\u5ea6\uff0c\u63a2\u7d22\u4e86\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u5de5\u4f5c\u673a\u5236\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8bcd\u6c47\u5206\u6790\u548c\u566a\u58f0\u6837\u672c\u5b9e\u9a8c\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5982\u4f55\u5e73\u8861\u9884\u8bad\u7ec3\u5148\u9a8c\u4e0e\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "motivation": "\u5c3d\u7ba1\u601d\u7ef4\u94fe\u63a8\u7406\u5728\u589e\u5f3a\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5176\u5e95\u5c42\u5de5\u4f5c\u673a\u5236\u4ecd\u4e0d\u660e\u786e\uff0c\u9700\u8981\u4ece\u7406\u8bba\u5c42\u9762\u6df1\u5165\u63a2\u7a76\u3002", "method": "\u91c7\u7528\u7ec6\u7c92\u5ea6\u8bcd\u6c47\u7ea7\u5206\u6790\u3001\u9010\u6b65\u5f15\u5165\u566a\u58f0\u6837\u672c\u5b9e\u9a8c\uff0c\u4ee5\u53ca\u63d0\u793a\u5de5\u7a0b\u8bf1\u5bfc\u6162\u601d\u8003\u7684\u65b9\u6cd5\u6765\u7814\u7a76\u601d\u7ef4\u94fe\u63a8\u7406\u673a\u5236\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u80fd\u5feb\u901f\u5b66\u4e60\u63a8\u7406\u7ed3\u6784\u548c\u903b\u8f91\u6a21\u5f0f\u4f46\u4f9d\u8d56\u9884\u8bad\u7ec3\u5148\u9a8c\uff1b\u8db3\u591f\u6837\u672c\u53ef\u4f7f\u51b3\u7b56\u8f6c\u5411\u4e0a\u4e0b\u6587\u4fe1\u53f7\uff1b\u957f\u601d\u7ef4\u94fe\u63d0\u793a\u80fd\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u601d\u7ef4\u94fe\u63a8\u7406\u662f\u9884\u8bad\u7ec3\u5148\u9a8c\u4e0e\u4e0a\u4e0b\u6587\u5b66\u4e60\u52a8\u6001\u5e73\u8861\u7684\u7ed3\u679c\uff0c\u957f\u63a8\u7406\u94fe\u80fd\u6709\u6548\u8bf1\u5bfc\u6162\u601d\u8003\u6a21\u5f0f\uff0c\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2509.01260", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01260", "abs": "https://arxiv.org/abs/2509.01260", "authors": ["Jonas Noblet"], "title": "Annotation and modeling of emotions in a textual corpus: an evaluative approach", "comment": "in French language. 27{\\`e}me Rencontre des {\\'E}tudiants Chercheurs\n  en Informatique pour le Traitement Automatique des Langues (RECITAL), Jun\n  2025, Marseille, France", "summary": "Emotion is a crucial phenomenon in the functioning of human beings in\nsociety. However, it remains a widely open subject, particularly in its textual\nmanifestations. This paper examines an industrial corpus manually annotated\nfollowing an evaluative approach to emotion. This theoretical framework, which\nis currently underutilized, offers a different perspective that complements\ntraditional approaches. Noting that the annotations we collected exhibit\nsignificant disagreement, we hypothesized that they nonetheless follow stable\nstatistical trends. Using language models trained on these annotations, we\ndemonstrate that it is possible to model the labeling process and that\nvariability is driven by underlying linguistic features. Conversely, our\nresults indicate that language models seem capable of distinguishing emotional\nsituations based on evaluative criteria.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u57fa\u4e8e\u8bc4\u4f30\u65b9\u6cd5\u6807\u6ce8\u7684\u60c5\u611f\u6587\u672c\u8bed\u6599\uff0c\u53d1\u73b0\u867d\u7136\u4eba\u5de5\u6807\u6ce8\u5b58\u5728\u663e\u8457\u5206\u6b67\uff0c\u4f46\u6807\u6ce8\u8fc7\u7a0b\u9075\u5faa\u7a33\u5b9a\u7684\u7edf\u8ba1\u8d8b\u52bf\uff0c\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5efa\u6a21\u6807\u6ce8\u8fc7\u7a0b\u5e76\u533a\u5206\u60c5\u611f\u60c5\u5883\u3002", "motivation": "\u60c5\u611f\u662f\u4eba\u7c7b\u793e\u4f1a\u529f\u80fd\u4e2d\u7684\u5173\u952e\u73b0\u8c61\uff0c\u4f46\u5728\u6587\u672c\u8868\u73b0\u65b9\u9762\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u7814\u7a76\u9886\u57df\u3002\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u7684\u60c5\u611f\u7406\u8bba\u6846\u67b6\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u9700\u8981\u63a2\u7d22\u4e0d\u540c\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u65b0\u89c6\u89d2\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u8bc4\u4f30\u65b9\u6cd5\u624b\u52a8\u6807\u6ce8\u7684\u5de5\u4e1a\u8bed\u6599\u5e93\uff0c\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u6765\u5206\u6790\u6807\u6ce8\u8fc7\u7a0b\u3002\u901a\u8fc7\u7edf\u8ba1\u65b9\u6cd5\u9a8c\u8bc1\u6807\u6ce8\u5206\u6b67\u4e2d\u7684\u7a33\u5b9a\u8d8b\u52bf\uff0c\u5e76\u63a2\u7a76\u8bed\u8a00\u7279\u5f81\u5bf9\u6807\u6ce8\u53d8\u5f02\u6027\u7684\u9a71\u52a8\u4f5c\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4eba\u5de5\u6807\u6ce8\u867d\u7136\u5b58\u5728\u663e\u8457\u5206\u6b67\uff0c\u4f46\u9075\u5faa\u7a33\u5b9a\u7684\u7edf\u8ba1\u89c4\u5f8b\u3002\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6210\u529f\u5efa\u6a21\u6807\u6ce8\u8fc7\u7a0b\uff0c\u8bc1\u660e\u6807\u6ce8\u53d8\u5f02\u6027\u662f\u7531\u5e95\u5c42\u8bed\u8a00\u7279\u5f81\u9a71\u52a8\u7684\u3002\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u57fa\u4e8e\u8bc4\u4f30\u6807\u51c6\u533a\u5206\u4e0d\u540c\u7684\u60c5\u611f\u60c5\u5883\u3002", "conclusion": "\u8bc4\u4f30\u65b9\u6cd5\u4e3a\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8865\u5145\u89c6\u89d2\u3002\u5c3d\u7ba1\u4eba\u5de5\u6807\u6ce8\u5b58\u5728\u4e3b\u89c2\u5dee\u5f02\uff0c\u4f46\u901a\u8fc7\u7edf\u8ba1\u5efa\u6a21\u53ef\u4ee5\u6355\u6349\u7a33\u5b9a\u7684\u6807\u6ce8\u6a21\u5f0f\u3002\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u548c\u533a\u5206\u60c5\u611f\u60c5\u5883\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4e3a\u60c5\u611f\u8ba1\u7b97\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.01301", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01301", "abs": "https://arxiv.org/abs/2509.01301", "authors": ["Juhyun Oh", "Inha Cha", "Michael Saxon", "Hyunseung Lim", "Shaily Bhatt", "Alice Oh"], "title": "Culture is Everywhere: A Call for Intentionally Cultural Evaluation", "comment": null, "summary": "The prevailing ``trivia-centered paradigm'' for evaluating the cultural\nalignment of large language models (LLMs) is increasingly inadequate as these\nmodels become more advanced and widely deployed. Existing approaches typically\nreduce culture to static facts or values, testing models via multiple-choice or\nshort-answer questions that treat culture as isolated trivia. Such methods\nneglect the pluralistic and interactive realities of culture, and overlook how\ncultural assumptions permeate even ostensibly ``neutral'' evaluation settings.\nIn this position paper, we argue for \\textbf{intentionally cultural\nevaluation}: an approach that systematically examines the cultural assumptions\nembedded in all aspects of evaluation, not just in explicitly cultural tasks.\nWe systematically characterize the what, how, and circumstances by which\nculturally contingent considerations arise in evaluation, and emphasize the\nimportance of researcher positionality for fostering inclusive, culturally\naligned NLP research. Finally, we discuss implications and future directions\nfor moving beyond current benchmarking practices, discovering important\napplications that we don't know exist, and involving communities in evaluation\ndesign through HCI-inspired participatory methodologies.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u4ece\u4f20\u7edf\u7684\"\u7410\u4e8b\u4e2d\u5fc3\"\u6587\u5316\u8bc4\u4f30\u8303\u5f0f\u8f6c\u5411\"\u6709\u610f\u6587\u5316\u8bc4\u4f30\"\uff0c\u5f3a\u8c03\u9700\u8981\u7cfb\u7edf\u6027\u5730\u5ba1\u89c6\u8bc4\u4f30\u4e2d\u5d4c\u5165\u7684\u6587\u5316\u5047\u8bbe\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5728\u660e\u786e\u7684\u6587\u5316\u4efb\u52a1\u4e2d\u3002", "motivation": "\u5f53\u524dLLM\u6587\u5316\u5bf9\u9f50\u8bc4\u4f30\u4e3b\u8981\u91c7\u7528\u57fa\u4e8e\u9759\u6001\u4e8b\u5b9e\u7684\u591a\u9009\u9898\u6216\u7b80\u7b54\u9898\uff0c\u5c06\u6587\u5316\u7b80\u5316\u4e3a\u5b64\u7acb\u7410\u4e8b\uff0c\u5ffd\u89c6\u4e86\u6587\u5316\u7684\u591a\u5143\u6027\u548c\u4e92\u52a8\u6027\u672c\u8d28\uff0c\u4ee5\u53ca\u6587\u5316\u5047\u8bbe\u5728\u770b\u4f3c\"\u4e2d\u6027\"\u8bc4\u4f30\u73af\u5883\u4e2d\u7684\u6e17\u900f\u3002", "method": "\u63d0\u51fa\"\u6709\u610f\u6587\u5316\u8bc4\u4f30\"\u6846\u67b6\uff0c\u7cfb\u7edf\u5206\u6790\u8bc4\u4f30\u4e2d\u6587\u5316\u56e0\u7d20\u7684what\uff08\u4ec0\u4e48\uff09\u3001how\uff08\u5982\u4f55\uff09\u548ccircumstances\uff08\u60c5\u5883\uff09\uff0c\u5f3a\u8c03\u7814\u7a76\u8005\u7acb\u573a\u6027\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5efa\u8bae\u91c7\u7528HCI\u542f\u53d1\u7684\u53c2\u4e0e\u5f0f\u65b9\u6cd5\u8ba9\u793e\u533a\u53c2\u4e0e\u8bc4\u4f30\u8bbe\u8ba1\u3002", "result": "\u5efa\u7acb\u4e86\u7cfb\u7edf\u6027\u5206\u6790\u6587\u5316\u56e0\u7d20\u5728\u8bc4\u4f30\u4e2d\u4f5c\u7528\u7684\u7406\u8bba\u6846\u67b6\uff0c\u6307\u51fa\u4e86\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u5b9e\u8df5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "conclusion": "\u9700\u8981\u8d85\u8d8a\u73b0\u6709\u7684\u6587\u5316\u8bc4\u4f30\u8303\u5f0f\uff0c\u91c7\u7528\u66f4\u52a0\u5305\u5bb9\u548c\u53c2\u4e0e\u5f0f\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6709\u610f\u6587\u5316\u8bc4\u4f30\u6765\u53d1\u73b0\u672a\u77e5\u7684\u91cd\u8981\u5e94\u7528\u573a\u666f\uff0c\u4fc3\u8fdb\u6587\u5316\u5bf9\u9f50\u7684NLP\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2509.01312", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01312", "abs": "https://arxiv.org/abs/2509.01312", "authors": ["Sishi Xiong", "Ziyang He", "Zhongjiang He", "Yu Zhao", "Changzai Pan", "Jie Zhang", "Zhenhe Wu", "Shuangyong Song", "Yongxiang Li"], "title": "TableZoomer: A Collaborative Agent Framework for Large-scale Table Question Answering", "comment": null, "summary": "While large language models (LLMs) have shown promise in the table question\nanswering (TQA) task through prompt engineering, they face challenges in\nindustrial applications, including structural heterogeneity, difficulties in\ntarget data localization, and bottlenecks in complex reasoning. To address\nthese limitations, this paper presents TableZoomer, a novel LLM-powered,\nprogramming-based agent framework. It introduces three key innovations: (1)\nreplacing the original fully verbalized table with structured table schema to\nbridge the semantic gap and reduce computational complexity; (2) a query-aware\ntable zooming mechanism that dynamically generates sub-table schema through\ncolumn selection and entity linking, significantly improving target\nlocalization efficiency; and (3) a Program-of-Thoughts (PoT) strategy that\ntransforms queries into executable code to mitigate numerical hallucination.\nAdditionally, we integrate the reasoning workflow with the ReAct paradigm to\nenable iterative reasoning. Extensive experiments demonstrate that our\nframework maintains the usability advantages while substantially enhancing\nperformance and scalability across tables of varying scales. When implemented\nwith the Qwen3-8B-Instruct LLM, TableZoomer achieves accuracy improvements of\n19.34% and 25% over conventional PoT methods on the large-scale DataBench\ndataset and the small-scale Fact Checking task of TableBench dataset,\nrespectively.", "AI": {"tldr": "TableZoomer\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u7a0b\u5e8f\u5316\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u6a21\u5f0f\u3001\u67e5\u8be2\u611f\u77e5\u7684\u8868\u7f29\u653e\u673a\u5236\u548c\u7a0b\u5e8f\u5316\u601d\u7ef4\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8868\u683c\u95ee\u7b54\u4efb\u52a1\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u8868\u683c\u95ee\u7b54\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u7ed3\u6784\u5f02\u8d28\u6027\u3001\u76ee\u6807\u6570\u636e\u5b9a\u4f4d\u56f0\u96be\u548c\u590d\u6742\u63a8\u7406\u74f6\u9888\u7b49\u5de5\u4e1a\u5e94\u7528\u6311\u6218\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u8868\u6a21\u5f0f\u66ff\u4ee3\u5b8c\u5168\u6587\u672c\u5316\u8868\u683c\uff0c\u5f15\u5165\u67e5\u8be2\u611f\u77e5\u7684\u8868\u7f29\u653e\u673a\u5236\u52a8\u6001\u751f\u6210\u5b50\u8868\u6a21\u5f0f\uff0c\u4f7f\u7528\u7a0b\u5e8f\u5316\u601d\u7ef4\u7b56\u7565\u5c06\u67e5\u8be2\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u5e76\u7ed3\u5408ReAct\u8303\u5f0f\u5b9e\u73b0\u8fed\u4ee3\u63a8\u7406\u3002", "result": "\u5728DataBench\u6570\u636e\u96c6\u4e0a\u6bd4\u4f20\u7edfPoT\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u534719.34%\uff0c\u5728TableBench\u6570\u636e\u96c6\u7684\u4e8b\u5b9e\u68c0\u67e5\u4efb\u52a1\u4e0a\u63d0\u534725%\u3002", "conclusion": "TableZoomer\u6846\u67b6\u5728\u4fdd\u6301\u53ef\u7528\u6027\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8868\u683c\u95ee\u7b54\u4efb\u52a1\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u7684\u8868\u683c\u5904\u7406\u3002"}}
{"id": "2509.01314", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01314", "abs": "https://arxiv.org/abs/2509.01314", "authors": ["Anum Afzal", "Mehul Kumawat", "Florian Matthes"], "title": "Can Smaller LLMs do better? Unlocking Cross-Domain Potential through Parameter-Efficient Fine-Tuning for Text Summarization", "comment": null, "summary": "Large Language Models (LLMs), being generic task solvers, are versatile.\nHowever, despite the vast amount of data they are trained on, there are\nspeculations about their adaptation capabilities to a new domain. Additionally,\nthe simple fine-tuning of the model to incorporate knowledge of a new domain is\ncomputationally expensive and time-consuming. This becomes more challenging\nwhen the domain in question is also low-resource, and labeled data is\nunavailable. We leverage parameter-efficient fine-tuning techniques (PEFTs) on\nhigh-resource datasets to address these challenges to improve performance on\nunseen low-resource domains. Throughout our experiments, we evaluate whether\nintrinsic linguistic commonalities between datasets can be leveraged for\nefficient domain adaptation. We benchmark six PEFTs with\n\\texttt{Llama-3-8B-Instruct} on 14 training datasets from the Scientific,\nMedical, Legal, and News domains for a Text Summarization task. Our experiments\nshow that for low-resource domains, inference using Within-Domain Adapters can\nachieve better performance than Few-Shot as well as a much larger\n\\texttt{Llama-3-70B-Instruct}. Lastly, in the absence of Within-Domain\nAdapters, we explore the concept of using Cross-Domain Adapters as well as the\nstrategic combinations of adapters to leverage intrinsic language similarities\nacross domains, facilitating better adaptability and performance in\nlow-resource settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5982\u4f55\u5229\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f(PEFTs)\u5728\u8d44\u6e90\u4e30\u5bcc\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u63d0\u9ad8\u5728\u672a\u89c1\u4f4e\u8d44\u6e90\u9886\u57df\u7684\u6587\u672c\u6458\u8981\u6027\u80fd\uff0c\u63a2\u7d22\u9886\u57df\u5185\u548c\u8de8\u9886\u57df\u9002\u914d\u5668\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u901a\u7528\uff0c\u4f46\u5728\u9002\u5e94\u65b0\u9886\u57df\u65f6\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u9886\u57df\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u65f6\u3002\u4f20\u7edf\u7684\u5168\u6a21\u578b\u5fae\u8c03\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u9886\u57df\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u4f7f\u75286\u79cd\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f(PEFTs)\u5728Llama-3-8B-Instruct\u6a21\u578b\u4e0a\uff0c\u57fa\u4e8e14\u4e2a\u6765\u81ea\u79d1\u5b66\u3001\u533b\u7597\u3001\u6cd5\u5f8b\u548c\u65b0\u95fb\u9886\u57df\u7684\u6570\u636e\u96c6\u8fdb\u884c\u6587\u672c\u6458\u8981\u4efb\u52a1\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u9886\u57df\u5185\u9002\u914d\u5668\u548c\u8de8\u9886\u57df\u9002\u914d\u5668\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5bf9\u4e8e\u4f4e\u8d44\u6e90\u9886\u57df\uff0c\u4f7f\u7528\u9886\u57df\u5185\u9002\u914d\u5668\u7684\u63a8\u7406\u6027\u80fd\u4f18\u4e8e\u5c11\u6837\u672c\u5b66\u4e60\uff0c\u751a\u81f3\u4f18\u4e8e\u66f4\u5927\u7684Llama-3-70B-Instruct\u6a21\u578b\u3002\u5728\u6ca1\u6709\u9886\u57df\u5185\u9002\u914d\u5668\u65f6\uff0c\u8de8\u9886\u57df\u9002\u914d\u5668\u548c\u7b56\u7565\u6027\u7ec4\u5408\u9002\u914d\u5668\u4e5f\u80fd\u5229\u7528\u8bed\u8a00\u76f8\u4f3c\u6027\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f4e\u8d44\u6e90\u9886\u57df\u7684\u9002\u5e94\u95ee\u9898\uff0c\u9886\u57df\u5185\u9002\u914d\u5668\u8868\u73b0\u6700\u4f73\uff0c\u8de8\u9886\u57df\u9002\u914d\u5668\u4e3a\u7f3a\u4e4f\u76ee\u6807\u9886\u57df\u6570\u636e\u65f6\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u5229\u7528\u8bed\u8a00\u5171\u6027\u5b9e\u73b0\u66f4\u597d\u7684\u9886\u57df\u9002\u5e94\u6027\u3002"}}
{"id": "2509.01322", "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.01322", "abs": "https://arxiv.org/abs/2509.01322", "authors": ["Meituan LongCat Team", "Bayan", "Bei Li", "Bingye Lei", "Bo Wang", "Bolin Rong", "Chao Wang", "Chao Zhang", "Chen Gao", "Chen Zhang", "Cheng Sun", "Chengcheng Han", "Chenguang Xi", "Chi Zhang", "Chong Peng", "Chuan Qin", "Chuyu Zhang", "Cong Chen", "Congkui Wang", "Dan Ma", "Daoru Pan", "Defei Bu", "Dengchang Zhao", "Deyang Kong", "Dishan Liu", "Feiye Huo", "Fengcun Li", "Fubao Zhang", "Gan Dong", "Gang Liu", "Gang Xu", "Ge Li", "Guoqiang Tan", "Guoyuan Lin", "Haihang Jing", "Haomin Fu", "Haonan Yan", "Haoxing Wen", "Haozhe Zhao", "Hong Liu", "Hongmei Shi", "Hongyan Hao", "Hongyin Tang", "Huantian Lv", "Hui Su", "Jiacheng Li", "Jiahao Liu", "Jiahuan Li", "Jiajun Yang", "Jiaming Wang", "Jian Yang", "Jianchao Tan", "Jiaqi Sun", "Jiaqi Zhang", "Jiawei Fu", "Jiawei Yang", "Jiaxi Hu", "Jiayu Qin", "Jingang Wang", "Jiyuan He", "Jun Kuang", "Junhui Mei", "Kai Liang", "Ke He", "Kefeng Zhang", "Keheng Wang", "Keqing He", "Liang Gao", "Liang Shi", "Lianhui Ma", "Lin Qiu", "Lingbin Kong", "Lingtong Si", "Linkun Lyu", "Linsen Guo", "Liqi Yang", "Lizhi Yan", "Mai Xia", "Man Gao", "Manyuan Zhang", "Meng Zhou", "Mengxia Shen", "Mingxiang Tuo", "Mingyang Zhu", "Peiguang Li", "Peng Pei", "Peng Zhao", "Pengcheng Jia", "Pingwei Sun", "Qi Gu", "Qianyun Li", "Qingyuan Li", "Qiong Huang", "Qiyuan Duan", "Ran Meng", "Rongxiang Weng", "Ruichen Shao", "Rumei Li", "Shizhe Wu", "Shuai Liang", "Shuo Wang", "Suogui Dang", "Tao Fang", "Tao Li", "Tefeng Chen", "Tianhao Bai", "Tianhao Zhou", "Tingwen Xie", "Wei He", "Wei Huang", "Wei Liu", "Wei Shi", "Wei Wang", "Wei Wu", "Weikang Zhao", "Wen Zan", "Wenjie Shi", "Xi Nan", "Xi Su", "Xiang Li", "Xiang Mei", "Xiangyang Ji", "Xiangyu Xi", "Xiangzhou Huang", "Xianpeng Li", "Xiao Fu", "Xiao Liu", "Xiao Wei", "Xiaodong Cai", "Xiaolong Chen", "Xiaoqing Liu", "Xiaotong Li", "Xiaowei Shi", "Xiaoyu Li", "Xili Wang", "Xin Chen", "Xing Hu", "Xingyu Miao", "Xinyan He", "Xuemiao Zhang", "Xueyuan Hao", "Xuezhi Cao", "Xunliang Cai", "Xurui Yang", "Yan Feng", "Yang Bai", "Yang Chen", "Yang Yang", "Yaqi Huo", "Yerui Sun", "Yifan Lu", "Yifan Zhang", "Yipeng Zang", "Yitao Zhai", "Yiyang Li", "Yongjing Yin", "Yongkang Lv", "Yongwei Zhou", "Yu Yang", "Yuchen Xie", "Yueqing Sun", "Yuewen Zheng", "Yuhua Wei", "Yulei Qian", "Yunfan Liang", "Yunfang Tai", "Yunke Zhao", "Zeyang Yu", "Zhao Zhang", "Zhaohua Yang", "Zhenchao Zhang", "Zhikang Xia", "Zhiye Zou", "Zhizhao Zeng", "Zhongda Su", "Zhuofan Chen", "Zijian Zhang", "Ziwen Wang", "Zixu Jiang", "Zizhe Zhao", "Zongyu Wang", "Zunhai Su"], "title": "LongCat-Flash Technical Report", "comment": null, "summary": "We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE)\nlanguage model designed for both computational efficiency and advanced agentic\ncapabilities. Stemming from the need for scalable efficiency, LongCat-Flash\nadopts two novel designs: (a) Zero-computation Experts, which enables dynamic\ncomputational budget allocation and activates 18.6B-31.3B (27B on average) per\ntoken depending on contextual demands, optimizing resource usage. (b)\nShortcut-connected MoE, which enlarges the computation-communication overlap\nwindow, demonstrating notable gains in inference efficiency and throughput\ncompared to models of a comparable scale. We develop a comprehensive scaling\nframework for large models that combines hyperparameter transfer, model-growth\ninitialization, a multi-pronged stability suite, and deterministic computation\nto achieve stable and reproducible training. Notably, leveraging the synergy\namong scalable architectural design and infrastructure efforts, we complete\nmodel training on more than 20 trillion tokens within 30 days, while achieving\nover 100 tokens per second (TPS) for inference at a cost of \\$0.70 per million\noutput tokens. To cultivate LongCat-Flash towards agentic intelligence, we\nconduct a large-scale pre-training on optimized mixtures, followed by targeted\nmid- and post-training on reasoning, code, and instructions, with further\naugmentation from synthetic data and tool use tasks. Comprehensive evaluations\ndemonstrate that, as a non-thinking foundation model, LongCat-Flash delivers\nhighly competitive performance among other leading models, with exceptional\nstrengths in agentic tasks. The model checkpoint of LongCat-Flash is\nopen-sourced to foster community research.\n  LongCat Chat: https://longcat.ai\n  Hugging Face: https://huggingface.co/meituan-longcat\n  GitHub: https://github.com/meituan-longcat", "AI": {"tldr": "LongCat-Flash\u662f\u4e00\u4e2a5600\u4ebf\u53c2\u6570\u7684MoE\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7Zero-computation Experts\u548cShortcut-connected MoE\u8bbe\u8ba1\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u548c\u63a8\u7406\uff0c\u572830\u5929\u5185\u5b8c\u621020\u4e07\u4ebftoken\u8bad\u7ec3\uff0c\u63a8\u7406\u6210\u672c\u4f4e\u81f3\u6bcf\u767e\u4e07\u8f93\u51fatoken 0.70\u7f8e\u5143\uff0c\u5728\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u6311\u6218\uff0c\u540c\u65f6\u63d0\u5347\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u9700\u8981\u8bbe\u8ba1\u65e2\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\u53c8\u80fd\u4f18\u5316\u8d44\u6e90\u4f7f\u7528\u7684\u6a21\u578b\u67b6\u6784\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u521b\u65b0\u8bbe\u8ba1\uff1aZero-computation Experts\u5b9e\u73b0\u52a8\u6001\u8ba1\u7b97\u9884\u7b97\u5206\u914d\uff0cShortcut-connected MoE\u6269\u5927\u8ba1\u7b97-\u901a\u4fe1\u91cd\u53e0\u7a97\u53e3\uff1b\u7ed3\u5408\u8d85\u53c2\u6570\u8fc1\u79fb\u3001\u6a21\u578b\u589e\u957f\u521d\u59cb\u5316\u3001\u7a33\u5b9a\u6027\u5957\u4ef6\u548c\u786e\u5b9a\u6027\u8ba1\u7b97\u7684\u7efc\u5408\u6269\u5c55\u6846\u67b6\uff1b\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u540e\u9488\u5bf9\u63a8\u7406\u3001\u4ee3\u7801\u548c\u6307\u4ee4\u8fdb\u884c\u4e2d\u540e\u671f\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u572830\u5929\u5185\u5b8c\u621020\u4e07\u4ebftoken\u8bad\u7ec3\uff0c\u63a8\u7406\u901f\u5ea6\u8d85\u8fc7100 TPS\uff0c\u6210\u672c\u4e3a\u6bcf\u767e\u4e07\u8f93\u51fatoken 0.70\u7f8e\u5143\uff1b\u5728\u5404\u9879\u8bc4\u4f30\u4e2d\u8868\u73b0\u7ade\u4e89\u529b\u5f3a\uff0c\u5728\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "LongCat-Flash\u901a\u8fc7\u521b\u65b0\u7684MoE\u67b6\u6784\u8bbe\u8ba1\u548c\u9ad8\u6548\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u4e3a\u667a\u80fd\u4f53\u667a\u80fd\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u5df2\u5f00\u6e90\u4fc3\u8fdb\u793e\u533a\u7814\u7a76\u3002"}}
{"id": "2509.01324", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01324", "abs": "https://arxiv.org/abs/2509.01324", "authors": ["Jihyung Lee", "Daehui Kim", "Seonjeong Hwang", "Hyounghun Kim", "Gary Lee"], "title": "KoBLEX: Open Legal Question Answering with Multi-hop Reasoning", "comment": "EMNLP 2025 Main Conference", "summary": "Large Language Models (LLM) have achieved remarkable performances in general\ndomains and are now extending into the expert domain of law. Several benchmarks\nhave been proposed to evaluate LLMs' legal capabilities. However, these\nbenchmarks fail to evaluate open-ended and provision-grounded Question\nAnswering (QA). To address this, we introduce a Korean Benchmark for Legal\nEXplainable QA (KoBLEX), designed to evaluate provision-grounded, multi-hop\nlegal reasoning. KoBLEX includes 226 scenario-based QA instances and their\nsupporting provisions, created using a hybrid LLM-human expert pipeline. We\nalso propose a method called Parametric provision-guided Selection Retrieval\n(ParSeR), which uses LLM-generated parametric provisions to guide legally\ngrounded and reliable answers. ParSeR facilitates multi-hop reasoning on\ncomplex legal questions by generating parametric provisions and employing a\nthree-stage sequential retrieval process. Furthermore, to better evaluate the\nlegal fidelity of the generated answers, we propose Legal Fidelity Evaluation\n(LF-Eval). LF-Eval is an automatic metric that jointly considers the question,\nanswer, and supporting provisions and shows a high correlation with human\njudgments. Experimental results show that ParSeR consistently outperforms\nstrong baselines, achieving the best results across multiple LLMs. Notably,\ncompared to standard retrieval with GPT-4o, ParSeR achieves +37.91 higher F1\nand +30.81 higher LF-Eval. Further analyses reveal that ParSeR efficiently\ndelivers consistent performance across reasoning depths, with ablations\nconfirming the effectiveness of ParSeR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u97e9\u56fd\u6cd5\u5f8b\u53ef\u89e3\u91ca\u95ee\u7b54\u57fa\u51c6KoBLEX\u548c\u53c2\u6570\u5316\u6761\u6b3e\u5f15\u5bfc\u68c0\u7d22\u65b9\u6cd5ParSeR\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u6cd5\u5f8b\u6761\u6b3e\u7684\u591a\u8df3\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u9879\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6cd5\u5f8b\u8bc4\u4f30\u57fa\u51c6\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u5f00\u653e\u5f0f\u3001\u57fa\u4e8e\u6cd5\u5f8b\u6761\u6b3e\u7684\u95ee\u7b54\u4efb\u52a1\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u591a\u8df3\u6cd5\u5f8b\u63a8\u7406\u7684\u8bc4\u6d4b\u4f53\u7cfb\u3002", "method": "\u91c7\u7528\u6df7\u5408LLM-\u4e13\u5bb6\u6d41\u6c34\u7ebf\u6784\u5efa226\u4e2a\u573a\u666f\u5316\u95ee\u7b54\u5b9e\u4f8b\uff0c\u63d0\u51faParSeR\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u53c2\u6570\u5316\u6761\u6b3e\u8fdb\u884c\u4e09\u9636\u6bb5\u987a\u5e8f\u68c0\u7d22\uff0c\u5e76\u8bbe\u8ba1LF-Eval\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u3002", "result": "ParSeR\u5728\u591a\u4e2aLLM\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u76f8\u6bd4\u6807\u51c6\u68c0\u7d22\u65b9\u6cd5F1\u5206\u6570\u63d0\u534737.91\uff0cLF-Eval\u5206\u6570\u63d0\u534730.81\uff0c\u5728\u4e0d\u540c\u63a8\u7406\u6df1\u5ea6\u4e0a\u5747\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002", "conclusion": "KoBLEX\u57fa\u51c6\u548cParSeR\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6cd5\u5f8b\u9886\u57df\u591a\u8df3\u63a8\u7406\u7684\u8bc4\u4f30\u95ee\u9898\uff0cLF-Eval\u6307\u6807\u4e0e\u4eba\u5de5\u8bc4\u4f30\u9ad8\u5ea6\u76f8\u5173\uff0c\u4e3a\u6cd5\u5f8bAI\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2509.01328", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01328", "abs": "https://arxiv.org/abs/2509.01328", "authors": ["Wei Wang", "Fuqing Bie", "Junzhe Chen", "Dan Zhang", "Shiyu Huang", "Evgeny Kharlamov", "Jie Tang"], "title": "Can Large Language Models Master Complex Card Games?", "comment": null, "summary": "Complex games have long been an important benchmark for testing the progress\nof artificial intelligence algorithms. AlphaGo, AlphaZero, and MuZero have\ndefeated top human players in Go and Chess, garnering widespread societal\nattention towards artificial intelligence. Concurrently, large language models\n(LLMs) have exhibited remarkable capabilities across various tasks, raising the\nquestion of whether LLMs can achieve similar success in complex games. In this\npaper, we explore the potential of LLMs in mastering complex card games. We\nsystematically assess the learning capabilities of LLMs across eight diverse\ncard games, evaluating the impact of fine-tuning on high-quality gameplay data,\nand examining the models' ability to retain general capabilities while\nmastering these games. Our findings indicate that: (1) LLMs can approach the\nperformance of strong game AIs through supervised fine-tuning on high-quality\ndata, (2) LLMs can master multiple complex card games simultaneously, with\nperformance augmentation for games with similar rules and conflicts for\ndissimilar ones, and (3) LLMs experience a decline in general capabilities when\nmastering complex games, but this decline can be mitigated by integrating a\ncertain amount of general instruction data. The evaluation results demonstrate\nstrong learning ability and versatility of LLMs.", "AI": {"tldr": "LLMs\u901a\u8fc7\u9ad8\u8d28\u91cf\u6e38\u620f\u6570\u636e\u5fae\u8c03\u53ef\u4ee5\u63a5\u8fd1\u5f3a\u6e38\u620fAI\u6027\u80fd\uff0c\u80fd\u540c\u65f6\u638c\u63e1\u591a\u79cd\u590d\u6742\u5361\u724c\u6e38\u620f\uff0c\u4f46\u5728\u638c\u63e1\u590d\u6742\u6e38\u620f\u65f6\u901a\u7528\u80fd\u529b\u4f1a\u4e0b\u964d\uff0c\u53ef\u901a\u8fc7\u52a0\u5165\u901a\u7528\u6307\u4ee4\u6570\u636e\u7f13\u89e3\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u5361\u724c\u6e38\u620f\u4e2d\u7684\u6f5c\u529b\uff0c\u6d4b\u8bd5\u5176\u662f\u5426\u80fd\u50cfAlphaGo\u7b49AI\u5728\u590d\u6742\u6e38\u620f\u4e2d\u53d6\u5f97\u7c7b\u4f3c\u6210\u529f\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30LLMs\u57288\u79cd\u4e0d\u540c\u5361\u724c\u6e38\u620f\u4e2d\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u5206\u6790\u5fae\u8c03\u5bf9\u6e38\u620f\u8868\u73b0\u7684\u5f71\u54cd\uff0c\u68c0\u9a8c\u6a21\u578b\u5728\u638c\u63e1\u6e38\u620f\u65f6\u4fdd\u6301\u901a\u7528\u80fd\u529b\u7684\u80fd\u529b\u3002", "result": "LLMs\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u53ef\u63a5\u8fd1\u5f3a\u6e38\u620fAI\u6027\u80fd\uff1b\u80fd\u540c\u65f6\u638c\u63e1\u591a\u4e2a\u590d\u6742\u5361\u724c\u6e38\u620f\uff0c\u76f8\u4f3c\u89c4\u5219\u6e38\u620f\u6709\u6027\u80fd\u589e\u5f3a\uff0c\u4e0d\u76f8\u4f3c\u6e38\u620f\u5b58\u5728\u51b2\u7a81\uff1b\u638c\u63e1\u590d\u6742\u6e38\u620f\u65f6\u901a\u7528\u80fd\u529b\u4e0b\u964d\u4f46\u53ef\u901a\u8fc7\u52a0\u5165\u901a\u7528\u6307\u4ee4\u6570\u636e\u7f13\u89e3\u3002", "conclusion": "\u8bc4\u4f30\u7ed3\u679c\u5c55\u793a\u4e86LLMs\u5f3a\u5927\u7684\u5b66\u4e60\u80fd\u529b\u548c\u591a\u7528\u9014\u6027\uff0c\u5728\u590d\u6742\u6e38\u620f\u9886\u57df\u5177\u6709\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2509.01363", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01363", "abs": "https://arxiv.org/abs/2509.01363", "authors": ["Mohammad Zbeeb", "Hasan Abed Al Kader Hammoud", "Bernard Ghanem"], "title": "Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic", "comment": "Under Review", "summary": "Large language models often require costly optimization, such as\nreinforcement learning, to master complex reasoning tasks. This work\ndemonstrates that reasoning ability, once learned, can be extracted and\ntransferred between models as a compact task vector. We source two publicly\navailable, identically initialized Qwen2.5 models, one fine-tuned with\nsupervised fine-tuning (SFT) and the other with group relative policy\noptimization (GRPO) on the same dataset. From these, we extract a reasoning\nvector: $v_{\\text{reason}} = \\theta_{\\text{GRPO}} - \\theta_{\\text{SFT}}$. We\nhypothesize that this vector captures the reasoning capability instilled by\nreinforcement learning while factoring out shared knowledge from the SFT\nprocess. When added to compatible instruction-tuned models through simple\narithmetic, this vector consistently improves performance across diverse\nreasoning benchmarks: GSM8K (+4.9%), HumanEval (+4.3%), SciQ (+1.7%), and\nBigBenchHard (+12.3% for the 1.5B model). The performance improvements persist\nunder adversarial conditions. Conversely, subtracting the vector causes\nsignificant performance degradation (-11.8% on GSM8K), demonstrating the\nvector's strong contribution to the model's reasoning abilities. This work\nshows how reasoning capabilities, typically developed through expensive\ntraining, can be extracted from existing open-source models and reused through\nsimple tensor arithmetic, offering a practical way to enhance models by\nrecycling prior computational investments.", "AI": {"tldr": "\u901a\u8fc7\u63d0\u53d6\u7406\u6027\u4efb\u52a1\u5411\u91cf\uff0c\u5c06GRPO\u8c03\u4f18\u7684\u7406\u6027\u80fd\u529b\u8f6c\u79fb\u5230\u5176\u4ed6\u6a21\u578b\uff0c\u907f\u514d\u91cd\u590d\u8c03\u4f18\u6210\u672c", "motivation": "\u907f\u514d\u91cd\u590d\u8c03\u4f18\u7684\u9ad8\u6210\u672c\uff0c\u5c06\u5df2\u5b66\u4e60\u7684\u7406\u6027\u80fd\u529b\u4ece\u73b0\u6709\u6a21\u578b\u4e2d\u63d0\u53d6\u5e76\u91cd\u7528", "method": "\u4eceGRPO\u548cSFT\u6a21\u578b\u7684\u53c2\u6570\u5dee\u4e2d\u63d0\u53d6\u7406\u6027\u5411\u91cf\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u5411\u91cf\u52a0\u6cd5\u5e94\u7528\u5230\u5176\u4ed6\u6a21\u578b", "result": "\u5728\u591a\u4e2a\u7406\u6027\u6d4b\u8bd5\u4e2d\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff08GSM8K +4.9%\uff0cHumanEval +4.3%\uff0cBigBenchHard +12.3%\uff09\uff0c\u51cf\u6cd5\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d", "conclusion": "\u7406\u6027\u80fd\u529b\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u7684\u5f20\u91cf\u8fd0\u7b97\u63d0\u53d6\u548c\u8f6c\u79fb\uff0c\u4e3a\u6a21\u578b\u589e\u5f3a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u65b9\u6cd5"}}
{"id": "2509.01379", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01379", "abs": "https://arxiv.org/abs/2509.01379", "authors": ["Paloma Piot", "Diego S\u00e1nchez", "Javier Parapar"], "title": "WATCHED: A Web AI Agent Tool for Combating Hate Speech by Expanding Data", "comment": null, "summary": "Online harms are a growing problem in digital spaces, putting user safety at\nrisk and reducing trust in social media platforms. One of the most persistent\nforms of harm is hate speech. To address this, we need tools that combine the\nspeed and scale of automated systems with the judgment and insight of human\nmoderators. These tools should not only find harmful content but also explain\ntheir decisions clearly, helping to build trust and understanding. In this\npaper, we present WATCHED, a chatbot designed to support content moderators in\ntackling hate speech. The chatbot is built as an Artificial Intelligence Agent\nsystem that uses Large Language Models along with several specialised tools. It\ncompares new posts with real examples of hate speech and neutral content, uses\na BERT-based classifier to help flag harmful messages, looks up slang and\ninformal language using sources like Urban Dictionary, generates\nchain-of-thought reasoning, and checks platform guidelines to explain and\nsupport its decisions. This combination allows the chatbot not only to detect\nhate speech but to explain why content is considered harmful, grounded in both\nprecedent and policy. Experimental results show that our proposed method\nsurpasses existing state-of-the-art methods, reaching a macro F1 score of 0.91.\nDesigned for moderators, safety teams, and researchers, the tool helps reduce\nonline harms by supporting collaboration between AI and human oversight.", "AI": {"tldr": "WATCHED\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u4e13\u4e1a\u5de5\u5177\u5e2e\u52a9\u5185\u5bb9\u5ba1\u6838\u5458\u68c0\u6d4b\u548c\u89e3\u91ca\u4ec7\u6068\u8a00\u8bba\uff0c\u8fbe\u52300.91\u7684F1\u5206\u6570", "motivation": "\u89e3\u51b3\u6570\u5b57\u7a7a\u95f4\u4e2d\u65e5\u76ca\u589e\u957f\u7684\u4ec7\u6068\u8a00\u8bba\u95ee\u9898\uff0c\u9700\u8981\u7ed3\u5408\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u901f\u5ea6\u548c\u4eba\u7c7b\u5ba1\u6838\u5458\u7684\u5224\u65ad\u529b\uff0c\u5efa\u7acb\u53ef\u89e3\u91ca\u7684\u5ba1\u6838\u5de5\u5177", "method": "\u6784\u5efaAI\u4ee3\u7406\u7cfb\u7edf\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u914d\u5408\u4e13\u4e1a\u5de5\u5177\uff1a\u6bd4\u5bf9\u771f\u5b9e\u4ec7\u6068\u8a00\u8bba\u793a\u4f8b\u3001BERT\u5206\u7c7b\u5668\u6807\u8bb0\u6709\u5bb3\u4fe1\u606f\u3001\u67e5\u8be2\u4fda\u8bed\u8bcd\u5178\u3001\u751f\u6210\u601d\u7ef4\u94fe\u63a8\u7406\u3001\u68c0\u67e5\u5e73\u53f0\u6307\u5357", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8fbe\u52300.91\u7684\u5b8fF1\u5206\u6570", "conclusion": "\u8be5\u5de5\u5177\u901a\u8fc7\u652f\u6301AI\u4e0e\u4eba\u7c7b\u76d1\u7763\u7684\u534f\u4f5c\uff0c\u6709\u6548\u51cf\u5c11\u7f51\u7edc\u5371\u5bb3\uff0c\u4e3a\u5ba1\u6838\u5458\u3001\u5b89\u5168\u56e2\u961f\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u6709\u529b\u652f\u6301"}}
{"id": "2509.01387", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.01387", "abs": "https://arxiv.org/abs/2509.01387", "authors": ["Serwar Basch", "Ilia Kuznetsov", "Tom Hope", "Iryna Gurevych"], "title": "ABCD-LINK: Annotation Bootstrapping for Cross-Document Fine-Grained Links", "comment": null, "summary": "Understanding fine-grained relations between documents is crucial for many\napplication domains. However, the study of automated assistance is limited by\nthe lack of efficient methods to create training and evaluation datasets of\ncross-document links. To address this, we introduce a new domain-agnostic\nframework for selecting a best-performing approach and annotating\ncross-document links in a new domain from scratch. We first generate and\nvalidate semi-synthetic datasets of interconnected documents. This data is used\nto perform automatic evaluation, producing a shortlist of best-performing\nlinking approaches. These approaches are then used in an extensive human\nevaluation study, yielding performance estimates on natural text pairs. We\napply our framework in two distinct domains -- peer review and news -- and show\nthat combining retrieval models with LLMs achieves 78\\% link approval from\nhuman raters, more than doubling the precision of strong retrievers alone. Our\nframework enables systematic study of cross-document understanding across\napplication scenarios, and the resulting novel datasets lay foundation for\nnumerous cross-document tasks like media framing and peer review. We make the\ncode, data, and annotation protocols openly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9886\u57df\u65e0\u5173\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u8bc4\u4f30\u548c\u4eba\u5de5\u6807\u6ce8\u8de8\u6587\u6863\u94fe\u63a5\uff0c\u5728\u65b0\u95fb\u548c\u540c\u884c\u8bc4\u5ba1\u9886\u57df\u9a8c\u8bc1\u4e86\u68c0\u7d22\u6a21\u578b\u4e0eLLM\u7ed3\u5408\u7684\u65b9\u6cd5\u6548\u679c\u6700\u4f73", "motivation": "\u8de8\u6587\u6863\u5173\u7cfb\u7406\u89e3\u5bf9\u8bb8\u591a\u5e94\u7528\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u6570\u636e\u96c6\u521b\u5efa\u65b9\u6cd5\u9650\u5236\u4e86\u81ea\u52a8\u5316\u8f85\u52a9\u7814\u7a76\u7684\u53d1\u5c55", "method": "\u9996\u5148\u751f\u6210\u548c\u9a8c\u8bc1\u534a\u5408\u6210\u7684\u4e92\u8054\u6587\u6863\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u81ea\u52a8\u8bc4\u4f30\u7b5b\u9009\u51fa\u6700\u4f73\u94fe\u63a5\u65b9\u6cd5\uff0c\u7136\u540e\u5728\u81ea\u7136\u6587\u672c\u5bf9\u4e0a\u5f00\u5c55\u5e7f\u6cdb\u7684\u4eba\u5de5\u8bc4\u4f30\u7814\u7a76", "result": "\u5728\u65b0\u95fb\u548c\u540c\u884c\u8bc4\u5ba1\u4e24\u4e2a\u9886\u57df\u5e94\u7528\u8be5\u6846\u67b6\uff0c\u7ed3\u5408\u68c0\u7d22\u6a21\u578b\u548cLLM\u7684\u65b9\u6cd5\u83b7\u5f97\u4e8678%\u7684\u4eba\u5de5\u8bc4\u5206\u8005\u94fe\u63a5\u6279\u51c6\u7387\uff0c\u6bd4\u5355\u72ec\u4f7f\u7528\u5f3a\u68c0\u7d22\u5668\u7684\u7cbe\u786e\u5ea6\u63d0\u9ad8\u4e86\u4e00\u500d\u591a", "conclusion": "\u8be5\u6846\u67b6\u652f\u6301\u8de8\u5e94\u7528\u573a\u666f\u7684\u7cfb\u7edf\u6027\u8de8\u6587\u6863\u7406\u89e3\u7814\u7a76\uff0c\u4ea7\u751f\u7684\u65b0\u6570\u636e\u96c6\u4e3a\u5a92\u4f53\u6846\u67b6\u5206\u6790\u548c\u540c\u884c\u8bc4\u5ba1\u7b49\u4f17\u591a\u8de8\u6587\u6863\u4efb\u52a1\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2509.01390", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01390", "abs": "https://arxiv.org/abs/2509.01390", "authors": ["Joonyong Park", "Shinnosuke Takamichi", "David M. Chan", "Shunsuke Kando", "Yuki Saito", "Hiroshi Saruwatari"], "title": "Analysing the Language of Neural Audio Codecs", "comment": "In Proceedings of 2025 IEEE Automatic Speech Recognition and\n  Understanding Workshop (ASRU 2025)", "summary": "This study presents a comparative analysis of the statistical and linguistic\nproperties of neural audio codecs (NACs). We investigate discrete speech tokens\nproduced by various NAC models, examining their adherence to linguistic\nstatistical laws such as Zipf's law and Heaps' law, as well as their entropy\nand redundancy. To assess how these token-level properties relate to semantic\nand acoustic preservation in synthesized speech, we evaluate intelligibility\nusing error rates of automatic speech recognition, and quality using the UTMOS\nscore. Our results reveal that NAC tokens, particularly 3-grams, exhibit\nlanguage-like statistical patterns. Moreover, these properties, together with\nmeasures of information content, are found to correlate with improved\nperformances in speech recognition and resynthesis tasks. These findings offer\ninsights into the structure of NAC token sequences and inform the design of\nmore effective generative speech models.", "AI": {"tldr": "\u7814\u7a76\u5bf9\u6df1\u5ea6\u5b66\u4e60\u97f3\u9891\u7f16\u7801\u5668\u4ea7\u751f\u7684\u8bed\u8a00\u7c7b\u4ee3\u7801\u8fdb\u884c\u7edf\u8ba1\u7279\u6027\u5206\u6790\uff0c\u53d1\u73b0\u5176\u6ee1\u8db3\u8bed\u8a00\u7edf\u8ba1\u5b66\u5b9a\u5f8b\uff0c\u4e14\u8fd9\u4e9b\u7279\u6027\u4e0e\u8bed\u97f3\u8bc6\u522b\u548c\u91cd\u6784\u6027\u80fd\u6b63\u76f8\u5173", "motivation": "\u63a2\u7d22\u795e\u7ecf\u97f3\u9891\u7f16\u7801\u5668\u4ea7\u751f\u7684\u79bb\u6563\u4ee3\u7801\u7684\u7edf\u8ba1\u7279\u6027\u548c\u8bed\u8a00\u5b66\u89c4\u5f8b\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u7279\u6027\u5982\u4f55\u5f71\u54cd\u8bed\u97f3\u8bc6\u522b\u548c\u91cd\u6784\u8d28\u91cf", "method": "\u5bf9\u591a\u79cdNAC\u6a21\u578b\u4ea7\u751f\u7684\u79bb\u6563\u8bed\u97f3\u4ee3\u7801\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\uff0c\u68c0\u9a8cZipf\u5b9a\u5f8b\u3001Heaps\u5b9a\u5f8b\u3001\u71b5\u548c\u5197\u4f59\u5ea6\uff0c\u5e76\u901a\u8fc7\u8bed\u97f3\u8bc6\u522b\u9519\u8bef\u7387\u548cUTMOS\u8bc4\u5206\u8bc4\u4f30\u8bed\u4e49\u4fdd\u5b58\u548c\u97f3\u8d28\u8d28\u91cf", "result": "NAC\u4ee3\u7801\u7279\u522b\u662f3-gram\u663e\u793a\u51fa\u7c7b\u4f3c\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u7edf\u8ba1\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u7edf\u8ba1\u7279\u6027\u4e0e\u4fe1\u606f\u5185\u5bb9\u91cf\u6307\u6807\u4e00\u8d77\u4e0e\u8bed\u97f3\u8bc6\u522b\u548c\u91cd\u6784\u4efb\u52a1\u7684\u6027\u80fd\u63d0\u5347\u5448\u73b0\u6b63\u76f8\u5173", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86NAC\u4ee3\u7801\u5e8f\u5217\u7684\u7ed3\u6784\u7279\u5f81\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u751f\u6210\u5f0f\u8bed\u97f3\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u4f53\u7cfb\u7ed3\u6784\u548c\u6027\u80fd\u9884\u6d4b\u7684\u89c1\u89e3"}}
{"id": "2509.01395", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.01395", "abs": "https://arxiv.org/abs/2509.01395", "authors": ["KV Aditya Srivatsa", "Kaushal Kumar Maurya", "Ekaterina Kochmar"], "title": "LLMs cannot spot math errors, even when allowed to peek into the solution", "comment": "Accepted to EMNLP 2025", "summary": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance.", "AI": {"tldr": "LLMs\u5728\u6570\u5b66\u5e94\u7528\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5143\u63a8\u7406\u4efb\u52a1\uff08\u5982\u8bc6\u522b\u5b66\u751f\u89e3\u9898\u9519\u8bef\uff09\u4e0a\u5b58\u5728\u56f0\u96be\u3002\u672c\u6587\u7814\u7a76\u5b9a\u4f4d\u9010\u6b65\u89e3\u7b54\u4e2d\u7b2c\u4e00\u4e2a\u9519\u8bef\u6b65\u9aa4\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u751f\u6210\u4e2d\u95f4\u4fee\u6b63\u5b66\u751f\u89e3\u7b54\u7684\u65b9\u6cd5\u6765\u6539\u5584\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u5e94\u7528\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u8bc6\u522b\u5b66\u751f\u89e3\u9898\u9519\u8bef\u7b49\u5143\u63a8\u7406\u4efb\u52a1\u4e0a\u4ecd\u6709\u56f0\u96be\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u51c6\u786e\u5b9a\u4f4d\u9010\u6b65\u89e3\u7b54\u4e2d\u7684\u7b2c\u4e00\u4e2a\u9519\u8bef\u6b65\u9aa4\u3002", "method": "\u4f7f\u7528VtG\u548cPRM800K\u4e24\u4e2a\u9519\u8bef\u63a8\u7406\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\uff0c\u63d0\u51fa\u751f\u6210\u4e2d\u95f4\u4fee\u6b63\u5b66\u751f\u89e3\u7b54\u7684\u65b9\u6cd5\uff0c\u4f7f\u4fee\u6b63\u65b9\u6848\u66f4\u8d34\u8fd1\u539f\u59cb\u5b66\u751f\u89e3\u7b54\uff0c\u4ece\u800c\u63d0\u9ad8\u9519\u8bef\u5b9a\u4f4d\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u63d0\u4f9b\u53c2\u8003\u7b54\u6848\uff0c\u6700\u5148\u8fdb\u7684LLMs\u4e5f\u96be\u4ee5\u51c6\u786e\u5b9a\u4f4d\u5b66\u751f\u89e3\u7b54\u4e2d\u7684\u7b2c\u4e00\u4e2a\u9519\u8bef\u6b65\u9aa4\u3002\u63d0\u51fa\u7684\u4e2d\u95f4\u4fee\u6b63\u65b9\u6cd5\u6709\u52a9\u4e8e\u6539\u5584\u6027\u80fd\u3002", "conclusion": "\u5b9a\u4f4d\u9010\u6b65\u89e3\u7b54\u4e2d\u7684\u7b2c\u4e00\u4e2a\u9519\u8bef\u6b65\u9aa4\u5bf9LLMs\u5177\u6709\u6311\u6218\u6027\uff0c\u901a\u8fc7\u751f\u6210\u66f4\u8d34\u8fd1\u539f\u59cb\u5b66\u751f\u89e3\u7b54\u7684\u4e2d\u95f4\u4fee\u6b63\u65b9\u6848\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u9519\u8bef\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2509.01412", "categories": ["cs.CL", "68T07, 68T50, 68T05", "I.2.7; I.2.6; I.2.8; H.5.2"], "pdf": "https://arxiv.org/pdf/2509.01412", "abs": "https://arxiv.org/abs/2509.01412", "authors": ["Kaviraj Pather", "Elena Hadjigeorgiou", "Arben Krasniqi", "Claire Schmit", "Irina Rusu", "Marc Pons", "Kabir Khan"], "title": "Vis-CoT: A Human-in-the-Loop Framework for Interactive Visualization and Intervention in LLM Chain-of-Thought Reasoning", "comment": "12 pages, 7 figures", "summary": "Large language models (LLMs) show strong reasoning via chain-of-thought (CoT)\nprompting, but the process is opaque, which makes verification, debugging, and\ncontrol difficult in high-stakes settings. We present Vis-CoT, a\nhuman-in-the-loop framework that converts linear CoT text into an interactive\nreasoning graph. Users can visualize the logical flow, identify flawed steps,\nand intervene by pruning incorrect paths and grafting new, user-defined\npremises. This shifts interaction from passive observation to active\ncollaboration, steering models toward more accurate and trustworthy\nconclusions. Across GSM8K and StrategyQA, Vis-CoT improves final-answer\naccuracy by up to 24 percentage points over non-interactive baselines. A user\nstudy also shows large gains in perceived usability and trust. Vis-CoT points\nto a practical path for more reliable, understandable, and collaborative\nreasoning by combining LLMs with targeted human oversight.", "AI": {"tldr": "Vis-CoT\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u6846\u67b6\uff0c\u5c06\u94fe\u5f0f\u601d\u7ef4\u6587\u672c\u8f6c\u6362\u4e3a\u53ef\u89c6\u5316\u63a8\u7406\u56fe\uff0c\u5141\u8bb8\u7528\u6237\u8bc6\u522b\u9519\u8bef\u6b65\u9aa4\u5e76\u8fdb\u884c\u5e72\u9884\uff0c\u663e\u8457\u63d0\u9ad8LLM\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u8fc7\u7a0b\u4e0d\u900f\u660e\uff0c\u96be\u4ee5\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3001\u8c03\u8bd5\u548c\u63a7\u5236\uff0c\u9700\u8981\u66f4\u53ef\u9760\u548c\u53ef\u7406\u89e3\u7684\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1Vis-CoT\u6846\u67b6\uff0c\u5c06\u7ebf\u6027CoT\u6587\u672c\u8f6c\u6362\u4e3a\u4ea4\u4e92\u5f0f\u63a8\u7406\u56fe\uff0c\u652f\u6301\u7528\u6237\u53ef\u89c6\u5316\u903b\u8f91\u6d41\u7a0b\u3001\u8bc6\u522b\u9519\u8bef\u6b65\u9aa4\u3001\u4fee\u526a\u9519\u8bef\u8def\u5f84\u548c\u5ac1\u63a5\u65b0\u524d\u63d0\u3002", "result": "\u5728GSM8K\u548cStrategyQA\u6570\u636e\u96c6\u4e0a\uff0cVis-CoT\u6bd4\u975e\u4ea4\u4e92\u5f0f\u57fa\u7ebf\u63d0\u9ad8\u4e86\u6700\u591a24\u4e2a\u767e\u5206\u70b9\u7684\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u7387\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u53ef\u7528\u6027\u548c\u4fe1\u4efb\u5ea6\u5927\u5e45\u63d0\u5347\u3002", "conclusion": "Vis-CoT\u901a\u8fc7\u7ed3\u5408LLM\u548c\u9488\u5bf9\u6027\u4eba\u5de5\u76d1\u7763\uff0c\u4e3a\u5b9e\u73b0\u66f4\u53ef\u9760\u3001\u53ef\u7406\u89e3\u548c\u534f\u4f5c\u7684\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2509.01418", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01418", "abs": "https://arxiv.org/abs/2509.01418", "authors": ["Yang Liu", "Masahiro Kaneko", "Chenhui Chu"], "title": "On the Alignment of Large Language Models with Global Human Opinion", "comment": "23 pages, 19 figures", "summary": "Today's large language models (LLMs) are capable of supporting multilingual\nscenarios, allowing users to interact with LLMs in their native languages. When\nLLMs respond to subjective questions posed by users, they are expected to align\nwith the views of specific demographic groups or historical periods, shaped by\nthe language in which the user interacts with the model. Existing studies\nmainly focus on researching the opinions represented by LLMs among demographic\ngroups in the United States or a few countries, lacking worldwide country\nsamples and studies on human opinions in different historical periods, as well\nas lacking discussion on using language to steer LLMs. Moreover, they also\noverlook the potential influence of prompt language on the alignment of LLMs'\nopinions. In this study, our goal is to fill these gaps. To this end, we create\nan evaluation framework based on the World Values Survey (WVS) to\nsystematically assess the alignment of LLMs with human opinions across\ndifferent countries, languages, and historical periods around the world. We\nfind that LLMs appropriately or over-align the opinions with only a few\ncountries while under-aligning the opinions with most countries. Furthermore,\nchanging the language of the prompt to match the language used in the\nquestionnaire can effectively steer LLMs to align with the opinions of the\ncorresponding country more effectively than existing steering methods. At the\nsame time, LLMs are more aligned with the opinions of the contemporary\npopulation. To our knowledge, our study is the first comprehensive\ninvestigation of the topic of opinion alignment in LLMs across global,\nlanguage, and temporal dimensions. Our code and data are publicly available at\nhttps://github.com/nlply/global-opinion-alignment.", "AI": {"tldr": "\u672c\u7814\u7a76\u521b\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e16\u754c\u4ef7\u503c\u89c2\u8c03\u67e5\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f30LLMs\u5728\u4e0d\u540c\u56fd\u5bb6\u3001\u8bed\u8a00\u548c\u5386\u53f2\u65f6\u671f\u4e0e\u4eba\u7c7b\u610f\u89c1\u7684\u5bf9\u9f50\u60c5\u51b5\uff0c\u53d1\u73b0LLMs\u4ec5\u4e0e\u5c11\u6570\u56fd\u5bb6\u9002\u5f53\u6216\u8fc7\u5ea6\u5bf9\u9f50\uff0c\u800c\u4e0e\u5927\u591a\u6570\u56fd\u5bb6\u5bf9\u9f50\u4e0d\u8db3\uff0c\u540c\u65f6\u63d0\u793a\u8bed\u8a00\u6539\u53d8\u80fd\u6709\u6548\u5f15\u5bfcLLMs\u66f4\u597d\u5730\u4e0e\u5bf9\u5e94\u56fd\u5bb6\u610f\u89c1\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7f8e\u56fd\u6216\u5c11\u6570\u56fd\u5bb6\u7684\u4eba\u53e3\u7fa4\u4f53\u610f\u89c1\uff0c\u7f3a\u4e4f\u5168\u7403\u56fd\u5bb6\u6837\u672c\u3001\u4e0d\u540c\u5386\u53f2\u65f6\u671f\u4eba\u7c7b\u610f\u89c1\u7814\u7a76\uff0c\u4ee5\u53ca\u4f7f\u7528\u8bed\u8a00\u5f15\u5bfcLLMs\u7684\u8ba8\u8bba\uff0c\u540c\u65f6\u5ffd\u89c6\u4e86\u63d0\u793a\u8bed\u8a00\u5bf9LLMs\u610f\u89c1\u5bf9\u9f50\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "method": "\u57fa\u4e8e\u4e16\u754c\u4ef7\u503c\u89c2\u8c03\u67e5(WVS)\u521b\u5efa\u8bc4\u4f30\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f30LLMs\u5728\u5168\u7403\u4e0d\u540c\u56fd\u5bb6\u3001\u8bed\u8a00\u548c\u5386\u53f2\u65f6\u671f\u4e0e\u4eba\u7c7b\u610f\u89c1\u7684\u5bf9\u9f50\u60c5\u51b5\u3002", "result": "LLMs\u4ec5\u4e0e\u5c11\u6570\u56fd\u5bb6\u9002\u5f53\u6216\u8fc7\u5ea6\u5bf9\u9f50\u610f\u89c1\uff0c\u800c\u4e0e\u5927\u591a\u6570\u56fd\u5bb6\u5bf9\u9f50\u4e0d\u8db3\uff1b\u6539\u53d8\u63d0\u793a\u8bed\u8a00\u4ee5\u5339\u914d\u95ee\u5377\u8bed\u8a00\u80fd\u6bd4\u73b0\u6709\u5f15\u5bfc\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u4f7fLLMs\u4e0e\u5bf9\u5e94\u56fd\u5bb6\u610f\u89c1\u5bf9\u9f50\uff1bLLMs\u66f4\u7b26\u5408\u5f53\u4ee3\u4eba\u7fa4\u7684\u610f\u89c1\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5728\u5168\u7403\u3001\u8bed\u8a00\u548c\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u5168\u9762\u8c03\u67e5LLMs\u610f\u89c1\u5bf9\u9f50\u4e3b\u9898\u7684\u7814\u7a76\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u7406\u89e3LLMs\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u4e0b\u7684\u610f\u89c1\u8868\u8fbe\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2509.01455", "categories": ["cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.01455", "abs": "https://arxiv.org/abs/2509.01455", "authors": ["Markus Oehri", "Giulia Conti", "Kaviraj Pather", "Alexandre Rossi", "Laia Serra", "Adrian Parody", "Rogvi Johannesen", "Aviaja Petersen", "Arben Krasniqi"], "title": "Trusted Uncertainty in Large Language Models: A Unified Framework for Confidence Calibration and Risk-Controlled Refusal", "comment": "10 pages, 5 figures", "summary": "Deployed language models must decide not only what to answer but also when\nnot to answer. We present UniCR, a unified framework that turns heterogeneous\nuncertainty evidence including sequence likelihoods, self-consistency\ndispersion, retrieval compatibility, and tool or verifier feedback into a\ncalibrated probability of correctness and then enforces a user-specified error\nbudget via principled refusal. UniCR learns a lightweight calibration head with\ntemperature scaling and proper scoring, supports API-only models through\nblack-box features, and offers distribution-free guarantees using conformal\nrisk control. For long-form generation, we align confidence with semantic\nfidelity by supervising on atomic factuality scores derived from retrieved\nevidence, reducing confident hallucinations while preserving coverage.\nExperiments on short-form QA, code generation with execution tests, and\nretrieval-augmented long-form QA show consistent improvements in calibration\nmetrics, lower area under the risk-coverage curve, and higher coverage at fixed\nrisk compared to entropy or logit thresholds, post-hoc calibrators, and\nend-to-end selective baselines. Analyses reveal that evidence contradiction,\nsemantic dispersion, and tool inconsistency are the dominant drivers of\nabstention, yielding informative user-facing refusal messages. The result is a\nportable recipe of evidence fusion to calibrated probability to risk-controlled\ndecision that improves trustworthiness without fine-tuning the base model and\nremains valid under distribution shift.", "AI": {"tldr": "UniCR\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u8bc1\u636e\u8f6c\u6362\u4e3a\u6821\u51c6\u540e\u7684\u6b63\u786e\u6982\u7387\uff0c\u5e76\u901a\u8fc7\u539f\u5219\u6027\u62d2\u7edd\u6765\u6267\u884c\u7528\u6237\u6307\u5b9a\u7684\u9519\u8bef\u9884\u7b97\uff0c\u63d0\u9ad8\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u90e8\u7f72\u7684\u8bed\u8a00\u6a21\u578b\u9700\u8981\u51b3\u5b9a\u4f55\u65f6\u56de\u7b54\u3001\u4f55\u65f6\u4e0d\u56de\u7b54\uff0c\u4ee5\u907f\u514d\u9519\u8bef\u56de\u7b54\u3002\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u7f3a\u4e4f\u7edf\u4e00\u7684\u8bc1\u636e\u878d\u5408\u548c\u98ce\u9669\u63a7\u5236\u673a\u5236\u3002", "method": "\u4f7f\u7528\u6e29\u5ea6\u7f29\u653e\u548c\u9002\u5f53\u8bc4\u5206\u5b66\u4e60\u8f7b\u91cf\u7ea7\u6821\u51c6\u5934\uff0c\u652f\u6301\u9ed1\u76d2\u6a21\u578b\uff0c\u5229\u7528\u7b26\u5408\u98ce\u9669\u63a7\u5236\u63d0\u4f9b\u5206\u5e03\u65e0\u5173\u4fdd\u8bc1\uff0c\u901a\u8fc7\u68c0\u7d22\u8bc1\u636e\u76d1\u7763\u539f\u5b50\u4e8b\u5b9e\u6027\u5f97\u5206\u6765\u5bf9\u9f50\u957f\u6587\u672c\u751f\u6210\u7684\u7f6e\u4fe1\u5ea6\u3002", "result": "\u5728\u77ed\u95ee\u7b54\u3001\u4ee3\u7801\u751f\u6210\u548c\u68c0\u7d22\u589e\u5f3a\u957f\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u6821\u51c6\u6307\u6807\u6301\u7eed\u6539\u8fdb\uff0c\u98ce\u9669-\u8986\u76d6\u7387\u66f2\u7ebf\u4e0b\u9762\u79ef\u964d\u4f4e\uff0c\u56fa\u5b9a\u98ce\u9669\u4e0b\u8986\u76d6\u7387\u66f4\u9ad8\u3002", "conclusion": "UniCR\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u79fb\u690d\u7684\u8bc1\u636e\u878d\u5408\u5230\u6821\u51c6\u6982\u7387\u518d\u5230\u98ce\u9669\u63a7\u5236\u51b3\u7b56\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u5fae\u8c03\u57fa\u7840\u6a21\u578b\uff0c\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u4ecd\u6709\u6548\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2509.01468", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01468", "abs": "https://arxiv.org/abs/2509.01468", "authors": ["Yuchen Wu", "Liang Ding", "Li Shen", "Dacheng Tao"], "title": "Robust Knowledge Editing via Explicit Reasoning Chains for Distractor-Resilient Multi-Hop QA", "comment": "EMNLP 2025 Findings", "summary": "Large language models (LLMs) encode vast amounts of world knowledge but\nremain static once trained, making the timely integration of emerging facts\nprohibitively expensive via full retraining. Knowledge-editing techniques have\nthus emerged to inject or overwrite specific facts into LLMs, yet they either\nover-rely on superficial cues or incur complex, iterative pipelines that\ncollapse under noisy, multi-hop conditions. We introduce Reason-KE, an\nend-to-end reasoning-chain-based editing framework that steers a pretrained LLM\nthrough four structured stages-fact acknowledgment, relevance determination,\nselective application, and final reasoning-to filter distractors in a single\npass. Trained on MQuAKE-CF with up to four irrelevant facts, Reason-KE elevates\nQwen2.5-7B's multi-hop QA accuracy to 90.2% while suffering merely a 6.3% drop\nunder heavy distraction and <1% when answers are leaked. Our quantitative\nanalysis confirms Reason-KE's resilience and efficiency, establishing a new\nstate-of-the-art for reliable LLM knowledge updates.", "AI": {"tldr": "Reason-KE\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u57fa\u4e8e\u63a8\u7406\u94fe\u7684\u77e5\u8bc6\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u7ed3\u6784\u5316\u9636\u6bb5\u5f15\u5bfcLLM\u5728\u5355\u6b21\u5904\u7406\u4e2d\u8fc7\u6ee4\u5e72\u6270\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u591a\u8df3\u95ee\u7b54\u51c6\u786e\u7387\u81f390.2%\uff0c\u5728\u5f3a\u5e72\u6270\u4e0b\u4ec5\u4e0b\u964d6.3%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u540e\u77e5\u8bc6\u9759\u6001\u5316\uff0c\u5b8c\u6574\u91cd\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u6280\u672f\u8981\u4e48\u8fc7\u5ea6\u4f9d\u8d56\u8868\u9762\u7ebf\u7d22\uff0c\u8981\u4e48\u5728\u566a\u58f0\u591a\u8df3\u6761\u4ef6\u4e0b\u590d\u6742\u7684\u8fed\u4ee3\u6d41\u7a0b\u4f1a\u5d29\u6e83\u3002", "method": "\u63d0\u51faReason-KE\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u7ed3\u6784\u5316\u9636\u6bb5\uff08\u4e8b\u5b9e\u786e\u8ba4\u3001\u76f8\u5173\u6027\u5224\u5b9a\u3001\u9009\u62e9\u6027\u5e94\u7528\u548c\u6700\u7ec8\u63a8\u7406\uff09\u5f15\u5bfc\u9884\u8bad\u7ec3LLM\u5728\u5355\u6b21\u5904\u7406\u4e2d\u8fc7\u6ee4\u5e72\u6270\u4fe1\u606f\u3002\u5728MQuAKE-CF\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u6700\u591a\u5305\u542b\u56db\u4e2a\u65e0\u5173\u4e8b\u5b9e\u3002", "result": "\u5c06Qwen2.5-7B\u7684\u591a\u8df3\u95ee\u7b54\u51c6\u786e\u7387\u63d0\u5347\u81f390.2%\uff0c\u5728\u5f3a\u5e72\u6270\u4e0b\u4ec5\u4e0b\u964d6.3%\uff0c\u7b54\u6848\u6cc4\u9732\u65f6\u4e0b\u964d<1%\u3002\u5b9a\u91cf\u5206\u6790\u8bc1\u5b9e\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "conclusion": "Reason-KE\u5efa\u7acb\u4e86\u53ef\u9760LLM\u77e5\u8bc6\u66f4\u65b0\u7684\u65b0state-of-the-art\uff0c\u5c55\u793a\u4e86\u5728\u566a\u58f0\u591a\u8df3\u6761\u4ef6\u4e0b\u7684\u4f18\u5f02\u8868\u73b0\u548c\u6548\u7387\u3002"}}
{"id": "2509.01476", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.01476", "abs": "https://arxiv.org/abs/2509.01476", "authors": ["Youchao Zhou", "Heyan Huang", "Yicheng Liu", "Rui Dai", "Xinglin Wang", "Xingchen Zhang", "Shumin Shi", "Yang Deng"], "title": "Do Retrieval Augmented Language Models Know When They Don't Know?", "comment": "under review", "summary": "Existing Large Language Models (LLMs) occasionally generate plausible yet\nfactually incorrect responses, known as hallucinations. Researchers are\nprimarily using two approaches to mitigate hallucinations, namely Retrieval\nAugmented Language Models (RALMs) and refusal post-training. However, current\nresearch predominantly emphasizes their individual effectiveness while\noverlooking the evaluation of the refusal capability of RALMs. In this study,\nwe ask the fundamental question: Do RALMs know when they don't know?\nSpecifically, we ask three questions. First, are RALMs well-calibrated\nregarding different internal and external knowledge states? We examine the\ninfluence of various factors. Contrary to expectations, we find that LLMs\nexhibit significant \\textbf{over-refusal} behavior. Then, how does refusal\npost-training affect the over-refusal issue? We investigate the Refusal-aware\nInstruction Tuning and In-Context Fine-tuning methods. Our results show that\nthe over-refusal problem is mitigated by In-context fine-tuning. but magnified\nby R-tuning. However, we also find that the refusal ability may conflict with\nthe quality of the answer. Finally, we develop a simple yet effective refusal\nmethod for refusal post-trained models to improve their overall answer quality\nin terms of refusal and correct answers. Our study provides a more\ncomprehensive understanding of the influence of important factors on RALM\nsystems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u68c0\u7d22\u589e\u5f3a\u8bed\u8a00\u6a21\u578b(RALMs)\u7684\u62d2\u7edd\u80fd\u529b\uff0c\u53d1\u73b0LLMs\u5b58\u5728\u663e\u8457\u8fc7\u5ea6\u62d2\u7edd\u884c\u4e3a\uff0c\u7814\u7a76\u4e86\u4e0d\u540c\u540e\u8bad\u7ec3\u65b9\u6cd5\u5bf9\u8fc7\u5ea6\u62d2\u7edd\u95ee\u9898\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u62d2\u7edd\u65b9\u6cd5\u6765\u63d0\u9ad8\u7b54\u6848\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f1a\u4ea7\u751f\u4e8b\u5b9e\u9519\u8bef\u7684\u5e7b\u89c9\uff0c\u7814\u7a76\u8005\u4e3b\u8981\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u548c\u62d2\u7edd\u540e\u8bad\u7ec3\u4e24\u79cd\u65b9\u6cd5\uff0c\u4f46\u5f53\u524d\u7814\u7a76\u5ffd\u89c6\u4e86RALMs\u62d2\u7edd\u80fd\u529b\u7684\u8bc4\u4f30\uff0c\u9700\u8981\u63a2\u8ba8RALMs\u662f\u5426\u77e5\u9053\u4f55\u65f6\u4e0d\u77e5\u9053\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u5185\u90e8\u548c\u5916\u90e8\u77e5\u8bc6\u72b6\u6001\u4e0bRALMs\u7684\u6821\u51c6\u60c5\u51b5\uff0c\u7814\u7a76\u62d2\u7edd\u540e\u8bad\u7ec3\u5bf9\u8fc7\u5ea6\u62d2\u7edd\u95ee\u9898\u7684\u5f71\u54cd\uff0c\u6d4b\u8bd5Refusal-aware Instruction Tuning\u548cIn-Context Fine-tuning\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u65b0\u7684\u62d2\u7edd\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0LLMs\u5b58\u5728\u663e\u8457\u8fc7\u5ea6\u62d2\u7edd\u884c\u4e3a\uff1bIn-context fine-tuning\u80fd\u7f13\u89e3\u8fc7\u5ea6\u62d2\u7edd\u95ee\u9898\uff0c\u4f46R-tuning\u4f1a\u52a0\u5267\u8be5\u95ee\u9898\uff1b\u62d2\u7edd\u80fd\u529b\u53ef\u80fd\u4e0e\u7b54\u6848\u8d28\u91cf\u5b58\u5728\u51b2\u7a81\u3002", "conclusion": "\u7814\u7a76\u5f00\u53d1\u4e86\u7b80\u5355\u6709\u6548\u7684\u62d2\u7edd\u65b9\u6cd5\u6765\u63d0\u9ad8\u62d2\u7edd\u540e\u8bad\u7ec3\u6a21\u578b\u7684\u6574\u4f53\u7b54\u6848\u8d28\u91cf\uff0c\u4e3a\u7406\u89e3\u91cd\u8981\u56e0\u7d20\u5bf9RALM\u7cfb\u7edf\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8ba4\u8bc6\u3002"}}
{"id": "2509.01514", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.01514", "abs": "https://arxiv.org/abs/2509.01514", "authors": ["Andreas Ottem"], "title": "MeVe: A Modular System for Memory Verification and Effective Context Control in Language Models", "comment": "16 pages, 7 figures, held online presentation at NLPA 2025", "summary": "Retrieval-Augmented Generation (RAG) systems typically face constraints\nbecause of their inherent mechanism: a simple top-k semantic search [1]. The\napproach often leads to the incorporation of irrelevant or redundant\ninformation in the context, degrading performance and efficiency [10][11]. This\npaper presents MeVe, a novel modular architecture intended for Memory\nVerification and smart context composition. MeVe rethinks the RAG paradigm by\nproposing a five-phase modular design that distinctly breaks down the retrieval\nand context composition process into distinct, auditable, and independently\ntunable phases: initial retrieval, relevance verification, fallback retrieval,\ncontext prioritization, and token budgeting. This architecture enables\nfine-grained control of what knowledge is made available to an LLM, enabling\ntask-dependent filtering and adaptation. We release a reference implementation\nof MeVe as a proof of concept and evaluate its performance on knowledge-heavy\nQA tasks over a subset of English Wikipedia [22]. Our results demonstrate that\nby actively verifying information before composition, MeVe significantly\nimproves context efficiency, achieving a 57% reduction on the Wikipedia dataset\nand a 75% reduction on the more complex HotpotQA dataset compared to standard\nRAG implementations [25]. This work provides a framework for more scalable and\nreliable LLM applications. By refining and distilling contextual information,\nMeVe offers a path toward better grounding and more accurate factual support\n[16].", "AI": {"tldr": "MeVe\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6a21\u5757\u5316RAG\u67b6\u6784\uff0c\u901a\u8fc7\u4e94\u9636\u6bb5\u8bbe\u8ba1\u5b9e\u73b0\u5185\u5b58\u9a8c\u8bc1\u548c\u667a\u80fd\u4e0a\u4e0b\u6587\u7ec4\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e0a\u4e0b\u6587\u6548\u7387\uff0c\u5728Wikipedia\u6570\u636e\u96c6\u4e0a\u51cf\u5c1157%\uff0c\u5728HotpotQA\u4e0a\u51cf\u5c1175%\u7684\u4e0a\u4e0b\u6587\u4f7f\u7528\u3002", "motivation": "\u4f20\u7edfRAG\u7cfb\u7edf\u91c7\u7528\u7b80\u5355\u7684top-k\u8bed\u4e49\u641c\u7d22\u673a\u5236\uff0c\u7ecf\u5e38\u5bfc\u81f4\u5305\u542b\u4e0d\u76f8\u5173\u6216\u5197\u4f59\u4fe1\u606f\uff0c\u4ece\u800c\u964d\u4f4e\u6027\u80fd\u548c\u5904\u7406\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e94\u9636\u6bb5\u6a21\u5757\u5316\u8bbe\u8ba1\uff1a\u521d\u59cb\u68c0\u7d22\u3001\u76f8\u5173\u6027\u9a8c\u8bc1\u3001\u5907\u7528\u68c0\u7d22\u3001\u4e0a\u4e0b\u6587\u4f18\u5148\u7ea7\u6392\u5e8f\u548ctoken\u9884\u7b97\u7ba1\u7406\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u4efb\u52a1\u4f9d\u8d56\u7684\u8fc7\u6ee4\u9002\u914d\u3002", "result": "\u5728\u77e5\u8bc6\u5bc6\u96c6\u578bQA\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u6807\u51c6RAG\u5b9e\u73b0\uff0c\u5728Wikipedia\u6570\u636e\u96c6\u4e0a\u51cf\u5c1157%\u7684\u4e0a\u4e0b\u6587\u4f7f\u7528\uff0c\u5728\u66f4\u590d\u6742\u7684HotpotQA\u6570\u636e\u96c6\u4e0a\u51cf\u5c1175%\u3002", "conclusion": "MeVe\u901a\u8fc7\u4e3b\u52a8\u9a8c\u8bc1\u4fe1\u606f\u548c\u7cbe\u70bc\u4e0a\u4e0b\u6587\uff0c\u4e3a\u66f4\u53ef\u6269\u5c55\u548c\u53ef\u9760\u7684LLM\u5e94\u7528\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u57fa\u7840\u652f\u6301\u548c\u66f4\u51c6\u786e\u7684\u4e8b\u5b9e\u652f\u6491\u3002"}}
{"id": "2509.01529", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01529", "abs": "https://arxiv.org/abs/2509.01529", "authors": ["Thomas Compton"], "title": "Service, Solidarity, and Self-Help: A Comparative Topic Modeling Analysis of Community Unionism in the Boot and Shoe Union and Unite Community", "comment": "10 pages, 7 figures, conference paper", "summary": "This paper presents a comparative analysis of community unionism (CU) in two\ndistinct historical and organizational contexts: the National Boot and Shoe\nUnion (B\\&S) in the 1920s and Unite Community in the 2010s--2020s. Using\nBERTopic for thematic modeling and cTF-IDF weighting, alongside word frequency\nanalysis, the study examines the extent to which each union's discourse aligns\nwith key features of CU -- such as coalition-building, grassroots engagement,\nand action beyond the workplace. The results reveal significant differences in\nthematic focus and discursive coherence. While Unite Community demonstrates\nstronger alignment with outward-facing, social justice-oriented themes, the\nB\\&S corpus emphasizes internal administration, industrial relations, and\nmember services -- reflecting a more traditional, servicing-oriented union\nmodel. The analysis also highlights methodological insights, demonstrating how\nmodern NLP techniques can enhance the study of historical labor archives.\nUltimately, the findings suggest that while both unions engage with\ncommunity-related themes, their underlying models of engagement diverge\nsignificantly, challenging assumptions about the continuity and universality of\ncommunity unionism across time and sector.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7BERTopic\u548c\u8bcd\u9891\u5206\u6790\u6bd4\u8f83\u4e86\u4e24\u4e2a\u4e0d\u540c\u65f6\u671f\u793e\u533a\u5de5\u4f1a\u6a21\u5f0f\uff0c\u53d1\u73b0Unite Community\u66f4\u7d27\u5bc6\u5bf9\u51c6\u793e\u4f1a\u6b63\u4e49\u4e3b\u9898\uff0c\u800c\u80a1\u9774\u5de5\u4f1a\u5219\u4ee5\u4f20\u7edf\u670d\u52a1\u6a21\u5f0f\u4e3a\u4e3b\uff0c\u8bf4\u660e\u793e\u533a\u5de5\u4f1a\u6a21\u5f0f\u5728\u4e0d\u540c\u65f6\u671f\u548c\u90e8\u95e8\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u6bd4\u8f83\u4e0d\u540c\u5386\u53f2\u65f6\u671f\u548c\u7ec4\u7ec7\u80cc\u666f\u4e0b\u793e\u533a\u5de5\u4f1a\u6a21\u5f0f\u7684\u8bdd\u8bed\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u4ee5\u9a8c\u8bc1\u793e\u533a\u5de5\u4f1a\u6a21\u5f0f\u5728\u4e0d\u540c\u65f6\u4ee3\u548c\u90e8\u95e8\u7684\u8fde\u7eed\u6027\u548c\u666e\u904d\u6027\u5047\u8bbe\u3002", "method": "\u91c7\u7528BERTopic\u4e3b\u9898\u5efa\u6a21\u548ccTF-IDF\u6743\u91cd\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bcd\u9891\u5206\u6790\uff0c\u5bf9\u4e24\u4e2a\u5de5\u4f1a\u7684\u8bdd\u8bed\u8fdb\u884c\u5b9a\u91cf\u5206\u6790\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4e24\u4e2a\u5de5\u4f1a\u5728\u4e3b\u9898\u91cd\u70b9\u548c\u8bdd\u8bed\u4e00\u81f4\u6027\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1aUnite Community\u66f4\u5916\u5411\u5316\u3001\u91cd\u89c6\u793e\u4f1a\u6b63\u4e49\u4e3b\u9898\uff0c\u800c\u80a1\u9774\u5de5\u4f1a\u5219\u4ee5\u5185\u90e8\u7ba1\u7406\u3001\u5de5\u4e1a\u5173\u7cfb\u548c\u6210\u5458\u670d\u52a1\u4e3a\u4e3b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u8868\u660e\uff0c\u867d\u7136\u4e24\u4e2a\u5de5\u4f1a\u90fd\u6d89\u53ca\u793e\u533a\u76f8\u5173\u4e3b\u9898\uff0c\u4f46\u5b83\u4eec\u7684\u53c2\u4e0e\u6a21\u5f0f\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8fd9\u6316\u6210\u4e86\u5bf9\u793e\u533a\u5de5\u4f1a\u6a21\u5f0f\u5728\u4e0d\u540c\u65f6\u671f\u548c\u90e8\u95e8\u95f4\u8fde\u7eed\u6027\u548c\u666e\u904d\u6027\u7684\u5047\u8bbe\u3002\u540c\u65f6\u8bc1\u660e\u4e86\u73b0\u4ee3NLP\u6280\u672f\u5728\u5386\u53f2\u52b3\u5de5\u6863\u6848\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.01535", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.01535", "abs": "https://arxiv.org/abs/2509.01535", "authors": ["Kairong Han", "Wenshuo Zhao", "Ziyu Zhao", "JunJian Ye", "Lujia Pan", "Kun Kuang"], "title": "CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models", "comment": "Accepted to EMNLP2025 Main conference", "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains. However, a fundamental question remains: Can LLMs effectively utilize\ncausal knowledge for prediction and generation? Through empirical studies, we\nfind that LLMs trained directly on large-scale data often capture spurious\ncorrelations rather than true causal relationships, leading to suboptimal\nperformance, especially in out-of-distribution (OOD) scenarios. To address this\nchallenge, we propose Causal Attention Tuning (CAT), a novel approach that\ninjects fine-grained causal knowledge into the attention mechanism. We propose\nan automated pipeline that leverages human priors to automatically generate\ntoken-level causal signals and introduce the Re-Attention mechanism to guide\ntraining, helping the model focus on causal structures while mitigating noise\nand biases in attention scores. Experimental results on our proposed Spurious\nToken Game (STG) benchmark and multiple downstream tasks demonstrate that our\napproach effectively leverages causal knowledge for prediction and remains\nrobust in OOD scenarios. Implementation details can be found at\nhttps://github.com/Kairong-Han/CAT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCausal Attention Tuning (CAT)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7ec6\u7c92\u5ea6\u56e0\u679c\u77e5\u8bc6\u6ce8\u5165\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3LLMs\u5728OOD\u573a\u666f\u4e0b\u56e0\u6355\u6349\u4f2a\u76f8\u5173\u800c\u975e\u771f\u5b9e\u56e0\u679c\u5173\u7cfb\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u5927\u89c4\u6a21\u6570\u636e\u8bad\u7ec3\u4e2d\u5f80\u5f80\u6355\u6349\u7684\u662f\u4f2a\u76f8\u5173\u800c\u975e\u771f\u5b9e\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5bfc\u81f4\u5728\u5206\u5e03\u5916(OOD)\u573a\u666f\u4e0b\u6027\u80fd\u4e0d\u4f73\u3002\u9700\u8981\u63a2\u7d22LLMs\u662f\u5426\u80fd\u6709\u6548\u5229\u7528\u56e0\u679c\u77e5\u8bc6\u8fdb\u884c\u9884\u6d4b\u548c\u751f\u6210\u3002", "method": "\u63d0\u51faCausal Attention Tuning (CAT)\u65b9\u6cd5\uff1a1) \u5229\u7528\u4eba\u7c7b\u5148\u9a8c\u81ea\u52a8\u751f\u6210token\u7ea7\u522b\u7684\u56e0\u679c\u4fe1\u53f7\uff1b2) \u5f15\u5165Re-Attention\u673a\u5236\u6307\u5bfc\u8bad\u7ec3\uff0c\u4f7f\u6a21\u578b\u5173\u6ce8\u56e0\u679c\u7ed3\u6784\u540c\u65f6\u51cf\u8f7b\u6ce8\u610f\u529b\u5206\u6570\u4e2d\u7684\u566a\u58f0\u548c\u504f\u5dee\u3002", "result": "\u5728\u63d0\u51fa\u7684Spurious Token Game (STG)\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5229\u7528\u56e0\u679c\u77e5\u8bc6\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u5728OOD\u573a\u666f\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "CAT\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u56e0\u679c\u77e5\u8bc6\u6ce8\u5165LLMs\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u56e0\u679c\u63a8\u7406\u548cOOD\u6cdb\u5316\u65b9\u9762\u7684\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3LLMs\u4e2d\u7684\u4f2a\u76f8\u5173\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2509.01560", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.01560", "abs": "https://arxiv.org/abs/2509.01560", "authors": ["Seungkyu Lee", "Nalim Kim", "Yohan Jo"], "title": "In-N-Out: A Parameter-Level API Graph Dataset for Tool Agents", "comment": null, "summary": "Tool agents -- LLM-based systems that interact with external APIs -- offer a\nway to execute real-world tasks. However, as tasks become increasingly complex,\nthese agents struggle to identify and call the correct APIs in the proper\norder. To tackle this problem, we investigate converting API documentation into\na structured API graph that captures API dependencies and leveraging it for\nmulti-tool queries that require compositional API calls. To support this, we\nintroduce In-N-Out, the first expert-annotated dataset of API graphs built from\ntwo real-world API benchmarks and their documentation. Using In-N-Out\nsignificantly improves performance on both tool retrieval and multi-tool query\ngeneration, nearly doubling that of LLMs using documentation alone. Moreover,\ngraphs generated by models fine-tuned on In-N-Out close 90% of this gap,\nshowing that our dataset helps models learn to comprehend API documentation and\nparameter relationships. Our findings highlight the promise of using explicit\nAPI graphs for tool agents and the utility of In-N-Out as a valuable resource.\nWe will release the dataset and code publicly.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86In-N-Out\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5c06API\u6587\u6863\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316API\u56fe\u6765\u6539\u5584\u5de5\u5177\u4ee3\u7406\u5728\u591a\u5de5\u5177\u67e5\u8be2\u4e2d\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86API\u68c0\u7d22\u548c\u7ec4\u5408\u8c03\u7528\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u4efb\u52a1\u590d\u6742\u6027\u589e\u52a0\uff0c\u57fa\u4e8eLLM\u7684\u5de5\u5177\u4ee3\u7406\u96be\u4ee5\u6b63\u786e\u8bc6\u522b\u548c\u6309\u987a\u5e8f\u8c03\u7528API\uff0c\u9700\u8981\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u7406\u89e3API\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u5c06API\u6587\u6863\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316API\u56fe\uff0c\u6784\u5efa\u4e13\u5bb6\u6807\u6ce8\u7684API\u56fe\u6570\u636e\u96c6In-N-Out\uff0c\u5e76\u5229\u7528\u8be5\u6570\u636e\u96c6\u8fdb\u884c\u6a21\u578b\u5fae\u8c03\u3002", "result": "\u4f7f\u7528In-N-Out\u6570\u636e\u96c6\u540e\uff0c\u5de5\u5177\u68c0\u7d22\u548c\u591a\u5de5\u5177\u67e5\u8be2\u751f\u6210\u7684\u6027\u80fd\u51e0\u4e4e\u7ffb\u500d\uff0c\u5fae\u8c03\u6a21\u578b\u80fd\u591f\u586b\u886590%\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u663e\u5f0fAPI\u56fe\u5bf9\u5de5\u5177\u4ee3\u7406\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0cIn-N-Out\u6570\u636e\u96c6\u662f\u7406\u89e3API\u6587\u6863\u548c\u53c2\u6570\u5173\u7cfb\u7684\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2509.01564", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01564", "abs": "https://arxiv.org/abs/2509.01564", "authors": ["Zeguan Xiao", "Diyang Dou", "Boya Xiong", "Yun Chen", "Guanhua Chen"], "title": "Enhancing Uncertainty Estimation in LLMs with Expectation of Aggregated Internal Belief", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success across a wide\nrange of natural language tasks, but often exhibit overconfidence and generate\nplausible yet incorrect answers. This overconfidence, especially in models\nundergone Reinforcement Learning from Human Feedback (RLHF), poses significant\nchallenges for reliable uncertainty estimation and safe deployment. In this\npaper, we propose EAGLE (Expectation of AGgregated internaL bEief), a novel\nself-evaluation-based calibration method that leverages the internal hidden\nstates of LLMs to derive more accurate confidence scores. Instead of relying on\nthe model's final output, our approach extracts internal beliefs from multiple\nintermediate layers during self-evaluation. By aggregating these layer-wise\nbeliefs and calculating the expectation over the resulting confidence score\ndistribution, EAGLE produces a refined confidence score that more faithfully\nreflects the model's internal certainty. Extensive experiments on diverse\ndatasets and LLMs demonstrate that EAGLE significantly improves calibration\nperformance over existing baselines. We also provide an in-depth analysis of\nEAGLE, including a layer-wise examination of uncertainty patterns, a study of\nthe impact of self-evaluation prompts, and an analysis of the effect of\nself-evaluation score range.", "AI": {"tldr": "EAGLE\u662f\u4e00\u79cd\u57fa\u4e8e\u81ea\u8bc4\u4f30\u7684\u6821\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u5408LLM\u5185\u90e8\u9690\u85cf\u72b6\u6001\u7684\u591a\u5c42\u4fe1\u5ff5\u6765\u751f\u6210\u66f4\u51c6\u786e\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u663e\u8457\u6539\u5584\u4e86\u6a21\u578b\u6821\u51c6\u6027\u80fd", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728RLHF\u540e\u7ecf\u5e38\u8868\u73b0\u51fa\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u9519\u8bef\u7684\u7b54\u6848\uff0c\u8fd9\u7ed9\u53ef\u9760\u7684uncertainty estimation\u548c\u5b89\u5168\u90e8\u7f72\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218", "method": "\u63d0\u51faEAGLE\u65b9\u6cd5\uff0c\u4ece\u81ea\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u63d0\u53d6\u591a\u4e2a\u4e2d\u95f4\u5c42\u7684\u5185\u90e8\u4fe1\u5ff5\uff0c\u805a\u5408\u8fd9\u4e9b\u5c42\u95f4\u4fe1\u5ff5\u5e76\u8ba1\u7b97\u7f6e\u4fe1\u5ea6\u5206\u6570\u5206\u5e03\u7684\u671f\u671b\u503c", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548cLLM\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cEAGLE\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u7684\u6821\u51c6\u6027\u80fd", "conclusion": "EAGLE\u901a\u8fc7\u5229\u7528\u6a21\u578b\u5185\u90e8\u9690\u85cf\u72b6\u6001\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\uff0c\u4e3aLLM\u7684\u53ef\u9760\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.01606", "categories": ["cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.01606", "abs": "https://arxiv.org/abs/2509.01606", "authors": ["Vivi Nastase", "Paola Merlo"], "title": "Testing the assumptions about the geometry of sentence embedding spaces: the cosine measure need not apply", "comment": "25 pages, 6 tables, 10 figures", "summary": "Transformer models learn to encode and decode an input text, and produce\ncontextual token embeddings as a side-effect. The mapping from language into\nthe embedding space maps words expressing similar concepts onto points that are\nclose in the space. In practice, the reverse implication is also assumed: words\ncorresponding to close points in this space are similar or related, those that\nare further are not.\n  Does closeness in the embedding space extend to shared properties for\nsentence embeddings? We present an investigation of sentence embeddings and\nshow that the geometry of their embedding space is not predictive of their\nrelative performances on a variety of tasks.\n  We compute sentence embeddings in three ways: as averaged token embeddings,\nas the embedding of the special [CLS] token, and as the embedding of a random\ntoken from the sentence. We explore whether there is a correlation between the\ndistance between sentence embedding variations and their performance on\nlinguistic tasks, and whether despite their distances, they do encode the same\ninformation in the same manner.\n  The results show that the cosine similarity -- which treats dimensions\nshallowly -- captures (shallow) commonalities or differences between sentence\nembeddings, which are not predictive of their performance on specific tasks.\nLinguistic information is rather encoded in weighted combinations of different\ndimensions, which are not reflected in the geometry of the sentence embedding\nspace.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u53e5\u5b50\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u51e0\u4f55\u8ddd\u79bb\u5e76\u4e0d\u80fd\u9884\u6d4b\u5176\u5728\u5177\u4f53\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u8bed\u8a00\u4fe1\u606f\u662f\u901a\u8fc7\u4e0d\u540c\u7ef4\u5ea6\u7684\u52a0\u6743\u7ec4\u5408\u7f16\u7801\u7684\uff0c\u800c\u975e\u53cd\u6620\u5728\u5d4c\u5165\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\u4e2d\u3002", "motivation": "\u63a2\u7a76\u53e5\u5b50\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8ddd\u79bb\u76f8\u8fd1\u662f\u5426\u610f\u5473\u7740\u5171\u4eab\u76f8\u4f3c\u5c5e\u6027\uff0c\u4ee5\u53ca\u8fd9\u79cd\u51e0\u4f55\u5173\u7cfb\u662f\u5426\u80fd\u9884\u6d4b\u53e5\u5b50\u5d4c\u5165\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u76f8\u5bf9\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u4e09\u79cd\u65b9\u5f0f\u8ba1\u7b97\u53e5\u5b50\u5d4c\u5165\uff1a\u5e73\u5747\u8bcd\u5d4c\u5165\u3001\u7279\u6b8a[CLS]\u6807\u8bb0\u5d4c\u5165\u548c\u968f\u673a\u8bcd\u5d4c\u5165\uff0c\u5206\u6790\u8fd9\u4e9b\u53d8\u4f53\u4e4b\u95f4\u7684\u8ddd\u79bb\u4e0e\u8bed\u8a00\u4efb\u52a1\u6027\u80fd\u7684\u76f8\u5173\u6027\u3002", "result": "\u4f59\u5f26\u76f8\u4f3c\u5ea6\u53ea\u80fd\u6355\u6349\u53e5\u5b50\u5d4c\u5165\u4e4b\u95f4\u7684\u6d45\u5c42\u5171\u6027\u6216\u5dee\u5f02\uff0c\u8fd9\u4e9b\u5dee\u5f02\u4e0d\u80fd\u9884\u6d4b\u5177\u4f53\u4efb\u52a1\u6027\u80fd\u3002\u8bed\u8a00\u4fe1\u606f\u662f\u901a\u8fc7\u4e0d\u540c\u7ef4\u5ea6\u7684\u52a0\u6743\u7ec4\u5408\u7f16\u7801\u7684\u3002", "conclusion": "\u53e5\u5b50\u5d4c\u5165\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\u4e0d\u80fd\u6709\u6548\u9884\u6d4b\u5176\u4efb\u52a1\u6027\u80fd\uff0c\u8bed\u8a00\u4fe1\u606f\u7684\u7f16\u7801\u65b9\u5f0f\u6bd4\u7b80\u5355\u7684\u51e0\u4f55\u8ddd\u79bb\u5173\u7cfb\u66f4\u4e3a\u590d\u6742\u548c\u6df1\u5c42\u3002"}}
{"id": "2509.01620", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.01620", "abs": "https://arxiv.org/abs/2509.01620", "authors": ["Shanshan Wang", "Junchao Wu", "Fengying Ye", "Jingming Yao", "Lidia S. Chao", "Derek F. Wong"], "title": "Benchmarking the Detection of LLMs-Generated Modern Chinese Poetry", "comment": "Accepted by EMNLP 2025", "summary": "The rapid development of advanced large language models (LLMs) has made\nAI-generated text indistinguishable from human-written text. Previous work on\ndetecting AI-generated text has made effective progress, but has not involved\nmodern Chinese poetry. Due to the distinctive characteristics of modern Chinese\npoetry, it is difficult to identify whether a poem originated from humans or\nAI. The proliferation of AI-generated modern Chinese poetry has significantly\ndisrupted the poetry ecosystem. Based on the urgency of identifying\nAI-generated poetry in the real Chinese world, this paper proposes a novel\nbenchmark for detecting LLMs-generated modern Chinese poetry. We first\nconstruct a high-quality dataset, which includes both 800 poems written by six\nprofessional poets and 41,600 poems generated by four mainstream LLMs.\nSubsequently, we conduct systematic performance assessments of six detectors on\nthis dataset. Experimental results demonstrate that current detectors cannot be\nused as reliable tools to detect modern Chinese poems generated by LLMs. The\nmost difficult poetic features to detect are intrinsic qualities, especially\nstyle. The detection results verify the effectiveness and necessity of our\nproposed benchmark. Our work lays a foundation for future detection of\nAI-generated poetry.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u7528\u4e8e\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u73b0\u4ee3\u8bd7\u6b4c\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u68c0\u6d4b\u5668\u65e0\u6cd5\u53ef\u9760\u8bc6\u522bAI\u751f\u6210\u7684\u73b0\u4ee3\u8bd7\u6b4c\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0cAI\u751f\u6210\u6587\u672c\u4e0e\u4eba\u7c7b\u6587\u672c\u96be\u4ee5\u533a\u5206\uff0c\u800c\u73b0\u4ee3\u8bd7\u6b4c\u68c0\u6d4b\u9886\u57df\u5b58\u5728\u7a7a\u767d\uff0cAI\u751f\u6210\u8bd7\u6b4c\u7684\u6d41\u884c\u4e25\u91cd\u5e72\u6270\u4e86\u8bd7\u6b4c\u751f\u6001\u7cfb\u7edf\u3002", "method": "\u9996\u5148\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5305\u542b6\u4f4d\u4e13\u4e1a\u8bd7\u4eba\u7684800\u9996\u8bd7\u6b4c\u548c4\u4e2a\u4e3b\u6d41LLMs\u751f\u6210\u768441,600\u9996\u8bd7\u6b4c\uff0c\u7136\u540e\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e866\u4e2a\u68c0\u6d4b\u5668\u5728\u8fd9\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u68c0\u6d4b\u5668\u65e0\u6cd5\u4f5c\u4e3a\u53ef\u9760\u5de5\u5177\u68c0\u6d4bLLMs\u751f\u6210\u7684\u73b0\u4ee3\u8bd7\u6b4c\uff0c\u6700\u96be\u68c0\u6d4b\u7684\u8bd7\u6b4c\u7279\u5f81\u662f\u5185\u5728\u54c1\u8d28\uff08\u7279\u522b\u662f\u98ce\u683c\uff09\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u672a\u6765AI\u751f\u6210\u8bd7\u6b4c\u7684\u68c0\u6d4b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6240\u63d0\u51fa\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.01640", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.01640", "abs": "https://arxiv.org/abs/2509.01640", "authors": ["Hind Aljuaid", "Areej Alhothali", "Ohoud Al-Zamzami", "Hussein Assalahi"], "title": "TransGAT: Transformer-Based Graph Neural Networks for Multi-Dimensional Automated Essay Scoring", "comment": null, "summary": "Essay writing is a critical component of student assessment, yet manual\nscoring is labor-intensive and inconsistent. Automated Essay Scoring (AES)\noffers a promising alternative, but current approaches face limitations. Recent\nstudies have incorporated Graph Neural Networks (GNNs) into AES using static\nword embeddings that fail to capture contextual meaning, especially for\npolysemous words. Additionally, many methods rely on holistic scoring,\noverlooking specific writing aspects such as grammar, vocabulary, and cohesion.\nTo address these challenges, this study proposes TransGAT, a novel approach\nthat integrates fine-tuned Transformer models with GNNs for analytic scoring.\nTransGAT combines the contextual understanding of Transformers with the\nrelational modeling strength of Graph Attention Networks (GAT). It performs\ntwo-stream predictions by pairing each fine-tuned Transformer (BERT, RoBERTa,\nand DeBERTaV3) with a separate GAT. In each pair, the first stream generates\nessay-level predictions, while the second applies GAT to Transformer token\nembeddings, with edges constructed from syntactic dependencies. The model then\nfuses predictions from both streams to produce the final analytic score.\nExperiments on the ELLIPSE dataset show that TransGAT outperforms baseline\nmodels, achieving an average Quadratic Weighted Kappa (QWK) of 0.854 across all\nanalytic scoring dimensions. These findings highlight the potential of TransGAT\nto advance AES systems.", "AI": {"tldr": "TransGAT\u6a21\u578b\u7ed3\u5408\u5fae\u8c03Transformer\u548c\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u4f5c\u6587\u5206\u6790\u8bc4\u5206\uff0c\u5728ELLIPSE\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u4eba\u5de5\u8bc4\u5206\u8017\u65f6\u4e14\u4e0d\u4e00\u81f4\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u8bc4\u5206\u65b9\u6cd5\u4f7f\u7528\u9759\u6001\u8bcd\u5d4c\u5165\u65e0\u6cd5\u6355\u6349\u4e0a\u4e0b\u6587\u8bed\u4e49\uff0c\u4e14\u591a\u91c7\u7528\u6574\u4f53\u8bc4\u5206\u800c\u5ffd\u7565\u5177\u4f53\u5199\u4f5c\u7ef4\u5ea6\u3002", "method": "\u63d0\u51faTransGAT\u65b9\u6cd5\uff0c\u5c06\u5fae\u8c03Transformer\uff08BERT\u3001RoBERTa\u3001DeBERTaV3\uff09\u4e0e\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u914d\u5bf9\uff0c\u8fdb\u884c\u53cc\u6d41\u9884\u6d4b\uff1a\u4e00\u6d41\u751f\u6210\u6587\u7ae0\u7ea7\u9884\u6d4b\uff0c\u4e8c\u6d41\u5bf9Transformer\u8bcd\u5d4c\u5165\u5e94\u7528GAT\uff08\u57fa\u4e8e\u53e5\u6cd5\u4f9d\u8d56\u6784\u5efa\u8fb9\uff09\uff0c\u6700\u540e\u878d\u5408\u4e24\u6d41\u9884\u6d4b\u3002", "result": "\u5728ELLIPSE\u6570\u636e\u96c6\u4e0a\uff0cTransGAT\u5e73\u5747\u4e8c\u6b21\u52a0\u6743Kappa\u8fbe\u52300.854\uff0c\u5728\u6240\u6709\u5206\u6790\u8bc4\u5206\u7ef4\u5ea6\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "TransGAT\u5c55\u793a\u4e86\u7ed3\u5408Transformer\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u548cGAT\u5173\u7cfb\u5efa\u6a21\u4f18\u52bf\u7684\u6f5c\u529b\uff0c\u53ef\u63a8\u52a8\u81ea\u52a8\u5316\u4f5c\u6587\u8bc4\u5206\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2509.01654", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01654", "abs": "https://arxiv.org/abs/2509.01654", "authors": ["Dominic Plein"], "title": "Parallel Needleman-Wunsch on CUDA to measure word similarity based on phonetic transcriptions", "comment": "11 pages, 12 figures, accompanied by a YouTube video\n  (https://youtu.be/xbcpnItE3_4) and a GitHub repository\n  (https://github.com/Splines/phonetics-graph/)", "summary": "We present a method to calculate the similarity between words based on their\nphonetic transcription (their pronunciation) using the Needleman-Wunsch\nalgorithm. We implement this algorithm in Rust and parallelize it on both CPU\nand GPU to handle large datasets efficiently. The GPU implementation leverages\nCUDA and the cudarc Rust library to achieve significant performance\nimprovements. We validate our approach by constructing a fully-connected graph\nwhere nodes represent words and edges have weights according to the similarity\nbetween the words. This graph is then analyzed using clustering algorithms to\nidentify groups of phonetically similar words. Our results demonstrate the\nfeasibility and effectiveness of the proposed method in analyzing the phonetic\nstructure of languages. It might be easily expanded to other languages.", "AI": {"tldr": "\u57fa\u4e8eNeedleman-Wunsch\u7b97\u6cd5\u8ba1\u7b97\u8bcd\u8bed\u8bed\u97f3\u76f8\u4f3c\u5ea6\uff0c\u4f7f\u7528Rust\u5b9e\u73b0\u5e76\u5229\u7528CPU/GPU\u5e76\u884c\u5904\u7406\uff0c\u901a\u8fc7\u6784\u5efa\u5168\u8fde\u63a5\u56fe\u548c\u805a\u7c7b\u5206\u6790\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027", "motivation": "\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5206\u6790\u8bcd\u8bed\u4e4b\u95f4\u7684\u8bed\u97f3\u76f8\u4f3c\u6027\uff0c\u4ee5\u4fbf\u7814\u7a76\u8bed\u8a00\u7684\u8bed\u97f3\u7ed3\u6784\u7279\u5f81", "method": "\u4f7f\u7528Needleman-Wunsch\u7b97\u6cd5\u8ba1\u7b97\u8bed\u97f3\u8f6c\u5f55\u7684\u76f8\u4f3c\u5ea6\uff0c\u5728Rust\u4e2d\u5b9e\u73b0\u5e76\u5229\u7528CUDA\u548ccudarc\u5e93\u8fdb\u884cCPU\u548cGPU\u5e76\u884c\u5316\u5904\u7406\uff0c\u6784\u5efa\u5168\u8fde\u63a5\u56fe\u5e76\u901a\u8fc7\u805a\u7c7b\u7b97\u6cd5\u5206\u6790", "result": "\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\u65f6\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff0cGPU\u5b9e\u73b0\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u6210\u529f\u8bc6\u522b\u51fa\u8bed\u97f3\u76f8\u4f3c\u7684\u8bcd\u8bed\u7fa4\u7ec4", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5206\u6790\u8bed\u8a00\u8bed\u97f3\u7ed3\u6784\u65b9\u9762\u5177\u6709\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\uff0c\u4e14\u6613\u4e8e\u6269\u5c55\u5230\u5176\u4ed6\u8bed\u8a00"}}
{"id": "2509.01660", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01660", "abs": "https://arxiv.org/abs/2509.01660", "authors": ["Zhengjia Wang", "Qiang Sheng", "Danding Wang", "Beizhe Hu", "Juan Cao"], "title": "Bridging Thoughts and Words: Graph-Based Intent-Semantic Joint Learning for Fake News Detection", "comment": "Accepted to CIKM'25", "summary": "Fake news detection is an important and challenging task for defending online\ninformation integrity. Existing state-of-the-art approaches typically extract\nnews semantic clues, such as writing patterns that include emotional words,\nstylistic features, etc. However, detectors tuned solely to such semantic clues\ncan easily fall into surface detection patterns, which can shift rapidly in\ndynamic environments, leading to limited performance in the evolving news\nlandscape. To address this issue, this paper investigates a novel perspective\nby incorporating news intent into fake news detection, bridging intents and\nsemantics together. The core insight is that by considering news intents, one\ncan deeply understand the inherent thoughts behind news deception, rather than\nthe surface patterns within words alone. To achieve this goal, we propose\nGraph-based Intent-Semantic Joint Modeling (InSide) for fake news detection,\nwhich models deception clues from both semantic and intent signals via\ngraph-based joint learning. Specifically, InSide reformulates news semantic and\nintent signals into heterogeneous graph structures, enabling long-range context\ninteraction through entity guidance and capturing both holistic and\nimplementation-level intent via coarse-to-fine intent modeling. To achieve\nbetter alignment between semantics and intents, we further develop a dynamic\npathway-based graph alignment strategy for effective message passing and\naggregation across these signals by establishing a common space. Extensive\nexperiments on four benchmark datasets demonstrate the superiority of the\nproposed InSide compared to state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faInSide\u6a21\u578b\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u8054\u5408\u5efa\u6a21\u65b0\u95fb\u8bed\u4e49\u548c\u610f\u56fe\u4fe1\u53f7\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u5047\u65b0\u95fb\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8bed\u4e49\u7ebf\u7d22\uff08\u5982\u60c5\u611f\u8bcd\u3001\u6587\u4f53\u7279\u5f81\uff09\uff0c\u5bb9\u6613\u9677\u5165\u8868\u9762\u6a21\u5f0f\u68c0\u6d4b\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6027\u80fd\u6709\u9650\u3002\u672c\u6587\u4ece\u65b0\u95fb\u610f\u56fe\u7684\u65b0\u89c6\u89d2\u51fa\u53d1\uff0c\u8ba4\u4e3a\u7ed3\u5408\u610f\u56fe\u53ef\u4ee5\u66f4\u6df1\u5165\u7406\u89e3\u65b0\u95fb\u6b3a\u9a97\u7684\u5185\u5728\u601d\u60f3", "method": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u7684\u610f\u56fe-\u8bed\u4e49\u8054\u5408\u5efa\u6a21\uff08InSide\uff09\u65b9\u6cd5\uff1a1\uff09\u5c06\u8bed\u4e49\u548c\u610f\u56fe\u4fe1\u53f7\u91cd\u6784\u4e3a\u5f02\u6784\u56fe\u7ed3\u6784\uff1b2\uff09\u901a\u8fc7\u5b9e\u4f53\u5f15\u5bfc\u5b9e\u73b0\u957f\u8ddd\u79bb\u4e0a\u4e0b\u6587\u4ea4\u4e92\uff1b3\uff09\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u610f\u56fe\u5efa\u6a21\u6355\u83b7\u6574\u4f53\u548c\u5b9e\u65bd\u5c42\u9762\u7684\u610f\u56fe\uff1b4\uff09\u5f00\u53d1\u57fa\u4e8e\u52a8\u6001\u8def\u5f84\u7684\u56fe\u5bf9\u9f50\u7b56\u7565\uff0c\u5728\u5171\u540c\u7a7a\u95f4\u4e2d\u8fdb\u884c\u6709\u6548\u6d88\u606f\u4f20\u9012\u548c\u805a\u5408", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51faInSide\u65b9\u6cd5\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027", "conclusion": "\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u65b0\u95fb\u8bed\u4e49\u548c\u610f\u56fe\u4fe1\u53f7\uff0cInSide\u65b9\u6cd5\u80fd\u591f\u66f4\u6709\u6548\u5730\u68c0\u6d4b\u5047\u65b0\u95fb\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u8868\u9762\u8bed\u4e49\u6a21\u5f0f\u7684\u95ee\u9898"}}
{"id": "2509.01772", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.01772", "abs": "https://arxiv.org/abs/2509.01772", "authors": ["Abdelkrime Aries"], "title": "chDzDT: Word-level morphology-aware language model for Algerian social media text", "comment": null, "summary": "Pre-trained language models (PLMs) have substantially advanced natural\nlanguage processing by providing context-sensitive text representations.\nHowever, the Algerian dialect remains under-represented, with few dedicated\nmodels available. Processing this dialect is challenging due to its complex\nmorphology, frequent code-switching, multiple scripts, and strong lexical\ninfluences from other languages. These characteristics complicate tokenization\nand reduce the effectiveness of conventional word- or subword-level approaches.\n  To address this gap, we introduce chDzDT, a character-level pre-trained\nlanguage model tailored for Algerian morphology. Unlike conventional PLMs that\nrely on token sequences, chDzDT is trained on isolated words. This design\nallows the model to encode morphological patterns robustly, without depending\non token boundaries or standardized orthography. The training corpus draws from\ndiverse sources, including YouTube comments, French, English, and Berber\nWikipedia, as well as the Tatoeba project. It covers multiple scripts and\nlinguistic varieties, resulting in a substantial pre-training workload.\n  Our contributions are threefold: (i) a detailed morphological analysis of\nAlgerian dialect using YouTube comments; (ii) the construction of a\nmultilingual Algerian lexicon dataset; and (iii) the development and extensive\nevaluation of a character-level PLM as a morphology-focused encoder for\ndownstream tasks. The proposed approach demonstrates the potential of\ncharacter-level modeling for morphologically rich, low-resource dialects and\nlays a foundation for more inclusive and adaptable NLP systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86chDzDT\uff0c\u4e00\u4e2a\u9762\u5411\u963f\u5c14\u53ca\u5229\u4e9a\u65b9\u8a00\u7684\u5b57\u7b26\u7ea7\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u95e8\u5904\u7406\u8be5\u65b9\u8a00\u590d\u6742\u7684\u5f62\u6001\u5b66\u7279\u5f81\u548c\u591a\u8bed\u8a00\u6df7\u5408\u95ee\u9898", "motivation": "\u963f\u5c14\u53ca\u5229\u4e9a\u65b9\u8a00\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u5176\u590d\u6742\u7684\u5f62\u6001\u5b66\u3001\u9891\u7e41\u7684\u8bed\u7801\u8f6c\u6362\u3001\u591a\u6587\u5b57\u7cfb\u7edf\u548c\u5f3a\u8bcd\u6c47\u5f71\u54cd\u4f7f\u5f97\u4f20\u7edf\u8bcd\u7ea7\u6216\u5b50\u8bcd\u7ea7\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73", "method": "\u5f00\u53d1\u5b57\u7b26\u7ea7\u9884\u8bad\u7ec3\u6a21\u578bchDzDT\uff0c\u5728\u5b64\u7acb\u8bcd\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4e0d\u4f9d\u8d56\u5206\u8bcd\u8fb9\u754c\u6216\u6807\u51c6\u5316\u62fc\u5199\u3002\u8bad\u7ec3\u8bed\u6599\u6765\u81eaYouTube\u8bc4\u8bba\u3001\u591a\u8bed\u8a00\u7ef4\u57fa\u767e\u79d1\u548cTatoeba\u9879\u76ee\uff0c\u6db5\u76d6\u591a\u79cd\u6587\u5b57\u548c\u8bed\u8a00\u53d8\u4f53", "result": "\u6784\u5efa\u4e86\u963f\u5c14\u53ca\u5229\u4e9a\u65b9\u8a00\u7684\u591a\u8bed\u8a00\u8bcd\u5178\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86\u4e13\u6ce8\u4e8e\u5f62\u6001\u5b66\u7f16\u7801\u7684\u5b57\u7b26\u7ea7PLM\uff0c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u6709\u6548\u7f16\u7801\u5668", "conclusion": "\u5b57\u7b26\u7ea7\u5efa\u6a21\u5bf9\u4e8e\u5f62\u6001\u4e30\u5bcc\u3001\u8d44\u6e90\u7a00\u7f3a\u7684\u65b9\u8a00\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u6784\u5efa\u66f4\u5177\u5305\u5bb9\u6027\u548c\u9002\u5e94\u6027\u7684NLP\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2509.01790", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.01790", "abs": "https://arxiv.org/abs/2509.01790", "authors": ["Andong Hua", "Kenan Tang", "Chenhe Gu", "Jindong Gu", "Eric Wong", "Yao Qin"], "title": "Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Prompt sensitivity, referring to the phenomenon where paraphrasing (i.e.,\nrepeating something written or spoken using different words) leads to\nsignificant changes in large language model (LLM) performance, has been widely\naccepted as a core limitation of LLMs. In this work, we revisit this issue and\nask: Is the widely reported high prompt sensitivity truly an inherent weakness\nof LLMs, or is it largely an artifact of evaluation processes? To answer this\nquestion, we systematically evaluate 7 LLMs (e.g., GPT and Gemini family)\nacross 6 benchmarks, including both multiple-choice and open-ended tasks on 12\ndiverse prompt templates. We find that much of the prompt sensitivity stems\nfrom heuristic evaluation methods, including log-likelihood scoring and rigid\nanswer matching, which often overlook semantically correct responses expressed\nthrough alternative phrasings, such as synonyms or paraphrases. When we adopt\nLLM-as-a-Judge evaluations, we observe a substantial reduction in performance\nvariance and a consistently higher correlation in model rankings across\nprompts. Our findings suggest that modern LLMs are more robust to prompt\ntemplates than previously believed, and that prompt sensitivity may be more an\nartifact of evaluation than a flaw in the models.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0cLLM\u7684\u63d0\u793a\u654f\u611f\u6027\u66f4\u591a\u662f\u8bc4\u4f30\u65b9\u6cd5\u9020\u6210\u7684\u5047\u8c61\u800c\u975e\u6a21\u578b\u56fa\u6709\u7f3a\u9677\uff0c\u4f7f\u7528LLM-as-a-Judge\u8bc4\u4f30\u53ef\u663e\u8457\u964d\u4f4e\u6027\u80fd\u65b9\u5dee", "motivation": "\u91cd\u65b0\u5ba1\u89c6\u5e7f\u6cdb\u62a5\u9053\u7684LLM\u63d0\u793a\u654f\u611f\u6027\u95ee\u9898\uff0c\u63a2\u7a76\u8fd9\u662f\u5426\u662f\u6a21\u578b\u56fa\u6709\u5f31\u70b9\u8fd8\u662f\u8bc4\u4f30\u8fc7\u7a0b\u7684\u4eba\u4e3a\u4ea7\u7269", "method": "\u7cfb\u7edf\u8bc4\u4f307\u4e2aLLM\u6a21\u578b\u57286\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8868\u73b0\uff0c\u4f7f\u752812\u4e2a\u4e0d\u540c\u63d0\u793a\u6a21\u677f\uff0c\u5bf9\u6bd4\u542f\u53d1\u5f0f\u8bc4\u4f30\u65b9\u6cd5\u548cLLM-as-a-Judge\u65b9\u6cd5", "result": "\u53d1\u73b0\u63d0\u793a\u654f\u611f\u6027\u4e3b\u8981\u6e90\u4e8e\u542f\u53d1\u5f0f\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982\u5bf9\u6570\u4f3c\u7136\u8bc4\u5206\u548c\u4e25\u683c\u7b54\u6848\u5339\u914d\uff09\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u7ecf\u5e38\u5ffd\u7565\u8bed\u4e49\u6b63\u786e\u4f46\u8868\u8ff0\u4e0d\u540c\u7684\u56de\u7b54\u3002\u4f7f\u7528LLM-as-a-Judge\u8bc4\u4f30\u65f6\uff0c\u6027\u80fd\u65b9\u5dee\u663e\u8457\u964d\u4f4e\uff0c\u6a21\u578b\u6392\u540d\u76f8\u5173\u6027\u66f4\u9ad8", "conclusion": "\u73b0\u4ee3LLM\u5bf9\u63d0\u793a\u6a21\u677f\u7684\u9c81\u68d2\u6027\u6bd4\u4e4b\u524d\u8ba4\u4e3a\u7684\u8981\u5f3a\uff0c\u63d0\u793a\u654f\u611f\u6027\u66f4\u591a\u662f\u8bc4\u4f30\u65b9\u6cd5\u7684\u4ea7\u7269\u800c\u975e\u6a21\u578b\u7f3a\u9677"}}
{"id": "2509.01814", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.01814", "abs": "https://arxiv.org/abs/2509.01814", "authors": ["Shreyas Tirumala", "Nishant Jain", "Danny D. Leybzon", "Trent D. Buskirk"], "title": "Mic Drop or Data Flop? Evaluating the Fitness for Purpose of AI Voice Interviewers for Data Collection within Quantitative & Qualitative Research Contexts", "comment": null, "summary": "Transformer-based Large Language Models (LLMs) have paved the way for \"AI\ninterviewers\" that can administer voice-based surveys with respondents in\nreal-time. This position paper reviews emerging evidence to understand when\nsuch AI interviewing systems are fit for purpose for collecting data within\nquantitative and qualitative research contexts. We evaluate the capabilities of\nAI interviewers as well as current Interactive Voice Response (IVR) systems\nacross two dimensions: input/output performance (i.e., speech recognition,\nanswer recording, emotion handling) and verbal reasoning (i.e., ability to\nprobe, clarify, and handle branching logic). Field studies suggest that AI\ninterviewers already exceed IVR capabilities for both quantitative and\nqualitative data collection, but real-time transcription error rates, limited\nemotion detection abilities, and uneven follow-up quality indicate that the\nutility, use and adoption of current AI interviewer technology may be\ncontext-dependent for qualitative data collection efforts.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u57fa\u4e8eTransformer\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3aAI\u9762\u8bd5\u5b98\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u7814\u7a76\u4e2d\u7684\u6570\u636e\u6536\u96c6\u80fd\u529b\uff0c\u53d1\u73b0AI\u9762\u8bd5\u5b98\u5728\u8bed\u97f3\u8bc6\u522b\u3001\u56de\u7b54\u8bb0\u5f55\u548c\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5df2\u8d85\u8d8a\u4f20\u7edfIVR\u7cfb\u7edf\uff0c\u4f46\u5728\u5b9e\u65f6\u8f6c\u5f55\u51c6\u786e\u6027\u3001\u60c5\u611f\u68c0\u6d4b\u548c\u540e\u7eed\u95ee\u9898\u8d28\u91cf\u65b9\u9762\u4ecd\u6709\u5c40\u9650\u3002", "motivation": "\u968f\u7740\u57fa\u4e8eTransformer\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0cAI\u9762\u8bd5\u5b98\u7cfb\u7edf\u80fd\u591f\u5b9e\u65f6\u8fdb\u884c\u8bed\u97f3\u8c03\u67e5\u3002\u672c\u6587\u65e8\u5728\u8bc4\u4f30\u8fd9\u7c7b\u7cfb\u7edf\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u7814\u7a76\u4e2d\u7684\u6570\u636e\u6536\u96c6\u9002\u7528\u6027\uff0c\u6bd4\u8f83\u5176\u4e0e\u4f20\u7edf\u4ea4\u4e92\u5f0f\u8bed\u97f3\u5e94\u7b54\u7cfb\u7edf\u7684\u80fd\u529b\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u56de\u987e\u65b0\u5174\u8bc1\u636e\uff0c\u4ece\u4e24\u4e2a\u7ef4\u5ea6\u8bc4\u4f30AI\u9762\u8bd5\u5b98\u548c\u5f53\u524dIVR\u7cfb\u7edf\u7684\u80fd\u529b\uff1a\u8f93\u5165/\u8f93\u51fa\u6027\u80fd\uff08\u8bed\u97f3\u8bc6\u522b\u3001\u56de\u7b54\u8bb0\u5f55\u3001\u60c5\u611f\u5904\u7406\uff09\u548c\u8bed\u8a00\u63a8\u7406\u80fd\u529b\uff08\u8ffd\u95ee\u3001\u6f84\u6e05\u3001\u5904\u7406\u5206\u652f\u903b\u8f91\uff09\u3002", "result": "\u5b9e\u5730\u7814\u7a76\u8868\u660e\uff0cAI\u9762\u8bd5\u5b98\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u6570\u636e\u6536\u96c6\u65b9\u9762\u5df2\u8d85\u8d8aIVR\u7cfb\u7edf\u80fd\u529b\uff0c\u4f46\u5b58\u5728\u5b9e\u65f6\u8f6c\u5f55\u9519\u8bef\u7387\u8f83\u9ad8\u3001\u60c5\u611f\u68c0\u6d4b\u80fd\u529b\u6709\u9650\u3001\u540e\u7eed\u95ee\u9898\u8d28\u91cf\u4e0d\u5747\u7b49\u95ee\u9898\uff0c\u4f7f\u5f97\u5f53\u524dAI\u9762\u8bd5\u5b98\u6280\u672f\u5728\u5b9a\u6027\u6570\u636e\u6536\u96c6\u4e2d\u7684\u6548\u7528\u548c\u4f7f\u7528\u53ef\u80fd\u53d7\u60c5\u5883\u9650\u5236\u3002", "conclusion": "\u867d\u7136AI\u9762\u8bd5\u5b98\u6280\u672f\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u5b9a\u91cf\u7814\u7a76\u65b9\u9762\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u7684\u5b9a\u6027\u6570\u636e\u6536\u96c6\u65f6\u4ecd\u9700\u8981\u6839\u636e\u5177\u4f53\u60c5\u5883\u8c28\u614e\u91c7\u7528\uff0c\u6280\u672f\u6210\u719f\u5ea6\u8fd8\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002"}}
{"id": "2509.01885", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.01885", "abs": "https://arxiv.org/abs/2509.01885", "authors": ["Zhimeng Luo", "Abhibha Gupta", "Adam Frisch", "Daqing He"], "title": "Extracting OPQRST in Electronic Health Records using Large Language Models with Reasoning", "comment": null, "summary": "The extraction of critical patient information from Electronic Health Records\n(EHRs) poses significant challenges due to the complexity and unstructured\nnature of the data. Traditional machine learning approaches often fail to\ncapture pertinent details efficiently, making it difficult for clinicians to\nutilize these tools effectively in patient care. This paper introduces a novel\napproach to extracting the OPQRST assessment from EHRs by leveraging the\ncapabilities of Large Language Models (LLMs). We propose to reframe the task\nfrom sequence labeling to text generation, enabling the models to provide\nreasoning steps that mimic a physician's cognitive processes. This approach\nenhances interpretability and adapts to the limited availability of labeled\ndata in healthcare settings. Furthermore, we address the challenge of\nevaluating the accuracy of machine-generated text in clinical contexts by\nproposing a modification to traditional Named Entity Recognition (NER) metrics.\nThis includes the integration of semantic similarity measures, such as the BERT\nScore, to assess the alignment between generated text and the clinical intent\nof the original records. Our contributions demonstrate a significant\nadvancement in the use of AI in healthcare, offering a scalable solution that\nimproves the accuracy and usability of information extraction from EHRs,\nthereby aiding clinicians in making more informed decisions and enhancing\npatient care outcomes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u63d0\u53d6OPQRST\u8bc4\u4f30\u4fe1\u606f\u7684\u65b0\u65b9\u6cd5\uff0c\u5c06\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\u91cd\u6784\u4e3a\u6587\u672c\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u6539\u8fdb\u8bc4\u4f30\u6307\u6807\u4ee5\u5305\u542b\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u590d\u6742\u4e14\u975e\u7ed3\u6784\u5316\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6355\u6349\u5173\u952e\u4fe1\u606f\uff0c\u4e34\u5e8a\u533b\u751f\u96be\u4ee5\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u5de5\u5177\u8fdb\u884c\u60a3\u8005\u62a4\u7406\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06OPQRST\u8bc4\u4f30\u63d0\u53d6\u4efb\u52a1\u4ece\u5e8f\u5217\u6807\u6ce8\u91cd\u6784\u4e3a\u6587\u672c\u751f\u6210\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u63d0\u4f9b\u7c7b\u4f3c\u533b\u751f\u8ba4\u77e5\u8fc7\u7a0b\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u5e76\u6574\u5408BERT Score\u7b49\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u6765\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4ece\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u63d0\u53d6\u4fe1\u606f\u7684\u51c6\u786e\u6027\u548c\u53ef\u7528\u6027\uff0c\u4e3a\u4e34\u5e8a\u533b\u751f\u63d0\u4f9b\u66f4\u597d\u7684\u51b3\u7b56\u652f\u6301\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u533b\u7597\u4fdd\u5065AI\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u6539\u5584\u60a3\u8005\u62a4\u7406\u7ed3\u679c\u3002"}}
{"id": "2509.01899", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01899", "abs": "https://arxiv.org/abs/2509.01899", "authors": ["Zhimeng Luo", "Zhendong Wang", "Rui Meng", "Diyang Xue", "Adam Frisch", "Daqing He"], "title": "Weakly Supervised Medical Entity Extraction and Linking for Chief Complaints", "comment": null, "summary": "A Chief complaint (CC) is the reason for the medical visit as stated in the\npatient's own words. It helps medical professionals to quickly understand a\npatient's situation, and also serves as a short summary for medical text\nmining. However, chief complaint records often take a variety of entering\nmethods, resulting in a wide variation of medical notations, which makes it\ndifficult to standardize across different medical institutions for record\nkeeping or text mining. In this study, we propose a weakly supervised method to\nautomatically extract and link entities in chief complaints in the absence of\nhuman annotation. We first adopt a split-and-match algorithm to produce weak\nannotations, including entity mention spans and class labels, on 1.2 million\nreal-world de-identified and IRB approved chief complaint records. Then we\ntrain a BERT-based model with generated weak labels to locate entity mentions\nin chief complaint text and link them to a pre-defined ontology. We conducted\nextensive experiments, and the results showed that our Weakly Supervised Entity\nExtraction and Linking (\\ours) method produced superior performance over\nprevious methods without any human annotation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u63d0\u53d6\u548c\u94fe\u63a5\u4e3b\u8bc9\u4e2d\u7684\u5b9e\u4f53\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u5728120\u4e07\u6761\u771f\u5b9e\u4e16\u754c\u4e3b\u8bc9\u8bb0\u5f55\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4e3b\u8bc9\u8bb0\u5f55\u5b58\u5728\u591a\u79cd\u5f55\u5165\u65b9\u5f0f\uff0c\u5bfc\u81f4\u533b\u5b66\u672f\u8bed\u5dee\u5f02\u5f88\u5927\uff0c\u96be\u4ee5\u5728\u4e0d\u540c\u533b\u7597\u673a\u6784\u95f4\u8fdb\u884c\u6807\u51c6\u5316\u8bb0\u5f55\u4fdd\u5b58\u6216\u6587\u672c\u6316\u6398\u3002", "method": "\u91c7\u7528\u5206\u5272\u5339\u914d\u7b97\u6cd5\u751f\u6210\u5f31\u6807\u6ce8\uff08\u5b9e\u4f53\u63d0\u53ca\u8de8\u5ea6\u548c\u7c7b\u522b\u6807\u7b7e\uff09\uff0c\u7136\u540e\u57fa\u4e8eBERT\u6a21\u578b\u8bad\u7ec3\u6765\u5b9a\u4f4d\u5b9e\u4f53\u63d0\u53ca\u5e76\u5c06\u5176\u94fe\u63a5\u5230\u9884\u5b9a\u4e49\u672c\u4f53\u3002", "result": "\u5728\u5927\u91cf\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u5f31\u76d1\u7763\u5b9e\u4f53\u63d0\u53d6\u548c\u94fe\u63a5\u65b9\u6cd5\u5728\u6ca1\u6709\u4eba\u5de5\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4e3b\u8bc9\u8bb0\u5f55\u6807\u51c6\u5316\u95ee\u9898\uff0c\u4e3a\u533b\u7597\u6587\u672c\u6316\u6398\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u5f31\u76d1\u7763\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01962", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01962", "abs": "https://arxiv.org/abs/2509.01962", "authors": ["Sachin Pawar", "Manoj Apte", "Girish K. Palshikar", "Basit Ali", "Nitin Ramrakhiyani"], "title": "DRAssist: Dispute Resolution Assistance using Large Language Models", "comment": "Accepted at the 20th International Conference on Artificial\n  Intelligence and Law (ICAIL 2025)", "summary": "Disputes between two parties occur in almost all domains such as taxation,\ninsurance, banking, healthcare, etc. The disputes are generally resolved in a\nspecific forum (e.g., consumer court) where facts are presented, points of\ndisagreement are discussed, arguments as well as specific demands of the\nparties are heard, and finally a human judge resolves the dispute by often\nfavouring one of the two parties. In this paper, we explore the use of large\nlanguage models (LLMs) as assistants for the human judge to resolve such\ndisputes, as part of our DRAssist system. We focus on disputes from two\nspecific domains -- automobile insurance and domain name disputes. DRAssist\nidentifies certain key structural elements (e.g., facts, aspects or\ndisagreement, arguments) of the disputes and summarizes the unstructured\ndispute descriptions to produce a structured summary for each dispute. We then\nexplore multiple prompting strategies with multiple LLMs for their ability to\nassist in resolving the disputes in these domains. In DRAssist, these LLMs are\nprompted to produce the resolution output at three different levels -- (i)\nidentifying an overall stronger party in a dispute, (ii) decide whether each\nspecific demand of each contesting party can be accepted or not, (iii) evaluate\nwhether each argument by each contesting party is strong or weak. We evaluate\nthe performance of LLMs on all these tasks by comparing them with relevant\nbaselines using suitable evaluation metrics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u7d22\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4eba\u7c7b\u6cd5\u5b98\u52a9\u624b\u6765\u89e3\u51b3\u4e89\u8bae\uff0c\u5f00\u53d1\u4e86DRAssist\u7cfb\u7edf\uff0c\u4e13\u6ce8\u4e8e\u6c7d\u8f66\u4fdd\u9669\u548c\u57df\u540d\u4e89\u8bae\u9886\u57df\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u603b\u7ed3\u548c\u591a\u79cd\u63d0\u793a\u7b56\u7565\u6765\u8f85\u52a9\u4e89\u8bae\u89e3\u51b3\u3002", "motivation": "\u89e3\u51b3\u5404\u79cd\u9886\u57df\uff08\u5982\u7a0e\u52a1\u3001\u4fdd\u9669\u3001\u94f6\u884c\u3001\u533b\u7597\u7b49\uff09\u4e2d\u5e38\u89c1\u7684\u4e89\u8bae\u95ee\u9898\uff0c\u63a2\u7d22LLMs\u4f5c\u4e3a\u4eba\u7c7b\u6cd5\u5b98\u52a9\u624b\u7684\u6f5c\u529b\uff0c\u63d0\u9ad8\u4e89\u8bae\u89e3\u51b3\u6548\u7387\u3002", "method": "\u5f00\u53d1DRAssist\u7cfb\u7edf\uff0c\u8bc6\u522b\u4e89\u8bae\u7684\u5173\u952e\u7ed3\u6784\u5143\u7d20\uff08\u4e8b\u5b9e\u3001\u4e89\u8bae\u65b9\u9762\u3001\u8bba\u70b9\uff09\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u4e89\u8bae\u63cf\u8ff0\u603b\u7ed3\u4e3a\u7ed3\u6784\u5316\u6458\u8981\uff0c\u4f7f\u7528\u591a\u79cd\u63d0\u793a\u7b56\u7565\u548c\u591a\u4e2aLLMs\u5728\u4e09\u4e2a\u4e0d\u540c\u5c42\u6b21\u4e0a\u4ea7\u751f\u4e89\u8bae\u89e3\u51b3\u8f93\u51fa\u3002", "result": "\u901a\u8fc7\u6bd4\u8f83\u76f8\u5173\u57fa\u7ebf\u548c\u5408\u9002\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u8bc4\u4f30\u4e86LLMs\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "LLMs\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u7684\u4e89\u8bae\u89e3\u51b3\u52a9\u624b\uff0c\u5728\u8bc6\u522b\u5f3a\u52bf\u65b9\u3001\u8bc4\u4f30\u5177\u4f53\u8981\u6c42\u548c\u8bba\u70b9\u5f3a\u5ea6\u7b49\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4e3a\u81ea\u52a8\u5316\u4e89\u8bae\u89e3\u51b3\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2509.02033", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.02033", "abs": "https://arxiv.org/abs/2509.02033", "authors": ["Chao Xue", "Ziyuan Gao"], "title": "StructCoh: Structured Contrastive Learning for Context-Aware Text Semantic Matching", "comment": "Accepted by PRICAI 2025", "summary": "Text semantic matching requires nuanced understanding of both structural\nrelationships and fine-grained semantic distinctions. While pre-trained\nlanguage models excel at capturing token-level interactions, they often\noverlook hierarchical structural patterns and struggle with subtle semantic\ndiscrimination. In this paper, we proposed StructCoh, a graph-enhanced\ncontrastive learning framework that synergistically combines structural\nreasoning with representation space optimization. Our approach features two key\ninnovations: (1) A dual-graph encoder constructs semantic graphs via dependency\nparsing and topic modeling, then employs graph isomorphism networks to\npropagate structural features across syntactic dependencies and cross-document\nconcept nodes. (2) A hierarchical contrastive objective enforces consistency at\nmultiple granularities: node-level contrastive regularization preserves core\nsemantic units, while graph-aware contrastive learning aligns inter-document\nstructural semantics through both explicit and implicit negative sampling\nstrategies. Experiments on three legal document matching benchmarks and\nacademic plagiarism detection datasets demonstrate significant improvements\nover state-of-the-art methods. Notably, StructCoh achieves 86.7% F1-score\n(+6.2% absolute gain) on legal statute matching by effectively identifying\nargument structure similarities.", "AI": {"tldr": "StructCoh\u662f\u4e00\u4e2a\u56fe\u589e\u5f3a\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u63a8\u7406\u548c\u8868\u793a\u7a7a\u95f4\u4f18\u5316\u6765\u89e3\u51b3\u6587\u672c\u8bed\u4e49\u5339\u914d\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5728\u6355\u6349token\u7ea7\u4ea4\u4e92\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5f80\u5f80\u5ffd\u7565\u5c42\u6b21\u7ed3\u6784\u6a21\u5f0f\uff0c\u96be\u4ee5\u5904\u7406\u7ec6\u5fae\u7684\u8bed\u4e49\u533a\u5206\uff0c\u9700\u8981\u540c\u65f6\u7406\u89e3\u7ed3\u6784\u5173\u7cfb\u548c\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5dee\u5f02", "method": "\u91c7\u7528\u53cc\u56fe\u7f16\u7801\u5668\u6784\u5efa\u8bed\u4e49\u56fe\uff08\u4f9d\u8d56\u89e3\u6790\u548c\u4e3b\u9898\u5efa\u6a21\uff09\uff0c\u4f7f\u7528\u56fe\u540c\u6784\u7f51\u7edc\u4f20\u64ad\u7ed3\u6784\u7279\u5f81\uff1b\u8bbe\u8ba1\u5206\u5c42\u5bf9\u6bd4\u76ee\u6807\uff0c\u5728\u591a\u4e2a\u7c92\u5ea6\u4e0a\u5f3a\u5236\u6267\u884c\u4e00\u81f4\u6027\uff0c\u5305\u62ec\u8282\u70b9\u7ea7\u5bf9\u6bd4\u6b63\u5219\u5316\u548c\u56fe\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60", "result": "\u5728\u4e09\u4e2a\u6cd5\u5f8b\u6587\u6863\u5339\u914d\u57fa\u51c6\u548c\u5b66\u672f\u6284\u88ad\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u6cd5\u5f8b\u6cd5\u89c4\u5339\u914d\u4e0a\u8fbe\u523086.7%\u7684F1\u5206\u6570\uff08\u7edd\u5bf9\u63d0\u53476.2%\uff09\uff0c\u6709\u6548\u8bc6\u522b\u8bba\u8bc1\u7ed3\u6784\u76f8\u4f3c\u6027", "conclusion": "StructCoh\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u63a8\u7406\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6587\u672c\u8bed\u4e49\u5339\u914d\u4e2d\u7684\u7ed3\u6784\u6a21\u5f0f\u6355\u6349\u548c\u7ec6\u7c92\u5ea6\u8bed\u4e49\u533a\u5206\u95ee\u9898\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.02036", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02036", "abs": "https://arxiv.org/abs/2509.02036", "authors": ["Hexian Zhang", "Xinyu Yan", "Yanqi Yang", "Lijian Jin", "Ping Yang", "Junwen Wang"], "title": "DeepSeek performs better than other Large Language Models in Dental Cases", "comment": "Abstract word count: 171; Total word count: 3130; Total number of\n  tables: 2; Total number of figures: 3; Number of references: 32", "summary": "Large language models (LLMs) hold transformative potential in healthcare, yet\ntheir capacity to interpret longitudinal patient narratives remains\ninadequately explored. Dentistry, with its rich repository of structured\nclinical data, presents a unique opportunity to rigorously assess LLMs'\nreasoning abilities. While several commercial LLMs already exist, DeepSeek, a\nmodel that gained significant attention earlier this year, has also joined the\ncompetition. This study evaluated four state-of-the-art LLMs (GPT-4o, Gemini\n2.0 Flash, Copilot, and DeepSeek V3) on their ability to analyze longitudinal\ndental case vignettes through open-ended clinical tasks. Using 34 standardized\nlongitudinal periodontal cases (comprising 258 question-answer pairs), we\nassessed model performance via automated metrics and blinded evaluations by\nlicensed dentists. DeepSeek emerged as the top performer, demonstrating\nsuperior faithfulness (median score = 0.528 vs. 0.367-0.457) and higher expert\nratings (median = 4.5/5 vs. 4.0/5), without significantly compromising\nreadability. Our study positions DeepSeek as the leading LLM for case analysis,\nendorses its integration as an adjunct tool in both medical education and\nresearch, and highlights its potential as a domain-specific agent.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cd\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5206\u6790\u7259\u79d1\u7eb5\u5411\u75c5\u4f8b\u65b9\u9762\u7684\u8868\u73b0\uff0cDeepSeek V3\u5728\u5fe0\u5b9e\u5ea6\u548c\u4e13\u5bb6\u8bc4\u5206\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u6210\u4e3a\u533b\u7597\u6848\u4f8b\u5206\u6790\u7684\u9886\u5148\u6a21\u578b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u4f46\u5176\u89e3\u91ca\u7eb5\u5411\u60a3\u8005\u53d9\u8ff0\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u7259\u79d1\u9886\u57df\u4e30\u5bcc\u7684\u7ed3\u6784\u5316\u4e34\u5e8a\u6570\u636e\u4e3a\u8bc4\u4f30LLMs\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u72ec\u7279\u673a\u4f1a\u3002", "method": "\u4f7f\u752834\u4e2a\u6807\u51c6\u5316\u7eb5\u5411\u7259\u5468\u75c5\u4f8b\uff08\u5305\u542b258\u4e2a\u95ee\u7b54\u5bf9\uff09\uff0c\u901a\u8fc7\u5f00\u653e\u5f0f\u4e34\u5e8a\u4efb\u52a1\u8bc4\u4f30GPT-4o\u3001Gemini 2.0 Flash\u3001Copilot\u548cDeepSeek V3\u56db\u79cd\u6a21\u578b\u7684\u8868\u73b0\uff0c\u91c7\u7528\u81ea\u52a8\u5316\u6307\u6807\u548c\u6301\u8bc1\u7259\u533b\u7684\u76f2\u6cd5\u8bc4\u4f30\u3002", "result": "DeepSeek\u8868\u73b0\u6700\u4f73\uff0c\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u5fe0\u5b9e\u5ea6\uff08\u4e2d\u4f4d\u6570\u5f97\u52060.528 vs. 0.367-0.457\uff09\u548c\u66f4\u9ad8\u7684\u4e13\u5bb6\u8bc4\u5206\uff08\u4e2d\u4f4d\u65704.5/5 vs. 4.0/5\uff09\uff0c\u4e14\u672a\u663e\u8457\u5f71\u54cd\u53ef\u8bfb\u6027\u3002", "conclusion": "DeepSeek\u88ab\u5b9a\u4f4d\u4e3a\u6848\u4f8b\u5206\u6790\u7684\u9886\u5148LLM\uff0c\u652f\u6301\u5176\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\u6574\u5408\u5230\u533b\u5b66\u6559\u80b2\u548c\u7814\u7a76\u4e2d\uff0c\u5e76\u7a81\u663e\u5176\u4f5c\u4e3a\u9886\u57df\u7279\u5b9a\u4ee3\u7406\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.02040", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.02040", "abs": "https://arxiv.org/abs/2509.02040", "authors": ["Guangzeng Han", "Weisi Liu", "Xiaolei Huang"], "title": "Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm Simulators for Conditional Synthetic Data Generation", "comment": "Accepted to EMNLP2025 Findings", "summary": "Large Language Models (LLMs) excel at generating synthetic data, but ensuring\nits quality and diversity remains challenging. We propose Genetic Prompt, a\nnovel framework that combines genetic algorithms with LLMs to augment synthetic\ndata generation. Our approach treats semantic text attributes as gene sequences\nand leverages the LLM to simulate crossover and mutation operations. This\ngenetic process enhances data quality and diversity by creating novel attribute\ncombinations, yielding synthetic distributions closer to real-world data. To\noptimize parent selection, we also integrate an active learning scheme that\nexpands the offspring search space. Our experiments on multiple NLP tasks\nreveal several key findings: Genetic Prompt not only significantly outperforms\nstate-of-the-art baselines but also shows robust performance across various\ngenerator model sizes and scales. Moreover, we demonstrate that fusing our\nsynthetic data with the original training set significantly boosts downstream\nmodel performance, particularly for class-imbalanced scenarios. Our findings\nvalidate that Genetic Prompt is an effective method for producing high-quality\nsynthetic data for a wide range of NLP applications.", "AI": {"tldr": "Genetic Prompt\u662f\u4e00\u4e2a\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u548cLLM\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8bed\u4e49\u6587\u672c\u5c5e\u6027\u89c6\u4e3a\u57fa\u56e0\u5e8f\u5217\uff0c\u5229\u7528LLM\u6a21\u62df\u4ea4\u53c9\u548c\u53d8\u5f02\u64cd\u4f5c\u6765\u589e\u5f3a\u5408\u6210\u6570\u636e\u751f\u6210\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u64c5\u957f\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u4f46\u786e\u4fdd\u5176\u8d28\u91cf\u548c\u591a\u6837\u6027\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u4ea7\u751f\u66f4\u63a5\u8fd1\u771f\u5b9e\u4e16\u754c\u6570\u636e\u5206\u5e03\u7684\u5408\u6210\u6570\u636e\u3002", "method": "\u63d0\u51faGenetic Prompt\u6846\u67b6\uff0c\u5c06\u8bed\u4e49\u6587\u672c\u5c5e\u6027\u4f5c\u4e3a\u57fa\u56e0\u5e8f\u5217\uff0c\u5229\u7528LLM\u6a21\u62df\u9057\u4f20\u7b97\u6cd5\u4e2d\u7684\u4ea4\u53c9\u548c\u53d8\u5f02\u64cd\u4f5c\u3002\u96c6\u6210\u4e3b\u52a8\u5b66\u4e60\u65b9\u6848\u6765\u4f18\u5316\u7236\u4ee3\u9009\u62e9\uff0c\u6269\u5c55\u540e\u4ee3\u641c\u7d22\u7a7a\u95f4\u3002", "result": "\u5728\u591a\u4e2aNLP\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1aGenetic Prompt\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u751f\u6210\u6a21\u578b\u4e0a\u90fd\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u80fd\u3002\u5c06\u5408\u6210\u6570\u636e\u4e0e\u539f\u59cb\u8bad\u7ec3\u96c6\u878d\u5408\u80fd\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u573a\u666f\u4e2d\u3002", "conclusion": "Genetic Prompt\u662f\u751f\u4ea7\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684NLP\u5e94\u7528\uff0c\u80fd\u591f\u751f\u6210\u66f4\u63a5\u8fd1\u771f\u5b9e\u6570\u636e\u5206\u5e03\u7684\u5408\u6210\u6570\u636e\u3002"}}
{"id": "2509.02075", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.02075", "abs": "https://arxiv.org/abs/2509.02075", "authors": ["Elisabetta Rocchetti", "Alfio Ferrara"], "title": "How Instruction-Tuning Imparts Length Control: A Cross-Lingual Mechanistic Analysis", "comment": null, "summary": "Adhering to explicit length constraints, such as generating text with a\nprecise word count, remains a significant challenge for Large Language Models\n(LLMs). This study aims at investigating the differences between foundation\nmodels and their instruction-tuned counterparts, on length-controlled text\ngeneration in English and Italian. We analyze both performance and internal\ncomponent contributions using Cumulative Weighted Attribution, a metric derived\nfrom Direct Logit Attribution. Our findings reveal that instruction-tuning\nsubstantially improves length control, primarily by specializing components in\ndeeper model layers. Specifically, attention heads in later layers of IT models\nshow increasingly positive contributions, particularly in English. In Italian,\nwhile attention contributions are more attenuated, final-layer MLPs exhibit a\nstronger positive role, suggesting a compensatory mechanism. These results\nindicate that instruction-tuning reconfigures later layers for task adherence,\nwith component-level strategies potentially adapting to linguistic context.", "AI": {"tldr": "\u6307\u4ee4\u5fae\u8c03\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u5ea6\u63a7\u5236\u80fd\u529b\uff0c\u4e3b\u8981\u901a\u8fc7\u6df1\u5c42\u7f51\u7edc\u7ec4\u4ef6\u7684\u4e13\u4e1a\u5316\u5b9e\u73b0\uff0c\u5728\u82f1\u8bed\u548c\u610f\u5927\u5229\u8bed\u4e2d\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u673a\u5236\u9002\u5e94", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7cbe\u786e\u5b57\u6570\u63a7\u5236\u6587\u672c\u751f\u6210\u65b9\u9762\u7684\u6311\u6218\uff0c\u6bd4\u8f83\u57fa\u7840\u6a21\u578b\u548c\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u5728\u82f1\u8bed\u548c\u610f\u5927\u5229\u8bed\u4e2d\u7684\u8868\u73b0\u5dee\u5f02", "method": "\u4f7f\u7528\u7d2f\u79ef\u52a0\u6743\u5f52\u56e0\uff08\u6e90\u81ea\u76f4\u63a5\u5bf9\u6570\u5f52\u56e0\uff09\u5206\u6790\u6a21\u578b\u6027\u80fd\u548c\u5185\u90e8\u7ec4\u4ef6\u8d21\u732e\uff0c\u7279\u522b\u5173\u6ce8\u4e0d\u540c\u5c42\u7ea7\u7684\u6ce8\u610f\u529b\u5934\u548cMLP", "result": "\u6307\u4ee4\u5fae\u8c03\u5927\u5e45\u6539\u5584\u957f\u5ea6\u63a7\u5236\uff0c\u6df1\u5c42\u6ce8\u610f\u529b\u5934\u5728\u82f1\u8bed\u4e2d\u8d21\u732e\u663e\u8457\uff0c\u610f\u5927\u5229\u8bed\u4e2d\u6700\u7ec8\u5c42MLP\u8d77\u8865\u507f\u4f5c\u7528\uff0c\u663e\u793a\u8bed\u8a00\u9002\u5e94\u6027", "conclusion": "\u6307\u4ee4\u5fae\u8c03\u901a\u8fc7\u91cd\u65b0\u914d\u7f6e\u6df1\u5c42\u7f51\u7edc\u6765\u5b9e\u73b0\u4efb\u52a1\u9075\u5faa\uff0c\u7ec4\u4ef6\u7ea7\u7b56\u7565\u4f1a\u6839\u636e\u8bed\u8a00\u4e0a\u4e0b\u6587\u8fdb\u884c\u9002\u5e94\u6027\u8c03\u6574"}}
{"id": "2509.02093", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.02093", "abs": "https://arxiv.org/abs/2509.02093", "authors": ["Juhyeon Lee", "Wonduk Seo", "Hyunjin An", "Seunghyun Lee", "Yi Bu"], "title": "Better by Comparison: Retrieval-Augmented Contrastive Reasoning for Automatic Prompt Optimization", "comment": "Preprint", "summary": "Automatic prompt optimization has recently emerged as a strategy for\nimproving the quality of prompts used in Large Language Models (LLMs), with the\ngoal of generating more accurate and useful responses. However, most prior work\nfocuses on direct prompt refinement or model fine-tuning, overlooking the\npotential of leveraging LLMs' inherent reasoning capability to learn from\ncontrasting examples. In this paper, we present Contrastive Reasoning Prompt\nOptimization (CRPO), a novel framework that formulates prompt optimization as a\nretrieval augmented reasoning process. Our approach retrieves top k reference\nprompts from the HelpSteer2 dataset, an open-source collection annotated for\nhelpfulness, correctness, coherence, complexity, and verbosity, and constructs\ntwo complementary optimization paradigms: (1) tiered contrastive reasoning,\nwhere the LLM compares high, medium, and low quality prompts to refine its own\ngeneration through reflective reasoning, and (2) multi-metric contrastive\nreasoning, where the LLM analyzes the best prompts along each evaluation\ndimension and integrates their strengths into an optimized prompt. By\nexplicitly contrasting high and low quality exemplars, CRPO enables the model\nto deduce why certain prompts succeed while others fail, thereby achieving more\nrobust and interpretable optimization. Experimental results on the HelpSteer2\nbenchmark demonstrate that CRPO significantly outperforms baselines. Our\nfindings highlight the promise of contrastive, retrieval-augmented reasoning\nfor advancing automatic prompt optimization.", "AI": {"tldr": "CRPO\u6846\u67b6\u901a\u8fc7\u5bf9\u6bd4\u63a8\u7406\u4f18\u5316\u63d0\u793a\uff0c\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5728HelpSteer2\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u73b0\u6709\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u76f4\u63a5\u63d0\u793a\u7cbe\u70bc\u6216\u6a21\u578b\u5fae\u8c03\uff0c\u5ffd\u89c6\u4e86\u5229\u7528LLMs\u56fa\u6709\u63a8\u7406\u80fd\u529b\u4ece\u5bf9\u6bd4\u793a\u4f8b\u4e2d\u5b66\u4e60\u7684\u6f5c\u529b", "method": "\u63d0\u51fa\u5bf9\u6bd4\u63a8\u7406\u63d0\u793a\u4f18\u5316(CRPO)\u6846\u67b6\uff0c\u4eceHelpSteer2\u6570\u636e\u96c6\u4e2d\u68c0\u7d22\u53c2\u8003\u63d0\u793a\uff0c\u6784\u5efa\u5206\u5c42\u5bf9\u6bd4\u63a8\u7406\u548c\u591a\u6307\u6807\u5bf9\u6bd4\u63a8\u7406\u4e24\u79cd\u4f18\u5316\u8303\u5f0f", "result": "\u5728HelpSteer2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCRPO\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u5bf9\u6bd4\u6027\u3001\u68c0\u7d22\u589e\u5f3a\u7684\u63a8\u7406\u65b9\u6cd5\u5728\u63a8\u8fdb\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u65b9\u9762\u5177\u6709\u5e7f\u9614\u524d\u666f"}}
{"id": "2509.02097", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02097", "abs": "https://arxiv.org/abs/2509.02097", "authors": ["Zhichao Shi", "Xuhui Jiang", "Chengjin Xu", "Cangli Yao", "Zhenxin Huang", "Shengjie Ma", "Yinghan Shen", "Yuanzhuo Wang"], "title": "JudgeAgent: Dynamically Evaluate LLMs with Agent-as-Interviewer", "comment": null, "summary": "Evaluating the capabilities of large language models (LLMs) is an essential\nstep to ensure the successful application of LLMs across various domains. The\ncurrent evaluation of LLMs is based on a paradigm that involves querying them\nwith predefined question sets and assessing their outputs. This paradigm offers\ncontrollable processes and simplicity, but faces challenges such as limited\ninteraction with targets, insufficient difficulty control, and difficulties in\nverifying the validity of evaluation results, making it hard to precisely\ndetermine the knowledge and capability boundaries of target models. To address\nthese challenges, we propose JudgeAgent, a knowledge-target adaptive dynamic\nevaluation framework based on a new interviewer-style evaluation paradigm.\nJudgeAgent employs a comprehensive evaluation approach consisting of benchmark\ngrading, interactive extension, and evaluation feedback. It utilizes\nknowledge-driven data synthesis and target-adaptive difficulty adjustment\nmethods to conduct extended testing, providing accurate and effective\nevaluation results. We also introduce a novel insight into validating\nevaluation methods, demonstrating the effectiveness of JudgeAgent and its\ndynamic evaluation paradigm through extensive experiments.", "AI": {"tldr": "JudgeAgent\u662f\u4e00\u4e2a\u57fa\u4e8e\u9762\u8bd5\u5b98\u5f0f\u8bc4\u4f30\u8303\u5f0f\u7684\u77e5\u8bc6\u76ee\u6807\u81ea\u9002\u5e94\u52a8\u6001\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u51c6\u8bc4\u5206\u3001\u4ea4\u4e92\u6269\u5c55\u548c\u8bc4\u4f30\u53cd\u9988\u7684\u7efc\u5408\u65b9\u6cd5\uff0c\u89e3\u51b3\u4f20\u7edfLLM\u8bc4\u4f30\u4e2d\u4ea4\u4e92\u6709\u9650\u3001\u96be\u5ea6\u63a7\u5236\u4e0d\u8db3\u548c\u7ed3\u679c\u9a8c\u8bc1\u56f0\u96be\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u57fa\u4e8e\u9884\u5b9a\u4e49\u95ee\u9898\u96c6\u7684\u8303\u5f0f\u5b58\u5728\u4e0e\u76ee\u6807\u4ea4\u4e92\u6709\u9650\u3001\u96be\u5ea6\u63a7\u5236\u4e0d\u8db3\u3001\u8bc4\u4f30\u7ed3\u679c\u6709\u6548\u6027\u9a8c\u8bc1\u56f0\u96be\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u7cbe\u786e\u786e\u5b9a\u76ee\u6807\u6a21\u578b\u7684\u77e5\u8bc6\u548c\u80fd\u529b\u8fb9\u754c\u3002", "method": "\u63d0\u51faJudgeAgent\u6846\u67b6\uff0c\u91c7\u7528\u77e5\u8bc6\u9a71\u52a8\u7684\u6570\u636e\u5408\u6210\u548c\u76ee\u6807\u81ea\u9002\u5e94\u96be\u5ea6\u8c03\u6574\u65b9\u6cd5\uff0c\u5305\u542b\u57fa\u51c6\u8bc4\u5206\u3001\u4ea4\u4e92\u6269\u5c55\u548c\u8bc4\u4f30\u53cd\u9988\u4e09\u4e2a\u7ec4\u6210\u90e8\u5206\uff0c\u901a\u8fc7\u6269\u5c55\u6d4b\u8bd5\u63d0\u4f9b\u51c6\u786e\u6709\u6548\u7684\u8bc4\u4f30\u7ed3\u679c\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86JudgeAgent\u53ca\u5176\u52a8\u6001\u8bc4\u4f30\u8303\u5f0f\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8bc4\u4f30\u65b9\u6cd5\u7684\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "conclusion": "JudgeAgent\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f20\u7edfLLM\u8bc4\u4f30\u8303\u5f0f\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u548c\u6709\u6548\u7684\u6a21\u578b\u80fd\u529b\u8bc4\u4f30\uff0c\u4e3aLLM\u5728\u4e0d\u540c\u9886\u57df\u7684\u6210\u529f\u5e94\u7528\u63d0\u4f9b\u4fdd\u969c\u3002"}}
{"id": "2509.02123", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.02123", "abs": "https://arxiv.org/abs/2509.02123", "authors": ["Wang Chen", "Guanqiang Qi", "Weikang Li", "Yang Li"], "title": "CMRAG: Co-modality-based document retrieval and visual question answering", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a core paradigm in document\nquestion answering tasks. However, existing methods have limitations when\ndealing with multimodal documents: one category of methods relies on layout\nanalysis and text extraction, which can only utilize explicit text information\nand struggle to capture images or unstructured content; the other category\ntreats document segmentation as visual input and directly passes it to visual\nlanguage models (VLMs) for processing, yet it ignores the semantic advantages\nof text, leading to suboptimal generation results. This paper proposes\nco-modality-based RAG (CMRAG), which can simultaneously leverage text and\nimages for efficient retrieval and generation. Specifically, we first perform\nstructured parsing on documents to obtain co-modality representations of text\nsegments and image regions. Subsequently, in response to user queries, we\nretrieve candidate evidence from text and image channels, respectively, and\naggregate the results at the cross-modal retrieval level. Finally, we prompt\nthe VLM to generate the final response based on the co-modality retrieval\nresults. Experiments demonstrate that our method significantly outperforms\npure-vision-based RAG in visual document question answering tasks. The findings\nof this paper show that integrating co-modality information into the RAG\nframework in a unified manner is an effective approach to improving the\nperformance of complex document visual question-answering (VQA) systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86co-modality-based RAG (CMRAG)\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u65f6\u5229\u7528\u6587\u672c\u548c\u56fe\u50cf\u4fe1\u606f\u8fdb\u884c\u591a\u6a21\u6001\u6587\u6863\u95ee\u7b54\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7eaf\u89c6\u89c9RAG\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6a21\u6001\u6587\u6863\u65f6\u5b58\u5728\u5c40\u9650\uff1a\u57fa\u4e8e\u5e03\u5c40\u5206\u6790\u548c\u6587\u672c\u63d0\u53d6\u7684\u65b9\u6cd5\u53ea\u80fd\u5229\u7528\u663e\u5f0f\u6587\u672c\u4fe1\u606f\uff0c\u65e0\u6cd5\u5904\u7406\u56fe\u50cf\u5185\u5bb9\uff1b\u800c\u7eaf\u89c6\u89c9\u65b9\u6cd5\u867d\u7136\u80fd\u5904\u7406\u56fe\u50cf\u4f46\u5ffd\u7565\u4e86\u6587\u672c\u7684\u8bed\u4e49\u4f18\u52bf\uff0c\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u4e0d\u7406\u60f3\u3002", "method": "\u9996\u5148\u5bf9\u6587\u6863\u8fdb\u884c\u7ed3\u6784\u5316\u89e3\u6790\uff0c\u83b7\u5f97\u6587\u672c\u7247\u6bb5\u548c\u56fe\u50cf\u533a\u57df\u7684\u5171\u6a21\u6001\u8868\u793a\uff1b\u7136\u540e\u5206\u522b\u4ece\u6587\u672c\u548c\u56fe\u50cf\u901a\u9053\u68c0\u7d22\u5019\u9009\u8bc1\u636e\uff0c\u5728\u8de8\u6a21\u6001\u68c0\u7d22\u5c42\u9762\u805a\u5408\u7ed3\u679c\uff1b\u6700\u540e\u57fa\u4e8e\u5171\u6a21\u6001\u68c0\u7d22\u7ed3\u679c\u63d0\u793a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6700\u7ec8\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u6587\u6863\u95ee\u7b54\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u7eaf\u89c6\u89c9RAG\u65b9\u6cd5\u3002", "conclusion": "\u4ee5\u7edf\u4e00\u65b9\u5f0f\u5c06\u5171\u6a21\u6001\u4fe1\u606f\u6574\u5408\u5230RAG\u6846\u67b6\u4e2d\u662f\u63d0\u5347\u590d\u6742\u6587\u6863\u89c6\u89c9\u95ee\u7b54\u7cfb\u7edf\u6027\u80fd\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2509.02133", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.02133", "abs": "https://arxiv.org/abs/2509.02133", "authors": ["Snehasis Mukhopadhyay", "Aryan Kasat", "Shivam Dubey", "Rahul Karthikeyan", "Dhruv Sood", "Vinija Jain", "Aman Chadha", "Amitava Das"], "title": "AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models", "comment": null, "summary": "Large Language Models (LLMs) can inadvertently reflect societal biases\npresent in their training data, leading to harmful or prejudiced outputs. In\nthe Indian context, our empirical evaluations across a suite of models reveal\nthat biases around caste and religion are particularly salient. Yet, most\nexisting mitigation strategies are Western-centric and fail to address these\nlocal nuances. We propose AMBEDKAR, a framework inspired by the egalitarian\nvision of Dr B. R. Ambedkar, architect of the Indian Constitution, to guide LLM\noutputs toward fairness, neutrality, and inclusion in line with Articles 14 to\n17. Our approach introduces a Constitution-Aware Decoding Layer, guided by the\nAI Constitution of India and applied only at inference time, without any\nparameter updates to the base model. We incorporate a speculative decoding\nalgorithm that proactively reduces casteist and communal bias during\ngeneration. This mitigation layer operates directly within the decoding\nprocess, avoiding changes to model internals and lowering the computational and\ninfrastructural costs associated with retraining. We reinterpret speculative\ndecoding not merely as an efficiency tool but as a mechanism for fairness. In\nthis framework, a Small Language Model (SLM) acts as a potentially biased\ngenerator, while a constitutionally guided Large Language Model (LLM) serves as\nthe verifier. Rather than accelerating generation, the LLM enforces bias-robust\ntrajectories in the SLM outputs. This inversion of roles gives rise to a\nfairness-by-speculation paradigm. Our approach yields an absolute reduction of\nbias up to 26.41 percent compared to baseline. Our source code, datasets, and\nresults are available at https://anonymous.4open.science/r/AMBEDKAR-983B/", "AI": {"tldr": "\u63d0\u51fa\u4e86AMBEDKAR\u6846\u67b6\uff0c\u901a\u8fc7\u5baa\u6cd5\u611f\u77e5\u89e3\u7801\u5c42\u548c\u63a8\u6d4b\u89e3\u7801\u7b97\u6cd5\uff0c\u5728\u4e0d\u4fee\u6539\u57fa\u7840\u6a21\u578b\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u51cf\u5c11LLM\u5728\u79cd\u59d3\u548c\u5b97\u6559\u65b9\u9762\u7684\u504f\u89c1\uff0c\u504f\u89c1\u7edd\u5bf9\u51cf\u5c11\u8fbe26.41%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u53cd\u6620\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u7279\u522b\u662f\u5728\u5370\u5ea6\u8bed\u5883\u4e0b\uff0c\u79cd\u59d3\u548c\u5b97\u6559\u504f\u89c1\u5c24\u4e3a\u7a81\u51fa\u3002\u73b0\u6709\u7f13\u89e3\u7b56\u7565\u591a\u4e3a\u897f\u65b9\u4e2d\u5fc3\uff0c\u65e0\u6cd5\u89e3\u51b3\u672c\u5730\u5316\u504f\u89c1\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u5370\u5ea6\u5baa\u6cd5\u7b2c14-17\u6761\u7684\u5e73\u7b49\u613f\u666f\uff0c\u8bbe\u8ba1\u5baa\u6cd5\u611f\u77e5\u89e3\u7801\u5c42\uff0c\u5728\u63a8\u7406\u65f6\u5e94\u7528\u63a8\u6d4b\u89e3\u7801\u7b97\u6cd5\u3002\u4f7f\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u6f5c\u5728\u504f\u89c1\u751f\u6210\u5668\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5baa\u6cd5\u6307\u5bfc\u7684\u9a8c\u8bc1\u5668\uff0c\u5b9e\u65bd\u516c\u5e73\u6027\u63a8\u6d4b\u8303\u5f0f\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u504f\u89c1\u7edd\u5bf9\u51cf\u5c11\u6700\u9ad8\u8fbe26.41%\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u57fa\u7840\u8bbe\u65bd\u6210\u672c\u3002", "conclusion": "AMBEDKAR\u6846\u67b6\u6210\u529f\u5c06\u63a8\u6d4b\u89e3\u7801\u91cd\u65b0\u8be0\u91ca\u4e3a\u516c\u5e73\u6027\u673a\u5236\u800c\u975e\u6548\u7387\u5de5\u5177\uff0c\u4e3a\u5904\u7406\u672c\u5730\u5316\u793e\u4f1a\u504f\u89c1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u5370\u5ea6\u79cd\u59d3\u548c\u5b97\u6559\u504f\u89c1\u65b9\u9762\u3002"}}
{"id": "2509.02160", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02160", "abs": "https://arxiv.org/abs/2509.02160", "authors": ["David Demitri Africa", "Suchir Salhan", "Yuval Weiss", "Paula Buttery", "Richard Diehl Martinez"], "title": "Meta-Pretraining for Zero-Shot Cross-Lingual Named Entity Recognition in Low-Resource Philippine Languages", "comment": null, "summary": "Named-entity recognition (NER) in low-resource languages is usually tackled\nby finetuning very large multilingual LMs, an option that is often infeasible\nin memory- or latency-constrained settings. We ask whether small decoder LMs\ncan be pretrained so that they adapt quickly and transfer zero-shot to\nlanguages unseen during pretraining. To this end we replace part of the\nautoregressive objective with first-order model-agnostic meta-learning (MAML).\nTagalog and Cebuano are typologically similar yet structurally different in\ntheir actor/non-actor voice systems, and hence serve as a challenging test-bed.\nAcross four model sizes (11 M - 570 M) MAML lifts zero-shot micro-F1 by 2-6 pp\nunder head-only tuning and 1-3 pp after full tuning, while cutting convergence\ntime by up to 8%. Gains are largest for single-token person entities that\nco-occur with Tagalog case particles si/ni, highlighting the importance of\nsurface anchors.", "AI": {"tldr": "\u901a\u8fc7\u7ee7\u627f\u5b66\u4e60\u65b9\u6cd5(MAML)\u9884\u8bad\u7ec3\u5c0f\u578b\u89e3\u7801\u5668\u6a21\u578b\uff0c\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u9002\u5e94\u548c\u66f4\u597d\u7684\u96f6\u6837\u672c\u8f6c\u79fb\u6027\u80fd", "motivation": "\u89e3\u51b3\u5728\u5185\u5b58\u6216\u5ef6\u8fdf\u53d7\u9650\u73af\u5883\u4e0b\uff0c\u65e0\u6cd5\u4f7f\u7528\u5927\u578b\u591a\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4f4e\u8d44\u6e90\u8bed\u8a00\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u95ee\u9898", "method": "\u5c06\u81ea\u56de\u5f52\u76ee\u6807\u4e0e\u4e00\u9636\u6a21\u578b\u65e0\u5173\u5143\u5b66\u4e60(MAML)\u7ed3\u5408\uff0c\u9884\u8bad\u7ec3\u5c0f\u578b\u89e3\u7801\u5668\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u5feb\u901f\u9002\u5e94\u5e76\u5411\u9884\u8bad\u7ec3\u65f6\u672a\u89c1\u8fc7\u7684\u8bed\u8a00\u8fdb\u884c\u96f6\u6837\u672c\u8f6c\u79fb", "result": "\u572811M-570M\u56db\u79cd\u6a21\u578b\u89c4\u6a21\u4e0b\uff0cMAML\u65b9\u6cd5\u5728\u4ec5\u8c03\u6574\u5934\u90e8\u65f6\u5fae\u89c2F1\u63d0\u53472-6\u4e2a\u767e\u5206\u70b9\uff0c\u5168\u6a21\u578b\u5fae\u8c03\u65f6\u63d0\u53471-3\u4e2a\u767e\u5206\u70b9\uff0c\u6536\u655b\u901f\u5ea6\u63d0\u5347\u8fbe8%\uff0c\u7279\u522b\u5728\u6807\u8bb0\u4eba\u540d\u5b9e\u4f53\u65f6\u6548\u679c\u663e\u8457", "conclusion": "\u7ee7\u627f\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u5c0f\u578b\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00NER\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027\u548c\u8f6c\u79fb\u6027\u80fd\uff0c\u8868\u9762\u6807\u8bb0(\u5982\u8bed\u6cd5\u52a9\u8bcd)\u5728\u8bc6\u522b\u5b9e\u4f53\u65f6\u8d77\u91cd\u8981\u4f5c\u7528"}}
{"id": "2509.02170", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02170", "abs": "https://arxiv.org/abs/2509.02170", "authors": ["Kyeongman Park", "Nakyeong Yang", "Kyomin Jung"], "title": "Avoidance Decoding for Diverse Multi-Branch Story Generation", "comment": null, "summary": "Large Language Models (LLMs) often generate repetitive and monotonous\noutputs, especially in tasks like story generation, due to limited creative\ndiversity when given the same input prompt. To address this challenge, we\npropose a novel decoding strategy, Avoidance Decoding, that modifies token\nlogits by penalizing similarity to previously generated outputs, thereby\nencouraging more diverse multi-branch stories. This penalty adaptively balances\ntwo similarity measures: (1) Concept-level Similarity Penalty, which is\nprioritized in early stages to diversify initial story concepts, and (2)\nNarrative-level Similarity Penalty, which is increasingly emphasized later to\nensure natural yet diverse plot development. Notably, our method achieves up to\n2.6 times higher output diversity and reduces repetition by an average of 30%\ncompared to strong baselines, while effectively mitigating text degeneration.\nFurthermore, we reveal that our method activates a broader range of neurons,\ndemonstrating that it leverages the model's intrinsic creativity.", "AI": {"tldr": "\u63d0\u51faAvoidance Decoding\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u60e9\u7f5a\u4e0e\u5148\u524d\u751f\u6210\u5185\u5bb9\u7684\u76f8\u4f3c\u6027\u6765\u63d0\u5347LLM\u751f\u6210\u6545\u4e8b\u7684\u591a\u6837\u6027\uff0c\u51cf\u5c11\u91cd\u590d\u6027", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u76f8\u540c\u8f93\u5165\u63d0\u793a\u4e0b\u5f80\u5f80\u751f\u6210\u91cd\u590d\u5355\u8c03\u7684\u8f93\u51fa\uff0c\u7279\u522b\u662f\u5728\u6545\u4e8b\u751f\u6210\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u521b\u610f\u591a\u6837\u6027", "method": "\u4fee\u6539token logits\uff0c\u901a\u8fc7\u4e24\u4e2a\u81ea\u9002\u5e94\u5e73\u8861\u7684\u76f8\u4f3c\u6027\u60e9\u7f5a\uff1a\u65e9\u671f\u4f18\u5148\u6982\u5ff5\u7ea7\u76f8\u4f3c\u6027\u60e9\u7f5a\u4ee5\u591a\u6837\u5316\u521d\u59cb\u6545\u4e8b\u6982\u5ff5\uff0c\u540e\u671f\u589e\u52a0\u53d9\u4e8b\u7ea7\u76f8\u4f3c\u6027\u60e9\u7f5a\u4ee5\u786e\u4fdd\u81ea\u7136\u4e14\u591a\u6837\u7684\u60c5\u8282\u53d1\u5c55", "result": "\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8f93\u51fa\u591a\u6837\u6027\u63d0\u53472.6\u500d\uff0c\u91cd\u590d\u6027\u5e73\u5747\u51cf\u5c1130%\uff0c\u540c\u65f6\u6709\u6548\u7f13\u89e3\u6587\u672c\u9000\u5316\u95ee\u9898\uff0c\u6fc0\u6d3b\u4e86\u66f4\u5e7f\u6cdb\u7684\u795e\u7ecf\u5143", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u63d0\u5347\u4e86LLM\u751f\u6210\u5185\u5bb9\u7684\u591a\u6837\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u80fd\u591f\u6709\u6548\u5229\u7528\u6a21\u578b\u7684\u5185\u5728\u521b\u9020\u529b"}}
{"id": "2509.02198", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.02198", "abs": "https://arxiv.org/abs/2509.02198", "authors": ["Anum Afzal", "Juraj Vladika", "Florian Matthes"], "title": "FActBench: A Benchmark for Fine-grained Automatic Evaluation of LLM-Generated Text in the Medical Domain", "comment": null, "summary": "Large Language Models tend to struggle when dealing with specialized domains.\nWhile all aspects of evaluation hold importance, factuality is the most\ncritical one. Similarly, reliable fact-checking tools and data sources are\nessential for hallucination mitigation. We address these issues by providing a\ncomprehensive Fact-checking Benchmark FActBench covering four generation tasks\nand six state-of-the-art Large Language Models (LLMs) for the Medical domain.\nWe use two state-of-the-art Fact-checking techniques: Chain-of-Thought (CoT)\nPrompting and Natural Language Inference (NLI). Our experiments show that the\nfact-checking scores acquired through the Unanimous Voting of both techniques\ncorrelate best with Domain Expert Evaluation.", "AI": {"tldr": "FActBench\u662f\u4e00\u4e2a\u9488\u5bf9\u533b\u5b66\u9886\u57df\u7684\u5168\u9762\u4e8b\u5b9e\u6838\u67e5\u57fa\u51c6\uff0c\u6db5\u76d64\u4e2a\u751f\u6210\u4efb\u52a1\u548c6\u4e2a\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u7528CoT\u63d0\u793a\u548cNLI\u4e24\u79cd\u6280\u672f\u8fdb\u884c\u4e8b\u5b9e\u6838\u67e5\uff0c\u53d1\u73b0\u4e00\u81f4\u6295\u7968\u7ed3\u679c\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u76f8\u5173\u6027\u6700\u4f73", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u4e8b\u5b9e\u51c6\u786e\u6027\u662f\u6700\u5173\u952e\u7684\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u9700\u8981\u53ef\u9760\u7684\u4e8b\u5b9e\u6838\u67e5\u5de5\u5177\u548c\u6570\u636e\u6e90\u6765\u89e3\u51b3\u5e7b\u89c9\u95ee\u9898", "method": "\u6784\u5efa\u533b\u5b66\u9886\u57df\u4e8b\u5b9e\u6838\u67e5\u57fa\u51c6FActBench\uff0c\u6db5\u76d64\u4e2a\u751f\u6210\u4efb\u52a1\u548c6\u4e2aLLM\uff0c\u91c7\u7528Chain-of-Thought\u63d0\u793a\u548c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4e24\u79cd\u5148\u8fdb\u4e8b\u5b9e\u6838\u67e5\u6280\u672f", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u4e24\u79cd\u6280\u672f\u7684\u4e00\u81f4\u6295\u7968\u83b7\u5f97\u7684\u4e8b\u5b9e\u6838\u67e5\u5206\u6570\u4e0e\u9886\u57df\u4e13\u5bb6\u8bc4\u4f30\u7684\u76f8\u5173\u6027\u6700\u597d", "conclusion": "FActBench\u4e3a\u533b\u5b66\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4e8b\u5b9e\u6838\u67e5\u57fa\u51c6\uff0c\u4e00\u81f4\u6295\u7968\u65b9\u6cd5\u80fd\u591f\u53ef\u9760\u5730\u8bc4\u4f30LLM\u7684\u4e8b\u5b9e\u51c6\u786e\u6027"}}
{"id": "2509.02225", "categories": ["cs.CL", "I.2.7; I.7"], "pdf": "https://arxiv.org/pdf/2509.02225", "abs": "https://arxiv.org/abs/2509.02225", "authors": ["Jaime Collado-Monta\u00f1ez", "L. Alfonso Ure\u00f1a-L\u00f3pez", "Arturo Montejo-R\u00e1ez"], "title": "Towards Fundamental Language Models: Does Linguistic Competence Scale with Model Size?", "comment": "13 pages, 2 figures", "summary": "Large Language Models offer impressive language capabilities but suffer from\nwell-known limitations, including hallucinations, biases, privacy concerns, and\nhigh computational costs. These issues are largely driven by the combination of\nlinguistic competence and factual memorization within a single monolithic\nmodel. This paper introduces and empirically supports the Fundamental Language\nModel (FLM) paradigm, which advocates for smaller, linguistically competent\nmodels that offload factual retrieval to external tools. We evaluate models\nranging from 135M to 32B parameters across three dimensions: linguistic\ncompetence, external factual knowledge, and internal factual knowledge. Our\nfindings reveal that while both linguistic competence and factual knowledge\nimprove with scale, internal factual knowledge grows significantly faster,\nsuggesting that model size is more closely tied to memorization than to core\nlanguage ability. These results support a modular approach to language\nmodeling, where compact, linguistically proficient models serve as the\nfoundation for tool-augmented systems. The FLM paradigm offers a path toward\nmore efficient, interpretable, and sustainable NLP solutions.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u3001\u504f\u89c1\u3001\u9690\u79c1\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7b49\u95ee\u9898\uff0c\u4e3b\u8981\u6e90\u4e8e\u8bed\u8a00\u80fd\u529b\u548c\u4e8b\u5b9e\u8bb0\u5fc6\u7684\u6df7\u5408\u3002\u672c\u6587\u63d0\u51fa\u57fa\u7840\u8bed\u8a00\u6a21\u578b\uff08FLM\uff09\u8303\u5f0f\uff0c\u4f7f\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u914d\u5408\u5916\u90e8\u5de5\u5177\u8fdb\u884c\u4e8b\u5b9e\u68c0\u7d22\uff0c\u5b9e\u8bc1\u8868\u660e\u6a21\u578b\u89c4\u6a21\u4e0e\u8bb0\u5fc6\u80fd\u529b\u800c\u975e\u6838\u5fc3\u8bed\u8a00\u80fd\u529b\u66f4\u76f8\u5173\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u5e7b\u89c9\u3001\u504f\u89c1\u3001\u9690\u79c1\u6cc4\u9732\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7b49\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u7684\u6839\u6e90\u5728\u4e8e\u5c06\u8bed\u8a00\u80fd\u529b\u548c\u4e8b\u5b9e\u8bb0\u5fc6\u529f\u80fd\u6df7\u5408\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u3002", "method": "\u63d0\u51fa\u57fa\u7840\u8bed\u8a00\u6a21\u578b\uff08FLM\uff09\u8303\u5f0f\uff0c\u4f7f\u7528\u8f83\u5c0f\u53c2\u6570\u89c4\u6a21\uff081.35\u4ebf\u5230320\u4ebf\uff09\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u4e8b\u5b9e\u68c0\u7d22\u529f\u80fd\u5378\u8f7d\u5230\u5916\u90e8\u5de5\u5177\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u8bed\u8a00\u80fd\u529b\u3001\u5916\u90e8\u4e8b\u5b9e\u77e5\u8bc6\u548c\u5185\u90e8\u4e8b\u5b9e\u77e5\u8bc6\u4e09\u4e2a\u7ef4\u5ea6\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u80fd\u529b\u548c\u4e8b\u5b9e\u77e5\u8bc6\u90fd\u968f\u6a21\u578b\u89c4\u6a21\u589e\u957f\u800c\u63d0\u5347\uff0c\u4f46\u5185\u90e8\u4e8b\u5b9e\u77e5\u8bc6\u589e\u957f\u66f4\u5feb\uff0c\u8868\u660e\u6a21\u578b\u89c4\u6a21\u4e0e\u8bb0\u5fc6\u80fd\u529b\u800c\u975e\u6838\u5fc3\u8bed\u8a00\u80fd\u529b\u66f4\u76f8\u5173\u3002", "conclusion": "\u652f\u6301\u6a21\u5757\u5316\u8bed\u8a00\u5efa\u6a21\u65b9\u6cd5\uff0c\u4f7f\u7528\u7d27\u51d1\u3001\u8bed\u8a00\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u4f5c\u4e3a\u5de5\u5177\u589e\u5f3a\u7cfb\u7edf\u7684\u57fa\u7840\uff0cFLM\u8303\u5f0f\u4e3a\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6301\u7eed\u7684NLP\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u8def\u5f84\u3002"}}
{"id": "2509.02292", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.02292", "abs": "https://arxiv.org/abs/2509.02292", "authors": ["Katharine Kowalyshyn", "Matthias Scheutz"], "title": "LLMs and their Limited Theory of Mind: Evaluating Mental State Annotations in Situated Dialogue", "comment": null, "summary": "What if large language models could not only infer human mindsets but also\nexpose every blind spot in team dialogue such as discrepancies in the team\nmembers' joint understanding? We present a novel, two-step framework that\nleverages large language models (LLMs) both as human-style annotators of team\ndialogues to track the team's shared mental models (SMMs) and as automated\ndiscrepancy detectors among individuals' mental states. In the first step, an\nLLM generates annotations by identifying SMM elements within task-oriented\ndialogues from the Cooperative Remote Search Task (CReST) corpus. Then, a\nsecondary LLM compares these LLM-derived annotations and human annotations\nagainst gold-standard labels to detect and characterize divergences. We define\nan SMM coherence evaluation framework for this use case and apply it to six\nCReST dialogues, ultimately producing: (1) a dataset of human and LLM\nannotations; (2) a reproducible evaluation framework for SMM coherence; and (3)\nan empirical assessment of LLM-based discrepancy detection. Our results reveal\nthat, although LLMs exhibit apparent coherence on straightforward\nnatural-language annotation tasks, they systematically err in scenarios\nrequiring spatial reasoning or disambiguation of prosodic cues.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u56e2\u961f\u5bf9\u8bdd\u6807\u6ce8\u5668\u548c\u5dee\u5f02\u68c0\u6d4b\u5668\uff0c\u6765\u8ffd\u8e2a\u56e2\u961f\u7684\u5171\u4eab\u5fc3\u667a\u6a21\u578b\u5e76\u53d1\u73b0\u6210\u5458\u95f4\u7684\u8ba4\u77e5\u5dee\u5f02\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u4e0d\u4ec5\u80fd\u63a8\u65ad\u4eba\u7c7b\u601d\u7ef4\uff0c\u8fd8\u80fd\u63ed\u793a\u56e2\u961f\u5bf9\u8bdd\u4e2d\u7684\u76f2\u70b9\uff0c\u7279\u522b\u662f\u56e2\u961f\u6210\u5458\u5728\u5171\u540c\u7406\u89e3\u4e0a\u7684\u5dee\u5f02\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u9aa4\u65b9\u6cd5\uff1a\u9996\u5148\u7528LLM\u751f\u6210\u56e2\u961f\u5bf9\u8bdd\u7684\u5171\u4eab\u5fc3\u667a\u6a21\u578b\u6807\u6ce8\uff0c\u7136\u540e\u7528\u53e6\u4e00\u4e2aLLM\u6bd4\u8f83LLM\u6807\u6ce8\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u5dee\u5f02\uff0c\u68c0\u6d4b\u8ba4\u77e5\u5206\u6b67\u3002\u57fa\u4e8eCReST\u8bed\u6599\u5e93\u6784\u5efa\u8bc4\u4f30\u6846\u67b6\u3002", "result": "LLM\u5728\u7b80\u5355\u81ea\u7136\u8bed\u8a00\u6807\u6ce8\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u4e00\u81f4\u6027\uff0c\u4f46\u5728\u9700\u8981\u7a7a\u95f4\u63a8\u7406\u6216\u97f5\u5f8b\u7ebf\u7d22\u6d88\u6b67\u7684\u573a\u666f\u4e2d\u4f1a\u7cfb\u7edf\u6027\u51fa\u9519\u3002", "conclusion": "\u867d\u7136LLM\u5728\u57fa\u7840\u6807\u6ce8\u4efb\u52a1\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u590d\u6742\u63a8\u7406\u573a\u666f\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3a\u56e2\u961f\u8ba4\u77e5\u5dee\u5f02\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u4f46\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2509.02333", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02333", "abs": "https://arxiv.org/abs/2509.02333", "authors": ["Shihui Yang", "Chengfeng Dou", "Peidong Guo", "Kai Lu", "Qiang Ju", "Fei Deng", "Rihui Xin"], "title": "DCPO: Dynamic Clipping Policy Optimization", "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a\npromising framework for enhancing the reasoning capabilities of large language\nmodels. However, existing approaches such as GRPO often suffer from zero\ngradients. This problem arises primarily due to fixed clipping bounds for\ntoken-level probability ratios and the standardization of identical rewards,\nwhich can lead to ineffective gradient updates and underutilization of\ngenerated responses. In this work, we propose Dynamic Clipping Policy\nOptimization (DCPO), which introduces a dynamic clipping strategy that\nadaptively adjusts the clipping bounds based on token-specific prior\nprobabilities to enhance token-level exploration, and a smooth advantage\nstandardization technique that standardizes rewards across cumulative training\nsteps to improve the response-level effective utilization of generated\nresponses. DCPO achieved state-of-the-art performance on four benchmarks based\non four different models. In particular, DCPO achieved an Avg@1 of 46.7 under\ngreedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24\nbenchmark, surpassing both DAPO (36.7/31.6) and GRPO (36.7/32.1) on the\nQwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO\nachieves a performance of (23.3/19.0), surpassing GRPO (13.3/10.5) and DAPO\n(20.0/15.3). Furthermore, DCPO achieved an average 28% improvement in the\nnonzero advantage over GRPO in four models, doubled the training efficiency\nover DAPO, and significantly reduced the token clipping ratio by an order of\nmagnitude compared to both GRPO and DAPO, while achieving superior performance.\nThese results highlight DCPO's effectiveness in leveraging generated data more\nefficiently for reinforcement learning in large language models.", "AI": {"tldr": "DCPO\u63d0\u51fa\u52a8\u6001\u526a\u5207\u7b56\u7565\u548c\u5e73\u6ed1\u4f18\u52bf\u6807\u51c6\u5316\u6280\u672f\uff0c\u89e3\u51b3RLVR\u4e2d\u56e0\u56fa\u5b9a\u526a\u5207\u8fb9\u754c\u548c\u5956\u52b1\u6807\u51c6\u5316\u5bfc\u81f4\u7684\u96f6\u68af\u5ea6\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u5982GRPO\u5b58\u5728\u96f6\u68af\u5ea6\u95ee\u9898\uff0c\u4e3b\u8981\u7531\u4e8e\u56fa\u5b9a\u526a\u5207\u8fb9\u754c\u548c\u76f8\u540c\u5956\u52b1\u6807\u51c6\u5316\u5bfc\u81f4\u7684\u65e0\u6548\u68af\u5ea6\u66f4\u65b0\u548c\u751f\u6210\u54cd\u5e94\u5229\u7528\u4e0d\u5145\u5206\u3002", "method": "\u63d0\u51faDCPO\u65b9\u6cd5\uff1a1\uff09\u52a8\u6001\u526a\u5207\u7b56\u7565\uff0c\u6839\u636e\u6807\u8bb0\u7279\u5b9a\u5148\u9a8c\u6982\u7387\u81ea\u9002\u5e94\u8c03\u6574\u526a\u5207\u8fb9\u754c\u4ee5\u589e\u5f3a\u6807\u8bb0\u7ea7\u63a2\u7d22\uff1b2\uff09\u5e73\u6ed1\u4f18\u52bf\u6807\u51c6\u5316\u6280\u672f\uff0c\u5728\u7d2f\u8ba1\u8bad\u7ec3\u6b65\u9aa4\u4e2d\u6807\u51c6\u5316\u5956\u52b1\u4ee5\u63d0\u9ad8\u54cd\u5e94\u7ea7\u5229\u7528\u6548\u679c\u3002", "result": "\u57284\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f73\u6027\u80fd\uff1aAIME24\u57fa\u51c6\u4e0aAvg@1 46.7\uff08\u8d85\u8fc7DAPO 36.7\u548cGRPO 36.7\uff09\uff0cAIME25\u57fa\u51c6\u4e0a23.3\uff08\u8d85\u8fc7GRPO 13.3\u548cDAPO 20.0\uff09\u3002\u975e\u96f6\u4f18\u52bf\u63d0\u9ad828%\uff0c\u8bad\u7ec3\u6548\u7387\u63d0\u9ad8\u4e00\u500d\uff0c\u6807\u8bb0\u526a\u5207\u6bd4\u7387\u660e\u663e\u964d\u4f4e\u3002", "conclusion": "DCPO\u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u751f\u6210\u6570\u636e\u8fdb\u884c\u52a0\u5f3a\u5b66\u4e60\uff0c\u901a\u8fc7\u52a8\u6001\u526a\u5207\u548c\u4f18\u52bf\u6807\u51c6\u5316\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u5728\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\u65b9\u9762\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2509.02350", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02350", "abs": "https://arxiv.org/abs/2509.02350", "authors": ["Jindong Li", "Yali Fu", "Li Fan", "Jiahong Liu", "Yao Shu", "Chengwei Qin", "Menglin Yang", "Irwin King", "Rex Ying"], "title": "Implicit Reasoning in Large Language Models: A Comprehensive Survey", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong generalization across a\nwide range of tasks. Reasoning with LLMs is central to solving multi-step\nproblems and complex decision-making. To support efficient reasoning, recent\nstudies have shifted attention from explicit chain-of-thought prompting toward\nimplicit reasoning, where reasoning occurs silently via latent structures\nwithout emitting intermediate textual steps. Implicit reasoning brings\nadvantages such as lower generation cost, faster inference, and better\nalignment with internal computation. Although prior surveys have discussed\nlatent representations in the context of reasoning, a dedicated and\nmechanism-level examination of how reasoning unfolds internally within LLMs\nremains absent. This survey fills that gap by introducing a taxonomy centered\non execution paradigms, shifting the focus from representational forms to\ncomputational strategies. We organize existing methods into three execution\nparadigms based on \\textbf{\\textit{how and where internal computation\nunfolds}}: latent optimization, signal-guided control, and layer-recurrent\nexecution. We also review structural, behavioral and representation-based\nevidence that supports the presence of implicit reasoning in LLMs. We further\nprovide a structured overview of the evaluation metrics and benchmarks used in\nexisting works to assess the effectiveness and reliability of implicit\nreasoning.We maintain a continuously updated project at:\nhttps://github.com/digailab/awesome-llm-implicit-reasoning.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u662f\u4e00\u4e2a\u5173\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u9690\u5f0f\u63a8\u7406\u7684\u7efc\u8ff0\uff0c\u4ecb\u7ecd\u4e86\u4e0d\u53d1\u751f\u6587\u672c\u4e2d\u95f4\u6b65\u9aa4\u7684\u5185\u90e8\u8ba1\u7b97\u63a8\u7406\u65b9\u6cd5\uff0c\u5305\u62ec\u4e09\u79cd\u6267\u884c\u8303\u5f0b\u548c\u76f8\u5173\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u5c3d\u7ba1\u4e4b\u524d\u7684\u7814\u7a76\u8ba8\u8bba\u4e86\u63a8\u7406\u4e2d\u7684\u6f5c\u5728\u8868\u5f81\uff0c\u4f46\u7f3a\u4e4f\u4e00\u4e2a\u4e13\u95e8\u4ece\u673a\u5236\u5c42\u9762\u5206\u6790LLM\u5185\u90e8\u5982\u4f55\u5c55\u5f00\u63a8\u7406\u7684\u7efc\u8ff0\u3002", "method": "\u901a\u8fc7\u4ee5\u6267\u884c\u8303\u5f0b\u4e3a\u4e2d\u5fc3\u7684\u5206\u7c7b\u6cd5\uff0c\u5c06\u73b0\u6709\u65b9\u6cd5\u7ec4\u7ec7\u4e3a\u4e09\u7c7b\uff1a\u6f5c\u5728\u4f18\u5316\u3001\u4fe1\u53f7\u5bfc\u5411\u63a7\u5236\u548c\u5c42\u5faa\u73af\u6267\u884c\uff0c\u5e76\u7efc\u8ff0\u652f\u6301\u9690\u5f0f\u63a8\u7406\u7684\u7ed3\u6784\u3001\u884c\u4e3a\u548c\u8868\u5f81\u8bc1\u636e\u3002", "result": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u89c6\u89d2\u6765\u7406\u89e3LLM\u5185\u90e8\u63a8\u7406\u673a\u5236\uff0c\u5e76\u7efc\u8ff0\u4e86\u76f8\u5173\u8bc4\u4f30\u6307\u6807\u548c\u6d4b\u8bd5\u96c6\u3002", "conclusion": "\u9690\u5f0f\u63a8\u7406\u5177\u6709\u751f\u6210\u6210\u672c\u4f4e\u3001\u63a8\u7406\u901f\u5ea6\u5feb\u3001\u4e0e\u5185\u90e8\u8ba1\u7b97\u66f4\u4e00\u81f4\u7b49\u4f18\u52bf\uff0c\u8fd9\u4e2a\u7efc\u8ff0\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u673a\u5236\u5c42\u9762\u7684\u6df1\u5165\u5206\u6790\u3002"}}
{"id": "2509.02363", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.02363", "abs": "https://arxiv.org/abs/2509.02363", "authors": ["Gaurav Negi", "Atul Kr. Ojha", "Omnia Zayed", "Paul Buitelaar"], "title": "Towards Temporal Knowledge-Base Creation for Fine-Grained Opinion Analysis with Language Models", "comment": null, "summary": "We propose a scalable method for constructing a temporal opinion knowledge\nbase with large language models (LLMs) as automated annotators. Despite the\ndemonstrated utility of time-series opinion analysis of text for downstream\napplications such as forecasting and trend analysis, existing methodologies\nunderexploit this potential due to the absence of temporally grounded\nfine-grained annotations. Our approach addresses this gap by integrating\nwell-established opinion mining formulations into a declarative LLM annotation\npipeline, enabling structured opinion extraction without manual prompt\nengineering. We define three data models grounded in sentiment and opinion\nmining literature, serving as schemas for structured representation. We perform\nrigorous quantitative evaluation of our pipeline using human-annotated test\nsamples. We carry out the final annotations using two separate LLMs, and\ninter-annotator agreement is computed label-wise across the fine-grained\nopinion dimensions, analogous to human annotation protocols. The resulting\nknowledge base encapsulates time-aligned, structured opinions and is compatible\nwith applications in Retrieval-Augmented Generation (RAG), temporal question\nanswering, and timeline summarisation.", "AI": {"tldr": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u65f6\u5e8f\u610f\u89c1\u77e5\u8bc6\u5e93\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u58f0\u660e\u5f0f\u6ce8\u91ca\u6d41\u6c34\u7ebf\u5b9e\u73b0\u7ed3\u6784\u5316\u610f\u89c1\u63d0\u53d6\uff0c\u65e0\u9700\u624b\u52a8\u63d0\u793a\u5de5\u7a0b", "motivation": "\u73b0\u6709\u65b9\u6cd5\u6ca1\u6709\u5145\u5206\u5229\u7528\u65f6\u95f4\u5e8f\u5217\u610f\u89c1\u5206\u6790\u7684\u6f5c\u529b\uff0c\u7f3a\u4e4f\u65f6\u6001\u57fa\u7840\u7684\u7ec6\u7c92\u5ea6\u6ce8\u91ca", "method": "\u5c06\u610f\u89c1\u6316\u6398\u5f62\u5f0f\u96c6\u6210\u5230\u58f0\u660e\u5f0fLLM\u6ce8\u91ca\u6d41\u6c34\u7ebf\u4e2d\uff0c\u5b9a\u4e49\u4e09\u79cd\u57fa\u4e8e\u60c5\u611f\u548c\u610f\u89c1\u6316\u6398\u6587\u732e\u7684\u6570\u636e\u6a21\u578b", "result": "\u4f7f\u7528\u4eba\u5de5\u6ce8\u91ca\u6d4b\u8bd5\u6837\u672c\u8fdb\u884c\u4e25\u683c\u7684\u5b9a\u91cf\u8bc4\u4f30\uff0c\u8ba1\u7b97\u4e86\u4e24\u4e2aLLM\u4e4b\u95f4\u7684\u8bc4\u6ce8\u8005\u4e00\u81f4\u6027", "conclusion": "\u6784\u5efa\u7684\u77e5\u8bc6\u5e93\u5305\u542b\u65f6\u95f4\u5bf9\u9f50\u7684\u7ed3\u6784\u5316\u610f\u89c1\uff0c\u517c\u5bb9RAG\u3001\u65f6\u6001\u95ee\u7b54\u548c\u65f6\u95f4\u7ebf\u6458\u8981\u7b49\u5e94\u7528"}}
{"id": "2509.02446", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02446", "abs": "https://arxiv.org/abs/2509.02446", "authors": ["Ali Hamdi", "Malak Mohamed", "Rokaia Emad", "Khaled Shaban"], "title": "An Ensemble Classification Approach in A Multi-Layered Large Language Model Framework for Disease Prediction", "comment": null, "summary": "Social telehealth has made remarkable progress in healthcare by allowing\npatients to post symptoms and participate in medical consultations remotely.\nUsers frequently post symptoms on social media and online health platforms,\ncreating a huge repository of medical data that can be leveraged for disease\nclassification. Large language models (LLMs) such as LLAMA3 and GPT-3.5, along\nwith transformer-based models like BERT, have demonstrated strong capabilities\nin processing complex medical text. In this study, we evaluate three Arabic\nmedical text preprocessing methods such as summarization, refinement, and Named\nEntity Recognition (NER) before applying fine-tuned Arabic transformer models\n(CAMeLBERT, AraBERT, and AsafayaBERT). To enhance robustness, we adopt a\nmajority voting ensemble that combines predictions from original and\npreprocessed text representations. This approach achieved the best\nclassification accuracy of 80.56%, thus showing its effectiveness in leveraging\nvarious text representations and model predictions to improve the understanding\nof medical texts. To the best of our knowledge, this is the first work that\nintegrates LLM-based preprocessing with fine-tuned Arabic transformer models\nand ensemble learning for disease classification in Arabic social telehealth\ndata.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u793e\u4ea4\u8fdc\u7a0b\u5065\u5eb7\u6587\u672c\u7684\u75be\u75c5\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u9884\u5904\u7406\u6280\u672f\u3001\u7cbe\u8c03\u963f\u62c9\u4f2f\u8bedtransformer\u6a21\u578b\u548c\u591a\u6570\u6295\u7968\u96c6\u6210\u5b66\u4e60\uff0c\u8fbe\u5230\u4e8680.56%\u7684\u6700\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u793e\u4ea4\u8fdc\u7a0b\u5065\u5eb7\u5e73\u53f0\u4e0a\u5b58\u5728\u5927\u91cf\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u6587\u672c\u6570\u636e\uff0c\u9700\u8981\u6709\u6548\u7684\u75be\u75c5\u5206\u7c7b\u65b9\u6cd5\u6765\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u3002\u867d\u7136LLM\u548ctransformer\u6a21\u578b\u5728\u533b\u7597\u6587\u672c\u5904\u7406\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u5728\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u9884\u5904\u7406\u548c\u96c6\u6210\u5b66\u4e60\u65b9\u9762\u4ecd\u7136\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u6587\u672c\u9884\u5904\u7406\u65b9\u6cd5\uff1a\u6458\u8981\u5316\u3001\u7cbe\u7ec6\u5316\u548c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b(NER)\uff0c\u7136\u540e\u5e94\u7528\u7cbe\u8c03\u7684\u963f\u62c9\u4f2f\u8bedtransformer\u6a21\u578b(CAMeLBERT\u3001AraBERT\u3001AsafayaBERT)\u3002\u91c7\u7528\u591a\u6570\u6295\u7968\u96c6\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u539f\u59cb\u6587\u672c\u548c\u9884\u5904\u7406\u6587\u672c\u8868\u5f81\u7684\u9884\u6d4b\u7ed3\u679c\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u963f\u62c9\u4f2f\u8bed\u793e\u4ea4\u8fdc\u7a0b\u5065\u5eb7\u6570\u636e\u4e0a\u8fbe\u5230\u4e8680.56%\u7684\u75be\u75c5\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u663e\u793a\u51fa\u901a\u8fc7\u5229\u7528\u4e0d\u540c\u6587\u672c\u8868\u5f81\u548c\u6a21\u578b\u9884\u6d4b\u6765\u63d0\u9ad8\u533b\u7597\u6587\u672c\u7406\u89e3\u80fd\u529b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5c06LLM\u57fa\u4e8e\u9884\u5904\u7406\u6280\u672f\u4e0e\u7cbe\u8c03\u963f\u62c9\u4f2f\u8bedtransformer\u6a21\u578b\u548c\u96c6\u6210\u5b66\u4e60\u76f8\u7ed3\u5408\u7528\u4e8e\u963f\u62c9\u4f2f\u8bed\u793e\u4ea4\u8fdc\u7a0b\u5065\u5eb7\u6570\u636e\u7684\u75be\u75c5\u5206\u7c7b\u7814\u7a76\uff0c\u4e3a\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u6587\u672c\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.02450", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02450", "abs": "https://arxiv.org/abs/2509.02450", "authors": ["Lingzhi Shen", "Xiaohao Cai", "Yunfei Long", "Imran Razzak", "Guanming Chen", "Shoaib Jameel"], "title": "EmoPerso: Enhancing Personality Detection with Self-Supervised Emotion-Aware Modelling", "comment": null, "summary": "Personality detection from text is commonly performed by analysing users'\nsocial media posts. However, existing methods heavily rely on large-scale\nannotated datasets, making it challenging to obtain high-quality personality\nlabels. Moreover, most studies treat emotion and personality as independent\nvariables, overlooking their interactions. In this paper, we propose a novel\nself-supervised framework, EmoPerso, which improves personality detection\nthrough emotion-aware modelling. EmoPerso first leverages generative mechanisms\nfor synthetic data augmentation and rich representation learning. It then\nextracts pseudo-labeled emotion features and jointly optimizes them with\npersonality prediction via multi-task learning. A cross-attention module is\nemployed to capture fine-grained interactions between personality traits and\nthe inferred emotional representations. To further refine relational reasoning,\nEmoPerso adopts a self-taught strategy to enhance the model's reasoning\ncapabilities iteratively. Extensive experiments on two benchmark datasets\ndemonstrate that EmoPerso surpasses state-of-the-art models. The source code is\navailable at https://github.com/slz0925/EmoPerso.", "AI": {"tldr": "EmoPerso\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u60c5\u611f\u611f\u77e5\u5efa\u6a21\u6539\u8fdb\u4eba\u683c\u68c0\u6d4b\uff0c\u5229\u7528\u751f\u6210\u673a\u5236\u8fdb\u884c\u6570\u636e\u589e\u5f3a\u548c\u8868\u793a\u5b66\u4e60\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u6355\u6349\u4eba\u683c\u4e0e\u60c5\u611f\u7684\u7ec6\u7c92\u5ea6\u4ea4\u4e92\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u683c\u68c0\u6d4b\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u83b7\u5f97\u9ad8\u8d28\u91cf\u4eba\u683c\u6807\u7b7e\uff0c\u4e14\u5927\u591a\u5c06\u60c5\u611f\u548c\u4eba\u683c\u89c6\u4e3a\u72ec\u7acb\u53d8\u91cf\uff0c\u5ffd\u89c6\u4e86\u5b83\u4eec\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u63d0\u51faEmoPerso\u6846\u67b6\uff1a1\uff09\u5229\u7528\u751f\u6210\u673a\u5236\u8fdb\u884c\u5408\u6210\u6570\u636e\u589e\u5f3a\u548c\u4e30\u5bcc\u8868\u793a\u5b66\u4e60\uff1b2\uff09\u63d0\u53d6\u4f2a\u6807\u7b7e\u60c5\u611f\u7279\u5f81\u5e76\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u4e0e\u4eba\u683c\u9884\u6d4b\u8054\u5408\u4f18\u5316\uff1b3\uff09\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u6355\u6349\u4eba\u683c\u7279\u8d28\u4e0e\u60c5\u611f\u8868\u793a\u7684\u7ec6\u7c92\u5ea6\u4ea4\u4e92\uff1b4\uff09\u91c7\u7528\u81ea\u5b66\u4e60\u7b56\u7565\u8fed\u4ee3\u589e\u5f3a\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cEmoPerso\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002", "conclusion": "EmoPerso\u901a\u8fc7\u60c5\u611f\u611f\u77e5\u5efa\u6a21\u6709\u6548\u63d0\u5347\u4e86\u4eba\u683c\u68c0\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u60c5\u611f\u4e0e\u4eba\u683c\u4e4b\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u81ea\u76d1\u7763\u4eba\u683c\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.02452", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02452", "abs": "https://arxiv.org/abs/2509.02452", "authors": ["Seyedali Mohammadi", "Bhaskara Hanuma Vedula", "Hemank Lamba", "Edward Raff", "Ponnurangam Kumaraguru", "Francis Ferraro", "Manas Gaur"], "title": "Do LLMs Adhere to Label Definitions? Examining Their Receptivity to External Label Definitions", "comment": "To appear in EMNLP 2025, Main Conference", "summary": "Do LLMs genuinely incorporate external definitions, or do they primarily rely\non their parametric knowledge? To address these questions, we conduct\ncontrolled experiments across multiple explanation benchmark datasets (general\nand domain-specific) and label definition conditions, including expert-curated,\nLLM-generated, perturbed, and swapped definitions. Our results reveal that\nwhile explicit label definitions can enhance accuracy and explainability, their\nintegration into an LLM's task-solving processes is neither guaranteed nor\nconsistent, suggesting reliance on internalized representations in many cases.\nModels often default to their internal representations, particularly in general\ntasks, whereas domain-specific tasks benefit more from explicit definitions.\nThese findings underscore the need for a deeper understanding of how LLMs\nprocess external knowledge alongside their pre-existing capabilities.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u5728\u5904\u7406\u5916\u90e8\u6807\u7b7e\u5b9a\u4e49\u65f6\u5e76\u4e0d\u603b\u662f\u6574\u5408\u8fd9\u4e9b\u5b9a\u4e49\uff0c\u800c\u662f\u7ecf\u5e38\u4f9d\u8d56\u5185\u90e8\u53c2\u6570\u5316\u77e5\u8bc6\uff0c\u7279\u522b\u662f\u5728\u901a\u7528\u4efb\u52a1\u4e2d\uff0c\u800c\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u66f4\u80fd\u4ece\u663e\u5f0f\u5b9a\u4e49\u4e2d\u53d7\u76ca", "motivation": "\u63a2\u7a76LLM\u662f\u5426\u771f\u6b63\u6574\u5408\u5916\u90e8\u5b9a\u4e49\uff0c\u8fd8\u662f\u4e3b\u8981\u4f9d\u8d56\u5176\u53c2\u6570\u5316\u77e5\u8bc6\u6765\u5904\u7406\u4efb\u52a1", "method": "\u5728\u591a\u4e2a\u89e3\u91ca\u57fa\u51c6\u6570\u636e\u96c6\uff08\u901a\u7528\u548c\u9886\u57df\u7279\u5b9a\uff09\u4e0a\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\uff0c\u4f7f\u7528\u4e13\u5bb6\u7b56\u5212\u3001LLM\u751f\u6210\u3001\u6270\u52a8\u548c\u4ea4\u6362\u7684\u5b9a\u4e49\u6761\u4ef6", "result": "\u663e\u5f0f\u6807\u7b7e\u5b9a\u4e49\u53ef\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4f46LLM\u6574\u5408\u8fd9\u4e9b\u5b9a\u4e49\u5230\u4efb\u52a1\u89e3\u51b3\u8fc7\u7a0b\u4e2d\u65e2\u4e0d\u4fdd\u8bc1\u4e5f\u4e0d\u4e00\u81f4\uff0c\u6a21\u578b\u7ecf\u5e38\u9ed8\u8ba4\u4f7f\u7528\u5185\u90e8\u8868\u793a", "conclusion": "\u9700\u8981\u66f4\u6df1\u5165\u7406\u89e3LLM\u5982\u4f55\u5904\u7406\u5916\u90e8\u77e5\u8bc6\u4e0e\u5176\u73b0\u6709\u80fd\u529b\u7684\u5173\u7cfb\uff0c\u6a21\u578b\u5bf9\u5185\u90e8\u8868\u793a\u7684\u4f9d\u8d56\u8868\u660e\u9700\u8981\u66f4\u597d\u7684\u5916\u90e8\u77e5\u8bc6\u6574\u5408\u673a\u5236"}}
{"id": "2509.02464", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.02464", "abs": "https://arxiv.org/abs/2509.02464", "authors": ["Ahmed Ahmed", "Kevin Klyman", "Yi Zeng", "Sanmi Koyejo", "Percy Liang"], "title": "SpecEval: Evaluating Model Adherence to Behavior Specifications", "comment": null, "summary": "Companies that develop foundation models publish behavioral guidelines they\npledge their models will follow, but it remains unclear if models actually do\nso. While providers such as OpenAI, Anthropic, and Google have published\ndetailed specifications describing both desired safety constraints and\nqualitative traits for their models, there has been no systematic audit of\nadherence to these guidelines. We introduce an automated framework that audits\nmodels against their providers specifications by parsing behavioral statements,\ngenerating targeted prompts, and using models to judge adherence. Our central\nfocus is on three way consistency between a provider specification, its model\noutputs, and its own models as judges; an extension of prior two way generator\nvalidator consistency. This establishes a necessary baseline: at minimum, a\nfoundation model should consistently satisfy the developer behavioral\nspecifications when judged by the developer evaluator models. We apply our\nframework to 16 models from six developers across more than 100 behavioral\nstatements, finding systematic inconsistencies including compliance gaps of up\nto 20 percent across providers.", "AI": {"tldr": "\u81ea\u52a8\u5316\u6846\u67b6\u5ba1\u8ba1\u57fa\u7840\u6a21\u578b\u662f\u5426\u9075\u5b88\u53d1\u5e03\u5546\u7684\u884c\u4e3a\u6307\u5357\uff0c\u53d1\u73b0\u7cfb\u7edf\u6027\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898", "motivation": "\u57fa\u7840\u6a21\u578b\u53d1\u5e03\u5546\u5ba3\u79f0\u6a21\u578b\u9075\u5b88\u884c\u4e3a\u6307\u5357\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u5ba1\u8ba1\u65b9\u6cd5\u6765\u9a8c\u8bc1\u5b9e\u9645\u9075\u5b88\u60c5\u51b5", "method": "\u5f00\u53d1\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u6790\u884c\u4e3a\u58f0\u660e\u3001\u751f\u6210\u9488\u5bf9\u6027\u63d0\u793a\u3001\u4f7f\u7528\u6a21\u578b\u4f5c\u4e3a\u5224\u65ad\u8005\u6765\u68c0\u9a8c\u4e09\u65b9\u4e00\u81f4\u6027\uff08\u89c4\u8303-\u8f93\u51fa-\u8bc4\u4f30\u6a21\u578b\uff09", "result": "\u572816\u4e2a\u6a21\u578b\u548c100\u591a\u4e2a\u884c\u4e3a\u58f0\u660e\u4e2d\u53d1\u73b0\u7cfb\u7edf\u6027\u4e0d\u4e00\u81f4\u6027\uff0c\u6700\u9ad8\u8fbe20%\u7684\u9075\u5faa\u95ee\u9898", "conclusion": "\u57fa\u7840\u6a21\u578b\u5e76\u975e\u59cb\u7ec8\u9075\u5b88\u53d1\u5e03\u5546\u6307\u5357\uff0c\u9700\u8981\u5efa\u7acb\u5fc5\u8981\u7684\u57fa\u51c6\u68c0\u9a8c\u673a\u5236\u786e\u4fdd\u4e09\u65b9\u4e00\u81f4\u6027"}}
{"id": "2509.02492", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02492", "abs": "https://arxiv.org/abs/2509.02492", "authors": ["Chenglong Wang", "Yongyu Mu", "Hang Zhou", "Yifu Huo", "Ziming Zhu", "Jiali Zeng", "Murun Yang", "Bei Li", "Tong Xiao", "Xiaoyang Hao", "Chunliang Zhang", "Fandong Meng", "Jingbo Zhu"], "title": "GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning", "comment": null, "summary": "Significant progress in reward modeling over recent years has been driven by\na paradigm shift from task-specific designs towards generalist reward models.\nDespite this trend, developing effective reward models remains a fundamental\nchallenge: the heavy reliance on large-scale labeled preference data.\nPre-training on abundant unlabeled data offers a promising direction, but\nexisting approaches fall short of instilling explicit reasoning into reward\nmodels. To bridge this gap, we propose a self-training approach that leverages\nunlabeled data to elicit reward reasoning in reward models. Based on this\napproach, we develop GRAM-R$^2$, a generative reward model trained to produce\nnot only preference labels but also accompanying reward rationales. GRAM-R$^2$\ncan serve as a foundation model for reward reasoning and can be applied to a\nwide range of tasks with minimal or no additional fine-tuning. It can support\ndownstream applications such as response ranking and task-specific reward\ntuning. Experiments on response ranking, task adaptation, and reinforcement\nlearning from human feedback demonstrate that GRAM-R$^2$ consistently delivers\nstrong performance, outperforming several strong discriminative and generative\nbaselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86GRAM-R\u00b2\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u8bad\u7ec3\u65b9\u6cd5\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u4e0d\u4ec5\u80fd\u8f93\u51fa\u504f\u597d\u6807\u7b7e\u8fd8\u80fd\u751f\u6210\u5956\u52b1\u7406\u7531\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u4e25\u91cd\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u504f\u597d\u6570\u636e\uff0c\u800c\u9884\u8bad\u7ec3\u65b9\u6cd5\u7f3a\u4e4f\u660e\u786e\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8fdb\u884c\u663e\u5f0f\u63a8\u7406\u7684\u5956\u52b1\u6a21\u578b", "method": "\u91c7\u7528\u81ea\u8bad\u7ec3\u65b9\u6cd5\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u6fc0\u53d1\u5956\u52b1\u63a8\u7406\uff0c\u5f00\u53d1GRAM-R\u00b2\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b\uff0c\u540c\u65f6\u8f93\u51fa\u504f\u597d\u6807\u7b7e\u548c\u5956\u52b1\u7406\u7531", "result": "\u5728\u54cd\u5e94\u6392\u5e8f\u3001\u4efb\u52a1\u9002\u5e94\u548c\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u591a\u4e2a\u5224\u522b\u5f0f\u548c\u751f\u6210\u5f0f\u57fa\u7ebf\u6a21\u578b", "conclusion": "GRAM-R\u00b2\u53ef\u4f5c\u4e3a\u5956\u52b1\u63a8\u7406\u7684\u57fa\u7840\u6a21\u578b\uff0c\u652f\u6301\u4e0b\u6e38\u5e94\u7528\u5982\u54cd\u5e94\u6392\u5e8f\u548c\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u8c03\u4f18\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027"}}
{"id": "2509.02499", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02499", "abs": "https://arxiv.org/abs/2509.02499", "authors": ["Junxi Wu", "Jinpeng Wang", "Zheng Liu", "Bin Chen", "Dongjian Hu", "Hao Wu", "Shu-Tao Xiu"], "title": "MoSEs: Uncertainty-Aware AI-Generated Text Detection via Mixture of Stylistics Experts with Conditional Thresholds", "comment": "EMNLP 2025", "summary": "The rapid advancement of large language models has intensified public\nconcerns about the potential misuse. Therefore, it is important to build\ntrustworthy AI-generated text detection systems. Existing methods neglect\nstylistic modeling and mostly rely on static thresholds, which greatly limits\nthe detection performance. In this paper, we propose the Mixture of Stylistic\nExperts (MoSEs) framework that enables stylistics-aware uncertainty\nquantification through conditional threshold estimation. MoSEs contain three\ncore components, namely, the Stylistics Reference Repository (SRR), the\nStylistics-Aware Router (SAR), and the Conditional Threshold Estimator (CTE).\nFor input text, SRR can activate the appropriate reference data in SRR and\nprovide them to CTE. Subsequently, CTE jointly models the linguistic\nstatistical properties and semantic features to dynamically determine the\noptimal threshold. With a discrimination score, MoSEs yields prediction labels\nwith the corresponding confidence level. Our framework achieves an average\nimprovement 11.34% in detection performance compared to baselines. More\ninspiringly, MoSEs shows a more evident improvement 39.15% in the low-resource\ncase. Our code is available at https://github.com/creator-xi/MoSEs.", "AI": {"tldr": "\u63d0\u51fa\u4e86MoSEs\u6846\u67b6\uff0c\u901a\u8fc7\u6761\u4ef6\u9608\u503c\u4f30\u8ba1\u5b9e\u73b0\u98ce\u683c\u611f\u77e5\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u663e\u8457\u63d0\u5347AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u6027\u80fd", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u5f15\u53d1\u4e86\u5bf9\u6ee5\u7528\u7684\u62c5\u5fe7\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u98ce\u683c\u5efa\u6a21\u4e14\u4f9d\u8d56\u9759\u6001\u9608\u503c\uff0c\u9650\u5236\u4e86\u68c0\u6d4b\u6027\u80fd", "method": "MoSEs\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u98ce\u683c\u53c2\u8003\u5e93(SRR)\u3001\u98ce\u683c\u611f\u77e5\u8def\u7531\u5668(SAR)\u548c\u6761\u4ef6\u9608\u503c\u4f30\u8ba1\u5668(CTE)\uff0c\u901a\u8fc7\u52a8\u6001\u786e\u5b9a\u6700\u4f18\u9608\u503c\u8fdb\u884c\u68c0\u6d4b", "result": "\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5e73\u5747\u63d0\u534711.34%\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u5728\u4f4e\u8d44\u6e90\u60c5\u51b5\u4e0b\u63d0\u5347\u66f4\u663e\u8457\u8fbe39.15%", "conclusion": "MoSEs\u6846\u67b6\u901a\u8fc7\u98ce\u683c\u611f\u77e5\u7684\u6761\u4ef6\u9608\u503c\u4f30\u8ba1\u6709\u6548\u63d0\u5347\u4e86AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027"}}
{"id": "2509.02503", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02503", "abs": "https://arxiv.org/abs/2509.02503", "authors": ["Nishant Tanksale", "Tanmay Kokate", "Darshan Gohad", "Sarvadnyaa Barate", "Raviraj Joshi"], "title": "L3Cube-IndicHeadline-ID: A Dataset for Headline Identification and Semantic Evaluation in Low-Resource Indian Languages", "comment": null, "summary": "Semantic evaluation in low-resource languages remains a major challenge in\nNLP. While sentence transformers have shown strong performance in high-resource\nsettings, their effectiveness in Indic languages is underexplored due to a lack\nof high-quality benchmarks. To bridge this gap, we introduce\nL3Cube-IndicHeadline-ID, a curated headline identification dataset spanning ten\nlow-resource Indic languages: Marathi, Hindi, Tamil, Gujarati, Odia, Kannada,\nMalayalam, Punjabi, Telugu, Bengali and English. Each language includes 20,000\nnews articles paired with four headline variants: the original, a semantically\nsimilar version, a lexically similar version, and an unrelated one, designed to\ntest fine-grained semantic understanding. The task requires selecting the\ncorrect headline from the options using article-headline similarity. We\nbenchmark several sentence transformers, including multilingual and\nlanguage-specific models, using cosine similarity. Results show that\nmultilingual models consistently perform well, while language-specific models\nvary in effectiveness. Given the rising use of similarity models in\nRetrieval-Augmented Generation (RAG) pipelines, this dataset also serves as a\nvaluable resource for evaluating and improving semantic understanding in such\napplications. Additionally, the dataset can be repurposed for multiple-choice\nquestion answering, headline classification, or other task-specific evaluations\nof LLMs, making it a versatile benchmark for Indic NLP. The dataset is shared\npublicly at https://github.com/l3cube-pune/indic-nlp", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4e3a\u5370\u5ea6\u8bed\u8a00\u5f00\u53d1\u4e86L3Cube-IndicHeadline-ID\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u53e5\u5b50\u7f16\u7801\u5668\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u5305\u542b10\u79cd\u5370\u5ea6\u8bed\u8a00\u768420,000\u7bc7\u65b0\u95fb\u6587\u7ae0\u548c\u56db\u79cd\u6807\u9898\u53d8\u4f53\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u8bed\u4e49\u8bc4\u4f30\u7684\u6311\u6218\uff0c\u586b\u8865\u5370\u5ea6\u8bed\u8a00\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u57fa\u51c6\u6d4b\u8bd5\u96c6\u7684\u7a7a\u767d\uff0c\u4ee5\u652f\u6301\u53e5\u5b50\u7f16\u7801\u5668\u5728\u8fd9\u4e9b\u8bed\u8a00\u4e2d\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b10\u79cd\u5370\u5ea6\u8bed\u8a00\u548c\u82f1\u8bed\u7684\u65b0\u95fb\u6807\u9898\u8bc6\u522b\u6570\u636e\u96c6\uff0c\u6bcf\u7bc7\u6587\u7ae0\u914d\u6709\u56db\u79cd\u6807\u9898\u53d8\u4f53\uff1a\u539f\u59cb\u6807\u9898\u3001\u8bed\u4e49\u76f8\u4f3c\u7248\u672c\u3001\u8bcd\u6c47\u76f8\u4f3c\u7248\u672c\u548c\u65e0\u5173\u6807\u9898\u3002\u4f7f\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5bf9\u591a\u8bed\u8a00\u548c\u8bed\u8a00\u7279\u5b9a\u53e5\u5b50\u7f16\u7801\u5668\u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5\u3002", "result": "\u591a\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u4e00\u81f4\u826f\u597d\uff0c\u800c\u8bed\u8a00\u7279\u5b9a\u6a21\u578b\u7684\u6548\u679c\u5b58\u5728\u5dee\u5f02\u3002\u8be5\u6570\u636e\u96c6\u4e0d\u4ec5\u53ef\u7528\u4e8e\u8bed\u4e49\u7406\u89e3\u8bc4\u4f30\uff0c\u8fd8\u53ef\u91cd\u65b0\u7528\u4e8e\u591a\u9009\u9898\u7b54\u9898\u3001\u6807\u9898\u5206\u7c7b\u7b49\u5176\u4ed6NLP\u4efb\u52a1\u3002", "conclusion": "L3Cube-IndicHeadline-ID\u6570\u636e\u96c6\u4e3a\u5370\u5ea6\u8bed\u8a00\u7684\u8bed\u4e49\u7406\u89e3\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u57fa\u51c6\u8d44\u6e90\uff0c\u5c24\u5176\u5728RAG\u7cfb\u7edf\u4e2d\u5177\u6709\u5e94\u7528\u4ef7\u503c\uff0c\u5e76\u5c55\u793a\u4e86\u591a\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2509.02506", "categories": ["cs.CL", "I.2"], "pdf": "https://arxiv.org/pdf/2509.02506", "abs": "https://arxiv.org/abs/2509.02506", "authors": ["Jean-Marie Le Ray"], "title": "The Forgotten Code: Validating a Century-Old Translation System with AI", "comment": "Preprint, 35 pages, 14 figures, 9 appendices", "summary": "A pioneering rule-based mechanical translation system (precursor of modern\nRBMTs) was first presented in December 1929 by its inventor, Federico Pucci,\nwho later published the full method in a book titled \"Il traduttore meccanico\ned il metodo per corrispondersi fra Europei conoscendo ciascuno solo la propria\nlingua: Parte I\", in Salerno (Italy), in 1931. This study illustrates how AI\nbreathes new life into the system of international keys and ideograms devised\nby Pucci to translate from/into any Romance language (at least as a first\nstep). The methodology involves having the AIs retranslate, following Pucci's\nmethod, the two text excerpts originally translated in 1931 and clearly\ndocumented in his publication: a passage from Dante's La Vita Nuova, translated\nfrom Italian into French, and a passage from Voltaire's Zadig, translated from\nFrench into Italian. The result is notable: the two texts, translated 94 years\napart using the same method--by Pucci in 1931 and by AIs in 2025--show a low\naverage difference, with only minor variations observed. With Pucci's system\nthus validated, it became feasible to have the AIs reproduce the excerpts in\nEnglish, Spanish, and German according to his method. The results were\nconsistent, and Pucci--via Artificial Intelligence--was tasked with translating\nmore modern and technical texts, thereby reviving, nearly a century later, an\ninvention that had remained almost entirely unknown and never applied beyond\nits creator, now brought to wider attention and opened to possible\nexperimentation. Such a demonstration would not only affirm Pucci's historical\nstatus but also place him among the precursors and intellectual contributors to\nmachine translation, whose work merits examination alongside figures such as\nTroyanskij, Booth, and Weaver, with possible consequences for how the history\nof the field is understood.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7AI\u91cd\u73b0\u4e861929\u5e74Federico Pucci\u53d1\u660e\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u572894\u5e74\u540e\u4ecd\u80fd\u4ea7\u751f\u76f8\u4f3c\u7ed3\u679c\uff0c\u5e76\u5c06\u8fd9\u4e00\u88ab\u9057\u5fd8\u7684\u65e9\u671f\u673a\u5668\u7ffb\u8bd1\u5148\u9a71\u91cd\u65b0\u7eb3\u5165\u5386\u53f2\u89c6\u91ce\u3002", "motivation": "\u91cd\u65b0\u53d1\u6398\u548c\u9a8c\u8bc1Federico Pucci\u57281929\u5e74\u63d0\u51fa\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u8fd9\u4e00\u65e9\u671f\u53d1\u660e\u51e0\u4e4e\u88ab\u5b8c\u5168\u9057\u5fd8\uff0c\u4ece\u672a\u5728\u521b\u9020\u8005\u4e4b\u5916\u5f97\u5230\u5e94\u7528\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u73b0\u4ee3AI\u6280\u672f\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u786e\u7acbPucci\u5728\u673a\u5668\u7ffb\u8bd1\u5386\u53f2\u4e0a\u7684\u5730\u4f4d\u3002", "method": "\u4f7f\u7528AI\u6309\u7167Pucci\u7684\u65b9\u6cd5\u91cd\u65b0\u7ffb\u8bd11931\u5e74\u6587\u732e\u4e2d\u8bb0\u5f55\u7684\u4e24\u4e2a\u6587\u672c\u7247\u6bb5\uff1a\u4f46\u4e01\u300a\u65b0\u751f\u300b\u7684\u610f\u5927\u5229\u8bed\u5230\u6cd5\u8bed\u7ffb\u8bd1\uff0c\u4ee5\u53ca\u4f0f\u5c14\u6cf0\u300a\u67e5\u7b2c\u683c\u300b\u7684\u6cd5\u8bed\u5230\u610f\u5927\u5229\u8bed\u7ffb\u8bd1\u3002\u968f\u540e\u5c06\u65b9\u6cd5\u6269\u5c55\u5230\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u5fb7\u8bed\uff0c\u5e76\u5c1d\u8bd5\u7ffb\u8bd1\u66f4\u73b0\u4ee3\u7684\u79d1\u6280\u6587\u672c\u3002", "result": "AI\u6309\u7167Pucci\u65b9\u6cd5\u7ffb\u8bd1\u7684\u7ed3\u679c\u4e0e94\u5e74\u524d\u7684\u539f\u59cb\u7ffb\u8bd1\u76f8\u6bd4\u5dee\u5f02\u5f88\u5c0f\uff0c\u53ea\u6709\u5fae\u5c0f\u53d8\u5316\u3002\u65b9\u6cd5\u5728\u591a\u79cd\u8bed\u8a00\u95f4\u8f6c\u6362\u90fd\u8868\u73b0\u4e00\u81f4\uff0c\u8bc1\u660e\u8be5\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "Pucci\u7684\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u7ecf\u8fc7\u8fd1\u4e00\u4e2a\u4e16\u7eaa\u540e\u4ecd\u80fd\u6709\u6548\u5de5\u4f5c\uff0c\u5e94\u88ab\u786e\u8ba4\u4e3a\u673a\u5668\u7ffb\u8bd1\u9886\u57df\u7684\u65e9\u671f\u5148\u9a71\uff0c\u5176\u8d21\u732e\u5e94\u4e0eTroyanskij\u3001Booth\u548cWeaver\u7b49\u77e5\u540d\u5148\u9a71\u5e76\u5217\uff0c\u8fd9\u53ef\u80fd\u6539\u53d8\u5bf9\u673a\u5668\u7ffb\u8bd1\u5386\u53f2\u7684\u7406\u89e3\u3002"}}
{"id": "2509.02510", "categories": ["cs.CL", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.02510", "abs": "https://arxiv.org/abs/2509.02510", "authors": ["Erfan Baghaei Potraghloo", "Seyedarmin Azizi", "Souvik Kundu", "Massoud Pedram"], "title": "Top-H Decoding: Adapting the Creativity and Coherence with Bounded Entropy in Text Generation", "comment": null, "summary": "Large language models (LLMs), despite their impressive performance across a\nwide range of tasks, often struggle to balance two competing objectives in\nopen-ended text generation: fostering diversity and creativity while preserving\nlogical coherence. Existing truncated sampling techniques, including\ntemperature scaling, top-\\$p\\$ (nucleus) sampling, and min-\\$p\\$ sampling, aim\nto manage this trade-off. However, they exhibit limitations, particularly in\nthe effective incorporation of the confidence of the model into the\ncorresponding sampling strategy. For example, min-\\$p\\$ sampling relies on a\nsingle top token as a heuristic for confidence, eventually underutilizing the\ninformation of the probability distribution. Toward effective incorporation of\nthe confidence of the model, in this paper, we present **top-H** decoding. We\nfirst establish the theoretical foundation of the interplay between creativity\nand coherence in truncated sampling by formulating an **entropy-constrained\nminimum divergence** problem. We then prove this minimization problem to be\nequivalent to an **entropy-constrained mass maximization** (ECMM) problem,\nwhich is NP-hard. Finally, we present top-H decoding, a computationally\nefficient greedy algorithm to solve the ECMM problem. Extensive empirical\nevaluations demonstrate that top-H outperforms the state-of-the-art (SoTA)\nalternative of min-\\$p\\$ sampling by up to **25.63%** on creative writing\nbenchmarks, while maintaining robustness on question-answering datasets such as\nGPQA, GSM8K, and MT-Bench. Additionally, an *LLM-as-judge* evaluation confirms\nthat top-H indeed produces coherent outputs even at higher temperatures, where\ncreativity is especially critical. In summary, top-H advances SoTA in\nopen-ended text generation and can be *easily integrated* into creative writing\napplications. The code is available at\nhttps://github.com/ErfanBaghaei/Top-H-Decoding.", "AI": {"tldr": "\u63d0\u51fa\u4e86top-H\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u71b5\u7ea6\u675f\u8d28\u91cf\u6700\u5927\u5316\u6765\u89e3\u51b3LLM\u5728\u5f00\u653e\u6587\u672c\u751f\u6210\u4e2d\u591a\u6837\u6027\u4e0e\u903b\u8f91\u8fde\u8d2f\u6027\u7684\u5e73\u8861\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u663e\u8457", "motivation": "\u73b0\u6709\u622a\u65ad\u91c7\u6837\u6280\u672f\uff08\u5982\u6e29\u5ea6\u7f29\u653e\u3001top-p\u91c7\u6837\u3001min-p\u91c7\u6837\uff09\u5728\u6709\u6548\u878d\u5165\u6a21\u578b\u7f6e\u4fe1\u5ea6\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662fmin-p\u91c7\u6837\u4ec5\u4f9d\u8d56\u5355\u4e2a\u9876\u90e8token\u4f5c\u4e3a\u7f6e\u4fe1\u5ea6\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u6982\u7387\u5206\u5e03\u4fe1\u606f", "method": "\u9996\u5148\u5efa\u7acb\u71b5\u7ea6\u675f\u6700\u5c0f\u6563\u5ea6\u95ee\u9898\u7684\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u5176\u7b49\u4ef7\u4e8eNP\u96be\u7684\u71b5\u7ea6\u675f\u8d28\u91cf\u6700\u5927\u5316\u95ee\u9898\uff0c\u7136\u540e\u63d0\u51fatop-H\u89e3\u7801\u8fd9\u4e00\u8ba1\u7b97\u9ad8\u6548\u8d2a\u5fc3\u7b97\u6cd5\u6765\u89e3\u51b3\u8be5\u95ee\u9898", "result": "\u5728\u521b\u610f\u5199\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2dtop-H\u6bd4\u6700\u5148\u8fdb\u7684min-p\u91c7\u6837\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe25.63%\uff0c\u5728GPQA\u3001GSM8K\u548cMT-Bench\u7b49\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u9c81\u68d2\u6027\uff0cLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u8bc4\u4f30\u786e\u8ba4\u5373\u4f7f\u5728\u66f4\u9ad8\u6e29\u5ea6\u4e0b\u4e5f\u80fd\u4ea7\u751f\u8fde\u8d2f\u8f93\u51fa", "conclusion": "top-H\u63a8\u8fdb\u4e86\u5f00\u653e\u6587\u672c\u751f\u6210\u7684\u6700\u65b0\u6280\u672f\u6c34\u5e73\uff0c\u53ef\u8f7b\u677e\u96c6\u6210\u5230\u521b\u610f\u5199\u4f5c\u5e94\u7528\u4e2d\uff0c\u6709\u6548\u5e73\u8861\u4e86\u521b\u9020\u6027\u4e0e\u8fde\u8d2f\u6027"}}
{"id": "2509.02514", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02514", "abs": "https://arxiv.org/abs/2509.02514", "authors": ["Mayur Shirke", "Amey Shembade", "Pavan Thorat", "Madhushri Wagh", "Raviraj Joshi"], "title": "Comparative Study of Pre-Trained BERT and Large Language Models for Code-Mixed Named Entity Recognition", "comment": null, "summary": "Named Entity Recognition (NER) in code-mixed text, particularly Hindi-English\n(Hinglish), presents unique challenges due to informal structure,\ntransliteration, and frequent language switching. This study conducts a\ncomparative evaluation of code-mixed fine-tuned models and non-code-mixed\nmultilingual models, along with zero-shot generative large language models\n(LLMs). Specifically, we evaluate HingBERT, HingMBERT, and HingRoBERTa (trained\non code-mixed data), and BERT Base Cased, IndicBERT, RoBERTa and MuRIL (trained\non non-code-mixed multilingual data). We also assess the performance of Google\nGemini in a zero-shot setting using a modified version of the dataset with NER\ntags removed. All models are tested on a benchmark Hinglish NER dataset using\nPrecision, Recall, and F1-score. Results show that code-mixed models,\nparticularly HingRoBERTa and HingBERT-based fine-tuned models, outperform\nothers - including closed-source LLMs like Google Gemini - due to\ndomain-specific pretraining. Non-code-mixed models perform reasonably but show\nlimited adaptability. Notably, Google Gemini exhibits competitive zero-shot\nperformance, underlining the generalization strength of modern LLMs. This study\nprovides key insights into the effectiveness of specialized versus generalized\nmodels for code-mixed NER tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u9488\u5bf9\u5370\u5730\u8bed-\u82f1\u8bed\u6df7\u5408\u6587\u672c\u7684\u4e13\u95e8\u5fae\u8c03\u6a21\u578b\u3001\u591a\u8bed\u8a00\u6a21\u578b\u548c\u96f6\u6837\u672c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e13\u95e8\u9488\u5bf9\u6df7\u5408\u6587\u672c\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\u6700\u4f73", "motivation": "\u5370\u5730\u8bed-\u82f1\u8bed\u6df7\u5408\u6587\u672c\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u9762\u4e34\u975e\u6b63\u5f0f\u7ed3\u6784\u3001\u97f3\u8bd1\u548c\u9891\u7e41\u8bed\u8a00\u8f6c\u6362\u7b49\u72ec\u7279\u6311\u6218\uff0c\u9700\u8981\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd", "method": "\u8bc4\u4f30\u4e86HingBERT\u3001HingMBERT\u3001HingRoBERTa\u7b49\u4e13\u95e8\u5fae\u8c03\u6a21\u578b\uff0cBERT Base Cased\u3001IndicBERT\u7b49\u591a\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u53caGoogle Gemini\u96f6\u6837\u672c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u7528\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5", "result": "\u4e13\u95e8\u9488\u5bf9\u6df7\u5408\u6587\u672c\u8bad\u7ec3\u7684\u6a21\u578b\uff08\u7279\u522b\u662fHingRoBERTa\u548cHingBERT\uff09\u6027\u80fd\u6700\u4f18\uff0c\u8d85\u8d8a\u4e86\u5305\u62ecGoogle Gemini\u5728\u5185\u7684\u95ed\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u3002\u975e\u6df7\u5408\u6587\u672c\u6a21\u578b\u8868\u73b0\u5c1a\u53ef\u4f46\u9002\u5e94\u6027\u6709\u9650\uff0cGemini\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5c55\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd", "conclusion": "\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u9884\u8bad\u7ec3\u7684\u4e13\u95e8\u6a21\u578b\u5728\u4ee3\u7801\u6df7\u5408\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u4e5f\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u4e13\u95e8\u5316\u4e0e\u901a\u7528\u5316\u6a21\u578b\u7684\u9009\u62e9\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3"}}
{"id": "2509.02522", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02522", "abs": "https://arxiv.org/abs/2509.02522", "authors": ["Jiaming Li", "Longze Chen", "Ze Gong", "Yukun Chen", "Lu Wang", "Wanwei He", "Run Luo", "Min Yang"], "title": "Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR", "comment": null, "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have\nempowered large language models (LLMs) to tackle challenging reasoning tasks\nsuch as mathematics and programming. RLVR leverages verifiable outcome rewards\nto guide policy optimization, enabling LLMs to progressively improve output\nquality in a grounded and reliable manner. Despite its promise, the RLVR\nparadigm poses significant challenges, as existing methods often suffer from\nsparse reward signals and unstable policy gradient updates, particularly in\nRL-based approaches. To address the challenges, we propose $\\textbf{PACS}$, a\nnovel RLVR framework that achieves im$\\textbf{P}$licit $\\textbf{A}$ctor\n$\\textbf{C}$ritic coupling via a $\\textbf{S}$upervised learning framework. By\ntreating the outcome reward as a predictable label, we reformulate the RLVR\nproblem into a supervised learning task over a score function parameterized by\nthe policy model and optimized using cross-entropy loss. A detailed gradient\nanalysis shows that this supervised formulation inherently recovers the\nclassical policy gradient update while implicitly coupling actor and critic\nroles, yielding more stable and efficient training. Benchmarking on challenging\nmathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as\nPPO and GRPO, achieving superior reasoning performance. For instance, PACS\nachieves 59.78\\% at pass@256 on AIME 2025, representing improvements of 13.32\nand 14.36 points over PPO and GRPO. This simple yet powerful framework offers a\npromising avenue for LLMs post-training with verifiable rewards. Our code and\ndata are available as open source at https://github.com/ritzz-ai/PACS.", "AI": {"tldr": "PACS\u662f\u4e00\u4e2a\u65b0\u7684RLVR\u6846\u67b6\uff0c\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u5b9e\u73b0\u9690\u5f0fActor-Critic\u8026\u5408\uff0c\u5c06\u53ef\u9a8c\u8bc1\u5956\u52b1\u95ee\u9898\u8f6c\u5316\u4e3a\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8ePPO\u548cGRPO\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684RLVR\u65b9\u6cd5\u5b58\u5728\u5956\u52b1\u4fe1\u53f7\u7a00\u758f\u548c\u7b56\u7565\u68af\u5ea6\u66f4\u65b0\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u57fa\u4e8eRL\u7684\u65b9\u6cd5\u4e2d\uff0c\u9700\u8981\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u8bad\u7ec3\u6846\u67b6\u3002", "method": "\u5c06\u7ed3\u679c\u5956\u52b1\u89c6\u4e3a\u53ef\u9884\u6d4b\u6807\u7b7e\uff0c\u5c06RLVR\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u5bf9\u7b56\u7565\u6a21\u578b\u53c2\u6570\u5316\u8bc4\u5206\u51fd\u6570\u7684\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\uff0c\u4f7f\u7528\u4ea4\u53c9\u71b5\u635f\u5931\u8fdb\u884c\u4f18\u5316\uff0c\u5b9e\u73b0\u9690\u5f0f\u7684Actor-Critic\u8026\u5408\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPACS\u663e\u8457\u4f18\u4e8ePPO\u548cGRPO\u7b49\u5f3a\u57fa\u7ebf\uff0c\u5728AIME 2025\u4e0a\u8fbe\u523059.78%\u7684pass@256\uff0c\u6bd4PPO\u548cGRPO\u5206\u522b\u63d0\u534713.32\u548c14.36\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "PACS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u4e3a\u5177\u6709\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684LLM\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u8bad\u7ec3\u3002"}}
{"id": "2509.02534", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02534", "abs": "https://arxiv.org/abs/2509.02534", "authors": ["Tianjian Li", "Yiming Zhang", "Ping Yu", "Swarnadeep Saha", "Daniel Khashabi", "Jason Weston", "Jack Lanchantin", "Tianlu Wang"], "title": "Jointly Reinforcing Diversity and Quality in Language Model Generations", "comment": "29 pages, 11 figures", "summary": "Post-training of Large Language Models (LMs) often prioritizes accuracy and\nhelpfulness at the expense of diversity. This creates a tension: while\npost-training improves response quality, it also sharpens output distributions\nand reduces the range of ideas, limiting the usefulness of LMs in creative and\nexploratory tasks such as brainstorming, storytelling, or problem solving. We\naddress this challenge with Diversity-Aware Reinforcement Learning (DARLING), a\nframework that jointly optimizes for response quality and semantic diversity.\nAt its core, DARLING introduces a learned partition function to measure\ndiversity beyond surface-level lexical variations. This diversity signal is\nthen combined with a quality reward during online reinforcement learning,\nencouraging models to generate outputs that are both high-quality and distinct.\nExperiments across multiple model families and sizes show that DARLING\ngeneralizes to two regimes: non-verifiable tasks (instruction following and\ncreative writing) and verifiable tasks (competition math). On five benchmarks\nin the first setting, DARLING consistently outperforms quality-only RL\nbaselines, producing outputs that are simultaneously of higher quality and\nnovelty. In the second setting, DARLING achieves higher pass@1 (solution\nquality) and pass@k (solution variety). Most strikingly, explicitly optimizing\nfor diversity catalyzes exploration in online RL, which manifests itself as\nhigher-quality responses.", "AI": {"tldr": "DARLING\u6846\u67b6\u901a\u8fc7\u8054\u5408\u4f18\u5316\u54cd\u5e94\u8d28\u91cf\u548c\u8bed\u4e49\u591a\u6837\u6027\uff0c\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u4e2d\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5728\u521b\u610f\u4efb\u52a1\u548c\u53ef\u9a8c\u8bc1\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u5f80\u5f80\u8fc7\u5206\u5173\u6ce8\u51c6\u786e\u6027\u548c\u6709\u7528\u6027\uff0c\u5bfc\u81f4\u8f93\u51fa\u5206\u5e03\u8fc7\u4e8e\u96c6\u4e2d\uff0c\u9650\u5236\u4e86\u5728\u521b\u610f\u63a2\u7d22\u4efb\u52a1\uff08\u5982\u5934\u8111\u98ce\u66b4\u3001\u6545\u4e8b\u521b\u4f5c\u3001\u95ee\u9898\u89e3\u51b3\uff09\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u9700\u8981\u5e73\u8861\u8d28\u91cf\u4e0e\u591a\u6837\u6027\u3002", "method": "\u63d0\u51fa\u591a\u6837\u6027\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff08DARLING\uff09\u6846\u67b6\uff0c\u5f15\u5165\u5b66\u4e60\u7684\u5206\u533a\u51fd\u6570\u6765\u6d4b\u91cf\u8d85\u8d8a\u8868\u9762\u8bcd\u6c47\u53d8\u5316\u7684\u8bed\u4e49\u591a\u6837\u6027\uff0c\u5c06\u591a\u6837\u6027\u4fe1\u53f7\u4e0e\u8d28\u91cf\u5956\u52b1\u7ed3\u5408\u8fdb\u884c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u5bb6\u65cf\u548c\u89c4\u6a21\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDARLING\u5728\u975e\u53ef\u9a8c\u8bc1\u4efb\u52a1\uff08\u6307\u4ee4\u8ddf\u968f\u548c\u521b\u610f\u5199\u4f5c\uff09\u548c\u53ef\u9a8c\u8bc1\u4efb\u52a1\uff08\u7ade\u8d5b\u6570\u5b66\uff09\u4e0a\u90fd\u4f18\u4e8e\u4ec5\u4f18\u5316\u8d28\u91cf\u7684RL\u57fa\u7ebf\uff0c\u540c\u65f6\u4ea7\u751f\u66f4\u9ad8\u8d28\u91cf\u548c\u66f4\u65b0\u9896\u7684\u8f93\u51fa\u3002", "conclusion": "\u660e\u786e\u4f18\u5316\u591a\u6837\u6027\u53ef\u4ee5\u50ac\u5316\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\uff0c\u8868\u73b0\u4e3a\u66f4\u9ad8\u8d28\u91cf\u7684\u54cd\u5e94\uff0c\u8bc1\u660e\u4e86\u5728\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u540c\u65f6\u8003\u8651\u8d28\u91cf\u548c\u591a\u6837\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.02550", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.02550", "abs": "https://arxiv.org/abs/2509.02550", "authors": ["Fakhraddin Alwajih", "Abdellah El Mekki", "Hamdy Mubarak", "Majd Hawasly", "Abubakr Mohamed", "Muhammad Abdul-Mageed"], "title": "PalmX 2025: The First Shared Task on Benchmarking LLMs on Arabic and Islamic Culture", "comment": "https://palmx.dlnlp.ai/", "summary": "Large Language Models (LLMs) inherently reflect the vast data distributions\nthey encounter during their pre-training phase. As this data is predominantly\nsourced from the web, there is a high chance it will be skewed towards\nhigh-resourced languages and cultures, such as those of the West. Consequently,\nLLMs often exhibit a diminished understanding of certain communities, a gap\nthat is particularly evident in their knowledge of Arabic and Islamic cultures.\nThis issue becomes even more pronounced with increasingly under-represented\ntopics. To address this critical challenge, we introduce PalmX 2025, the first\nshared task designed to benchmark the cultural competence of LLMs in these\nspecific domains. The task is composed of two subtasks featuring\nmultiple-choice questions (MCQs) in Modern Standard Arabic (MSA): General\nArabic Culture and General Islamic Culture. These subtasks cover a wide range\nof topics, including traditions, food, history, religious practices, and\nlanguage expressions from across 22 Arab countries. The initiative drew\nconsiderable interest, with 26 teams registering for Subtask 1 and 19 for\nSubtask 2, culminating in nine and six valid submissions, respectively. Our\nfindings reveal that task-specific fine-tuning substantially boosts performance\nover baseline models. The top-performing systems achieved an accuracy of 72.15%\non cultural questions and 84.22% on Islamic knowledge. Parameter-efficient\nfine-tuning emerged as the predominant and most effective approach among\nparticipants, while the utility of data augmentation was found to be\ndomain-dependent.", "AI": {"tldr": "PalmX 2025\u662f\u9996\u4e2a\u9488\u5bf9LLMs\u5728\u963f\u62c9\u4f2f\u548c\u4f0a\u65af\u5170\u6587\u5316\u9886\u57df\u6587\u5316\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u4efb\u52a1\uff0c\u5305\u542b\u963f\u62c9\u4f2f\u6587\u5316\u548c\u4f0a\u65af\u5170\u6587\u5316\u4e24\u4e2a\u5b50\u4efb\u52a1\uff0c\u7ed3\u679c\u663e\u793a\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "LLMs\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u4e3b\u8981\u63a5\u89e6\u897f\u65b9\u9ad8\u8d44\u6e90\u8bed\u8a00\u548c\u6587\u5316\u6570\u636e\uff0c\u5bfc\u81f4\u5bf9\u963f\u62c9\u4f2f\u548c\u4f0a\u65af\u5170\u6587\u5316\u7684\u7406\u89e3\u5b58\u5728\u660e\u663e\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u4e3b\u9898\u4e0a\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b\u591a\u9879\u9009\u62e9\u9898\u7684\u4e24\u4e2a\u5b50\u4efb\u52a1\uff1a\u901a\u7528\u963f\u62c9\u4f2f\u6587\u5316\u548c\u901a\u7528\u4f0a\u65af\u5170\u6587\u5316\uff0c\u6db5\u76d622\u4e2a\u963f\u62c9\u4f2f\u56fd\u5bb6\u7684\u4f20\u7edf\u3001\u98df\u7269\u3001\u5386\u53f2\u3001\u5b97\u6559\u5b9e\u8df5\u548c\u8bed\u8a00\u8868\u8fbe\u7b49\u5e7f\u6cdb\u4e3b\u9898\u3002", "result": "\u4efb\u52a1\u5438\u5f15\u4e8626\u4e2a\u56e2\u961f\u6ce8\u518c\u5b50\u4efb\u52a11\u548c19\u4e2a\u56e2\u961f\u6ce8\u518c\u5b50\u4efb\u52a12\uff0c\u6700\u7ec8\u5206\u522b\u67099\u4e2a\u548c6\u4e2a\u6709\u6548\u63d0\u4ea4\u3002\u6700\u4f73\u7cfb\u7edf\u5728\u6587\u5316\u95ee\u9898\u4e0a\u8fbe\u523072.15%\u51c6\u786e\u7387\uff0c\u5728\u4f0a\u65af\u5170\u77e5\u8bc6\u4e0a\u8fbe\u523084.22%\u51c6\u786e\u7387\u3002", "conclusion": "\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u662f\u6700\u4e3b\u8981\u4e14\u6700\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u800c\u6570\u636e\u589e\u5f3a\u7684\u6548\u7528\u5177\u6709\u9886\u57df\u4f9d\u8d56\u6027\u3002"}}
