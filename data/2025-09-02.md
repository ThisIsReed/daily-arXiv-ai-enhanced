<div id=toc></div>

# Table of Contents

- [cs.SD](#cs.SD) [Total: 2]


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [1] [WaveLLDM: Design and Development of a Lightweight Latent Diffusion Model for Speech Enhancement and Restoration](https://arxiv.org/abs/2508.21153)
*Kevin Putra Santoso,Rizka Wakhidatus Sholikah,Raden Venantius Hari Ginardi*

Main category: cs.SD

TL;DR: WaveLLDM是一种轻量级潜在扩散模型，通过神经音频编解码器和潜在扩散的结合，在压缩潜在空间中处理音频，降低计算复杂度同时保持重建质量。


<details>
  <summary>Details</summary>
Motivation: 高质量音频在多种应用中至关重要，但噪声、压缩和传输伪影导致的音频退化仍是主要挑战。传统扩散模型需要大量计算资源且难以处理较长缺失片段。

Method: 提出WaveLLDM架构，将高效神经音频编解码器与潜在扩散相结合，在压缩潜在空间中进行音频修复和去噪，而非传统的时间或频谱域处理。

Result: 在Voicebank+DEMAND测试集上，WaveLLDM实现了准确的频谱重建（LSD分数0.48-0.60），但对未见数据的适应性良好。在感知质量和语音清晰度方面（WB-PESQ 1.62-1.71，STOI 0.76-0.78）仍落后于最先进方法。

Conclusion: 尽管存在架构调优不足、缺乏微调和训练时间不足等限制，但结合神经音频编解码器和潜在扩散模型的灵活架构为未来发展提供了坚实基础。

Abstract: High-quality audio is essential in a wide range of applications, including
online communication, virtual assistants, and the multimedia industry. However,
degradation caused by noise, compression, and transmission artifacts remains a
major challenge. While diffusion models have proven effective for audio
restoration, they typically require significant computational resources and
struggle to handle longer missing segments. This study introduces WaveLLDM
(Wave Lightweight Latent Diffusion Model), an architecture that integrates an
efficient neural audio codec with latent diffusion for audio restoration and
denoising. Unlike conventional approaches that operate in the time or spectral
domain, WaveLLDM processes audio in a compressed latent space, reducing
computational complexity while preserving reconstruction quality. Empirical
evaluations on the Voicebank+DEMAND test set demonstrate that WaveLLDM achieves
accurate spectral reconstruction with low Log-Spectral Distance (LSD) scores
(0.48 to 0.60) and good adaptability to unseen data. However, it still
underperforms compared to state-of-the-art methods in terms of perceptual
quality and speech clarity, with WB-PESQ scores ranging from 1.62 to 1.71 and
STOI scores between 0.76 and 0.78. These limitations are attributed to
suboptimal architectural tuning, the absence of fine-tuning, and insufficient
training duration. Nevertheless, the flexible architecture that combines a
neural audio codec and latent diffusion model provides a strong foundation for
future development.

</details>


### [2] [Full-Frequency Temporal Patching and Structured Masking for Enhanced Audio Classification](https://arxiv.org/abs/2508.21243)
*Aditya Makineni,Baocheng Geng,Qing Tian*

Main category: cs.SD

TL;DR: 提出了Full-Frequency Temporal Patching (FFTP)和SpecMask方法，通过在音频谱图上使用全频率时间分块策略和谱图增强技术，显著提升了音频分类性能并大幅降低了计算量。


<details>
  <summary>Details</summary>
Motivation: 现有的音频分类模型如AST和AuM采用计算机视觉中的方形分块方法，这会破坏连续频率模式，产生过多分块，导致训练速度慢且计算量大。

Method: 1) FFTP分块策略：跨越全频率带同时保持局部时间上下文，保留谐波结构并减少分块数量和计算量；2) SpecMask增强：在固定掩码预算下结合全频率和局部时频掩码，增强时间鲁棒性同时保持频谱连续性。

Result: 在AST和AuM模型上应用该方法后，AudioSet-18k的mAP提升高达+6.76，SpeechCommandsV2的准确率提升高达+8.46，同时计算量减少高达83.26%。

Conclusion: FFTP和SpecMask方法能够有效解决音频谱图的时间-频率不对称性问题，在提升音频分类性能的同时显著提高计算效率。

Abstract: Transformers and State-Space Models (SSMs) have advanced audio classification
by modeling spectrograms as sequences of patches. However, existing models such
as the Audio Spectrogram Transformer (AST) and Audio Mamba (AuM) adopt square
patching from computer vision, which disrupts continuous frequency patterns and
produces an excessive number of patches, slowing training, and increasing
computation. We propose Full-Frequency Temporal Patching (FFTP), a patching
strategy that better matches the time-frequency asymmetry of spectrograms by
spanning full frequency bands with localized temporal context, preserving
harmonic structure, and significantly reducing patch count and computation. We
also introduce SpecMask, a patch-aligned spectrogram augmentation that combines
full-frequency and localized time-frequency masks under a fixed masking budget,
enhancing temporal robustness while preserving spectral continuity. When
applied on both AST and AuM, our patching method with SpecMask improves mAP by
up to +6.76 on AudioSet-18k and accuracy by up to +8.46 on SpeechCommandsV2,
while reducing computation by up to 83.26%, demonstrating both performance and
efficiency gains.

</details>
