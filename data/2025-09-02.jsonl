{"id": "2508.21153", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.21153", "abs": "https://arxiv.org/abs/2508.21153", "authors": ["Kevin Putra Santoso", "Rizka Wakhidatus Sholikah", "Raden Venantius Hari Ginardi"], "title": "WaveLLDM: Design and Development of a Lightweight Latent Diffusion Model for Speech Enhancement and Restoration", "comment": null, "summary": "High-quality audio is essential in a wide range of applications, including\nonline communication, virtual assistants, and the multimedia industry. However,\ndegradation caused by noise, compression, and transmission artifacts remains a\nmajor challenge. While diffusion models have proven effective for audio\nrestoration, they typically require significant computational resources and\nstruggle to handle longer missing segments. This study introduces WaveLLDM\n(Wave Lightweight Latent Diffusion Model), an architecture that integrates an\nefficient neural audio codec with latent diffusion for audio restoration and\ndenoising. Unlike conventional approaches that operate in the time or spectral\ndomain, WaveLLDM processes audio in a compressed latent space, reducing\ncomputational complexity while preserving reconstruction quality. Empirical\nevaluations on the Voicebank+DEMAND test set demonstrate that WaveLLDM achieves\naccurate spectral reconstruction with low Log-Spectral Distance (LSD) scores\n(0.48 to 0.60) and good adaptability to unseen data. However, it still\nunderperforms compared to state-of-the-art methods in terms of perceptual\nquality and speech clarity, with WB-PESQ scores ranging from 1.62 to 1.71 and\nSTOI scores between 0.76 and 0.78. These limitations are attributed to\nsuboptimal architectural tuning, the absence of fine-tuning, and insufficient\ntraining duration. Nevertheless, the flexible architecture that combines a\nneural audio codec and latent diffusion model provides a strong foundation for\nfuture development."}
{"id": "2508.21243", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21243", "abs": "https://arxiv.org/abs/2508.21243", "authors": ["Aditya Makineni", "Baocheng Geng", "Qing Tian"], "title": "Full-Frequency Temporal Patching and Structured Masking for Enhanced Audio Classification", "comment": null, "summary": "Transformers and State-Space Models (SSMs) have advanced audio classification\nby modeling spectrograms as sequences of patches. However, existing models such\nas the Audio Spectrogram Transformer (AST) and Audio Mamba (AuM) adopt square\npatching from computer vision, which disrupts continuous frequency patterns and\nproduces an excessive number of patches, slowing training, and increasing\ncomputation. We propose Full-Frequency Temporal Patching (FFTP), a patching\nstrategy that better matches the time-frequency asymmetry of spectrograms by\nspanning full frequency bands with localized temporal context, preserving\nharmonic structure, and significantly reducing patch count and computation. We\nalso introduce SpecMask, a patch-aligned spectrogram augmentation that combines\nfull-frequency and localized time-frequency masks under a fixed masking budget,\nenhancing temporal robustness while preserving spectral continuity. When\napplied on both AST and AuM, our patching method with SpecMask improves mAP by\nup to +6.76 on AudioSet-18k and accuracy by up to +8.46 on SpeechCommandsV2,\nwhile reducing computation by up to 83.26%, demonstrating both performance and\nefficiency gains."}
