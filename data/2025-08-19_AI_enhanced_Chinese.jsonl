{"id": "2508.11676", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11676", "abs": "https://arxiv.org/abs/2508.11676", "authors": ["Maksym Shamrai", "Vladyslav Hamolia"], "title": "Deep Language Geometry: Constructing a Metric Space from LLM Weights", "comment": "18 pages, accepted to RANLP 2025", "summary": "We introduce a novel framework that utilizes the internal weight activations\nof modern Large Language Models (LLMs) to construct a metric space of\nlanguages. Unlike traditional approaches based on hand-crafted linguistic\nfeatures, our method automatically derives high-dimensional vector\nrepresentations by computing weight importance scores via an adapted pruning\nalgorithm. Our approach captures intrinsic language characteristics that\nreflect linguistic phenomena. We validate our approach across diverse datasets\nand multilingual LLMs, covering 106 languages. The results align well with\nestablished linguistic families while also revealing unexpected inter-language\nconnections that may indicate historical contact or language evolution. The\nsource code, computed language latent vectors, and visualization tool are made\npublicly available at https://github.com/mshamrai/deep-language-geometry.", "AI": {"tldr": "\u4f7f\u7528LLM\u5185\u90e8\u6743\u91cd\u6fc0\u6d3b\u6784\u5efa\u8bed\u8a00\u5ea6\u91cf\u7a7a\u95f4\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u526a\u679d\u7b97\u6cd5\u81ea\u52a8\u751f\u6210\u9ad8\u7ef4\u8bed\u8a00\u8868\u5f81\uff0c\u53d1\u73b0\u4e86\u4e0e\u8bed\u8a00\u5bb6\u65cf\u76f8\u7b26\u7684\u5173\u8054\u548c\u610f\u5916\u7684\u8bed\u8a00\u8fde\u63a5", "motivation": "\u4f20\u7edf\u8bed\u8a00\u5ea6\u91cf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u624b\u5de5\u7f16\u7801\u7684\u8bed\u8a00\u7279\u5f81\uff0c\u672c\u6587\u65b9\u6cd5\u5229\u7528LLM\u5185\u90e8\u6743\u91cd\u6fc0\u6d3b\u81ea\u52a8\u63d0\u53d6\u8bed\u8a00\u8868\u5f81\uff0c\u4ee5\u53cd\u6620\u8bed\u8a00\u73b0\u8c61\u7684\u672c\u8d28\u7279\u5f81", "method": "\u901a\u8fc7\u9002\u914d\u7684\u526a\u679d\u7b97\u6cd5\u8ba1\u7b97\u6743\u91cd\u91cd\u8981\u6027\u5206\u6570\uff0c\u81ea\u52a8\u6d3e\u751f\u9ad8\u7ef4\u5411\u91cf\u8868\u5f81\uff0c\u6784\u5efa\u8bed\u8a00\u5ea6\u91cf\u7a7a\u95f4\uff0c\u5728106\u79cd\u8bed\u8a00\u548c\u591a\u8bed\u8a00LLM\u4e0a\u9a8c\u8bc1", "result": "\u7ed3\u679c\u4e0e\u5df2\u77e5\u8bed\u8a00\u5bb6\u65cf\u5f88\u597d\u5bf9\u9f50\uff0c\u540c\u65f6\u53d1\u73b0\u4e86\u610f\u5916\u7684\u8bed\u8a00\u95f4\u8fde\u63a5\uff0c\u53ef\u80fd\u8868\u660e\u5386\u53f2\u63a5\u89e6\u6216\u8bed\u8a00\u8fdb\u5316\u5173\u7cfb", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u53d6\u8bed\u8a00\u7684\u672c\u8d28\u7279\u5f81\uff0c\u4e3a\u8bed\u8a00\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u5ea6\u91cf\u5316\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u4e86\u4f20\u7edf\u8bed\u8a00\u5b66\u65b9\u6cd5\u53ef\u80fd\u6ca1\u6709\u6d89\u53ca\u7684\u8bed\u8a00\u5173\u8054"}}
{"id": "2508.11758", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11758", "abs": "https://arxiv.org/abs/2508.11758", "authors": ["Jonas van Elburg", "Peter van der Putten", "Maarten Marx"], "title": "Can we Evaluate RAGs with Synthetic Data?", "comment": "Accepted for the SynDAiTE workshop at the European Conference on\n  Machine Learning and Principles and Practice of Knowledge Discovery in\n  Databases (ECML-PKDD 2025), September 15, 2025 - Porto, Portugal", "summary": "We investigate whether synthetic question-answer (QA) data generated by large\nlanguage models (LLMs) can serve as an effective proxy for human-labeled\nbenchmarks when such data is unavailable. We assess the reliability of\nsynthetic benchmarks across two experiments: one varying retriever parameters\nwhile keeping the generator fixed, and another varying the generator with fixed\nretriever parameters. Across four datasets, of which two open-domain and two\nproprietary, we find that synthetic benchmarks reliably rank the RAGs varying\nin terms of retriever configuration, aligning well with human-labeled benchmark\nbaselines. However, they fail to produce consistent RAG rankings when comparing\ngenerator architectures. The breakdown possibly arises from a combination of\ntask mismatch between the synthetic and human benchmarks, and stylistic bias\nfavoring certain generators.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u751f\u6210\u7684\u5408\u6210\u95ee\u7b54\u6570\u636e\u5728\u8bc4\u4f30\u68c0\u7d22\u5668\u914d\u7f6e\u65f6\u80fd\u53ef\u9760\u66ff\u4ee3\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6\uff0c\u4f46\u5728\u8bc4\u4f30\u751f\u6210\u5668\u67b6\u6784\u65f6\u6548\u679c\u4e0d\u4f73\u3002", "motivation": "\u63a2\u7d22\u5f53\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6\u4e0d\u53ef\u7528\u65f6\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u95ee\u7b54\u6570\u636e\u662f\u5426\u80fd\u6709\u6548\u66ff\u4ee3\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6\u6765\u8bc4\u4f30\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\u8fdb\u884c\u8bc4\u4f30\uff1a1\uff09\u56fa\u5b9a\u751f\u6210\u5668\uff0c\u53d8\u5316\u68c0\u7d22\u5668\u53c2\u6570\uff1b2\uff09\u56fa\u5b9a\u68c0\u7d22\u5668\u53c2\u6570\uff0c\u53d8\u5316\u751f\u6210\u5668\u67b6\u6784\u3002\u5728\u56db\u4e2a\u6570\u636e\u96c6\uff08\u4e24\u4e2a\u5f00\u653e\u57df\u548c\u4e24\u4e2a\u4e13\u6709\u6570\u636e\u96c6\uff09\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5408\u6210\u57fa\u51c6\u5728\u8bc4\u4f30\u4e0d\u540c\u68c0\u7d22\u5668\u914d\u7f6e\u7684RAG\u7cfb\u7edf\u65f6\u8868\u73b0\u53ef\u9760\uff0c\u4e0e\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff1b\u4f46\u5728\u6bd4\u8f83\u4e0d\u540c\u751f\u6210\u5668\u67b6\u6784\u65f6\u65e0\u6cd5\u4ea7\u751f\u4e00\u81f4\u7684RAG\u6392\u540d\u3002", "conclusion": "\u5408\u6210\u57fa\u51c6\u5728\u8bc4\u4f30\u68c0\u7d22\u7ec4\u4ef6\u65f6\u6709\u6548\uff0c\u4f46\u7531\u4e8e\u4efb\u52a1\u4e0d\u5339\u914d\u548c\u98ce\u683c\u504f\u89c1\u95ee\u9898\uff0c\u5728\u8bc4\u4f30\u751f\u6210\u7ec4\u4ef6\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2508.11767", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11767", "abs": "https://arxiv.org/abs/2508.11767", "authors": ["Noah Kasmanoff", "Rahul Zalkikar"], "title": "Limitation Learning: Catching Adverse Dialog with GAIL", "comment": "Paper from 2021", "summary": "Imitation learning is a proven method for creating a policy in the absence of\nrewards, by leveraging expert demonstrations. In this work, we apply imitation\nlearning to conversation. In doing so, we recover a policy capable of talking\nto a user given a prompt (input state), and a discriminator capable of\nclassifying between expert and synthetic conversation. While our policy is\neffective, we recover results from our discriminator that indicate the\nlimitations of dialog models. We argue that this technique can be used to\nidentify adverse behavior of arbitrary data models common for dialog oriented\ntasks.", "AI": {"tldr": "\u4f7f\u7528\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5e94\u7528\u4e8e\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u8bad\u7ec3\u51fa\u80fd\u591f\u6839\u636e\u63d0\u793a\u4e0e\u7528\u6237\u4ea4\u8c08\u7684\u7b56\u7565\uff0c\u540c\u65f6\u901a\u8fc7\u8fa8\u522b\u5668\u53d1\u73b0\u5bf9\u8bdd\u6a21\u578b\u7684\u5c40\u9650\u6027\u95ee\u9898\u3002", "motivation": "\u5728\u7f3a\u4e4f\u5956\u52b1\u51fd\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u4e13\u5bb6\u793a\u8303\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u6784\u5efa\u5bf9\u8bdd\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u8fa8\u522b\u5668\u8bc6\u522b\u6a21\u578b\u7684\u4e0d\u826f\u884c\u4e3a\u3002", "method": "\u91c7\u7528\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u8bad\u7ec3\u4e00\u4e2a\u80fd\u591f\u6839\u636e\u8f93\u5165\u63d0\u793a\u4e0e\u7528\u6237\u8fdb\u884c\u5bf9\u8bdd\u7684\u7b56\u7565\uff0c\u540c\u65f6\u8bad\u7ec3\u4e00\u4e2a\u80fd\u591f\u533a\u5206\u4e13\u5bb6\u5bf9\u8bdd\u548c\u5408\u6210\u5bf9\u8bdd\u7684\u8fa8\u522b\u5668\u3002", "result": "\u7b56\u7565\u8868\u73b0\u6709\u6548\uff0c\u4f46\u8fa8\u522b\u5668\u7ed3\u679c\u663e\u793a\u4e86\u5bf9\u8bdd\u6a21\u578b\u7684\u660e\u663e\u5c40\u9650\u6027\uff0c\u5305\u62ec\u4e0d\u826f\u884c\u4e3a\u7684\u8bc6\u522b\u95ee\u9898\u3002", "conclusion": "\u8fd9\u79cd\u6280\u672f\u53ef\u4ee5\u7528\u4e8e\u8bc6\u522b\u5e38\u89c1\u4e8e\u5bf9\u8bdd\u4efb\u52a1\u7684\u968f\u673a\u6570\u636e\u6a21\u578b\u7684\u4e0d\u826f\u884c\u4e3a\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2508.12020", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2508.12020", "abs": "https://arxiv.org/abs/2508.12020", "authors": ["Zhilin Gao", "Yunhao Li", "Sijing Wu", "Yuqin Cao", "Huiyu Duan", "Guangtao Zhai"], "title": "Ges-QA: A Multidimensional Quality Assessment Dataset for Audio-to-3D Gesture Generation", "comment": null, "summary": "The Audio-to-3D-Gesture (A2G) task has enormous potential for various\napplications in virtual reality and computer graphics, etc. However, current\nevaluation metrics, such as Fr\\'echet Gesture Distance or Beat Constancy, fail\nat reflecting the human preference of the generated 3D gestures. To cope with\nthis problem, exploring human preference and an objective quality assessment\nmetric for AI-generated 3D human gestures is becoming increasingly significant.\nIn this paper, we introduce the Ges-QA dataset, which includes 1,400 samples\nwith multidimensional scores for gesture quality and audio-gesture consistency.\nMoreover, we collect binary classification labels to determine whether the\ngenerated gestures match the emotions of the audio. Equipped with our Ges-QA\ndataset, we propose a multi-modal transformer-based neural network with 3\nbranches for video, audio and 3D skeleton modalities, which can score A2G\ncontents in multiple dimensions. Comparative experimental results and ablation\nstudies demonstrate that Ges-QAer yields state-of-the-art performance on our\ndataset.", "AI": {"tldr": "\u63d0\u51faGes-QA\u6570\u636e\u96c6\u548c\u591a\u6a21\u6001\u8f6c\u6362\u5668\u7f51\u7edcGes-QAer\uff0c\u7528\u4e8e\u8bc4\u4f30\u97f3\u9891\u52303D\u624b\u52bf\u751f\u6210\u8d28\u91cf\u548c\u97f3\u624b\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u73b0\u6709\u6307\u6807\u4e0e\u4eba\u7c7b\u504f\u597d\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u97f3\u9891\u52303D\u624b\u52bf\u751f\u6210\u8bc4\u4f30\u6307\u6807\uff08\u5982FGD\u3001Beat Constancy\uff09\u65e0\u6cd5\u53cd\u6620\u4eba\u7c7b\u504f\u597d\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u7b26\u5408\u4eba\u7c7b\u89c2\u611f\u7684\u5ba2\u89c2\u8d28\u91cf\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u6784\u5efaGes-QA\u6570\u636e\u96c6\uff081,400\u4e2a\u6837\u672c\uff09\uff0c\u5305\u542b\u591a\u7ef4\u5ea6\u8d28\u91cf\u5206\u6570\u548c\u97f3\u624b\u4e00\u81f4\u6027\u6807\u7b7e\uff1b\u8bbe\u8ba1\u591a\u6a21\u6001\u8f6c\u6362\u5668\u7f51\u7edcGes-QAer\uff0c\u901a\u8fc7\u89c6\u9891\u3001\u97f3\u9891\u548c3D\u9aa8\u67b6\u4e09\u4e2a\u5206\u652f\u8fdb\u884c\u591a\u7ef4\u5ea6\u8bc4\u5206\u3002", "result": "Ges-QAer\u5728Ges-QA\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5b9e\u9a8c\u7ed3\u679c\u548c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aA2G\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u7684\u5ba2\u89c2\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0cGes-QA\u6570\u636e\u96c6\u548cGes-QAer\u6a21\u578b\u5bf9\u865a\u62df\u73b0\u5b9e\u548c\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.11771", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11771", "abs": "https://arxiv.org/abs/2508.11771", "authors": ["Leo Peckham", "Michael Ong", "Naomi Nagy", "Ewan Dunbar"], "title": "Investigating Transcription Normalization in the Faetar ASR Benchmark", "comment": null, "summary": "We examine the role of transcription inconsistencies in the Faetar Automatic\nSpeech Recognition benchmark, a challenging low-resource ASR benchmark. With\nthe help of a small, hand-constructed lexicon, we conclude that find that,\nwhile inconsistencies do exist in the transcriptions, they are not the main\nchallenge in the task. We also demonstrate that bigram word-based language\nmodelling is of no added benefit, but that constraining decoding to a finite\nlexicon can be beneficial. The task remains extremely difficult.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0Faetar ASR\u57fa\u51c6\u4e2d\u7684\u8f6c\u5f55\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u5b58\u5728\u4f46\u4e0d\u662f\u4e3b\u8981\u6311\u6218\uff0c\u6709\u9650\u8bcd\u5178\u7ea6\u675f\u89e3\u7801\u6709\u76ca\uff0c\u4f46\u4efb\u52a1\u4ecd\u7136\u6781\u5176\u56f0\u96be", "motivation": "\u68c0\u9a8cFaetar\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u57fa\u51c6\u4e2d\u8f6c\u5f55\u4e0d\u4e00\u81f4\u6027\u7684\u4f5c\u7528\uff0c\u8fd9\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4f4e\u8d44\u6e90ASR\u57fa\u51c6", "method": "\u4f7f\u7528\u5c0f\u578b\u624b\u5de5\u6784\u5efa\u7684\u8bcd\u5178\u8fdb\u884c\u5206\u6790\uff0c\u6bd4\u8f83\u4e86bigram\u8bcd\u7ea7\u8bed\u8a00\u5efa\u6a21\u548c\u6709\u9650\u8bcd\u5178\u7ea6\u675f\u89e3\u7801\u7684\u6548\u679c", "result": "\u8f6c\u5f55\u4e0d\u4e00\u81f4\u786e\u5b9e\u5b58\u5728\u4f46\u4e0d\u662f\u4e3b\u8981\u6311\u6218\uff0cbigram\u8bed\u8a00\u5efa\u6a21\u65e0\u989d\u5916\u76ca\u5904\uff0c\u6709\u9650\u8bcd\u5178\u7ea6\u675f\u89e3\u7801\u6709\u76ca", "conclusion": "Faetar ASR\u4efb\u52a1\u4ecd\u7136\u6781\u5176\u56f0\u96be\uff0c\u8f6c\u5f55\u4e0d\u4e00\u81f4\u4e0d\u662f\u4e3b\u8981\u95ee\u9898\uff0c\u8bcd\u5178\u7ea6\u675f\u53ef\u80fd\u63d0\u4f9b\u6539\u8fdb\u65b9\u5411"}}
{"id": "2508.12368", "categories": ["cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.12368", "abs": "https://arxiv.org/abs/2508.12368", "authors": ["Kangyi Wu", "Pengna Li", "Jingwen Fu", "Yang Wu", "Yuhan Liu", "Sanping Zhou", "Jinjun Wang"], "title": "CEM-Net: Cross-Emotion Memory Network for Emotional Talking Face Generation", "comment": null, "summary": "Emotional talking face generation aims to animate a human face in given\nreference images and generate a talking video that matches the content and\nemotion of driving audio. However, existing methods neglect that reference\nimages may have a strong emotion that conflicts with the audio emotion, leading\nto severe emotion inaccuracy and distorted generated results. To tackle the\nissue, we introduce a cross-emotion memory network(CEM-Net), designed to\ngenerate emotional talking faces aligned with the driving audio when reference\nimages exhibit strong emotion. Specifically, an Audio Emotion Enhancement\nmodule(AEE) is first devised with the cross-reconstruction training strategy to\nenhance audio emotion, overcoming the disruption from reference image emotion.\nSecondly, since reference images cannot provide sufficient facial motion\ninformation of the speaker under audio emotion, an Emotion Bridging Memory\nmodule(EBM) is utilized to compensate for the lacked information. It brings in\nexpression displacement from the reference image emotion to the audio emotion\nand stores it in the memory.Given a cross-emotion feature as a query, the\nmatching displacement can be retrieved at inference time. Extensive experiments\nhave demonstrated that our CEM-Net can synthesize expressive, natural and\nlip-synced talking face videos with better emotion accuracy.", "AI": {"tldr": "\u8de8\u60c5\u611f\u5185\u5b58\u7f51\u7edc(CEM-Net)\u89e3\u51b3\u53c2\u8003\u56fe\u7247\u60c5\u611f\u4e0e\u97f3\u9891\u60c5\u611f\u51b2\u7a81\u95ee\u9898\uff0c\u901a\u8fc7\u97f3\u9891\u60c5\u611f\u589e\u5f3a\u548c\u60c5\u611f\u6865\u63a5\u5185\u5b58\u6a21\u5757\uff0c\u751f\u6210\u66f4\u51c6\u786e\u3001\u81ea\u7136\u7684\u60c5\u611f\u8bf4\u8bdd\u4eba\u8138\u89c6\u9891", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u53c2\u8003\u56fe\u7247\u53ef\u80fd\u5305\u542b\u4e0e\u97f3\u9891\u60c5\u611f\u51b2\u7a81\u7684\u5f3a\u60c5\u611f\uff0c\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u60c5\u611f\u4e0d\u51c6\u786e\u548c\u53d8\u5f62", "method": "\u63d0\u51faCEM-Net\uff0c\u5305\u542b\u97f3\u9891\u60c5\u611f\u589e\u5f3a\u6a21\u5757(AEE)\u548c\u60c5\u611f\u6865\u63a5\u5185\u5b58\u6a21\u5757(EBM)\uff0c\u901a\u8fc7\u8de8\u91cd\u6784\u8bad\u7ec3\u7b56\u7565\u589e\u5f3a\u97f3\u9891\u60c5\u611f\uff0c\u5e76\u4f7f\u7528\u5185\u5b58\u7f51\u7edc\u8865\u507f\u7f3a\u4e4f\u7684\u9762\u90e8\u8fd0\u52a8\u4fe1\u606f", "result": "\u5b9e\u9a8c\u8868\u660eCEM-Net\u80fd\u591f\u5408\u6210\u8868\u60c5\u4e30\u5bcc\u3001\u81ea\u7136\u3001\u5507\u5782\u540c\u6b65\u7684\u8bf4\u8bdd\u4eba\u8138\u89c6\u9891\uff0c\u5177\u6709\u66f4\u597d\u7684\u60c5\u611f\u51c6\u786e\u6027", "conclusion": "CEM-Net\u901a\u8fc7\u5904\u7406\u53c2\u8003\u56fe\u7247\u4e0e\u97f3\u9891\u60c5\u611f\u51b2\u7a81\u7684\u95ee\u9898\uff0c\u6709\u6548\u63d0\u5347\u4e86\u60c5\u611f\u8bf4\u8bdd\u4eba\u8138\u751f\u6210\u7684\u8d28\u91cf\u548c\u51c6\u786e\u6027"}}
{"id": "2508.11632", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.11632", "abs": "https://arxiv.org/abs/2508.11632", "authors": ["Ian Jacob Cabansag", "Paul Ntegeka"], "title": "Prediction of Spotify Chart Success Using Audio and Streaming Features", "comment": null, "summary": "Spotify's streaming charts offer a real-time lens into music popularity,\ndriving discovery, playlists, and even revenue potential. Understanding what\ninfluences a song's rise in ranks on these charts-especially early on-can guide\nmarketing efforts, investment decisions, and even artistic direction. In this\nproject, we developed a classification pipeline to predict a song's chart\nsuccess based on its musical characteristics and early engagement data. Using\nall 2024 U.S. Top 200 Spotify Daily Charts and the Spotify Web API, we built a\ndataset containing both metadata and audio features for 14,639 unique songs.\n  The project was structured in two phases. First, we benchmarked four models:\nLogistic Regression, K Nearest Neighbors, Random Forest, and XGBoost-using a\nstandard train-test split. In the second phase, we incorporated\ncross-validation, hyperparameter tuning, and detailed class-level evaluation to\nensure robustness. Tree-based models consistently outperformed the rest, with\nRandom Forest and XGBoost achieving macro F1-scores near 0.95 and accuracy\naround 97%.\n  Even when stream count and rank history were excluded, models trained solely\non audio attributes retained predictive power. These findings validate the\npotential of audio-based modeling in A&R scouting, playlist optimization, and\nhit forecasting-long before a track reaches critical mass.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5206\u7c7b\u7ba1\u9053\uff0c\u57fa\u4e8e\u97f3\u4e50\u7279\u5f81\u548c\u65e9\u671f\u53c2\u4e0e\u6570\u636e\u9884\u6d4b\u6b4c\u66f2\u5728Spotify\u6392\u884c\u699c\u4e0a\u7684\u6210\u529f\uff0c\u6811\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u7ea697%", "motivation": "\u7406\u89e3\u5f71\u54cd\u6b4c\u66f2\u5728Spotify\u6392\u884c\u699c\u4e0a\u6392\u540d\u4e0a\u5347\u7684\u56e0\u7d20\uff0c\u7279\u522b\u662f\u65e9\u671f\u9636\u6bb5\uff0c\u53ef\u4ee5\u6307\u5bfc\u8425\u9500\u3001\u6295\u8d44\u51b3\u7b56\u548c\u827a\u672f\u65b9\u5411", "method": "\u4f7f\u75282024\u5e74\u7f8e\u56fdTop 200 Spotify\u6bcf\u65e5\u6392\u884c\u699c\u6570\u636e\u548cSpotify Web API\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5305\u542b14,639\u9996\u72ec\u7279\u6b4c\u66f2\u7684\u5143\u6570\u636e\u548c\u97f3\u9891\u7279\u5f81\u3002\u5206\u4e24\u9636\u6bb5\uff1a\u57fa\u51c6\u6d4b\u8bd5\u56db\u79cd\u6a21\u578b\uff08\u903b\u8f91\u56de\u5f52\u3001K\u8fd1\u90bb\u3001\u968f\u673a\u68ee\u6797\u3001XGBoost\uff09\uff0c\u7136\u540e\u8fdb\u884c\u4ea4\u53c9\u9a8c\u8bc1\u3001\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u8be6\u7ec6\u7c7b\u522b\u8bc4\u4f30", "result": "\u6811\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u968f\u673a\u68ee\u6797\u548cXGBoost\u7684\u5b8f\u89c2F1\u5206\u6570\u63a5\u8fd10.95\uff0c\u51c6\u786e\u7387\u7ea697%\u3002\u5373\u4f7f\u6392\u9664\u6d41\u5a92\u4f53\u8ba1\u6570\u548c\u6392\u540d\u5386\u53f2\uff0c\u4ec5\u57fa\u4e8e\u97f3\u9891\u5c5e\u6027\u7684\u6a21\u578b\u4ecd\u4fdd\u6301\u9884\u6d4b\u80fd\u529b", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u97f3\u9891\u7684\u5efa\u6a21\u5728A&R\u53d1\u6398\u3001\u64ad\u653e\u5217\u8868\u4f18\u5316\u548c\u70ed\u95e8\u9884\u6d4b\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u8fdc\u5728\u6b4c\u66f2\u8fbe\u5230\u4e34\u754c\u8d28\u91cf\u4e4b\u524d"}}
{"id": "2508.11779", "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2508.11779", "abs": "https://arxiv.org/abs/2508.11779", "authors": ["Tianyi Li", "Yu Qin", "Olivia R. Liu Sheng"], "title": "A Multi-Task Evaluation of LLMs' Processing of Academic Text Input", "comment": null, "summary": "How much large language models (LLMs) can aid scientific discovery, notably\nin assisting academic peer review, is in heated debate. Between a literature\ndigest and a human-comparable research assistant lies their practical\napplication potential. We organize individual tasks that computer science\nstudies employ in separate terms into a guided and robust workflow to evaluate\nLLMs' processing of academic text input. We employ four tasks in the\nassessment: content reproduction/comparison/scoring/reflection, each demanding\na specific role of the LLM (oracle/judgmental arbiter/knowledgeable\narbiter/collaborator) in assisting scholarly works, and altogether testing LLMs\nwith questions that increasingly require intellectual capabilities towards a\nsolid understanding of scientific texts to yield desirable solutions. We\nexemplify a rigorous performance evaluation with detailed instructions on the\nprompts. Adopting first-rate Information Systems articles at three top journals\nas the input texts and an abundant set of text metrics, we record a compromised\nperformance of the leading LLM - Google's Gemini: its summary and paraphrase of\nacademic text is acceptably reliable; using it to rank texts through pairwise\ntext comparison is faintly scalable; asking it to grade academic texts is prone\nto poor discrimination; its qualitative reflection on the text is\nself-consistent yet hardly insightful to inspire meaningful research. This\nevidence against an endorsement of LLMs' text-processing capabilities is\nconsistent across metric-based internal (linguistic assessment), external\n(comparing to the ground truth), and human evaluation, and is robust to the\nvariations of the prompt. Overall, we do not recommend an unchecked use of LLMs\nin constructing peer reviews.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u5316\u6d41\u7a0b\u8bc4\u4f30Gemini\u5728\u5b66\u672f\u6587\u672c\u5904\u7406\u4e2d\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5b83\u5728\u6458\u8981\u3001\u6bd4\u8f83\u3001\u8bc4\u5206\u548c\u53cd\u601d\u4efb\u52a1\u4e2d\u8868\u73b0\u5c40\u9650\uff0c\u4e0d\u5efa\u8bae\u5728\u540c\u884c\u5ba1\u67e5\u4e2d\u65e0\u68c0\u67e5\u5730\u4f7f\u7528LLM\u3002", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b66\u672f\u53d1\u73b0\u548c\u540c\u884c\u5ba1\u67e5\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u89e3\u51b3\u5f53\u524d\u5173\u4e8eLLM\u80fd\u5426\u5e2e\u52a9\u5b66\u672f\u5de5\u4f5c\u7684\u4e89\u8bba\u3002", "method": "\u8bbe\u8ba1\u5305\u542b\u56db\u4e2a\u4efb\u52a1\u7684\u7cfb\u7edf\u5316\u6d41\u7a0b\uff1a\u5185\u5bb9\u590d\u73b0/\u6bd4\u8f83/\u6253\u5206/\u53cd\u601d\uff0c\u6bcf\u4e2a\u4efb\u52a1\u8981\u6c42LLM\u626e\u6f14\u4e0d\u540c\u89d2\u8272\uff08\u795e\u8c15/\u5224\u65ad\u8005/\u77e5\u8bc6\u4e30\u5bcc\u8005/\u5408\u4f5c\u8005\uff09\u3002\u4f7f\u7528\u9876\u7ea7\u4fe1\u606f\u7cfb\u7edf\u6742\u5fd7\u7684\u6587\u7ae0\u4f5c\u4e3a\u8f93\u5165\uff0c\u91c7\u7528\u591a\u79cd\u6587\u672c\u6307\u6807\u8fdb\u884c\u8be6\u7ec6\u7684\u63d0\u793a\u8bbe\u8ba1\u548c\u6027\u80fd\u8bc4\u4f30\u3002", "result": "Gemini\u5728\u5b66\u672f\u6587\u672c\u5904\u7406\u4e2d\u8868\u73b0\u504f\u5dee\uff1a\u6458\u8981\u548c\u91cd\u5199\u53ef\u9760\u6027\u53ef\u63a5\u53d7\uff1b\u901a\u8fc7\u6210\u5bf9\u6bd4\u8f83\u8fdb\u884c\u6587\u672c\u6392\u540d\u7f3a\u4e4f\u6269\u5c55\u6027\uff1b\u5b66\u672f\u6587\u672c\u6253\u5206\u533a\u5206\u5ea6\u5dee\uff1b\u5b9a\u6027\u53cd\u601d\u81ea\u76f8\u4e00\u81f4\u4f46\u7f3a\u4e4f\u6df1\u5ea6\u3002\u8fd9\u4e9b\u7ed3\u679c\u5728\u8bed\u8a00\u3001\u771f\u5b9e\u6570\u636e\u548c\u4eba\u5de5\u8bc4\u4f30\u4e2d\u5747\u4e00\u81f4\uff0c\u4e14\u5bf9\u63d0\u793a\u53d8\u5316\u5f3a\u52b2\u3002", "conclusion": "LLM\u5728\u5904\u7406\u5b66\u672f\u6587\u672c\u65f6\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u4e0d\u5efa\u8bae\u5728\u540c\u884c\u5ba1\u67e5\u4e2d\u65e0\u68c0\u67e5\u5730\u4f7f\u7528\u3002\u7814\u7a76\u7ed3\u679c\u4e0e\u652f\u6301LLM\u6587\u672c\u5904\u7406\u80fd\u529b\u7684\u8bc4\u4f30\u76f8\u51b2\u7a81\uff0c\u5f3a\u8c03\u4e86\u5728\u5b66\u672f\u5e94\u7528\u4e2d\u9700\u8981\u66f4\u4e25\u683c\u7684\u9a8c\u8bc1\u3002"}}
{"id": "2508.12992", "categories": ["cs.MM", "I.2"], "pdf": "https://arxiv.org/pdf/2508.12992", "abs": "https://arxiv.org/abs/2508.12992", "authors": ["Xiangxian Li", "Yawen Zheng", "Baiqiao Zhang", "Yijia Ma", "XianhuiCao XianhuiCao", "Juan Liu", "Yulong Bian", "Jin Huang", "Chenglei Yang"], "title": "MAGNeT: Multimodal Adaptive Gaussian Networks for Intent Inference in Moving Target Selection across Complex Scenarios", "comment": "Accepted by ACM MM 2025", "summary": "Moving target selection in multimedia interactive systems faces unprecedented\nchallenges as users increasingly interact across diverse and dynamic\ncontexts-from live streaming in moving vehicles to VR gaming in varying\nenvironments. Existing approaches rely on probabilistic models that relate\nendpoint distribution to target properties such as size and speed. However,\nthese methods require substantial training data for each new context and lack\ntransferability across scenarios, limiting their practical deployment in\ndiverse multimedia environments where rich multimodal contextual information is\nreadily available. This paper introduces MAGNeT (Multimodal Adaptive Gaussian\nNetworks), which addresses these problems by combining classical statistical\nmodeling with a context-aware multimodal method. MAGNeT dynamically fuses\npre-fitted Ternary-Gaussian models from various scenarios based on real-time\ncontextual cues, enabling effective adaptation with minimal training data while\npreserving model interpretability. We conduct experiments on self-constructed\n2D and 3D moving target selection datasets under in-vehicle vibration\nconditions. Extensive experiments demonstrate that MAGNeT achieves lower error\nrates with few-shot samples by applying context-aware fusion of Gaussian\nexperts from multi-factor conditions.", "AI": {"tldr": "MAGNeT\u662f\u4e00\u79cd\u591a\u6a21\u6001\u81ea\u9002\u5e94\u9ad8\u65af\u7f51\u7edc\uff0c\u901a\u8fc7\u7ed3\u5408\u7ecf\u5178\u7edf\u8ba1\u5efa\u6a21\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5728\u5c11\u6837\u672c\u6761\u4ef6\u4e0b\u5bf9\u79fb\u52a8\u76ee\u6807\u9009\u62e9\u7684\u4f4e\u9519\u8bef\u7387\u3002", "motivation": "\u591a\u5a92\u4f53\u4ea4\u4e92\u7cfb\u7edf\u4e2d\u79fb\u52a8\u76ee\u6807\u9009\u62e9\u9762\u4e34\u65b0\u6311\u6218\uff0c\u73b0\u6709\u6982\u7387\u6a21\u578b\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u4e14\u7f3a\u4e4f\u8de8\u573a\u666f\u8fc1\u79fb\u80fd\u529b\uff0c\u65e0\u6cd5\u9002\u5e94\u591a\u6837\u5316\u591a\u5a92\u4f53\u73af\u5883\u3002", "method": "MAGNeT\u52a8\u6001\u878d\u5408\u6765\u81ea\u4e0d\u540c\u573a\u666f\u7684\u9884\u62df\u5408\u4e09\u5143\u9ad8\u65af\u6a21\u578b\uff0c\u57fa\u4e8e\u5b9e\u65f6\u4e0a\u4e0b\u6587\u7ebf\u7d22\u8fdb\u884c\u591a\u6a21\u6001\u81ea\u9002\u5e94\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u5c11\u6837\u672c\u6709\u6548\u9002\u5e94\u3002", "result": "\u5728\u8f66\u8f7d\u632f\u52a8\u6761\u4ef6\u4e0b\u76842D\u548c3D\u79fb\u52a8\u76ee\u6807\u9009\u62e9\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\uff0cMAGNeT\u901a\u8fc7\u591a\u56e0\u7d20\u6761\u4ef6\u7684\u9ad8\u65af\u4e13\u5bb6\u4e0a\u4e0b\u6587\u611f\u77e5\u878d\u5408\uff0c\u5b9e\u73b0\u4e86\u8f83\u4f4e\u7684\u5c11\u6837\u672c\u9519\u8bef\u7387\u3002", "conclusion": "MAGNeT\u6210\u529f\u89e3\u51b3\u4e86\u8de8\u573a\u666f\u8fc1\u79fb\u95ee\u9898\uff0c\u5728\u5c11\u6837\u672c\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u79fb\u52a8\u76ee\u6807\u9009\u62e9\u7684\u6027\u80fd\uff0c\u4e3a\u591a\u6837\u5316\u591a\u5a92\u4f53\u73af\u5883\u4e2d\u7684\u4ea4\u4e92\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11818", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11818", "abs": "https://arxiv.org/abs/2508.11818", "authors": ["Zhifeng Kong", "Arushi Goel", "Joao Felipe Santos", "Sreyan Ghosh", "Rafael Valle", "Wei Ping", "Bryan Catanzaro"], "title": "Audio Flamingo Sound-CoT Technical Report: Improving Chain-of-Thought Reasoning in Sound Understanding", "comment": null, "summary": "Chain-of-thought reasoning has demonstrated significant improvements in large\nlanguage models and vision language models, yet its potential for audio\nlanguage models remains largely unexplored. In this technical report, we take a\npreliminary step towards closing this gap. For better assessment of sound\nreasoning, we propose AF-Reasoning-Eval, a benchmark targeting common-sense\nreasoning and the ability to discriminate among closely related choices. To\nprepare training corpus for sound reasoning abilities, we propose automatic\npipelines that transform existing audio question answering and classification\ndata into explicit reasoning chains, yielding AF-CoT-Train with 1.24M samples.\nWe study the effect of finetuning Audio Flamingo series on AF-CoT-Train and\nobserve considerable improvements on several reasoning benchmarks, validating\nthe effectiveness of chain-of-thought finetuning on advanced sound\nunderstanding.", "AI": {"tldr": "\u901a\u8fc7\u6784\u5efaAF-CoT-Train\u8bad\u7ec3\u6570\u636e\u96c6\u548cAF-Reasoning-Eval\u8bc4\u6d4b\u6807\u51c6\uff0c\u8bc1\u660e\u4e86\u94fe\u5f0f\u601d\u7ef4\u5fae\u8c03\u5728\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6548\u679c", "motivation": "\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u5728\u6587\u672c\u548c\u56fe\u50cf\u6a21\u578b\u4e2d\u5df2\u663e\u793a\u663e\u8457\u6539\u5584\uff0c\u4f46\u5728\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22", "method": "\u6784\u5efaAF-Reasoning-Eval\u8bc4\u6d4b\u6807\u51c6\uff0c\u5f00\u53d1\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\u5c06\u73b0\u6709\u97f3\u9891\u6570\u636e\u8f6c\u6362\u4e3a\u660e\u786e\u7684\u63a8\u7406\u94fe\uff0c\u751f\u6210\u5305\u542b124\u4e07\u6837\u672c\u7684AF-CoT-Train\u6570\u636e\u96c6\uff0c\u5bf9Audio Flamingo\u7cfb\u5217\u6a21\u578b\u8fdb\u884c\u5fae\u8c03", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u8bc4\u6d4b\u6807\u51c6\u4e0a\u89c2\u5bdf\u5230\u663e\u8457\u6539\u8fdb\uff0c\u9a8c\u8bc1\u4e86\u94fe\u5f0f\u601d\u7ef4\u5fae\u8c03\u5bf9\u9ad8\u7ea7\u97f3\u54cd\u7406\u89e3\u80fd\u529b\u7684\u6709\u6548\u6027", "conclusion": "\u94fe\u5f0f\u601d\u7ef4\u5b66\u4e60\u5728\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\u540c\u6837\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4e3a\u97f3\u54cd\u7406\u89e3\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u53d1\u5c55\u65b9\u5411"}}
{"id": "2508.11816", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11816", "abs": "https://arxiv.org/abs/2508.11816", "authors": ["Krishna Chaitanya Marturi", "Heba H. Elwazzan"], "title": "LLM-Guided Planning and Summary-Based Scientific Text Simplification: DS@GT at CLEF 2025 SimpleText", "comment": "Text Simplification, hallucination detection, LLMs, CLEF 2025,\n  SimpleText, CEUR-WS", "summary": "In this paper, we present our approach for the CLEF 2025 SimpleText Task 1,\nwhich addresses both sentence-level and document-level scientific text\nsimplification. For sentence-level simplification, our methodology employs\nlarge language models (LLMs) to first generate a structured plan, followed by\nplan-driven simplification of individual sentences. At the document level, we\nleverage LLMs to produce concise summaries and subsequently guide the\nsimplification process using these summaries. This two-stage, LLM-based\nframework enables more coherent and contextually faithful simplifications of\nscientific text.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u79d1\u5b66\u6587\u672c\u7b80\u5316\uff0c\u5305\u62ec\u53e5\u5b50\u7ea7\u548c\u6587\u6863\u7ea7\u4e24\u4e2a\u9636\u6bb5\uff1a\u53e5\u5b50\u7ea7\u901a\u8fc7\u751f\u6210\u7ed3\u6784\u5316\u8ba1\u5212\u6307\u5bfc\u7b80\u5316\uff0c\u6587\u6863\u7ea7\u901a\u8fc7\u6458\u8981\u5f15\u5bfc\u7b80\u5316\u8fc7\u7a0b", "motivation": "\u89e3\u51b3\u79d1\u5b66\u6587\u672c\u7b80\u5316\u4efb\u52a1\uff0c\u9700\u8981\u540c\u65f6\u5904\u7406\u53e5\u5b50\u7ea7\u548c\u6587\u6863\u7ea7\u7684\u7b80\u5316\uff0c\u786e\u4fdd\u7b80\u5316\u540e\u7684\u6587\u672c\u65e2\u4fdd\u6301\u8fde\u8d2f\u6027\u53c8\u5fe0\u5b9e\u4e8e\u539f\u6587\u5185\u5bb9", "method": "\u4e24\u9636\u6bb5LLM\u6846\u67b6\uff1a\u53e5\u5b50\u7ea7\u4f7f\u7528LLM\u751f\u6210\u7ed3\u6784\u5316\u8ba1\u5212\u540e\u6307\u5bfc\u53e5\u5b50\u7b80\u5316\uff1b\u6587\u6863\u7ea7\u4f7f\u7528LLM\u751f\u6210\u6458\u8981\u540e\u5f15\u5bfc\u6574\u4f53\u7b80\u5316\u8fc7\u7a0b", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u66f4\u8fde\u8d2f\u4e14\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u7684\u79d1\u5b66\u6587\u672c\u7b80\u5316\u7ed3\u679c", "conclusion": "\u57fa\u4e8eLLM\u7684\u4e24\u9636\u6bb5\u6846\u67b6\u5728\u79d1\u5b66\u6587\u672c\u7b80\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3aCLEF 2025 SimpleText Task 1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12334", "categories": ["cs.SD", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.12334", "abs": "https://arxiv.org/abs/2508.12334", "authors": ["Qing Wang", "Ya Jiang", "Hang Chen", "Sabato Marco Siniscalchi", "Jun Du", "Jianqing Gao"], "title": "Cross-Modal Knowledge Distillation with Multi-Level Data Augmentation for Low-Resource Audio-Visual Sound Event Localization and Detection", "comment": "34 pages, 7 figures", "summary": "This work presents a cross-modal knowledge distillation (CMKD) framework\ncombined with multi-level data augmentation for low-resource audio-visual (AV)\nsound event localization and detection (SELD). An audio-only SELD model acts as\nthe teacher, transferring knowledge to an AV student model through both output\nresponses and intermediate feature representations. To enhance learning, data\naugmentation is applied by mixing features randomly selected from multiple\nnetwork layers and associated loss functions tailored to the SELD task.\nExtensive experiments on the DCASE 2023 and 2024 SELD datasets show that the\nproposed method significantly improves AV SELD performance, yielding relative\ngains of 22%~36% in the overall metric over the baseline. Notably, our approach\nachieves results comparable to or better than teacher models trained on much\nlarger datasets, surpassing state-of-the-art methods on both DCASE 2023 and\n2024 SELD tasks.", "AI": {"tldr": "\u901a\u8fc7\u8de8\u6a21\u6001\u77e5\u8bc6\u840c\u84c8\u548c\u591a\u5c42\u6b21\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4f4e\u8d44\u6e90\u60c5\u51b5\u4e0b\u63d0\u5347\u97f3\u89c6\u9891\u58f0\u97f3\u4e8b\u4ef6\u5b9a\u4f4d\u68c0\u6d4b\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u5728DCASE\u6bd4\u8d5b\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u60c5\u51b5\u4e0b\u97f3\u89c6\u9891\u58f0\u97f3\u4e8b\u4ef6\u5b9a\u4f4d\u68c0\u6d4b(SELD)\u6027\u80fd\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u77e5\u8bc6\u840c\u84c8\u6280\u672f\u5229\u7528\u97f3\u9891\u5355\u6a21\u6001\u6a21\u578b\u7684\u77e5\u8bc6\u6765\u63d0\u5347\u97f3\u89c6\u9891\u6a21\u578b\u7684\u8868\u73b0", "method": "\u4f7f\u7528\u97f3\u9891\u5355\u6a21\u6001SELD\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\uff0c\u901a\u8fc7\u8f93\u51fa\u54cd\u5e94\u548c\u4e2d\u95f4\u7279\u5f81\u8868\u793a\u5411\u97f3\u89c6\u9891\u5b66\u751f\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u840c\u84c8\uff0c\u7ed3\u5408\u591a\u5c42\u6b21\u7f51\u7edc\u7279\u5f81\u7684\u968f\u673a\u6df7\u5408\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u4ee5\u53ca\u4e13\u95e8\u4e3aSELD\u4efb\u52a1\u8bbe\u8ba1\u7684\u635f\u5931\u51fd\u6570", "result": "\u5728DCASE 2023\u548c2024 SELD\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u65b9\u6cd5\u5728\u6574\u4f53\u6307\u6807\u4e0a\u76f8\u6bd4\u57fa\u7ebf\u63d0\u534722%~36%\uff0c\u8fbe\u5230\u4e86\u4e0e\u5728\u66f4\u5927\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6559\u5e08\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u5728\u4e24\u4e2aDCASE\u4efb\u52a1\u4e0a\u90fd\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u840c\u84c8\u6846\u67b6\u7ed3\u5408\u591a\u5c42\u6b21\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u4f4e\u8d44\u6e90\u60c5\u51b5\u4e0b\u97f3\u89c6\u9891SELD\u7684\u6027\u80fd\uff0c\u4e3a\u8de8\u6a21\u6001\u5b66\u4e60\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.11845", "categories": ["cs.SD", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11845", "abs": "https://arxiv.org/abs/2508.11845", "authors": ["Marius Miron", "David Robinson", "Milad Alizadeh", "Ellen Gilsenan-McMahon", "Gagan Narula", "Olivier Pietquin", "Matthieu Geist", "Emmanuel Chemla", "Maddie Cusimano", "Felix Effenberger", "Masato Hagiwara", "Benjamin Hoffman", "Sara Keen", "Diane Kim", "Jane Lawton", "Jen-Yu Liu", "Aza Raskin"], "title": "What Matters for Bioacoustic Encoding", "comment": null, "summary": "Bioacoustics, the study of sounds produced by living organisms, plays a vital\nrole in conservation, biodiversity monitoring, and behavioral studies. Many\ntasks in this field, such as species, individual, and behavior classification\nand detection, are well-suited to machine learning. However, they often suffer\nfrom limited annotated data, highlighting the need for a general-purpose\nbioacoustic encoder capable of extracting useful representations for diverse\ndownstream tasks. Such encoders have been proposed before, but are often\nlimited in scope due to a focus on a narrow range of species (typically birds),\nand a reliance on a single model architecture or training paradigm. Moreover,\nthey are usually evaluated on a small set of tasks and datasets. In this work,\nwe present a large-scale empirical study that covers aspects of bioacoustics\nthat are relevant to research but have previously been scarcely considered:\ntraining data diversity and scale, model architectures and training recipes,\nand the breadth of evaluation tasks and datasets. We obtain encoders that are\nstate-of-the-art on the existing and proposed benchmarks. We also identify what\nmatters for training these encoders, such that this work can be extended when\nmore data are available or better architectures are proposed. Specifically,\nacross 26 datasets with tasks including species classification, detection,\nindividual ID, and vocal repertoire discovery, we find self-supervised\npre-training followed by supervised post-training on a mixed bioacoustics +\ngeneral-audio corpus yields the strongest in- and out-of-distribution\nperformance. We show the importance of data diversity in both stages. To\nsupport ongoing research and application, we will release the model\ncheckpoints.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u751f\u7269\u58f0\u5b66\u7f16\u7801\u5668\u7814\u7a76\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u540e\u8bad\u7ec3\u7ec4\u5408\uff0c\u572826\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u72ec\u6811\u4e00\u5e84\u7684\u6027\u80fd\uff0c\u5e76\u786e\u8ba4\u4e86\u6570\u636e\u591a\u6837\u6027\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u751f\u7269\u58f0\u5b66\u9886\u57df\u7f3a\u4e4f\u6ce8\u89e3\u6570\u636e\uff0c\u9700\u8981\u901a\u7528\u7684\u7f16\u7801\u5668\u6765\u63d0\u53d6\u6709\u7528\u8868\u5f81\u3002\u4ee5\u5f80\u7814\u7a76\u5b58\u5728\u8303\u56f4\u7a84\u5c40\u9650\u6027\uff08\u4e3b\u8981\u805a\u7126\u9e1f\u7c7b\uff09\u3001\u6a21\u578b\u67b6\u6784\u548c\u8bc4\u4f30\u4efb\u52a1\u7684\u7f3a\u4e4f\u7b49\u95ee\u9898\u3002", "method": "\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u9a8c\u7814\u7a76\uff0c\u5305\u62ec\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6837\u6027\u548c\u89c4\u6a21\u3001\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6848\u3001\u8bc4\u4f30\u4efb\u52a1\u548c\u6570\u636e\u96c6\u7684\u5e7f\u5ea6\u3002\u91c7\u7528\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7ecf\u9a8c\u540e\u76d1\u7763\u540e\u8bad\u7ec3\u7684\u7ec4\u5408\u65b9\u5f0f\uff0c\u4f7f\u7528\u6df7\u5408\u751f\u7269\u58f0\u5b66+\u901a\u7528\u97f3\u9891\u8bed\u6599\u5e93\u3002", "result": "\u572826\u4e2a\u6570\u636e\u96c6\u4e0a\uff08\u5305\u62ec\u7269\u79cd\u5206\u7c7b\u3001\u68c0\u6d4b\u3001\u4e2a\u4f53\u8bc6\u522b\u3001\u53d1\u58f0\u5e93\u53d1\u73b0\u7b49\u4efb\u52a1\uff09\u83b7\u5f97\u4e86\u72ec\u6811\u4e00\u5e84\u7684\u6027\u80fd\uff0c\u663e\u793a\u4e86\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u90fd\u6709\u4f18\u5f02\u8868\u73b0\u3002\u8bc1\u660e\u4e86\u4e24\u4e2a\u8bad\u7ec3\u9636\u6bb5\u4e2d\u6570\u636e\u591a\u6837\u6027\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u8bc4\u4f30\uff0c\u63d0\u51fa\u4e86\u6700\u4f18\u7684\u751f\u7269\u58f0\u5b66\u7f16\u7801\u5668\u8bad\u7ec3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002\u5c06\u91ca\u653e\u6a21\u578b\u68c0\u67e5\u70b9\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2508.11823", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11823", "abs": "https://arxiv.org/abs/2508.11823", "authors": ["Krishna Chaitanya Marturi", "Heba H. Elwazzan"], "title": "Hallucination Detection and Mitigation in Scientific Text Simplification using Ensemble Approaches: DS@GT at CLEF 2025 SimpleText", "comment": "Text Simplification, hallucination detection, LLMs, CLEF 2025,\n  SimpleText, CEUR-WS", "summary": "In this paper, we describe our methodology for the CLEF 2025 SimpleText Task\n2, which focuses on detecting and evaluating creative generation and\ninformation distortion in scientific text simplification. Our solution\nintegrates multiple strategies: we construct an ensemble framework that\nleverages BERT-based classifier, semantic similarity measure, natural language\ninference model, and large language model (LLM) reasoning. These diverse\nsignals are combined using meta-classifiers to enhance the robustness of\nspurious and distortion detection. Additionally, for grounded generation, we\nemploy an LLM-based post-editing system that revises simplifications based on\nthe original input texts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u591a\u79cd\u7b56\u7565\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u79d1\u5b66\u6587\u672c\u7b80\u5316\u4e2d\u7684\u521b\u9020\u6027\u751f\u6210\u548c\u4fe1\u606f\u5931\u771f\uff0c\u5305\u62ecBERT\u5206\u7c7b\u5668\u3001\u8bed\u4e49\u76f8\u4f3c\u5ea6\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u548cLLM\u63a8\u7406\uff0c\u5e76\u4f7f\u7528\u5143\u5206\u7c7b\u5668\u6574\u5408\u8fd9\u4e9b\u4fe1\u53f7\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u79d1\u5b66\u6587\u672c\u7b80\u5316\u4efb\u52a1\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u521b\u9020\u6027\u751f\u6210\u548c\u4fe1\u606f\u5931\u771f\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u51c6\u786e\u68c0\u6d4b\u8fd9\u4e9b\u95ee\u9898\u7684\u9c81\u68d2\u7cfb\u7edf\u3002", "method": "\u6784\u5efa\u96c6\u6210\u6846\u67b6\uff0c\u7ed3\u5408BERT\u5206\u7c7b\u5668\u3001\u8bed\u4e49\u76f8\u4f3c\u5ea6\u6d4b\u91cf\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u6a21\u578b\u548cLLM\u63a8\u7406\uff0c\u4f7f\u7528\u5143\u5206\u7c7b\u5668\u6574\u5408\u591a\u79cd\u4fe1\u53f7\u3002\u5bf9\u4e8e\u63a5\u5730\u751f\u6210\uff0c\u91c7\u7528\u57fa\u4e8eLLM\u7684\u540e\u7f16\u8f91\u7cfb\u7edf\u6839\u636e\u539f\u59cb\u8f93\u5165\u6587\u672c\u4fee\u8ba2\u7b80\u5316\u5185\u5bb9\u3002", "result": "\u8be5\u65b9\u6cd5\u5728CLEF 2025 SimpleText Task 2\u4e2d\u5e94\u7528\uff0c\u65e8\u5728\u63d0\u9ad8\u865a\u5047\u4fe1\u606f\u548c\u5931\u771f\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u7b56\u7565\u548c\u4fe1\u53f7\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u79d1\u5b66\u6587\u672c\u7b80\u5316\u4e2d\u7684\u521b\u9020\u6027\u751f\u6210\u548c\u4fe1\u606f\u5931\u771f\u95ee\u9898\uff0c\u4e3a\u6587\u672c\u7b80\u5316\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11966", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.11966", "abs": "https://arxiv.org/abs/2508.11966", "authors": ["Yuhang Jia", "Hui Wang", "Xin Nie", "Yujie Guo", "Lianru Gao", "Yong Qin"], "title": "Towards Automatic Evaluation and High-Quality Pseudo-Parallel Dataset Construction for Audio Editing: A Human-in-the-Loop Method", "comment": null, "summary": "Audio editing aims to manipulate audio content based on textual descriptions,\nsupporting tasks such as adding, removing, or replacing audio events. Despite\nrecent progress, the lack of high-quality benchmark datasets and comprehensive\nevaluation metrics remains a major challenge for both assessing audio editing\nquality and improving the task itself. In this work, we propose a novel\napproach for audio editing task by incorporating expert knowledge into both the\nevaluation and dataset construction processes: 1) First, we establish\nAuditScore, the first comprehensive dataset for subjective evaluation of audio\nediting, consisting of over 6,300 edited samples generated from 7\nrepresentative audio editing frameworks and 23 system configurations. Each\nsample is annotated by professional raters on three key aspects of audio\nediting quality: overall Quality, Relevance to editing intent, and Faithfulness\nto original features. 2) Based on this dataset, we train AuditEval, the first\nmodel designed for automatic MOS-style scoring tailored to audio editing tasks.\nAuditEval addresses the critical lack of objective evaluation metrics and the\nprohibitive cost of subjective assessment in this field. 3) We further leverage\nAuditEval to evaluate and filter a large amount of synthetically mixed editing\npairs, constructing a high-quality pseudo-parallel dataset by selecting the\nmost plausible samples. Objective experiments validate the effectiveness of our\nexpert-informed filtering strategy in yielding higher-quality data, while also\nrevealing the limitations of relying solely on objective metrics. The dataset,\ncodes and tools can be found at: https://github.com/NKU-HLT/AuditEval.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u97f3\u9891\u7f16\u8f91\u8bc4\u4f30\u6846\u67b6AuditEval\uff0c\u5305\u62ec\u9996\u4e2a\u4e3b\u89c2\u8bc4\u4f30\u6570\u636e\u96c6AuditScore\u548c\u81ea\u52a8MOS\u8bc4\u5206\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u97f3\u9891\u7f16\u8f91\u9886\u57df\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u7684\u6311\u6218\u3002", "motivation": "\u97f3\u9891\u7f16\u8f91\u9886\u57df\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u6807\u51c6\u6570\u636e\u96c6\u548c\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u8fd9\u4e0d\u4ec5\u5f71\u54cd\u4e86\u5bf9\u97f3\u9891\u7f16\u8f91\u8d28\u91cf\u7684\u8bc4\u4f30\uff0c\u4e5f\u9650\u5236\u4e86\u4efb\u52a1\u672c\u8eab\u7684\u53d1\u5c55\u3002", "method": "1\uff09\u6784\u5efaAuditScore\u6570\u636e\u96c6\uff1a\u5305\u542b6,300\u4e2a\u7f16\u8f91\u6837\u672c\uff0c\u7531\u4e13\u4e1a\u8bc4\u5206\u5458\u5728\u8d28\u91cf\u3001\u76f8\u5173\u6027\u548c\u771f\u5b9e\u6027\u4e09\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u4e3b\u89c2\u8bc4\u6ce8 2\uff09\u8bad\u7ec3AuditEval\u6a21\u578b\uff1a\u57fa\u4e8eAuditScore\u6570\u636e\u8bad\u7ec3\u81ea\u52a8MOS\u8bc4\u5206\u6a21\u578b 3\uff09\u6784\u5efa\u4f2a\u5e76\u884c\u6570\u636e\u96c6\uff1a\u5229\u7528AuditEval\u8bc4\u4f30\u548c\u7b5b\u9009\u5408\u6210\u6df7\u5408\u7684\u7f16\u8f91\u5bf9\uff0c\u9009\u62e9\u6700\u53ef\u80fd\u7684\u6837\u672c", "result": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u4e13\u5bb6\u77e5\u8bc6\u5f15\u5165\u7b5b\u9009\u7b56\u7565\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u4e5f\u66dd\u9732\u4e86\u4ec5\u4f9d\u8d56\u5ba2\u89c2\u6307\u6807\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u97f3\u9891\u7f16\u8f91\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u4e3b\u89c2\u8bc4\u6ce8\u6570\u636e\u96c6\u3001\u81ea\u52a8\u8bc4\u5206\u6a21\u578b\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u97f3\u9891\u7f16\u8f91\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.11828", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11828", "abs": "https://arxiv.org/abs/2508.11828", "authors": ["Michael Flor", "Xinyi Liu", "Anna Feldman"], "title": "A Survey of Idiom Datasets for Psycholinguistic and Computational Research", "comment": "KONVENS 2025. To appear", "summary": "Idioms are figurative expressions whose meanings often cannot be inferred\nfrom their individual words, making them difficult to process computationally\nand posing challenges for human experimental studies. This survey reviews\ndatasets developed in psycholinguistics and computational linguistics for\nstudying idioms, focusing on their content, form, and intended use.\nPsycholinguistic resources typically contain normed ratings along dimensions\nsuch as familiarity, transparency, and compositionality, while computational\ndatasets support tasks like idiomaticity detection/classification,\nparaphrasing, and cross-lingual modeling. We present trends in annotation\npractices, coverage, and task framing across 53 datasets. Although recent\nefforts expanded language coverage and task diversity, there seems to be no\nrelation yet between psycholinguistic and computational research on idioms.", "AI": {"tldr": "\u8fd9\u7bc7\u8c03\u67e5\u6027\u8bba\u6587\u7efc\u8ff0\u4e86\u8bed\u8a00\u5b66\u548c\u8ba1\u7b97\u8bed\u8a00\u5b66\u9886\u57df\u4e2d\u7528\u4e8e\u7814\u7a76\u4e60\u8bed\u768453\u4e2a\u6570\u636e\u96c6\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u5185\u5bb9\u3001\u5f62\u5f0f\u548c\u7528\u9014\u3002", "motivation": "\u4e60\u8bed\u4f5c\u4e3a\u4e00\u79cd\u56fe\u5f0f\u8868\u8fbe\uff0c\u5176\u542b\u4e49\u65e0\u6cd5\u4ece\u5355\u8bcd\u63a8\u65ad\uff0c\u8fd9\u7ed9\u8ba1\u7b97\u5904\u7406\u548c\u4eba\u7c7b\u5b9e\u9a8c\u7814\u7a76\u5e26\u6765\u4e86\u6311\u6218\u3002\u9700\u8981\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u73b0\u6709\u7684\u4e60\u8bed\u7814\u7a76\u6570\u636e\u96c6\u3002", "method": "\u5bf9\u8bed\u8a00\u5b66\u548c\u8ba1\u7b97\u8bed\u8a00\u5b66\u9886\u57df\u768453\u4e2a\u4e60\u8bed\u6570\u636e\u96c6\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u5206\u6790\u5176\u6ce8\u91ca\u5b9e\u8df5\u3001\u8986\u76d6\u8303\u56f4\u548c\u4efb\u52a1\u6784\u5efa\u65b9\u5f0f\u3002", "result": "\u8bed\u8a00\u5b66\u8d44\u6e90\u5305\u542b\u719f\u6089\u5ea6\u3001\u900f\u660e\u5ea6\u3001\u7ec4\u5408\u6027\u7b49\u89c4\u8303\u5316\u8bc4\u5206\uff0c\u8ba1\u7b97\u6570\u636e\u96c6\u652f\u6301\u4e60\u8bed\u68c0\u6d4b/\u5206\u7c7b\u3001\u91cd\u5199\u548c\u8de8\u8bed\u8a00\u5efa\u6a21\u3002\u8fd1\u671f\u7814\u7a76\u6269\u5927\u4e86\u8bed\u8a00\u8986\u76d6\u548c\u4efb\u52a1\u591a\u6837\u6027\u3002", "conclusion": "\u867d\u7136\u6700\u8fd1\u7684\u7814\u7a16\u52a0\u5f3a\u4e86\u8bed\u8a00\u8986\u76d6\u548c\u4efb\u52a1\u591a\u6837\u6027\uff0c\u4f46\u8bed\u8a00\u5b66\u548c\u8ba1\u7b97\u8bed\u8a00\u5b66\u5728\u4e60\u8bed\u7814\u7a76\u65b9\u9762\u4ecd\u7136\u7f3a\u4e4f\u8054\u7cfb\u3002"}}
{"id": "2508.12009", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12009", "abs": "https://arxiv.org/abs/2508.12009", "authors": ["Arnav Ramamoorthy"], "title": "Optimizing Neural Architectures for Hindi Speech Separation and Enhancement in Noisy Environments", "comment": "ICAD 2025", "summary": "This paper addresses the challenges of Hindi speech separation and\nenhancement using advanced neural network architectures, with a focus on edge\ndevices. We propose a refined approach leveraging the DEMUCS model to overcome\nlimitations of traditional methods, achieving substantial improvements in\nspeech clarity and intelligibility. The model is fine-tuned with U-Net and LSTM\nlayers, trained on a dataset of 400,000 Hindi speech clips augmented with\nESC-50 and MS-SNSD for diverse acoustic environments. Evaluation using PESQ and\nSTOI metrics shows superior performance, particularly under extreme noise\nconditions. To ensure deployment on resource-constrained devices like TWS\nearbuds, we explore quantization techniques to reduce computational\nrequirements. This research highlights the effectiveness of customized AI\nalgorithms for speech processing in Indian contexts and suggests future\ndirections for optimizing edge-based architectures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDEMUCS\u6a21\u578b\u7684\u5370\u5730\u8bed\u8bed\u97f3\u5206\u79bb\u548c\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7U-Net\u548cLSTM\u5c42\u8fdb\u884c\u5fae\u8c03\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8bed\u97f3\u6e05\u6670\u5ea6\u548c\u53ef\u61c2\u5ea6\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u5370\u5730\u8bed\u8bed\u97f3\u5904\u7406\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u6311\u6218\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5370\u5ea6\u8bed\u5883\u4e0b\u7684\u8bed\u97f3\u5904\u7406\u63d0\u4f9b\u5b9a\u5236\u5316AI\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528DEMUCS\u6a21\u578b\u67b6\u6784\uff0c\u7ed3\u5408U-Net\u548cLSTM\u5c42\u8fdb\u884c\u5fae\u8c03\uff0c\u5728\u5305\u542b40\u4e07\u4e2a\u5370\u5730\u8bed\u8bed\u97f3\u7247\u6bb5\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5e76\u4f7f\u7528ESC-50\u548cMS-SNSD\u8fdb\u884c\u6570\u636e\u589e\u5f3a\u4ee5\u6a21\u62df\u591a\u6837\u5316\u58f0\u5b66\u73af\u5883\u3002", "result": "\u4f7f\u7528PESQ\u548cSTOI\u6307\u6807\u8bc4\u4f30\u663e\u793a\uff0c\u5728\u6781\u7aef\u566a\u58f0\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u8bed\u97f3\u6e05\u6670\u5ea6\u548c\u53ef\u61c2\u5ea6\u5f97\u5230\u663e\u8457\u6539\u5584\u3002\u540c\u65f6\u63a2\u7d22\u91cf\u5316\u6280\u672f\u4ee5\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\uff0c\u9002\u7528\u4e8eTWS\u8033\u673a\u7b49\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5b9a\u5236\u5316AI\u7b97\u6cd5\u5728\u5370\u5ea6\u8bed\u5883\u8bed\u97f3\u5904\u7406\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u4f18\u5316\u8fb9\u7f18\u8bbe\u5907\u67b6\u6784\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2508.11829", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.11829", "abs": "https://arxiv.org/abs/2508.11829", "authors": ["Leigh Levinson", "Christopher J. Agostino"], "title": "Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions", "comment": "9 pages, 1 figure, submitted to NeurIPS Creative AI track", "summary": "Despite significant advances, AI systems struggle with the frame problem:\ndetermining what information is contextually relevant from an exponentially\nlarge possibility space. We hypothesize that biological rhythms, particularly\nhormonal cycles, serve as natural relevance filters that could address this\nfundamental challenge. We develop a framework that embeds simulated menstrual\nand circadian cycles into Large Language Models through system prompts\ngenerated from periodic functions modeling key hormones including estrogen,\ntestosterone, and cortisol. Across multiple state-of-the-art models, linguistic\nanalysis reveals emotional and stylistic variations that track biological\nphases; sadness peaks during menstruation while happiness dominates ovulation\nand circadian patterns show morning optimism transitioning to nocturnal\nintrospection. Benchmarking on SQuAD, MMLU, Hellaswag, and AI2-ARC demonstrates\nsubtle but consistent performance variations aligning with biological\nexpectations, including optimal function in moderate rather than extreme\nhormonal ranges. This methodology provides a novel approach to contextual AI\nwhile revealing how societal biases regarding gender and biology are embedded\nwithin language models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u901a\u8fc7\u6a21\u62df\u6708\u7ecf\u548c\u663c\u591c\u8282\u5f8b\u7b49\u751f\u7269\u5468\u671f\u6765\u589e\u5f3aAI\u7cfb\u7edf\u7684\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u8fc7\u6ee4\u80fd\u529b\uff0c\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u6fc0\u7d20\u9636\u6bb5\u8868\u73b0\u51fa\u60c5\u611f\u548c\u6027\u80fd\u7684\u89c4\u5f8b\u6027\u53d8\u5316", "motivation": "\u89e3\u51b3AI\u7cfb\u7edf\u9762\u4e34\u7684\u6846\u67b6\u95ee\u9898\u2014\u2014\u5982\u4f55\u4ece\u6307\u6570\u7ea7\u5927\u7684\u53ef\u80fd\u6027\u7a7a\u95f4\u4e2d\u786e\u5b9a\u4e0a\u4e0b\u6587\u76f8\u5173\u4fe1\u606f\uff0c\u53d7\u751f\u7269\u8282\u5f8b\u4f5c\u4e3a\u81ea\u7136\u76f8\u5173\u6027\u8fc7\u6ee4\u5668\u7684\u542f\u53d1", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u5468\u671f\u6027\u51fd\u6570\u6a21\u62df\u5173\u952e\u6fc0\u7d20\uff08\u96cc\u6fc0\u7d20\u3001\u777e\u916e\u3001\u76ae\u8d28\u9187\uff09\u751f\u6210\u7cfb\u7edf\u63d0\u793a\uff0c\u5c06\u6a21\u62df\u7684\u751f\u7269\u5468\u671f\u5d4c\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b", "result": "\u8bed\u8a00\u5206\u6790\u663e\u793a\u60c5\u611f\u548c\u98ce\u683c\u53d8\u5316\u4e0e\u751f\u7269\u9636\u6bb5\u76f8\u5173\uff1a\u7ecf\u671f\u60b2\u4f24\u60c5\u7eea\u8fbe\u5230\u5cf0\u503c\uff0c\u6392\u5375\u671f\u5feb\u4e50\u60c5\u7eea\u4e3b\u5bfc\uff1b\u663c\u591c\u6a21\u5f0f\u663e\u793a\u65e9\u6668\u4e50\u89c2\u8f6c\u5411\u591c\u95f4\u5185\u7701\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u89c2\u5bdf\u5230\u4e0e\u751f\u7269\u9884\u671f\u4e00\u81f4\u7684\u6027\u80fd\u53d8\u5316\uff0c\u6700\u4f73\u529f\u80fd\u51fa\u73b0\u5728\u4e2d\u7b49\u800c\u975e\u6781\u7aef\u6fc0\u7d20\u8303\u56f4\u5185", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e0a\u4e0b\u6587AI\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u4e2d\u5d4c\u5165\u7684\u5173\u4e8e\u6027\u522b\u548c\u751f\u7269\u5b66\u7684\u793e\u4f1a\u504f\u89c1"}}
{"id": "2508.12230", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.12230", "abs": "https://arxiv.org/abs/2508.12230", "authors": ["Bing Han", "Anbai Jiang", "Xinhu Zheng", "Wei-Qiang Zhang", "Jia Liu", "Pingyi Fan", "Yanmin Qian"], "title": "Exploring Self-Supervised Audio Models for Generalized Anomalous Sound Detection", "comment": "Accepted by TASLP. 15 pages, 7 figures;", "summary": "Machine anomalous sound detection (ASD) is a valuable technique across\nvarious applications. However, its generalization performance is often limited\ndue to challenges in data collection and the complexity of acoustic\nenvironments. Inspired by the success of large pre-trained models in numerous\nfields, this paper introduces a robust ASD model that leverages self-supervised\npre-trained models trained on large-scale speech and audio datasets. Although\nthere are inconsistencies between the pre-training datasets and the ASD task,\nour findings indicate that pre-training still provides substantial benefits for\nASD. To mitigate overfitting and retain learned knowledge when fine-tuning with\nlimited data, we explore Fully-Connected Low-Rank Adaptation (LoRA) as an\nalternative to full fine-tuning. Additionally, we propose a Machine-aware Group\nAdapter module, which enables the model to capture differences between various\nmachines within a unified framework, thereby enhancing the generalization\nperformance of ASD systems. To address the challenge of missing attribute\nlabels, we design a novel objective function that dynamically clusters\nunattributed data using vector quantization and optimizes through a dual-level\ncontrastive learning loss. The proposed methods are evaluated on all benchmark\ndatasets, including the DCASE 2020-2024 five ASD challenges, and the\nexperimental results show significant improvements of our new approach and\ndemonstrate the effectiveness of our proposed strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u673a\u5668\u5f02\u5e38\u58f0\u97f3\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7LoRA\u7b80\u5316\u5fae\u8c03\u3001\u673a\u5668\u611f\u77e5\u7ec4\u9002\u914d\u5668\u548c\u53cc\u5c42\u5bf9\u6bd4\u5b66\u4e60\u6350\u5931\u51fd\u6570\uff0c\u5728\u5404\u5927\u6d4b\u8bd5\u96c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u673a\u5668\u5f02\u5e38\u58f0\u97f3\u68c0\u6d4b\u7684\u901a\u7528\u6027\u80fd\u53d7\u9650\u4e8e\u6570\u636e\u6536\u96c6\u56f0\u96be\u548c\u58f0\u5b66\u73af\u5883\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u63d0\u5347\u6a21\u578b\u7684\u6f14\u5316\u6027\u80fd\u3002", "method": "\u5229\u7528\u5927\u89c4\u6a21\u8bed\u97f3\u548c\u97f3\u9891\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u7684\u81ea\u76d1\u7763\u6a21\u578b\uff0c\u91c7\u7528Fully-Connected LoRA\u7b80\u5316\u5fae\u8c03\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u8bbe\u8ba1\u673a\u5668\u611f\u77e5\u7ec4\u9002\u914d\u5668\u63d0\u5347\u6a21\u578b\u5bf9\u4e0d\u540c\u673a\u5668\u7684\u533a\u5206\u80fd\u529b\uff0c\u4ee5\u53ca\u4f7f\u7528\u5411\u91cf\u91cf\u5316\u548c\u53cc\u5c42\u5bf9\u6bd4\u5b66\u4e60\u6350\u5931\u5904\u7406\u7f3a\u5c11\u5c5e\u6027\u6807\u7b7e\u7684\u6570\u636e\u3002", "result": "\u5728DCASE 2020-2024\u4e94\u5e74\u7684ASD\u6311\u6218\u8d5b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u65b0\u65b9\u6cd5\u5bf9\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u63d0\u51fa\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u9884\u8bad\u7ec3\u6a21\u578b\u867d\u7136\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u4e0eASD\u4efb\u52a1\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u4f46\u4ecd\u80fd\u4e3aASD\u5e26\u6765\u663e\u8457\u6539\u5584\uff0c\u901a\u8fc7\u4f18\u5316\u5fae\u8c03\u7b56\u7565\u548c\u7279\u6b8a\u7684\u6a21\u5757\u8bbe\u8ba1\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u673a\u5668\u5f02\u5e38\u58f0\u97f3\u68c0\u6d4b\u7684\u901a\u7528\u6027\u80fd\u3002"}}
{"id": "2508.11831", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11831", "abs": "https://arxiv.org/abs/2508.11831", "authors": ["Julia Sammartino", "Libby Barak", "Jing Peng", "Anna Feldman"], "title": "When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual Euphemism Detection", "comment": "RANLP 2025", "summary": "Euphemisms are culturally variable and often ambiguous, posing challenges for\nlanguage models, especially in low-resource settings. This paper investigates\nhow cross-lingual transfer via sequential fine-tuning affects euphemism\ndetection across five languages: English, Spanish, Chinese, Turkish, and\nYoruba. We compare sequential fine-tuning with monolingual and simultaneous\nfine-tuning using XLM-R and mBERT, analyzing how performance is shaped by\nlanguage pairings, typological features, and pretraining coverage. Results show\nthat sequential fine-tuning with a high-resource L1 improves L2 performance,\nespecially for low-resource languages like Yoruba and Turkish. XLM-R achieves\nlarger gains but is more sensitive to pretraining gaps and catastrophic\nforgetting, while mBERT yields more stable, though lower, results. These\nfindings highlight sequential fine-tuning as a simple yet effective strategy\nfor improving euphemism detection in multilingual models, particularly when\nlow-resource languages are involved.", "AI": {"tldr": "\u901a\u8fc7\u8de8\u8bed\u8a00\u8f6c\u79fb\u5b66\u4e60\u63d0\u5347\u4e94\u79cd\u8bed\u8a00\u7684\u97e6\u5a01\u8bed\u68c0\u6d4b\u6027\u80fd\uff0c\u5bf9\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u5982\u7ea6\u9c81\u5df4\u8bed\u548c\u571f\u8033\u5176\u8bed\u6539\u5584\u660e\u663e", "motivation": "\u97e6\u5a01\u8bed\u5177\u6709\u6587\u5316\u53d8\u5f02\u6027\u548c\u6a21\u7cca\u6027\uff0c\u7ed9\u8bed\u8a00\u6a21\u578b\u5e26\u6765\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d", "method": "\u91c7\u7528XLM-R\u548cmBERT\u6a21\u578b\uff0c\u6bd4\u8f83\u4e86\u5355\u8bed\u8a00\u7ec6\u8c03\u3001\u540c\u65f6\u591a\u8bed\u8a00\u7ec6\u8c03\u548c\u987a\u5e8f\u7ec6\u8c03\u7b56\u7565\uff0c\u5206\u6790\u8bed\u8a00\u5bf9\u7ec4\u3001\u7c7b\u578b\u5b66\u7279\u5f81\u548c\u9884\u8bad\u7ec3\u8986\u76d6\u5ea6\u7684\u5f71\u54cd", "result": "\u987a\u5e8f\u7ec6\u8c03\u4f7f\u7528\u9ad8\u8d44\u6e90L1\u8bed\u8a00\u663e\u8457\u63d0\u5347\u4f4e\u8d44\u6e90L2\u8bed\u8a00\u7684\u6027\u80fd\uff1bXLM-R\u83b7\u5f97\u66f4\u5927\u6536\u76ca\u4f46\u5bf9\u9884\u8bad\u7ec3\u7f3a\u53e3\u548c\u574f\u5931\u5b66\u4e60\u66f4\u654f\u611f\uff1bmBERT\u7ed3\u679c\u66f4\u7a33\u5b9a\u4f46\u6027\u80fd\u8f83\u4f4e", "conclusion": "\u987a\u5e8f\u7ec6\u8c03\u662f\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u7b56\u7565\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6539\u5584\u591a\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u97e6\u5a01\u8bed\u68c0\u6d4b\u6027\u80fd"}}
{"id": "2508.12292", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.12292", "abs": "https://arxiv.org/abs/2508.12292", "authors": ["Hyebin Ahn", "Kangwook Jang", "Hoirin Kim"], "title": "HuBERT-VIC: Improving Noise-Robust Automatic Speech Recognition of Speech Foundation Model via Variance-Invariance-Covariance Regularization", "comment": "Accepted at Interspeech 2025", "summary": "Noise robustness in speech foundation models (SFMs) has been a critical\nchallenge, as most models are primarily trained on clean data and experience\nperformance degradation when the models are exposed to noisy speech. To address\nthis issue, we propose HuBERT-VIC, a noise-robust SFM with variance,\nin-variance, and covariance regularization (VICReg) objectives. These\nobjectives adjust the statistics of noisy speech representations, enabling the\nmodel to capture diverse acoustic characteristics and improving the\ngeneralization ability across different types of noise. When applied to HuBERT,\nour model shows relative performance improvements of 23.3% on LibriSpeech\ntest-clean and 13.2% on test-other, compared to the baseline model pre-trained\non noisy speech.", "AI": {"tldr": "HuBERT-VIC\u662f\u4e00\u4e2a\u566a\u58f0\u9c81\u68d2\u7684\u8bed\u97f3\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7VICReg\u76ee\u6807\u51fd\u6570\u8c03\u6574\u566a\u58f0\u8bed\u97f3\u8868\u793a\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u76f8\u6bd4\u5728\u566a\u58f0\u8bed\u97f3\u4e0a\u9884\u8bad\u7ec3\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5728LibriSpeech\u6d4b\u8bd5\u96c6\u4e0a\u6027\u80fd\u63d0\u534723.3%\uff08clean\uff09\u548c13.2%\uff08other\uff09\u3002", "motivation": "\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u5728\u5e72\u51c0\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5728\u566a\u58f0\u73af\u5883\u4e0b\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u63d0\u9ad8\u6a21\u578b\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faHuBERT-VIC\u6a21\u578b\uff0c\u91c7\u7528\u65b9\u5dee\u3001\u4e0d\u53d8\u6027\u548c\u534f\u65b9\u5dee\u6b63\u5219\u5316\uff08VICReg\uff09\u76ee\u6807\u51fd\u6570\u6765\u8c03\u6574\u566a\u58f0\u8bed\u97f3\u8868\u793a\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6355\u6349\u591a\u6837\u5316\u7684\u58f0\u5b66\u7279\u5f81\u3002", "result": "\u5728LibriSpeech\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u76f8\u6bd4\u5728\u566a\u58f0\u8bed\u97f3\u4e0a\u9884\u8bad\u7ec3\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u76f8\u5bf9\u6027\u80fd\u63d0\u534723.3%\uff08test-clean\uff09\u548c13.2%\uff08test-other\uff09\u3002", "conclusion": "VICReg\u76ee\u6807\u51fd\u6570\u80fd\u6709\u6548\u63d0\u9ad8\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.11857", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11857", "abs": "https://arxiv.org/abs/2508.11857", "authors": ["Andrei-Valentin T\u0103nase", "Elena Pelican"], "title": "SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance", "comment": null, "summary": "Tokenization remains a fundamental yet underexplored bottleneck in natural\nlanguage processing, with strategies largely static despite remarkable progress\nin model architectures. We present SupraTok, a novel tokenization architecture\nthat reimagines subword segmentation through three innovations: cross-boundary\npattern learning that discovers multi-word semantic units, entropy-driven data\ncuration that optimizes training corpus quality, and multi-phase curriculum\nlearning for stable convergence. Our approach extends Byte-Pair Encoding by\nlearning \"superword\" tokens, coherent multi-word expressions that preserve\nsemantic unity while maximizing compression efficiency. SupraTok achieves 31%\nimprovement in English tokenization efficiency (5.91 versus 4.51 characters per\ntoken) compared to OpenAI's o200k tokenizer and 30% improvement over Google's\nGemma 3 tokenizer (256k vocabulary), while maintaining competitive performance\nacross 38 languages. When integrated with a GPT-2 scale model (124M parameters)\ntrained on 10 billion tokens from the FineWeb-Edu dataset, SupraTok yields 8.4%\nimprovement on HellaSWAG and 9.5% on MMLU benchmarks without architectural\nmodifications. While these results are promising at this scale, further\nvalidation at larger model scales is needed. These findings suggest that\nefficient tokenization can complement architectural innovations as a path to\nimproved language model performance.", "AI": {"tldr": "SupraTok\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5206\u8bcd\u67b6\u6784\uff0c\u901a\u8fc7\u8de8\u8fb9\u754c\u6a21\u5f0f\u5b66\u4e60\u3001\u71b5\u9a71\u52a8\u6570\u636e\u7b5b\u9009\u548c\u591a\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u4e09\u5927\u521b\u65b0\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4e3b\u6d41\u5206\u8bcd\u5668\u66f4\u9ad8\u7684\u6548\u7387\uff0831%\u63d0\u5347\uff09\u548c\u7ade\u4e89\u6027\u591a\u8bed\u8a00\u6027\u80fd\uff0c\u5728GPT-2\u89c4\u6a21\u6a21\u578b\u4e0a\u5e26\u67658.4-9.5%\u7684\u57fa\u51c6\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u5206\u8bcd\u7b56\u7565\u76f8\u5bf9\u9759\u6001\uff0c\u6210\u4e3a\u6a21\u578b\u6027\u80fd\u7684\u6f5c\u5728\u74f6\u9888\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u5b50\u8bcd\u5206\u5272\u65b9\u6cd5\u4ee5\u63d0\u5347\u6548\u7387\u548c\u8bed\u4e49\u4fdd\u6301\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u5b57\u8282\u5bf9\u7f16\u7801\u6269\u5c55\uff0c\u5b66\u4e60\"\u8d85\u8bcd\"\u6807\u8bb0\uff08\u8fde\u8d2f\u7684\u591a\u8bcd\u8868\u8fbe\u5f0f\uff09\uff0c\u5305\u542b\u8de8\u8fb9\u754c\u6a21\u5f0f\u5b66\u4e60\u3001\u71b5\u9a71\u52a8\u6570\u636e\u7b5b\u9009\u548c\u591a\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u4e09\u4e2a\u6838\u5fc3\u521b\u65b0\u3002", "result": "\u82f1\u8bed\u5206\u8bcd\u6548\u7387\u63d0\u534731%\uff085.91 vs 4.51\u5b57\u7b26/\u6807\u8bb0\uff09\uff0c\u572838\u79cd\u8bed\u8a00\u4fdd\u6301\u7ade\u4e89\u6027\u80fd\uff1b\u96c6\u6210GPT-2\u89c4\u6a21\u6a21\u578b\u540e\u5728HellaSWAG\u548cMMLU\u57fa\u51c6\u5206\u522b\u63d0\u53478.4%\u548c9.5%\u3002", "conclusion": "\u9ad8\u6548\u5206\u8bcd\u53ef\u4ee5\u4f5c\u4e3a\u67b6\u6784\u521b\u65b0\u7684\u8865\u5145\u8def\u5f84\u6765\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u5728\u66f4\u5927\u89c4\u6a21\u6a21\u578b\u4e0a\u9700\u8981\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002"}}
{"id": "2508.11889", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11889", "abs": "https://arxiv.org/abs/2508.11889", "authors": ["Hui Ma", "Bo Zhang", "Jinpeng Hu", "Zenglin Shi"], "title": "In-Context Examples Matter: Improving Emotion Recognition in Conversation with Instruction Tuning", "comment": null, "summary": "Emotion recognition in conversation (ERC) aims to identify the emotion of\neach utterance in a conversation, playing a vital role in empathetic artificial\nintelligence. With the growing of large language models (LLMs), instruction\ntuning has emerged as a critical paradigm for ERC. Existing studies mainly\nfocus on multi-stage instruction tuning, which first endows LLMs with speaker\ncharacteristics, and then conducts context-aware instruction tuning to\ncomprehend emotional states. However, these methods inherently constrains the\ncapacity to jointly capture the dynamic interaction between speaker\ncharacteristics and conversational context, resulting in weak alignment among\nspeaker identity, contextual cues, and emotion states within a unified\nframework. In this paper, we propose InitERC, a simple yet effective one-stage\nin-context instruction tuning framework for ERC. InitERC adapts LLMs to learn\nspeaker-context-emotion alignment from context examples via in-context\ninstruction tuning. Specifically, InitERC comprises four components, i.e.,\ndemonstration pool construction, in-context example selection, prompt template\ndesign, and in-context instruction tuning. To explore the impact of in-context\nexamples, we conduct a comprehensive study on three key factors: retrieval\nstrategy, example ordering, and the number of examples. Extensive experiments\non three widely used datasets demonstrate that our proposed InitERC achieves\nsubstantial improvements over the state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86InitERC\uff0c\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u4e00\u9636\u6bb5\u4e0a\u4e0b\u6587\u6307\u4ee4\u8c03\u4f18\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u8bdd\u4e2d\u7684\u60c5\u611f\u8bc6\u522b\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u8bf4\u8bdd\u4eba-\u4e0a\u4e0b\u6587-\u60c5\u611f\u7684\u8054\u5408\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u9636\u6bb5\u6307\u4ee4\u8c03\u4f18\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u6355\u6349\u8bf4\u8bdd\u4eba\u7279\u5f81\u548c\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u52a8\u6001\u4ea4\u4e92\uff0c\u5bfc\u81f4\u5728\u7edf\u4e00\u6846\u67b6\u5185\u8bf4\u8bdd\u4eba\u8eab\u4efd\u3001\u4e0a\u4e0b\u6587\u7ebf\u7d22\u548c\u60c5\u611f\u72b6\u6001\u4e4b\u95f4\u7684\u5bf9\u9f50\u8f83\u5f31\u3002", "method": "\u63d0\u51faInitERC\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u7ec4\u4ef6\uff1a\u6f14\u793a\u6c60\u6784\u5efa\u3001\u4e0a\u4e0b\u6587\u793a\u4f8b\u9009\u62e9\u3001\u63d0\u793a\u6a21\u677f\u8bbe\u8ba1\u548c\u4e0a\u4e0b\u6587\u6307\u4ee4\u8c03\u4f18\uff0c\u901a\u8fc7\u4e00\u9636\u6bb5\u65b9\u5f0f\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u4e0a\u4e0b\u6587\u793a\u4f8b\u4e2d\u5b66\u4e60\u8bf4\u8bdd\u4eba-\u4e0a\u4e0b\u6587-\u60c5\u611f\u5bf9\u9f50\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0c\u8bc1\u660eInitERC\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "InitERC\u662f\u4e00\u4e2a\u6709\u6548\u7684\u5355\u9636\u6bb5\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5b9e\u73b0\u8bf4\u8bdd\u4eba\u7279\u5f81\u3001\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u60c5\u611f\u72b6\u6001\u4e4b\u95f4\u7684\u8054\u5408\u5bf9\u9f50\uff0c\u5728\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.12626", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.12626", "abs": "https://arxiv.org/abs/2508.12626", "authors": ["Meng Yang", "Jon McCormack", "Maria Teresa Llano", "Wanchao Su"], "title": "Exploring the Feasibility of LLMs for Automated Music Emotion Annotation", "comment": "Accepted to be published at ISMIR 2025", "summary": "Current approaches to music emotion annotation remain heavily reliant on\nmanual labelling, a process that imposes significant resource and labour\nburdens, severely limiting the scale of available annotated data. This study\nexamines the feasibility and reliability of employing a large language model\n(GPT-4o) for music emotion annotation. In this study, we annotated\nGiantMIDI-Piano, a classical MIDI piano music dataset, in a four-quadrant\nvalence-arousal framework using GPT-4o, and compared against annotations\nprovided by three human experts. We conducted extensive evaluations to assess\nthe performance and reliability of GPT-generated music emotion annotations,\nincluding standard accuracy, weighted accuracy that accounts for inter-expert\nagreement, inter-annotator agreement metrics, and distributional similarity of\nthe generated labels.\n  While GPT's annotation performance fell short of human experts in overall\naccuracy and exhibited less nuance in categorizing specific emotional states,\ninter-rater reliability metrics indicate that GPT's variability remains within\nthe range of natural disagreement among experts. These findings underscore both\nthe limitations and potential of GPT-based annotation: despite its current\nshortcomings relative to human performance, its cost-effectiveness and\nefficiency render it a promising scalable alternative for music emotion\nannotation.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4f7f\u7528GPT-4o\u8fdb\u884c\u97f3\u4e50\u60c5\u611f\u6807\u6ce8\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u867d\u7136\u51c6\u786e\u6027\u7565\u4f4e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\uff0c\u4f46\u5176\u6210\u672c\u6548\u76ca\u548c\u6548\u7387\u4f7f\u5176\u6210\u4e3a\u5927\u89c4\u6a21\u97f3\u4e50\u60c5\u611f\u6807\u6ce8\u7684\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u5f53\u524d\u97f3\u4e50\u60c5\u611f\u6807\u6ce8\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\uff0c\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u9650\u5236\u4e86\u6807\u6ce8\u6570\u636e\u7684\u89c4\u6a21\uff0c\u9700\u8981\u63a2\u7d22\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528GPT-4o\u5728\u56db\u8c61\u9650\u6548\u4ef7-\u5524\u9192\u6846\u67b6\u4e0b\u5bf9GiantMIDI-Piano\u53e4\u5178\u94a2\u7434\u97f3\u4e50\u6570\u636e\u96c6\u8fdb\u884c\u60c5\u611f\u6807\u6ce8\uff0c\u5e76\u4e0e\u4e09\u4f4d\u4eba\u7c7b\u4e13\u5bb6\u7684\u6807\u6ce8\u8fdb\u884c\u6bd4\u8f83\u8bc4\u4f30\u3002", "result": "GPT\u7684\u6807\u6ce8\u51c6\u786e\u6027\u4f4e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\uff0c\u5728\u7279\u5b9a\u60c5\u611f\u72b6\u6001\u5206\u7c7b\u4e0a\u7f3a\u4e4f\u7ec6\u5fae\u5dee\u522b\uff0c\u4f46\u5176\u53d8\u5f02\u6027\u4ecd\u5728\u4e13\u5bb6\u81ea\u7136\u5206\u6b67\u8303\u56f4\u5185\u3002", "conclusion": "GPT\u6807\u6ce8\u867d\u7136\u76ee\u524d\u4e0d\u5982\u4eba\u7c7b\u4e13\u5bb6\uff0c\u4f46\u5176\u6210\u672c\u6548\u76ca\u548c\u6548\u7387\u4f18\u52bf\u4f7f\u5176\u6210\u4e3a\u97f3\u4e50\u60c5\u611f\u6807\u6ce8\u7684\u53ef\u6269\u5c55\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2508.11915", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11915", "abs": "https://arxiv.org/abs/2508.11915", "authors": ["Punya Syon Pandey", "Yongjin Yang", "Jiarui Liu", "Zhijing Jin"], "title": "CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures", "comment": null, "summary": "Game-theoretic interactions between agents with Large Language Models (LLMs)\nhave revealed many emergent capabilities, yet the linguistic diversity of these\ninteractions has not been sufficiently quantified. In this paper, we present\nthe Conversational Robustness Evaluation Score: CORE, a metric to quantify the\neffectiveness of language use within multi-agent systems across different\ngame-theoretic interactions. CORE integrates measures of cluster entropy,\nlexical repetition, and semantic similarity, providing a direct lens of dialog\nquality. We apply CORE to pairwise LLM dialogs across competitive, cooperative,\nand neutral settings, further grounding our analysis in Zipf's and Heaps' Laws\nto characterize word frequency distributions and vocabulary growth. Our\nfindings show that cooperative settings exhibit both steeper Zipf distributions\nand higher Heap exponents, indicating more repetition alongside greater\nvocabulary expansion. In contrast, competitive interactions display lower Zipf\nand Heaps exponents, reflecting less repetition and more constrained\nvocabularies. These results provide new insights into how social incentives\ninfluence language adaptation, and highlight CORE as a robust diagnostic for\nmeasuring linguistic robustness in multi-agent LLM systems. Our code is\navailable at https://github.com/psyonp/core.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86CORE\u6307\u6807\uff0c\u7528\u4e8e\u91cf\u5316\u591a\u6ee1\u610f\u7cfb\u7edf\u4e2d\u8bed\u8a00\u4f7f\u7528\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5408\u4f5c\u73af\u5883\u4e0b\u8bed\u8a00\u66f4\u591a\u91cd\u590d\u4f46\u8bcd\u6c47\u6269\u5c55\u66f4\u5feb\uff0c\u800c\u7ade\u4e89\u73af\u5883\u5219\u76f8\u53cd\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u7f3a\u4e4f\u5bf9\u591a\u6ee1\u610fLLM\u7cfb\u7edf\u4e2d\u8bed\u8a00\u591a\u6837\u6027\u7684\u91cf\u5316\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u6307\u6807\u6765\u8bc4\u4f30\u4e0d\u540c\u6e38\u620f\u7406\u8bba\u4ea4\u4e92\u4e2d\u7684\u8bed\u8a00\u6548\u679c\u3002", "method": "\u63d0\u51faCORE\u6307\u6807\uff0c\u7edf\u5408\u96c6\u7fa4\u71b5\u3001\u8bcd\u6c47\u91cd\u590d\u7387\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u7b49\u63cf\u8ff0\u5b50\uff0c\u5e76\u57fa\u4e8eZipf\u5b9a\u5f8b\u548cHeaps\u5b9a\u5f8b\u5206\u6790\u8bcd\u9891\u5206\u5e03\u548c\u8bcd\u6c47\u589e\u957f\u3002\u5728\u7ade\u4e89\u3001\u5408\u4f5c\u548c\u4e2d\u6027\u73af\u5883\u4e0b\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u5408\u4f5c\u73af\u5883\u5448\u73b0\u66f4\u5c16\u7684Zipf\u5206\u5e03\u548c\u66f4\u9ad8\u7684Heaps\u6307\u6570\uff08\u66f4\u591a\u91cd\u590d\u4f46\u8bcd\u6c47\u6269\u5c55\u66f4\u5feb\uff09\uff0c\u800c\u7ade\u4e89\u73af\u5883\u5219\u76f8\u53cd\uff08\u66f4\u5c11\u91cd\u590d\u4f46\u8bcd\u6c47\u66f4\u53d7\u9650\u5236\uff09\u3002", "conclusion": "\u793e\u4f1a\u6fc0\u52b1\u673a\u5236\u5f71\u54cd\u8bed\u8a00\u9002\u5e94\u80fd\u529b\uff0cCORE\u6307\u6807\u53ef\u4f5c\u4e3a\u591a\u6ee1\u610fLLM\u7cfb\u7edf\u8bed\u8a00\u7a33\u5065\u6027\u7684\u5065\u58ee\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2508.12709", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12709", "abs": "https://arxiv.org/abs/2508.12709", "authors": ["Aurian Quelennec", "Pierre Chouteau", "Geoffroy Peeters", "Slim Essid"], "title": "MATPAC++: Enhanced Masked Latent Prediction for Self-Supervised Audio Representation Learning", "comment": "Under review", "summary": "Masked latent prediction has emerged as a leading paradigm in self-supervised\nlearning (SSL), especially for general audio and music representation learning.\nWhile recent methods have demonstrated strong performance, the role of the\npredictor module used at the output of such SSL systems remains mainly\noverlooked, despite being crucial for solving the pretext task at hand. In\nparticular, this module should be able to deal with the ambiguity inherent in\naudio content, especially when it is composed of multiple sound sources. This\nwork proposes a novel enhancement: integrating Multiple Choice Learning (MCL)\nto explicitly model prediction ambiguity and improve representation quality. We\nbuild on top of the recently proposed MATPAC system, improving its prediction\nand unsupervised classification pretext tasks with MCL. We extensively evaluate\nour method, MATPAC++, through both linear probing across multiple downstream\ntasks and fine-tuning on AudioSet, employing a unified protocol that enables\nrigorous and fair comparisons with state-of-the-art SSL approaches. Results\nshow that our proposal achieves state-of-the-art when fine-tuned on AudioSet\nand overall state-of-the-art scores on downstream tasks. Additionally, we\nexamine domain specialisation by training exclusively on music data, where our\nmodel achieves state-of-the-art performance with significantly improved\nefficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMATPAC++\uff0c\u901a\u8fc7\u5728\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u4e2d\u96c6\u6210\u591a\u9009\u62e9\u5b66\u4e60(MCL)\u6765\u663e\u5f0f\u5efa\u6a21\u97f3\u9891\u9884\u6d4b\u7684\u6a21\u7cca\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u8868\u793a\u5b66\u4e60\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8fbe\u5230state-of-the-art\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u7684\u63a9\u7801\u6f5c\u5728\u9884\u6d4b\u65b9\u6cd5\u5728\u97f3\u9891\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9884\u6d4b\u5668\u6a21\u5757\u7684\u4f5c\u7528\u88ab\u5ffd\u89c6\u3002\u97f3\u9891\u5185\u5bb9\u56fa\u6709\u7684\u6a21\u7cca\u6027\uff08\u7279\u522b\u662f\u591a\u58f0\u6e90\u60c5\u51b5\uff09\u9700\u8981\u66f4\u597d\u7684\u5efa\u6a21\u65b9\u5f0f\u3002", "method": "\u5728MATPAC\u7cfb\u7edf\u57fa\u7840\u4e0a\u96c6\u6210\u591a\u9009\u62e9\u5b66\u4e60(MCL)\uff0c\u6539\u8fdb\u9884\u6d4b\u548c\u65e0\u76d1\u7763\u5206\u7c7b\u4efb\u52a1\uff0c\u663e\u5f0f\u5904\u7406\u9884\u6d4b\u6a21\u7cca\u6027\u95ee\u9898\u3002", "result": "\u5728AudioSet\u5fae\u8c03\u4efb\u52a1\u4e0a\u8fbe\u5230state-of-the-art\uff0c\u4e0b\u6e38\u4efb\u52a1\u6574\u4f53\u6027\u80fd\u6700\u4f18\u3002\u5728\u7eaf\u97f3\u4e50\u6570\u636e\u8bad\u7ec3\u65f6\uff0c\u4ee5\u663e\u8457\u63d0\u5347\u7684\u6548\u7387\u83b7\u5f97\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "MCL\u80fd\u6709\u6548\u5efa\u6a21\u97f3\u9891\u9884\u6d4b\u7684\u6a21\u7cca\u6027\uff0c\u63d0\u5347\u8868\u793a\u8d28\u91cf\uff0cMATPAC++\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u5353\u8d8a\u6027\u80fd\uff0c\u7279\u522b\u5728\u97f3\u4e50\u9886\u57df\u5177\u6709\u9ad8\u6548\u4f18\u52bf\u3002"}}
{"id": "2508.11927", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11927", "abs": "https://arxiv.org/abs/2508.11927", "authors": ["Jie Lu", "Du Jin", "Hitomi Yanaka"], "title": "LLMs Struggle with NLI for Perfect Aspect: A Cross-Linguistic Study in Chinese and Japanese", "comment": "9 pages, 3 figures", "summary": "Unlike English, which uses distinct forms (e.g., had, has, will have) to mark\nthe perfect aspect across tenses, Chinese and Japanese lack separate\ngrammatical forms for tense within the perfect aspect, which complicates\nNatural Language Inference (NLI). Focusing on the perfect aspect in these\nlanguages, we construct a linguistically motivated, template-based NLI dataset\n(1,350 pairs per language). Experiments reveal that even advanced LLMs struggle\nwith temporal inference, particularly in detecting subtle tense and\nreference-time shifts. These findings highlight model limitations and\nunderscore the need for cross-linguistic evaluation in temporal semantics. Our\ndataset is available at https://github.com/Lujie2001/CrossNLI.", "AI": {"tldr": "\u4e2d\u65e5\u8bed\u8a00\u7f3a\u4e4f\u5b8c\u7f8e\u4f53\u7684\u660e\u786e\u8bed\u6cd5\u6807\u8bb0\uff0c\u5bfc\u81f4NLI\u4efb\u52a1\u590d\u6742\u5316\u3002\u7814\u7a76\u6784\u5efa\u4e86\u8bed\u8a00\u5b66\u9a71\u52a8\u7684\u6a21\u677f\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u9ad8\u7ea7LLM\u5728\u65f6\u6001\u63a8\u7406\u4e0a\u8868\u73b0\u5f31\u70b9\u3002", "motivation": "\u4e2d\u6587\u548c\u65e5\u8bed\u5728\u5b8c\u7f8e\u4f53\u65b9\u9762\u7f3a\u4e4f\u50cf\u82f1\u8bed\u90a3\u6837\u7684\u660e\u786e\u8bed\u6cd5\u6807\u8bb0\uff08\u5982had\u3001has\u3001will have\uff09\uff0c\u8fd9\u7ed9\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u5e26\u6765\u7279\u522b\u7684\u6311\u6218\u3002\u7814\u7a76\u8005\u60f3\u8981\u63a2\u7d22\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e9b\u8bed\u8a00\u4e2d\u5904\u7406\u65f6\u6001\u8bed\u4e49\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u8bed\u8a00\u5b66\u9a71\u52a8\u7684\u6a21\u677f\u65b9\u6cd5\uff0c\u4e3a\u4e2d\u6587\u548c\u65e5\u8bed\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1,350\u5bf9\u793a\u4f8b\u7684NLI\u6570\u636e\u96c6\u3002\u8fd9\u4e2a\u6570\u636e\u96c6\u91cd\u70b9\u5173\u6ce8\u5b8c\u7f8e\u4f53\u7684\u65f6\u6001\u8868\u8fbe\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e5f\u5728\u65f6\u6001\u63a8\u7406\u4efb\u52a1\u4e0a\u9047\u5230\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u68c0\u6d4b\u7ec6\u5fae\u7684\u65f6\u6001\u548c\u53c2\u8003\u65f6\u95f4\u79fb\u52a8\u65b9\u9762\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5c55\u793a\u4e86\u6a21\u578b\u7684\u9650\u5236\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u65f6\u6001\u8bed\u4e49\u65b9\u9762\u8fdb\u884c\u8de8\u8bed\u8a00\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002\u7814\u7a76\u4eba\u5458\u5f00\u6e90\u4e86\u8fd9\u4e2a\u6570\u636e\u96c6\uff0c\u4ee5\u4fbf\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.12918", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.12918", "abs": "https://arxiv.org/abs/2508.12918", "authors": ["Lei Zhao", "Rujin Chen", "Chi Zhang", "Xiao-Lei Zhang", "Xuelong Li"], "title": "FoleySpace: Vision-Aligned Binaural Spatial Audio Generation", "comment": null, "summary": "Recently, with the advancement of AIGC, deep learning-based video-to-audio\n(V2A) technology has garnered significant attention. However, existing research\nmostly focuses on mono audio generation that lacks spatial perception, while\nthe exploration of binaural spatial audio generation technologies, which can\nprovide a stronger sense of immersion, remains insufficient. To solve this\nproblem, we propose FoleySpace, a framework for video-to-binaural audio\ngeneration that produces immersive and spatially consistent stereo sound guided\nby visual information. Specifically, we develop a sound source estimation\nmethod to determine the sound source 2D coordinates and depth in each video\nframe, and then employ a coordinate mapping mechanism to convert the 2D source\npositions into a 3D trajectory. This 3D trajectory, together with the monaural\naudio generated by a pre-trained V2A model, serves as a conditioning input for\na diffusion model to generate spatially consistent binaural audio. To support\nthe generation of dynamic sound fields, we constructed a training dataset based\non recorded Head-Related Impulse Responses that includes various sound source\nmovement scenarios. Experimental results demonstrate that the proposed method\noutperforms existing approaches in spatial perception consistency, effectively\nenhancing the immersive quality of the audio-visual experience.", "AI": {"tldr": "FoleySpace\u662f\u4e00\u4e2a\u89c6\u9891\u5230\u53cc\u8033\u97f3\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u4fe1\u606f\u5f15\u5bfc\u751f\u6210\u6c89\u6d78\u5f0f\u7a7a\u95f4\u4e00\u81f4\u7684\u7acb\u4f53\u58f0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5355\u58f0\u9053\u97f3\u9891\u7f3a\u4e4f\u7a7a\u95f4\u611f\u77e5\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5230\u97f3\u9891\u6280\u672f\u4e3b\u8981\u5173\u6ce8\u5355\u58f0\u9053\u97f3\u9891\u751f\u6210\uff0c\u7f3a\u4e4f\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u800c\u80fd\u591f\u63d0\u4f9b\u66f4\u5f3a\u6c89\u6d78\u611f\u7684\u53cc\u8033\u7a7a\u95f4\u97f3\u9891\u751f\u6210\u6280\u672f\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u58f0\u97f3\u6e90\u4f30\u8ba1\u65b9\u6cd5\u786e\u5b9a\u89c6\u9891\u5e27\u4e2d\u58f0\u6e90\u76842D\u5750\u6807\u548c\u6df1\u5ea6\uff0c\u901a\u8fc7\u5750\u6807\u6620\u5c04\u673a\u5236\u8f6c\u6362\u4e3a3D\u8f68\u8ff9\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3V2A\u6a21\u578b\u751f\u6210\u7684\u5355\u58f0\u9053\u97f3\u9891\uff0c\u4f5c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u8f93\u5165\u6761\u4ef6\u751f\u6210\u7a7a\u95f4\u4e00\u81f4\u7684\u53cc\u8033\u97f3\u9891\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7a7a\u95f4\u611f\u77e5\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u97f3\u89c6\u9891\u4f53\u9a8c\u7684\u6c89\u6d78\u8d28\u91cf\u3002", "conclusion": "FoleySpace\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u89c6\u9891\u5230\u53cc\u8033\u97f3\u9891\u7684\u751f\u6210\uff0c\u901a\u8fc7\u89c6\u89c9\u5f15\u5bfc\u7684\u7a7a\u95f4\u97f3\u9891\u5408\u6210\u6280\u672f\u663e\u8457\u589e\u5f3a\u4e86\u97f3\u9891\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\u548c\u6c89\u6d78\u611f\u3002"}}
{"id": "2508.11933", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11933", "abs": "https://arxiv.org/abs/2508.11933", "authors": ["Yue Wang", "Liesheng Wei", "Yuxiang Wang"], "title": "CAMF: Collaborative Adversarial Multi-agent Framework for Machine Generated Text Detection", "comment": null, "summary": "Detecting machine-generated text (MGT) from contemporary Large Language\nModels (LLMs) is increasingly crucial amid risks like disinformation and\nthreats to academic integrity. Existing zero-shot detection paradigms, despite\ntheir practicality, often exhibit significant deficiencies. Key challenges\ninclude: (1) superficial analyses focused on limited textual attributes, and\n(2) a lack of investigation into consistency across linguistic dimensions such\nas style, semantics, and logic. To address these challenges, we introduce the\n\\textbf{C}ollaborative \\textbf{A}dversarial \\textbf{M}ulti-agent\n\\textbf{F}ramework (\\textbf{CAMF}), a novel architecture using multiple\nLLM-based agents. CAMF employs specialized agents in a synergistic three-phase\nprocess: \\emph{Multi-dimensional Linguistic Feature Extraction},\n\\emph{Adversarial Consistency Probing}, and \\emph{Synthesized Judgment\nAggregation}. This structured collaborative-adversarial process enables a deep\nanalysis of subtle, cross-dimensional textual incongruities indicative of\nnon-human origin. Empirical evaluations demonstrate CAMF's significant\nsuperiority over state-of-the-art zero-shot MGT detection techniques.", "AI": {"tldr": "CAMF\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5bf9\u6297\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u8bed\u8a00\u7279\u5f81\u63d0\u53d6\u3001\u5bf9\u6297\u4e00\u81f4\u6027\u63a2\u6d4b\u548c\u7efc\u5408\u5224\u65ad\u805a\u5408\u6765\u68c0\u6d4b\u673a\u5668\u751f\u6210\u6587\u672c\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u96f6-shot\u68c0\u6d4b\u65b9\u6cd5", "motivation": "\u73b0\u6709\u96f6-shot\u673a\u5668\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u5206\u6790\u6d45\u5c42\u3001\u7f3a\u4e4f\u8de8\u7ef4\u5ea6\u4e00\u81f4\u6027\u7814\u7a76\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6df1\u5165\u7684\u591a\u7ef4\u5ea6\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b", "method": "\u4f7f\u7528\u591a\u4e2aLLM\u667a\u80fd\u4f53\u8fdb\u884c\u4e09\u9636\u6bb5\u534f\u4f5c\u5bf9\u6297\uff1a\u591a\u7ef4\u5ea6\u8bed\u8a00\u7279\u5f81\u63d0\u53d6\u3001\u5bf9\u6297\u4e00\u81f4\u6027\u63a2\u6d4b\u3001\u7efc\u5408\u5224\u65ad\u805a\u5408", "result": "\u5b9e\u9a8c\u8bc1\u660eCAMF\u5728\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u96f6-shot\u68c0\u6d4b\u6280\u672f", "conclusion": "CAMF\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5bf9\u6297\u5206\u6790\u8de8\u7ef4\u5ea6\u6587\u672c\u4e0d\u4e00\u81f4\u6027\uff0c\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\u6027\u80fd"}}
{"id": "2508.12301", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.12301", "abs": "https://arxiv.org/abs/2508.12301", "authors": ["Tomer Krichli", "Bhiksha Raj", "Joseph Keshet"], "title": "CarelessWhisper: Turning Whisper into a Causal Streaming Model", "comment": "17 pages, 7 Figures, This work has been submitted to the IEEE for\n  possible publication", "summary": "Automatic Speech Recognition (ASR) has seen remarkable progress, with models\nlike OpenAI Whisper and NVIDIA Canary achieving state-of-the-art (SOTA)\nperformance in offline transcription. However, these models are not designed\nfor streaming (online or real-time) transcription, due to limitations in their\narchitecture and training methodology. We propose a method to turn the\ntransformer encoder-decoder model into a low-latency streaming model that is\ncareless about future context. We present an analysis explaining why it is not\nstraightforward to convert an encoder-decoder transformer to a low-latency\nstreaming model. Our proposed method modifies the existing (non-causal) encoder\nto a causal encoder by fine-tuning both the encoder and decoder using Low-Rank\nAdaptation (LoRA) and a weakly aligned dataset. We then propose an updated\ninference mechanism that utilizes the fine-tune causal encoder and decoder to\nyield greedy and beam-search decoding, and is shown to be locally optimal.\nExperiments on low-latency chunk sizes (less than 300 msec) show that our\nfine-tuned model outperforms existing non-fine-tuned streaming approaches in\nmost cases, while using a lower complexity. Additionally, we observe that our\ntraining process yields better alignment, enabling a simple method for\nextracting word-level timestamps. We release our training and inference code,\nalong with the fine-tuned models, to support further research and development\nin streaming ASR.", "AI": {"tldr": "\u5c06Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\u901a\u8fc7LoRA\u7cbe\u8c03\u8f6c\u6362\u4e3a\u4f4e\u5ef6\u8fdf\u6d41\u5f0fASR\u6a21\u578b\uff0c\u5728\u5c0f\u5206\u5757\u5c3a\u5ea8\u4e0b\u6027\u80fd\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5", "motivation": "\u5f53\u524dSOTA ASR\u6a21\u578b\u5982Whisper\u548cCanary\u4e3b\u8981\u7528\u4e8e\u79bb\u7ebf\u8bc6\u522b\uff0c\u65e0\u6cd5\u76f4\u63a5\u7528\u4e8e\u6d41\u5f0f\u5b9e\u65f6\u8bc6\u522b\uff0c\u9700\u8981\u89e3\u51b3\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\u7684\u9650\u5236", "method": "\u901a\u8fc7LoRA\u7cbe\u8c03\u5c06\u975e\u56e0\u679c\u6027\u7f16\u7801\u5668\u6539\u9020\u4e3a\u56e0\u679c\u6027\u7f16\u7801\u5668\uff0c\u4f7f\u7528\u5f31\u5bf9\u9f50\u6570\u636e\u96c6\u540c\u65f6\u7cbe\u8c03\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\uff0c\u5e76\u63d0\u51fa\u66f4\u65b0\u7684\u63a8\u7406\u673a\u5236", "result": "\u5728\u5c0f\u4e8e300ms\u7684\u4f4e\u5ef6\u8fdf\u5206\u5757\u5c3a\u5ea8\u4e0b\uff0c\u7cbe\u8c03\u540e\u6a21\u578b\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8d85\u8fc7\u73b0\u6709\u975e\u7cbe\u8c03\u6d41\u5f0f\u65b9\u6cd5\uff0c\u4e14\u590d\u6742\u5ea6\u66f4\u4f4e\uff0c\u8fd8\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u5355\u8bcd\u7ea7\u65f6\u95f4\u6233", "conclusion": "\u901a\u8fc7LoRA\u7cbe\u8c03\u53ef\u4ee5\u9ad8\u6548\u5c06\u73b0\u6709\u79bb\u7ebfASR\u6a21\u578b\u8f6c\u6362\u4e3a\u4f4e\u5ef6\u8fdf\u6d41\u5f0f\u6a21\u578b\uff0c\u4e3a\u5b9e\u65f6\u8bed\u97f3\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12031", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12031", "abs": "https://arxiv.org/abs/2508.12031", "authors": ["Shaozhe Yin", "Jinyu Guo", "Kai Shuang", "Xia Liu", "Ruize Ou"], "title": "Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases", "comment": null, "summary": "Continual Relation Extraction (CRE) aims to continually learn new emerging\nrelations while avoiding catastrophic forgetting. Existing CRE methods mainly\nuse memory replay and contrastive learning to mitigate catastrophic forgetting.\nHowever, these methods do not attach importance to the error cases that can\nreveal the model's cognitive biases more effectively. To address this issue, we\npropose an instruction-based continual contrastive tuning approach for Large\nLanguage Models (LLMs) in CRE. Different from existing CRE methods that\ntypically handle the training and memory data in a unified manner, this\napproach splits the training and memory data of each task into two parts\nrespectively based on the correctness of the initial responses and treats them\ndifferently through dual-task fine-tuning. In addition, leveraging the\nadvantages of LLM's instruction-following ability, we propose a novel\ninstruction-based contrastive tuning strategy for LLM to continuously correct\ncurrent cognitive biases with the guidance of previous data in an\ninstruction-tuning manner, which mitigates the gap between old and new\nrelations in a more suitable way for LLMs. We experimentally evaluate our model\non TACRED and FewRel, and the results show that our model achieves new\nstate-of-the-art CRE performance with significant improvements, demonstrating\nthe importance of specializing in exploiting error cases.", "AI": {"tldr": "\u57fa\u4e8e\u6307\u4ee4\u7684\u6307\u5bfc\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u9519\u8bef\u6848\u4f8b\u6765\u51cf\u8f7b\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5173\u7cfb\u63d0\u53d6\u4e2d\u7684\u5fd8\u5371\u95ee\u9898", "motivation": "\u73b0\u6709\u7684\u6301\u7eed\u5173\u7cfb\u63d0\u53d6\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u5185\u5b58\u56de\u653e\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4f46\u5ffd\u89c6\u4e86\u9519\u8bef\u6848\u4f8b\u6240\u53cd\u6620\u7684\u6a21\u578b\u8ba4\u77e5\u504f\u5dee\uff0c\u8fd9\u4e9b\u504f\u5dee\u80fd\u66f4\u6709\u6548\u5730\u63ed\u793a\u6a21\u578b\u7684\u5b66\u4e60\u95ee\u9898", "method": "\u63d0\u51fa\u6307\u4ee4\u57fa\u7840\u7684\u6301\u7eed\u5bf9\u6bd4\u5fae\u8c03\u65b9\u6cd5\uff1a1) \u5c06\u6bcf\u4e2a\u4efb\u52a1\u7684\u8bad\u7ec3\u548c\u5185\u5b58\u6570\u636e\u6309\u521d\u59cb\u54cd\u5e94\u6b63\u786e\u6027\u5206\u4e3a\u4e24\u90e8\u5206\uff1b2) \u901a\u8fc7\u53cc\u4efb\u52a1\u5fae\u8c03\u533a\u522b\u5904\u7406\uff1b3) \u5229\u7528LLM\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u8fdb\u884c\u6307\u4ee4\u5fae\u8c03\u65b9\u5f0f\u7684\u5bf9\u6bd4\u8c03\u6574", "result": "\u5728TACRED\u548cFewRel\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u6a21\u578b\u5b9e\u73b0\u4e86\u65b0\u7684state-of-the-art\u6301\u7eed\u5173\u7cfb\u63d0\u53d6\u6027\u80fd\uff0c\u5e76\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb", "conclusion": "\u4e13\u95e8\u5229\u7528\u9519\u8bef\u6848\u4f8b\u6765\u8c03\u6574\u6a21\u578b\u8ba4\u77e5\u504f\u5dee\u7684\u65b9\u6cd5\u5728\u6301\u7eed\u5173\u7cfb\u63d0\u53d6\u4efb\u52a1\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u80fd\u591f\u6709\u6548\u51cf\u8f7b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5fd8\u5371\u95ee\u9898"}}
{"id": "2508.12040", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12040", "abs": "https://arxiv.org/abs/2508.12040", "authors": ["Jinyi Han", "Tingyun Li", "Shisong Chen", "Jie Shi", "Xinyi Wang", "Guanglei Yue", "Jiaqing Liang", "Xin Lin", "Liqian Wen", "Zulong Chen", "Yanghua Xiao"], "title": "Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation", "comment": "The initial versin was made in August 2024", "summary": "While large language models (LLMs) have demonstrated remarkable performance\nacross diverse tasks, they fundamentally lack self-awareness and frequently\nexhibit overconfidence, assigning high confidence scores to incorrect\npredictions. Accurate confidence estimation is therefore critical for enhancing\nthe trustworthiness and reliability of LLM-generated outputs. However, existing\napproaches suffer from coarse-grained scoring mechanisms that fail to provide\nfine-grained, continuous confidence estimates throughout the generation\nprocess. To address these limitations, we introduce FineCE, a novel confidence\nestimation method that delivers accurate, fine-grained confidence scores during\ntext generation. Specifically, we first develop a comprehensive pipeline for\nconstructing training data that effectively captures the underlying\nprobabilistic distribution of LLM responses, and then train a model to predict\nconfidence scores for arbitrary text sequences in a supervised manner.\nFurthermore, we propose a Backward Confidence Integration (BCI) strategy that\nleverages information from the subsequent text to enhance confidence estimation\nfor the current sequence during inference. We also introduce three strategies\nfor identifying optimal positions to perform confidence estimation within the\ngeneration process. Extensive experiments on multiple benchmark datasets\ndemonstrate that FineCE consistently outperforms existing classical confidence\nestimation methods. Our code and all baselines used in the paper are available\non GitHub.", "AI": {"tldr": "FineCE\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7ec6\u7c92\u5ea6\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\u6765\u9884\u6d4b\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u8fde\u7eed\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u89e3\u51b3\u4e86LLM\u8fc7\u5ea6\u81ea\u4fe1\u7684\u95ee\u9898", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u81ea\u6211\u610f\u8bc6\uff0c\u7ecf\u5e38\u5bf9\u9519\u8bef\u9884\u6d4b\u7ed9\u51fa\u9ad8\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u7ec6\u7c92\u5ea6\u8fde\u7eed\u7f6e\u4fe1\u5ea6\u4f30\u8ba1", "method": "\u5f00\u53d1\u4e86\u6784\u5efa\u8bad\u7ec3\u6570\u636e\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u6355\u6349LLM\u54cd\u5e94\u7684\u6982\u7387\u5206\u5e03\uff0c\u7136\u540e\u4ee5\u76d1\u7763\u65b9\u5f0f\u8bad\u7ec3\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u6a21\u578b\uff1b\u63d0\u51fa\u540e\u5411\u7f6e\u4fe1\u5ea6\u96c6\u6210\u7b56\u7565\u548c\u4e09\u79cd\u6700\u4f18\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u4f4d\u7f6e\u8bc6\u522b\u7b56\u7565", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFineCE\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u7ecf\u5178\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5", "conclusion": "FineCE\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u663e\u8457\u63d0\u5347\u4e86LLM\u751f\u6210\u8f93\u51fa\u7684\u53ef\u4fe1\u5ea6\u548c\u53ef\u9760\u6027"}}
{"id": "2508.12591", "categories": ["cs.CL", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.12591", "abs": "https://arxiv.org/abs/2508.12591", "authors": ["Yu-Hsuan Fang", "Tien-Hong Lo", "Yao-Ting Sung", "Berlin Chen"], "title": "Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning", "comment": "Accepted at IEEE ASRU 2025", "summary": "Traditional Automated Speaking Assessment (ASA) systems exhibit inherent\nmodality limitations: text-based approaches lack acoustic information while\naudio-based methods miss semantic context. Multimodal Large Language Models\n(MLLM) offer unprecedented opportunities for comprehensive ASA by\nsimultaneously processing audio and text within unified frameworks. This paper\npresents a very first systematic study of MLLM for comprehensive ASA,\ndemonstrating the superior performance of MLLM across the aspects of content\nand language use . However, assessment on the delivery aspect reveals unique\nchallenges, which is deemed to require specialized training strategies. We thus\npropose Speech-First Multimodal Training (SFMT), leveraging a curriculum\nlearning principle to establish more robust modeling foundations of speech\nbefore cross-modal synergetic fusion. A series of experiments on a benchmark\ndataset show MLLM-based systems can elevate the holistic assessment performance\nfrom a PCC value of 0.783 to 0.846. In particular, SFMT excels in the\nevaluation of the delivery aspect, achieving an absolute accuracy improvement\nof 4% over conventional training approaches, which also paves a new avenue for\nASA.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6027\u7814\u7a76\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u8bf4\u8bdd\u8bc4\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e13\u95e8\u7684\u8bed\u97f3\u4f18\u5148\u591a\u6a21\u6001\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u8bf4\u8bdd\u8bc4\u6d4b\u7cfb\u7edf\u5b58\u5728\u5355\u4e00\u6a21\u6001\u9650\u5236\uff1a\u6587\u672c\u65b9\u6cd5\u7f3a\u4e4f\u97f3\u54cd\u4fe1\u606f\uff0c\u97f3\u9891\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u5168\u9762\u8bc4\u6d4b\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\u3002", "method": "\u63d0\u51fa\u8bed\u97f3\u4f18\u5148\u591a\u6a21\u6001\u8bad\u7ec3\uff08SFMT\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u8bfe\u7a0b\u5b66\u4e60\u539f\u7406\uff0c\u5148\u5efa\u7acb\u5065\u58ee\u7684\u8bed\u97f3\u6a21\u578b\u57fa\u7840\uff0c\u518d\u8fdb\u884c\u8de8\u6a21\u6001\u534f\u540c\u878d\u5408\u3002", "result": "\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u5c06\u5168\u9762\u8bc4\u6d4b\u6027\u80fd\u4ecePCC 0.783\u63d0\u5347\u52300.846\u3002\u5728\u8868\u8fbe\u65b9\u9762\u7684\u8bc4\u4f30\u4e2d\uff0cSFMT\u6bd4\u4f20\u7edf\u8bad\u7ec3\u65b9\u6cd5\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u9ad84%\u3002", "conclusion": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u8bf4\u8bdd\u8bc4\u6d4b\u4e2d\u663e\u793a\u51fa\u4f18\u5f02\u6027\u80fd\uff0c\u7279\u522b\u662f\u901a\u8fc7\u4e13\u95e8\u7684\u8bad\u7ec3\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u8868\u8fbe\u65b9\u9762\u7684\u6311\u6218\uff0c\u4e3a\u8be5\u9886\u57df\u5f00\u542f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.12086", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50, 90C29, 62F07", "I.2.7; I.2.6; G.1.6"], "pdf": "https://arxiv.org/pdf/2508.12086", "abs": "https://arxiv.org/abs/2508.12086", "authors": ["Yao Wu"], "title": "J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs", "comment": "9 pages, 3 tables, 1 algorithm", "summary": "In large language model (LLM) adaptation, balancing multiple optimization\nobjectives such as improving factuality (heat) and increasing confidence (via\nlow entropy) poses a fundamental challenge, especially when prompt parameters\n(e.g., hidden-layer insertions h and embedding modifications w) interact in\nnon-trivial ways. Existing multi-objective optimization strategies often rely\non scalar gradient aggregation, ignoring the deeper geometric structure between\nobjectives and parameters. We propose J6, a structured Jacobian-based method\nthat decomposes the gradient interaction matrix into six interpretable\ncomponents. This decomposition enables both hard decision-making (e.g.,\nchoosing the dominant update direction via argmax) and soft strategies (e.g.,\nattention-style weighting via softmax over J6), forming a dynamic update\nframework that adapts to local conflict and synergy. Moreover, the\ninterpretable structure of J6 provides insight into parameter attribution, task\ninterference, and geometry-aligned adaptation. Our work introduces a principled\nand extensible mechanism for conflict-aware prompt optimization, and opens a\nnew avenue for incorporating structured Jacobian reasoning into multi-objective\nneural tuning.", "AI": {"tldr": "J6\u65b9\u6cd5\u901a\u8fc7\u96c5\u53ef\u6bd4\u77e9\u9635\u5206\u89e3\u4e3a\u516d\u4e2a\u53ef\u89e3\u91ca\u7ec4\u4ef6\uff0c\u89e3\u51b3LLM\u591a\u76ee\u6807\u4f18\u5316\u4e2d\u7684\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u4f9b\u786c\u51b3\u7b56\u548c\u8f6f\u7b56\u7565\u7684\u52a8\u6001\u66f4\u65b0\u6846\u67b6", "motivation": "\u73b0\u6709LLM\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u6807\u91cf\u68af\u5ea6\u805a\u5408\uff0c\u5ffd\u7565\u4e86\u76ee\u6807\u4e0e\u53c2\u6570\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u96be\u4ee5\u5e73\u8861\u4e8b\u5b9e\u6027\u63d0\u5347\u548c\u7f6e\u4fe1\u5ea6\u589e\u52a0\u7b49\u51b2\u7a81\u76ee\u6807", "method": "\u63d0\u51fa\u7ed3\u6784\u5316\u96c5\u53ef\u6bd4\u65b9\u6cd5J6\uff0c\u5c06\u68af\u5ea6\u4ea4\u4e92\u77e9\u9635\u5206\u89e3\u4e3a\u516d\u4e2a\u53ef\u89e3\u91ca\u7ec4\u4ef6\uff0c\u652f\u6301argmax\u786c\u51b3\u7b56\u548csoftmax\u8f6f\u7b56\u7565\u7684\u52a8\u6001\u66f4\u65b0\u6846\u67b6", "result": "J6\u63d0\u4f9b\u4e86\u53c2\u6570\u5f52\u56e0\u3001\u4efb\u52a1\u5e72\u6270\u548c\u51e0\u4f55\u5bf9\u9f50\u9002\u5e94\u7684\u6df1\u5165\u6d1e\u5bdf\uff0c\u5f62\u6210\u4e86\u51b2\u7a81\u611f\u77e5\u7684\u63d0\u793a\u4f18\u5316\u673a\u5236", "conclusion": "J6\u4e3a\u591a\u76ee\u6807\u795e\u7ecf\u8c03\u4f18\u5f15\u5165\u4e86\u7ed3\u6784\u5316\u96c5\u53ef\u6bd4\u63a8\u7406\u7684\u65b0\u9014\u5f84\uff0c\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u548c\u53ef\u6269\u5c55\u7684\u4f18\u5316\u673a\u5236"}}
{"id": "2508.12096", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12096", "abs": "https://arxiv.org/abs/2508.12096", "authors": ["Haiquan Hu", "Jiazhi Jiang", "Shiyou Xu", "Ruhan Zeng", "Tian Wang"], "title": "STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples", "comment": "Submit to AAAI 2026", "summary": "Evaluating large language models (LLMs) has become increasingly challenging\nas model capabilities advance rapidly. While recent models often achieve higher\nscores on standard benchmarks, these improvements do not consistently reflect\nenhanced real-world reasoning capabilities. Moreover, widespread overfitting to\npublic benchmarks and the high computational cost of full evaluations have made\nit both expensive and less effective to distinguish meaningful differences\nbetween models. To address these challenges, we propose the \\textbf{S}tructured\n\\textbf{T}ransition \\textbf{E}valuation \\textbf{M}ethod (STEM), a lightweight\nand interpretable evaluation framework for efficiently estimating the relative\ncapabilities of LLMs. STEM identifies \\textit{significant transition samples}\n(STS) by analyzing consistent performance transitions among LLMs of the same\narchitecture but varying parameter scales. These samples enable STEM to\neffectively estimate the capability position of an unknown model. Qwen3 model\nfamily is applied to construct the STS pool on six diverse and representative\nbenchmarks. To assess generalizability. Experimental results indicate that STEM\nreliably captures performance trends, aligns with ground-truth rankings of\nmodel capability. These findings highlight STEM as a practical and scalable\nmethod for fine-grained, architecture-agnostic evaluation of LLMs.", "AI": {"tldr": "STEM\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u540c\u67b6\u6784\u4e0d\u540c\u53c2\u6570\u89c4\u6a21LLMs\u7684\u6027\u80fd\u8f6c\u53d8\u6765\u8bc6\u522b\u5173\u952e\u6837\u672c\uff0c\u4ece\u800c\u9ad8\u6548\u4f30\u8ba1\u672a\u77e5\u6a21\u578b\u7684\u80fd\u529b\u4f4d\u7f6e\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u5feb\u901f\u63d0\u5347\uff0c\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u9762\u4e34\u8fc7\u62df\u5408\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u96be\u4ee5\u533a\u5206\u6a21\u578b\u771f\u5b9e\u80fd\u529b\u5dee\u5f02\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSTEM\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u540c\u67b6\u6784\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u7684\u6027\u80fd\u8f6c\u53d8\u8bc6\u522b\u5173\u952e\u8fc7\u6e21\u6837\u672c(STS)\uff0c\u7528\u8fd9\u4e9b\u6837\u672c\u6765\u4f30\u8ba1\u672a\u77e5\u6a21\u578b\u7684\u80fd\u529b\u6392\u540d\u3002", "result": "\u5728\u516d\u4e2a\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u9a8c\u8bc1\uff0cSTEM\u80fd\u53ef\u9760\u6355\u6349\u6027\u80fd\u8d8b\u52bf\uff0c\u4e0e\u771f\u5b9e\u6a21\u578b\u80fd\u529b\u6392\u540d\u4e00\u81f4\uff0c\u5c55\u73b0\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "STEM\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u67b6\u6784\u7684LLMs\u8bc4\u4f30\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.12140", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12140", "abs": "https://arxiv.org/abs/2508.12140", "authors": ["Ziqian Bi", "Lu Chen", "Junhao Song", "Hongying Luo", "Enze Ge", "Junmin Huang", "Tianyang Wang", "Keyu Chen", "Chia Xin Liang", "Zihan Wei", "Huafeng Liu", "Chunjie Tian", "Jibin Guan", "Joe Yeong", "Yongzhi Xu", "Peng Wang", "Junfeng Hao"], "title": "Exploring Efficiency Frontiers of Thinking Budget in Medical Reasoning: Scaling Laws between Computational Resources and Reasoning Quality", "comment": null, "summary": "This study presents the first comprehensive evaluation of thinking budget\nmechanisms in medical reasoning tasks, revealing fundamental scaling laws\nbetween computational resources and reasoning quality. We systematically\nevaluated two major model families, Qwen3 (1.7B to 235B parameters) and\nDeepSeek-R1 (1.5B to 70B parameters), across 15 medical datasets spanning\ndiverse specialties and difficulty levels. Through controlled experiments with\nthinking budgets ranging from zero to unlimited tokens, we establish\nlogarithmic scaling relationships where accuracy improvements follow a\npredictable pattern with both thinking budget and model size. Our findings\nidentify three distinct efficiency regimes: high-efficiency (0 to 256 tokens)\nsuitable for real-time applications, balanced (256 to 512 tokens) offering\noptimal cost-performance tradeoffs for routine clinical support, and\nhigh-accuracy (above 512 tokens) justified only for critical diagnostic tasks.\nNotably, smaller models demonstrate disproportionately larger benefits from\nextended thinking, with 15 to 20% improvements compared to 5 to 10% for larger\nmodels, suggesting a complementary relationship where thinking budget provides\ngreater relative benefits for capacity-constrained models. Domain-specific\npatterns emerge clearly, with neurology and gastroenterology requiring\nsignificantly deeper reasoning processes than cardiovascular or respiratory\nmedicine. The consistency between Qwen3 native thinking budget API and our\nproposed truncation method for DeepSeek-R1 validates the generalizability of\nthinking budget concepts across architectures. These results establish thinking\nbudget control as a critical mechanism for optimizing medical AI systems,\nenabling dynamic resource allocation aligned with clinical needs while\nmaintaining the transparency essential for healthcare deployment.", "AI": {"tldr": "\u8fd9\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u533b\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u601d\u8003\u9884\u7b97\u673a\u5236\uff0c\u63ed\u793a\u4e86\u8ba1\u7b97\u8d44\u6e90\u4e0e\u63a8\u7406\u8d28\u91cf\u4e4b\u95f4\u7684\u57fa\u672c\u7f29\u653e\u89c4\u5f8b\uff0c\u4e3a\u533b\u7597AI\u7cfb\u7edf\u7684\u52a8\u6001\u8d44\u6e90\u5206\u914d\u63d0\u4f9b\u4e86\u5173\u952e\u673a\u5236\u3002", "motivation": "\u4e3a\u4e86\u7406\u89e3\u8ba1\u7b97\u8d44\u6e90\u4e0e\u533b\u5b66\u63a8\u7406\u8d28\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u601d\u8003\u9884\u7b97\u63a7\u5236\u6765\u4f18\u5316\u533b\u7597AI\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u6210\u672c\u6548\u76ca\u3002", "method": "\u7cfb\u7edf\u6027\u8bc4\u4f30Qwen3\u548cDeepSeek-R1\u4e24\u5927\u6a21\u578b\u5bb6\u65cf\uff0c\u6d89\u53ca1.5B\u5230235B\u53c2\u6570\u8303\u56f4\uff0c\u572815\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002\u901a\u8fc7\u63a7\u5236\u601d\u8003\u9884\u7b97\u4ece\u96f6\u5230\u65e0\u9650\u4ee4\u724c\u7684\u5b9e\u9a8c\uff0c\u5206\u6790\u7f29\u653e\u5173\u7cfb\u548c\u6548\u7387\u6cbb\u7406\u3002", "result": "\u53d1\u73b0\u4e86\u5bf9\u6570\u7f29\u653e\u5173\u7cfb\uff1a\u9ad8\u6548\u7387\u6cbb\u7406\uff080-256\u4ee4\u724c\uff09\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\uff0c\u5e73\u8861\u6cbb\u7406\uff08256-512\u4ee4\u724c\uff09\u63d0\u4f9b\u6700\u4f73\u6210\u672c\u6548\u76ca\uff0c\u9ad8\u51c6\u786e\u6cbb\u7406\uff08>512\u4ee4\u724c\uff09\u4ec5\u9002\u7528\u4e8e\u5173\u952e\u8bca\u65ad\u4efb\u52a1\u3002\u8f83\u5c0f\u6a21\u578b\u4ece\u6269\u5c55\u601d\u8003\u4e2d\u83b7\u5f97\u66f4\u5927\u6536\u76ca\uff0815-20%\u6536\u76ca\uff0c\u5927\u6a21\u578b\u4ec55-10%\uff09\u3002\u4e0d\u540c\u533b\u5b66\u4e13\u4e1a\u9700\u8981\u4e0d\u540c\u6df1\u5ea6\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "conclusion": "\u601d\u8003\u9884\u7b97\u63a7\u5236\u662f\u4f18\u5316\u533b\u7597AI\u7cfb\u7edf\u7684\u5173\u952e\u673a\u5236\uff0c\u80fd\u591f\u6839\u636e\u4e34\u5e8a\u9700\u6c42\u52a8\u6001\u5206\u914d\u8d44\u6e90\uff0c\u540c\u65f6\u4fdd\u6301\u900f\u660e\u6027\uff0c\u4e3a\u533b\u7597\u90e8\u7f72\u63d0\u4f9b\u91cd\u8981\u652f\u6491\u3002"}}
{"id": "2508.12158", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12158", "abs": "https://arxiv.org/abs/2508.12158", "authors": ["Stephen Meisenbacher", "Alexandra Klymenko", "Florian Matthes"], "title": "LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human and LLM Perceptions of Privacy in Textual Data", "comment": "13 pages, 3 figures, 4 tables. Accepted to HAIPS @ CCS 2025", "summary": "Despite advances in the field of privacy-preserving Natural Language\nProcessing (NLP), a significant challenge remains the accurate evaluation of\nprivacy. As a potential solution, using LLMs as a privacy evaluator presents a\npromising approach $\\unicode{x2013}$ a strategy inspired by its success in\nother subfields of NLP. In particular, the so-called $\\textit{LLM-as-a-Judge}$\nparadigm has achieved impressive results on a variety of natural language\nevaluation tasks, demonstrating high agreement rates with human annotators.\nRecognizing that privacy is both subjective and difficult to define, we\ninvestigate whether LLM-as-a-Judge can also be leveraged to evaluate the\nprivacy sensitivity of textual data. Furthermore, we measure how closely LLM\nevaluations align with human perceptions of privacy in text. Resulting from a\nstudy involving 10 datasets, 13 LLMs, and 677 human survey participants, we\nconfirm that privacy is indeed a difficult concept to measure empirically,\nexhibited by generally low inter-human agreement rates. Nevertheless, we find\nthat LLMs can accurately model a global human privacy perspective, and through\nan analysis of human and LLM reasoning patterns, we discuss the merits and\nlimitations of LLM-as-a-Judge for privacy evaluation in textual data. Our\nfindings pave the way for exploring the feasibility of LLMs as privacy\nevaluators, addressing a core challenge in solving pressing privacy issues with\ninnovative technical solutions.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4f7f\u7528LLM\u4f5c\u4e3a\u9690\u79c1\u8bc4\u4f30\u5668\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u6bd4\u8f83LLM\u4e0e\u4eba\u7c7b\u5bf9\u6587\u672c\u9690\u79c1\u654f\u611f\u5ea6\u7684\u8bc4\u4f30\uff0c\u53d1\u73b0LLM\u80fd\u591f\u51c6\u786e\u6a21\u62df\u4eba\u7c7b\u9690\u79c1\u89c6\u89d2\uff0c\u4e3a\u89e3\u51b3\u9690\u79c1\u8bc4\u4f30\u96be\u9898\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "motivation": "\u9690\u79c1\u4fdd\u62a4NLP\u9886\u57df\u9762\u4e34\u9690\u79c1\u8bc4\u4f30\u51c6\u786e\u6027\u7684\u6311\u6218\uff0c\u800cLLM\u5728\u5176\u4ed6NLP\u5b50\u9886\u57df\u4f5c\u4e3a\u8bc4\u4f30\u5668\u5df2\u53d6\u5f97\u663e\u8457\u6210\u529f\uff0c\u56e0\u6b64\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u5229\u7528LLM-as-a-Judge\u8303\u5f0f\u6765\u8bc4\u4f30\u6587\u672c\u6570\u636e\u7684\u9690\u79c1\u654f\u611f\u6027\u3002", "method": "\u7814\u7a76\u6d89\u53ca10\u4e2a\u6570\u636e\u96c6\u300113\u4e2aLLM\u6a21\u578b\u548c677\u540d\u4eba\u7c7b\u8c03\u67e5\u53c2\u4e0e\u8005\uff0c\u901a\u8fc7\u6bd4\u8f83LLM\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u9690\u79c1\u611f\u77e5\u7684\u4e00\u81f4\u6027\uff0c\u5206\u6790\u4eba\u7c7b\u548cLLM\u7684\u63a8\u7406\u6a21\u5f0f\u6765\u8bc4\u4f30LLM\u4f5c\u4e3a\u9690\u79c1\u8bc4\u4f30\u5668\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u9690\u79c1\u786e\u5b9e\u96be\u4ee5\u91cf\u5316\u6d4b\u91cf\uff08\u4eba\u7c7b\u95f4\u4e00\u81f4\u6027\u8f83\u4f4e\uff09\uff0c\u4f46LLM\u80fd\u591f\u51c6\u786e\u5efa\u6a21\u5168\u5c40\u4eba\u7c7b\u9690\u79c1\u89c6\u89d2\uff0c\u5728\u9690\u79c1\u8bc4\u4f30\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\u3002", "conclusion": "LLM\u4f5c\u4e3a\u9690\u79c1\u8bc4\u4f30\u5668\u5177\u6709\u53ef\u884c\u6027\uff0c\u4e3a\u901a\u8fc7\u521b\u65b0\u6280\u672f\u89e3\u51b3\u65b9\u6848\u89e3\u51b3\u7d27\u8feb\u9690\u79c1\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u63a2\u7d22\u65b9\u5411\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u8ba8\u8bba\u5176\u4f18\u7f3a\u70b9\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2508.12227", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12227", "abs": "https://arxiv.org/abs/2508.12227", "authors": ["Abdelhamid Haouhat", "Slimane Bellaouar", "Attia Nehar", "Hadda Cherroun", "Ahmed Abdelali"], "title": "Arabic Multimodal Machine Learning: Datasets, Applications, Approaches, and Challenges", "comment": null, "summary": "Multimodal Machine Learning (MML) aims to integrate and analyze information\nfrom diverse modalities, such as text, audio, and visuals, enabling machines to\naddress complex tasks like sentiment analysis, emotion recognition, and\nmultimedia retrieval. Recently, Arabic MML has reached a certain level of\nmaturity in its foundational development, making it time to conduct a\ncomprehensive survey. This paper explores Arabic MML by categorizing efforts\nthrough a novel taxonomy and analyzing existing research. Our taxonomy\norganizes these efforts into four key topics: datasets, applications,\napproaches, and challenges. By providing a structured overview, this survey\noffers insights into the current state of Arabic MML, highlighting areas that\nhave not been investigated and critical research gaps. Researchers will be\nempowered to build upon the identified opportunities and address challenges to\nadvance the field.", "AI": {"tldr": "\u963f\u62c9\u4f2f\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u7efc\u8ff0\u7814\u7a76\uff0c\u901a\u8fc7\u65b0\u7684\u5206\u7c7b\u6cd5\u5bf9\u6570\u636e\u96c6\u3001\u5e94\u7528\u3001\u65b9\u6cd5\u548c\u6311\u6218\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\uff0c\u6301\u70b9\u63d0\u51fa\u7814\u7a76\u7a7a\u767d\u548c\u53d1\u5c55\u673a\u9047\u3002", "motivation": "\u963f\u62c9\u4f2f\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u5df2\u7ecf\u53d1\u5c55\u5230\u4e00\u5b9a\u6210\u719f\u9636\u6bb5\uff0c\u9700\u8981\u8fdb\u884c\u5168\u9762\u7684\u7efc\u8ff0\u6027\u7814\u7a76\u6765\u6574\u7406\u76ee\u524d\u7684\u7814\u7a76\u72b6\u51b5\u548c\u6301\u70b9\u6307\u660e\u672a\u6765\u65b9\u5411\u3002", "method": "\u91c7\u7528\u65b0\u7684\u5206\u7c7b\u6cd5\u5bf9\u73b0\u6709\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u5206\u7c7b\uff0c\u4e3b\u8981\u5305\u62ec\u56db\u4e2a\u6838\u5fc3\u9886\u57df\uff1a\u6570\u636e\u96c6\u3001\u5e94\u7528\u573a\u666f\u3001\u6280\u672f\u65b9\u6cd5\u548c\u9762\u4e34\u7684\u6311\u6218\u3002", "result": "\u63d0\u4f9b\u4e86\u963f\u62c9\u4f2f\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u7ed3\u6784\u5316\u6982\u89c8\uff0c\u8bc6\u522b\u51fa\u4e86\u5f53\u524d\u7684\u7814\u7a76\u72b6\u6001\u3001\u5b58\u5728\u7684\u7814\u7a76\u7a7a\u767d\u548c\u5173\u952e\u7684\u7814\u7a76\u95ee\u9898\u3002", "conclusion": "\u672c\u7efc\u8ff0\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5efa\u8bae\u6027\u7684\u7814\u7a76\u6846\u67b6\uff0c\u5e2e\u52a9\u4ed6\u4eec\u6293\u4f4f\u9886\u57df\u53d1\u5c55\u673a\u9047\u5e76\u89e3\u51b3\u76f8\u5173\u6311\u6218\uff0c\u4ee5\u63a8\u52a8\u963f\u62c9\u4f2f\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2508.12243", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12243", "abs": "https://arxiv.org/abs/2508.12243", "authors": ["Wuttikorn Ponwitayarat", "Raymond Ng", "Jann Railey Montalan", "Thura Aung", "Jian Gang Ngui", "Yosephine Susanto", "William Tjhi", "Panuthep Tasawong", "Erik Cambria", "Ekapol Chuangsuwanich", "Sarana Nutanong", "Peerat Limkonchotiwat"], "title": "SEA-BED: Southeast Asia Embedding Benchmark", "comment": null, "summary": "Sentence embeddings are essential for NLP tasks such as semantic search,\nre-ranking, and textual similarity. Although multilingual benchmarks like MMTEB\nbroaden coverage, Southeast Asia (SEA) datasets are scarce and often\nmachine-translated, missing native linguistic properties. With nearly 700\nmillion speakers, the SEA region lacks a region-specific embedding benchmark.\nWe introduce SEA-BED, the first large-scale SEA embedding benchmark with 169\ndatasets across 9 tasks and 10 languages, where 71% are formulated by humans,\nnot machine generation or translation. We address three research questions: (1)\nwhich SEA languages and tasks are challenging, (2) whether SEA languages show\nunique performance gaps globally, and (3) how human vs. machine translations\naffect evaluation. We evaluate 17 embedding models across six studies,\nanalyzing task and language challenges, cross-benchmark comparisons, and\ntranslation trade-offs. Results show sharp ranking shifts, inconsistent model\nperformance among SEA languages, and the importance of human-curated datasets\nfor low-resource languages like Burmese.", "AI": {"tldr": "\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u4e1c\u5357\u4e9a\u8bed\u8a00\u7684\u5927\u89c4\u6a21\u5d4c\u5165\u6a21\u578b\u8bc4\u6d4b\u6807\u51c6SEA-BED\uff0c\u5305\u542b169\u4e2a\u6570\u636e\u96c6\u30019\u9879\u4efb\u52a1\u548c10\u79cd\u8bed\u8a00\uff0c71%\u4eba\u5de5\u6784\u5efa\uff0c\u53d1\u73b0\u4e1c\u5357\u4e9a\u8bed\u8a00\u5728\u5d4c\u5165\u6a21\u578b\u4e0a\u8868\u73b0\u5dee\u5f02\u663e\u8457\u4e14\u9700\u8981\u4eba\u5de5\u7cbe\u7ec6\u6570\u636e\u96c6\u652f\u6301", "motivation": "\u4e1c\u5357\u4e9a\u5730\u533a\u8fd1\u4e03\u4ebf\u4eba\u53e3\u4f46\u7f3a\u4e4f\u4e13\u95e8\u7684\u5d4c\u5165\u6a21\u578b\u8bc4\u6d4b\u6807\u51c6\uff0c\u73b0\u6709\u591a\u8bed\u8a00\u6807\u51c6\u4e2dSEA\u6570\u636e\u96c6\u7a00\u7f3a\u4e14\u591a\u4e3a\u673a\u5668\u7ffb\u8bd1\uff0c\u6ca1\u6709\u53cd\u6620\u539f\u751f\u8bed\u8a00\u7279\u6027", "method": "\u6784\u5efaSEA-BED\u6807\u51c6\uff0c\u5305\u542b169\u4e2a\u6570\u636e\u96c6\u30019\u4e2a\u4efb\u52a1\u7c7b\u578b\u548c10\u79cd\u8bed\u8a00\uff0c\u517671%\u4eba\u5de5\u6784\u5efa\uff1b\u8bc4\u6d4b17\u4e2a\u5d4c\u5165\u6a21\u578b\uff0c\u5206\u6790\u4efb\u52a1\u96be\u5ea6\u3001\u8bed\u8a00\u5dee\u5f02\u3001\u8de8\u6807\u51c6\u5bf9\u6bd4\u548c\u7ffb\u8bd1\u8d28\u91cf\u5f71\u54cd", "result": "\u53d1\u73b0\u6a21\u578b\u6392\u540d\u663e\u8457\u53d8\u5316\uff0c\u4e1c\u5357\u4e9a\u8bed\u8a00\u95f4\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u4eba\u5de5\u7cbe\u7ec6\u6570\u636e\u96c6\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u5982\u7f05\u7538\u8bed\u81f3\u5173\u91cd\u8981\uff0c\u673a\u5668\u7ffb\u8bd1\u4e0e\u4eba\u5de5\u7ffb\u8bd1\u5b58\u5728\u660e\u663e\u5dee\u5f02", "conclusion": "SEA-BED\u586b\u8865\u4e86\u4e1c\u5357\u4e9a\u8bed\u8a00\u5d4c\u5165\u6a21\u578b\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u8bc1\u660e\u4e86\u8bed\u8a00\u7279\u5b9a\u6807\u51c6\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u5f3a\u8c03\u4eba\u5de5\u6784\u5efa\u9ad8\u8d28\u91cf\u539f\u751f\u6570\u636e\u96c6\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00NLP\u7814\u7a76\u7684\u5173\u952e\u4f5c\u7528"}}
{"id": "2508.12255", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.12255", "abs": "https://arxiv.org/abs/2508.12255", "authors": ["Ankita Pasad"], "title": "What do Speech Foundation Models Learn? Analysis and Applications", "comment": "Ph.D. Thesis", "summary": "Speech foundation models (SFMs) are designed to serve as general-purpose\nrepresentations for a wide range of speech-processing tasks. The last five\nyears have seen an influx of increasingly successful self-supervised and\nsupervised pre-trained models with impressive performance on various downstream\ntasks.\n  Although the zoo of SFMs continues to grow, our understanding of the\nknowledge they acquire lags behind. This thesis presents a lightweight analysis\nframework using statistical tools and training-free tasks to investigate the\nacoustic and linguistic knowledge encoded in SFM layers. We conduct a\ncomparative study across multiple SFMs and statistical tools. Our study also\nshows that the analytical insights have concrete implications for downstream\ntask performance.\n  The effectiveness of an SFM is ultimately determined by its performance on\nspeech applications. Yet it remains unclear whether the benefits extend to\nspoken language understanding (SLU) tasks that require a deeper understanding\nthan widely studied ones, such as speech recognition. The limited exploration\nof SLU is primarily due to a lack of relevant datasets. To alleviate that, this\nthesis contributes tasks, specifically spoken named entity recognition (NER)\nand named entity localization (NEL), to the Spoken Language Understanding\nEvaluation benchmark. We develop SFM-based approaches for NER and NEL, and find\nthat end-to-end (E2E) models leveraging SFMs can surpass traditional cascaded\n(speech recognition followed by a text model) approaches. Further, we evaluate\nE2E SLU models across SFMs and adaptation strategies to assess the impact on\ntask performance.\n  Collectively, this thesis tackles previously unanswered questions about SFMs,\nproviding tools and datasets to further our understanding and to enable the\ncommunity to make informed design choices for future model development and\nadoption.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u7814\u7a76\u8bed\u97f3\u57fa\u7840\u6a21\u578b\uff08SFMs\uff09\u7f16\u7801\u7684\u97f3\u54cd\u548c\u8bed\u8a00\u77e5\u8bc6\uff0c\u5e76\u4e3a\u53e3\u8bed\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u53d1\u5c55\u5feb\u901f\uff0c\u4f46\u6211\u4eec\u5bf9\u5176\u6240\u83b7\u5f97\u77e5\u8bc6\u7684\u7406\u89e3\u8fdf\u8fdf\u8fdf\u540e\u3002\u540c\u65f6\uff0c\u5bf9\u9700\u8981\u6df1\u5c42\u7406\u89e3\u7684\u53e3\u8bed\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u7684\u7814\u7a76\u4ecd\u7136\u6709\u9650\uff0c\u4e3b\u8981\u56e0\u4e3a\u7f3a\u4e4f\u76f8\u5173\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528\u7edf\u8ba1\u5de5\u5177\u548c\u65e0\u9700\u8bad\u7ec3\u7684\u4efb\u52a1\u6784\u5efa\u8f7b\u91cf\u7ea7\u5206\u6790\u6846\u67b6\uff0c\u5bf9\u591a\u4e2aSFM\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u7814\u7a76\u3002\u540c\u65f6\u4e3a\u53e3\u8bed\u8bed\u8a00\u7406\u89e3\u8bc4\u6d4b\u63d0\u4f9b\u4e86\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u547d\u540d\u5b9e\u4f53\u5b9a\u4f4d\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8eSFM\u7684\u7aef\u5230\u7aef\u6a21\u578b\u3002", "result": "\u5206\u6790\u7ed3\u679c\u663e\u793a\u4e86\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u5177\u4f53\u542b\u4e49\u3002\u7aef\u5230\u7aef\u6a21\u578b\u5728\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u6d41\u6c34\u7ebf\u65b9\u6cd5\uff08\u8bed\u97f3\u8bc6\u522b+\u6587\u672c\u6a21\u578b\uff09\u3002", "conclusion": "\u672c\u8bba\u6587\u89e3\u51b3\u4e86\u5173\u4e8eSFM\u7684\u4e4b\u524d\u672a\u89e3\u51b3\u95ee\u9898\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u5206\u6790\u5de5\u5177\u548c\u6570\u636e\u96c6\uff0c\u4ee5\u4fbf\u5728\u672a\u6765\u6a21\u578b\u5f00\u53d1\u548c\u91c7\u7528\u4e2d\u505a\u51fa\u660e\u667a\u7684\u8bbe\u8ba1\u9009\u62e9\u3002"}}
{"id": "2508.12257", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12257", "abs": "https://arxiv.org/abs/2508.12257", "authors": ["Zheye Deng", "Chunkit Chan", "Tianshi Zheng", "Wei Fan", "Weiqi Wang", "Yangqiu Song"], "title": "Structuring the Unstructured: A Systematic Review of Text-to-Structure Generation for Agentic AI with a Universal Evaluation Framework", "comment": "Under Review", "summary": "The evolution of AI systems toward agentic operation and context-aware\nretrieval necessitates transforming unstructured text into structured formats\nlike tables, knowledge graphs, and charts. While such conversions enable\ncritical applications from summarization to data mining, current research lacks\na comprehensive synthesis of methodologies, datasets, and metrics. This\nsystematic review examines text-to-structure techniques and the encountered\nchallenges, evaluates current datasets and assessment criteria, and outlines\npotential directions for future research. We also introduce a universal\nevaluation framework for structured outputs, establishing text-to-structure as\nfoundational infrastructure for next-generation AI systems.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6587\u672c\u5230\u7ed3\u6784\u5316\u8f6c\u6362\u6280\u672f\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u5206\u6790\u4e86\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u63d0\u51fa\u4e86\u901a\u7528\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "AI\u7cfb\u7edf\u5411\u4ee3\u7406\u64cd\u4f5c\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u68c0\u7d22\u53d1\u5c55\u9700\u8981\u5c06\u975e\u7ed3\u6784\u5316\u6587\u672c\u8f6c\u6362\u4e3a\u8868\u683c\u3001\u77e5\u8bc6\u56fe\u8c31\u548c\u56fe\u8868\u7b49\u7ed3\u6784\u5316\u683c\u5f0f\uff0c\u4f46\u5f53\u524d\u7814\u7a76\u7f3a\u4e4f\u5bf9\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u548c\u6307\u6807\u7684\u7efc\u5408\u5206\u6790\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u6790\u6587\u672c\u5230\u7ed3\u6784\u8f6c\u6362\u6280\u672f\uff0c\u8bc4\u4f30\u73b0\u6709\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6807\u51c6\u3002", "result": "\u5efa\u7acb\u4e86\u6587\u672c\u5230\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u901a\u7528\u8bc4\u4f30\u6846\u67b6\uff0c\u786e\u8ba4\u6587\u672c\u5230\u7ed3\u6784\u8f6c\u6362\u4f5c\u4e3a\u4e0b\u4e00\u4ee3AI\u7cfb\u7edf\u7684\u57fa\u7840\u8bbe\u65bd\u5730\u4f4d\u3002", "conclusion": "\u6587\u672c\u5230\u7ed3\u6784\u8f6c\u6362\u662fAI\u7cfb\u7edf\u53d1\u5c55\u7684\u5173\u952e\u6280\u672f\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u65b9\u6cd5\u6765\u5e94\u5bf9\u73b0\u6709\u6311\u6218\u5e76\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2508.12265", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12265", "abs": "https://arxiv.org/abs/2508.12265", "authors": ["Xinda Jia", "Jinpeng Li", "Zezhong Wang", "Jingjing Li", "Xingshan Zeng", "Yasheng Wang", "Weinan Zhang", "Yong Yu", "Weiwen Liu"], "title": "Fast, Slow, and Tool-augmented Thinking for LLMs: A Review", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in\nreasoning across diverse domains. However, effective reasoning in real-world\ntasks requires adapting the reasoning strategy to the demands of the problem,\nranging from fast, intuitive responses to deliberate, step-by-step reasoning\nand tool-augmented thinking. Drawing inspiration from cognitive psychology, we\npropose a novel taxonomy of LLM reasoning strategies along two knowledge\nboundaries: a fast/slow boundary separating intuitive from deliberative\nprocesses, and an internal/external boundary distinguishing reasoning grounded\nin the model's parameters from reasoning augmented by external tools. We\nsystematically survey recent work on adaptive reasoning in LLMs and categorize\nmethods based on key decision factors. We conclude by highlighting open\nchallenges and future directions toward more adaptive, efficient, and reliable\nLLMs.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba4\u77e5\u5fc3\u7406\u5b66\u7684LLM\u63a8\u7406\u7b56\u7565\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7\u5feb\u6162\u8fb9\u754c\u548c\u5185\u5916\u8fb9\u754c\u6765\u7cfb\u7edf\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9002\u5e94\u6027\u63a8\u7406\u65b9\u6cd5\u3002", "motivation": "\u5b9e\u9645\u4efb\u52a1\u4e2d\u9700\u8981\u6839\u636e\u95ee\u9898\u9700\u6c42\u9002\u5e94\u5730\u9009\u62e9\u63a8\u7406\u7b56\u7565\uff0c\u4ece\u5feb\u901f\u76f4\u89c9\u5230\u6b65\u9aa4\u5f0f\u63a8\u7406\u548c\u5de5\u5177\u589e\u5f3a\u601d\u7ef4\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u7684\u5206\u7c7b\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u7c7b\u6cd5\uff1a\u901a\u8fc7\u5feb\u6162\u8fb9\u754c\uff08\u76f4\u89c9\u4e0e\u6c89\u601d\u8fc7\u7a0b\uff09\u548c\u5185\u5916\u8fb9\u754c\uff08\u6a21\u578b\u53c2\u6570\u5185\u90e8\u63a8\u7406\u4e0e\u5916\u90e8\u5de5\u5177\u589e\u5f3a\uff09\u6765\u7cfb\u7edf\u5206\u6790LLM\u63a8\u7406\u7b56\u7565\u3002\u5bf9\u6700\u65b0\u7684\u9002\u5e94\u6027\u63a8\u7406\u7814\u7a76\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8c03\u67e5\u548c\u5206\u7c7b\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7cfb\u7edf\u7684LLM\u63a8\u7406\u7b56\u7565\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u57fa\u4e8e\u5173\u952e\u51b3\u7b56\u56e0\u7d20\u5bf9\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\u3002\u8be6\u7ec6\u7684\u8c03\u67e5\u7ed3\u679c\u9700\u8981\u67e5\u770b\u8bba\u6587\u5b8c\u6574\u5185\u5bb9\u3002", "conclusion": "\u63d0\u51fa\u4e86\u5f00\u653e\u6027\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u9002\u5e94\u3001\u9ad8\u6548\u548c\u53ef\u9760\u7684\u5927\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u3002"}}
{"id": "2508.12277", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12277", "abs": "https://arxiv.org/abs/2508.12277", "authors": ["Elon Ezra", "Ariel Weizman", "Amos Azaria"], "title": "The Self-Execution Benchmark: Measuring LLMs' Attempts to Overcome Their Lack of Self-Execution", "comment": "11 pages, 9 figures", "summary": "Large language models (LLMs) are commonly evaluated on tasks that test their\nknowledge or reasoning abilities. In this paper, we explore a different type of\nevaluation: whether an LLM can predict aspects of its own responses. Since LLMs\nlack the ability to execute themselves, we introduce the Self-Execution\nBenchmark, which measures a model's ability to anticipate properties of its\noutput, such as whether a question will be difficult for it, whether it will\nrefuse to answer, or what kinds of associations it is likely to produce. Our\nexperiments show that models generally perform poorly on this benchmark, and\nthat increased model size or capability does not consistently lead to better\nperformance. These results suggest a fundamental limitation in how LLMs\nrepresent and reason about their own behavior.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u7d22\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u81ea\u8eab\u54cd\u5e94\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u81ea\u6211\u6267\u884c\u6d4b\u8bd5\u53d1\u73b0\u6a21\u578b\u8868\u73b0\u5dee\u5f3a\u4e14\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u4e0d\u80fd\u63d0\u5347\u6027\u80fd", "motivation": "\u5f53\u524d\u5bf9LLM\u7684\u8bc4\u4f30\u4e3b\u8981\u96c6\u4e2d\u5728\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u4e0a\uff0c\u672c\u6587\u60f3\u8981\u63a2\u7d22\u4e00\u79cd\u4e0d\u540c\u7684\u8bc4\u4f30\u65b9\u5f0f\uff1a\u6a21\u578b\u662f\u5426\u80fd\u9884\u6d4b\u81ea\u8eab\u7684\u54cd\u5e94\u7279\u6027", "method": "\u5f15\u5165\u4e86\u81ea\u6211\u6267\u884c\u6d4b\u8bd5(SEB)\uff0c\u6d4b\u91cf\u6a21\u578b\u9884\u6d4b\u5176\u8f93\u51fa\u7279\u6027\u7684\u80fd\u529b\uff0c\u5305\u62ec\u95ee\u9898\u96be\u5ea6\u9884\u6d4b\u3001\u62d2\u7edd\u56de\u7b54\u9884\u6d4b\u3001\u8054\u60f3\u503e\u5411\u9884\u6d4b\u7b49", "result": "\u6a21\u578b\u5728\u8be5\u6d4b\u8bd5\u4e0a\u8868\u73b0\u666e\u904d\u8f83\u5dee\uff0c\u6a21\u578b\u89c4\u6a21\u6216\u80fd\u529b\u7684\u63d0\u5347\u5e76\u4e0d\u80fd\u6301\u7eed\u6539\u5584\u6027\u80fd", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660eLLM\u5728\u8868\u5f81\u548c\u63a8\u7406\u81ea\u8eab\u884c\u4e3a\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u7684\u9650\u5236"}}
{"id": "2508.12281", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12281", "abs": "https://arxiv.org/abs/2508.12281", "authors": ["Xin Dai", "Buqiang Xu", "Zhenghao Liu", "Yukun Yan", "Huiyuan Xie", "Xiaoyuan Yi", "Shuo Wang", "Ge Yu"], "title": "Legal$\u0394$: Enhancing Legal Reasoning in LLMs via Reinforcement Learning with Chain-of-Thought Guided Information Gain", "comment": null, "summary": "Legal Artificial Intelligence (LegalAI) has achieved notable advances in\nautomating judicial decision-making with the support of Large Language Models\n(LLMs). However, existing legal LLMs still struggle to generate reliable and\ninterpretable reasoning processes. They often default to fast-thinking behavior\nby producing direct answers without explicit multi-step reasoning, limiting\ntheir effectiveness in complex legal scenarios that demand rigorous\njustification. To address this challenge, we propose Legal$\\Delta$, a\nreinforcement learning framework designed to enhance legal reasoning through\nchain-of-thought guided information gain. During training, Legal$\\Delta$\nemploys a dual-mode input setup-comprising direct answer and\nreasoning-augmented modes-and maximizes the information gain between them. This\nencourages the model to acquire meaningful reasoning patterns rather than\ngenerating superficial or redundant explanations. Legal$\\Delta$ follows a\ntwo-stage approach: (1) distilling latent reasoning capabilities from a\npowerful Large Reasoning Model (LRM), DeepSeek-R1, and (2) refining reasoning\nquality via differential comparisons, combined with a multidimensional reward\nmechanism that assesses both structural coherence and legal-domain specificity.\nExperimental results on multiple legal reasoning tasks demonstrate that\nLegal$\\Delta$ outperforms strong baselines in both accuracy and\ninterpretability. It consistently produces more robust and trustworthy legal\njudgments without relying on labeled preference data. All code and data will be\nreleased at https://github.com/NEUIR/LegalDelta.", "AI": {"tldr": "Legal\u0394\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u601d\u7ef4\u94fe\u5f15\u5bfc\u7684\u4fe1\u606f\u589e\u76ca\u6765\u589e\u5f3a\u6cd5\u5f8b\u63a8\u7406\u80fd\u529b\uff0c\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u6cd5\u5f8b\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fc7\u7a0b\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5f80\u5f80\u4ea7\u751f\u76f4\u63a5\u7b54\u6848\u800c\u7f3a\u4e4f\u591a\u6b65\u63a8\u7406\uff0c\u9650\u5236\u4e86\u5728\u590d\u6742\u6cd5\u5f8b\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u53cc\u6a21\u5f0f\u8f93\u5165\u8bbe\u7f6e\uff08\u76f4\u63a5\u7b54\u6848\u6a21\u5f0f\u548c\u63a8\u7406\u589e\u5f3a\u6a21\u5f0f\uff09\uff0c\u6700\u5927\u5316\u4e24\u8005\u95f4\u7684\u4fe1\u606f\u589e\u76ca\uff1b\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1)\u4eceDeepSeek-R1\u63d0\u53d6\u6f5c\u5728\u63a8\u7406\u80fd\u529b\uff0c2)\u901a\u8fc7\u5dee\u5f02\u6bd4\u8f83\u548c\u591a\u7ef4\u5956\u52b1\u673a\u5236\uff08\u8bc4\u4f30\u7ed3\u6784\u8fde\u8d2f\u6027\u548c\u6cd5\u5f8b\u9886\u57df\u7279\u5f02\u6027\uff09\u7cbe\u70bc\u63a8\u7406\u8d28\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u6cd5\u5f8b\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLegal\u0394\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u80fd\u591f\u4ea7\u751f\u66f4\u7a33\u5065\u548c\u53ef\u4fe1\u7684\u6cd5\u5f8b\u5224\u65ad\uff0c\u4e14\u4e0d\u4f9d\u8d56\u6807\u6ce8\u7684\u504f\u597d\u6570\u636e\u3002", "conclusion": "Legal\u0394\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u4fe1\u606f\u589e\u76ca\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u6cd5\u5f8bAI\u7684\u63a8\u7406\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u590d\u6742\u6cd5\u5f8b\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u81ea\u52a8\u5316\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2508.12282", "categories": ["cs.CL", "cs.IR", "68T50, 68P20", "I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2508.12282", "abs": "https://arxiv.org/abs/2508.12282", "authors": ["Ziyang Chen", "Erxue Min", "Xiang Zhao", "Yunxin Li", "Xin Jia", "Jinzhi Liao", "Jichao Li", "Shuaiqiang Wang", "Baotian Hu", "Dawei Yin"], "title": "A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation", "comment": "10 pages, 5 figures", "summary": "We introduce ChronoQA, a large-scale benchmark dataset for Chinese question\nanswering, specifically designed to evaluate temporal reasoning in\nRetrieval-Augmented Generation (RAG) systems. ChronoQA is constructed from over\n300,000 news articles published between 2019 and 2024, and contains 5,176\nhigh-quality questions covering absolute, aggregate, and relative temporal\ntypes with both explicit and implicit time expressions. The dataset supports\nboth single- and multi-document scenarios, reflecting the real-world\nrequirements for temporal alignment and logical consistency. ChronoQA features\ncomprehensive structural annotations and has undergone multi-stage validation,\nincluding rule-based, LLM-based, and human evaluation, to ensure data quality.\nBy providing a dynamic, reliable, and scalable resource, ChronoQA enables\nstructured evaluation across a wide range of temporal tasks, and serves as a\nrobust benchmark for advancing time-sensitive retrieval-augmented question\nanswering systems.", "AI": {"tldr": "ChronoQA\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u4e2d\u6587\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7cfb\u7edf\u4e2d\u7684\u65f6\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5305\u542b5,176\u4e2a\u9ad8\u8d28\u91cf\u95ee\u9898\uff0c\u8986\u76d6\u591a\u79cd\u65f6\u95f4\u7c7b\u578b\u548c\u8868\u8fbe\u65b9\u5f0f\u3002", "motivation": "\u73b0\u6709\u7684\u95ee\u7b54\u6570\u636e\u96c6\u5728\u65f6\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u4e2d\u6587RAG\u7cfb\u7edf\u7684\u65f6\u95f4\u654f\u611f\u6027\u8bc4\u4f30\u57fa\u51c6\uff0c\u4ee5\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u4e2d\u5bf9\u65f6\u95f4\u5bf9\u9f50\u548c\u903b\u8f91\u4e00\u81f4\u6027\u7684\u9700\u6c42\u3002", "method": "\u4ece2019-2024\u5e74\u768430\u591a\u4e07\u7bc7\u65b0\u95fb\u6587\u7ae0\u4e2d\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5305\u542b\u7edd\u5bf9\u3001\u805a\u5408\u548c\u76f8\u5bf9\u65f6\u95f4\u7c7b\u578b\u7684\u95ee\u9898\uff0c\u652f\u6301\u5355\u6587\u6863\u548c\u591a\u6587\u6863\u573a\u666f\uff0c\u91c7\u7528\u89c4\u5219\u3001LLM\u548c\u4eba\u5de5\u591a\u9636\u6bb5\u9a8c\u8bc1\u786e\u4fdd\u6570\u636e\u8d28\u91cf\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u52a8\u6001\u3001\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684ChronoQA\u6570\u636e\u96c6\uff0c\u5177\u6709\u5168\u9762\u7684\u7ed3\u6784\u6807\u6ce8\uff0c\u80fd\u591f\u652f\u6301\u5e7f\u6cdb\u65f6\u95f4\u4efb\u52a1\u7684\u7ed3\u6784\u5316\u8bc4\u4f30\u3002", "conclusion": "ChronoQA\u4e3a\u63a8\u8fdb\u65f6\u95f4\u654f\u611f\u7684\u68c0\u7d22\u589e\u5f3a\u95ee\u7b54\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u51c6\u8d44\u6e90\uff0c\u80fd\u591f\u6709\u6548\u8bc4\u4f30RAG\u7cfb\u7edf\u7684\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2508.12286", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12286", "abs": "https://arxiv.org/abs/2508.12286", "authors": ["Qinghua Wang", "Xu Zhang", "Lingyan Yang", "Rui Shao", "Bonan Wang", "Fang Wang", "Cunquan Qu"], "title": "Incorporating Legal Logic into Deep Learning: An Intelligent Approach to Probation Prediction", "comment": null, "summary": "Probation is a crucial institution in modern criminal law, embodying the\nprinciples of fairness and justice while contributing to the harmonious\ndevelopment of society. Despite its importance, the current Intelligent\nJudicial Assistant System (IJAS) lacks dedicated methods for probation\nprediction, and research on the underlying factors influencing probation\neligibility remains limited. In addition, probation eligibility requires a\ncomprehensive analysis of both criminal circumstances and remorse. Much of the\nexisting research in IJAS relies primarily on data-driven methodologies, which\noften overlooks the legal logic underpinning judicial decision-making. To\naddress this gap, we propose a novel approach that integrates legal logic into\ndeep learning models for probation prediction, implemented in three distinct\nstages. First, we construct a specialized probation dataset that includes fact\ndescriptions and probation legal elements (PLEs). Second, we design a distinct\nprobation prediction model named the Multi-Task Dual-Theory Probation\nPrediction Model (MT-DT), which is grounded in the legal logic of probation and\nthe \\textit{Dual-Track Theory of Punishment}. Finally, our experiments on the\nprobation dataset demonstrate that the MT-DT model outperforms baseline models,\nand an analysis of the underlying legal logic further validates the\neffectiveness of the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6cd5\u5f8b\u903b\u8f91\u878d\u5165\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u7f13\u5211\u9884\u6d4b\uff0c\u901a\u8fc7\u6784\u5efa\u4e13\u4e1a\u6570\u636e\u96c6\u548c\u57fa\u4e8e\u53cc\u8f68\u60e9\u7f5a\u7406\u8bba\u7684\u591a\u4efb\u52a1\u6a21\u578b\uff0c\u5728\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u90fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u53f8\u6cd5\u8f85\u52a9\u7cfb\u7edf\u7f3a\u4e4f\u4e13\u95e8\u7684\u7f13\u5211\u9884\u6d4b\u65b9\u6cd5\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u800c\u5ffd\u89c6\u4e86\u53f8\u6cd5\u51b3\u7b56\u7684\u6cd5\u5f8b\u903b\u8f91\u57fa\u7840\uff0c\u9700\u8981\u540c\u65f6\u8003\u8651\u72af\u7f6a\u60c5\u8282\u548c\u6094\u7f6a\u8868\u73b0\u7684\u7efc\u5408\u5206\u6790\u3002", "method": "\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1)\u6784\u5efa\u5305\u542b\u4e8b\u5b9e\u63cf\u8ff0\u548c\u7f13\u5211\u6cd5\u5f8b\u8981\u7d20\u7684\u4e13\u4e1a\u6570\u636e\u96c6\uff1b2)\u57fa\u4e8e\u7f13\u5211\u6cd5\u5f8b\u903b\u8f91\u548c\u53cc\u8f68\u60e9\u7f5a\u7406\u8bba\u8bbe\u8ba1\u591a\u4efb\u52a1\u53cc\u7406\u8bba\u7f13\u5211\u9884\u6d4b\u6a21\u578b(MT-DT)\uff1b3)\u5728\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "MT-DT\u6a21\u578b\u5728\u7f13\u5211\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5bf9\u5e95\u5c42\u6cd5\u5f8b\u903b\u8f91\u7684\u5206\u6790\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5c06\u6cd5\u5f8b\u903b\u8f91\u878d\u5165\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u7f13\u5211\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u667a\u80fd\u53f8\u6cd5\u8f85\u52a9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u7b26\u5408\u53f8\u6cd5\u5b9e\u8df5\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2508.12355", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12355", "abs": "https://arxiv.org/abs/2508.12355", "authors": ["Eviatar Nachshoni", "Arie Cattan", "Shmuel Amar", "Ori Shapira", "Ido Dagan"], "title": "Consensus or Conflict? Fine-Grained Evaluation of Conflicting Answers in Question-Answering", "comment": "no comments", "summary": "Large Language Models (LLMs) have demonstrated strong performance in question\nanswering (QA) tasks. However, Multi-Answer Question Answering (MAQA), where a\nquestion may have several valid answers, remains challenging. Traditional QA\nsettings often assume consistency across evidences, but MAQA can involve\nconflicting answers. Constructing datasets that reflect such conflicts is\ncostly and labor-intensive, while existing benchmarks often rely on synthetic\ndata, restrict the task to yes/no questions, or apply unverified automated\nannotation. To advance research in this area, we extend the conflict-aware MAQA\nsetting to require models not only to identify all valid answers, but also to\ndetect specific conflicting answer pairs, if any. To support this task, we\nintroduce a novel cost-effective methodology for leveraging fact-checking\ndatasets to construct NATCONFQA, a new benchmark for realistic, conflict-aware\nMAQA, enriched with detailed conflict labels, for all answer pairs. We evaluate\neight high-end LLMs on NATCONFQA, revealing their fragility in handling various\ntypes of conflicts and the flawed strategies they employ to resolve them.", "AI": {"tldr": "\u63d0\u51fa\u4e86NATCONFQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u591a\u7b54\u6848\u95ee\u7b54\u4efb\u52a1\u4e2d\u5904\u7406\u51b2\u7a81\u7b54\u6848\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u51b2\u7a81\u5904\u7406\u65b9\u9762\u5b58\u5728\u8106\u5f31\u6027\u3002", "motivation": "\u591a\u7b54\u6848\u95ee\u7b54\u4efb\u52a1\u4e2d\u53ef\u80fd\u5b58\u5728\u51b2\u7a81\u7b54\u6848\uff0c\u4f20\u7edfQA\u5047\u8bbe\u8bc1\u636e\u4e00\u81f4\u6027\uff0c\u4f46\u6784\u5efa\u5305\u542b\u771f\u5b9e\u51b2\u7a81\u7684\u6570\u636e\u96c6\u6210\u672c\u9ad8\u4e14\u52b3\u52a8\u5bc6\u96c6\uff0c\u73b0\u6709\u57fa\u51c6\u591a\u4f9d\u8d56\u5408\u6210\u6570\u636e\u6216\u81ea\u52a8\u6807\u6ce8\uff0c\u4e0d\u591f\u53ef\u9760\u3002", "method": "\u6269\u5c55\u51b2\u7a81\u611f\u77e5\u7684MAQA\u8bbe\u7f6e\uff0c\u8981\u6c42\u6a21\u578b\u4e0d\u4ec5\u8bc6\u522b\u6240\u6709\u6709\u6548\u7b54\u6848\uff0c\u8fd8\u8981\u68c0\u6d4b\u7279\u5b9a\u51b2\u7a81\u7b54\u6848\u5bf9\uff1b\u5229\u7528\u4e8b\u5b9e\u6838\u67e5\u6570\u636e\u96c6\u6784\u5efaNATCONFQA\u57fa\u51c6\uff0c\u5305\u542b\u8be6\u7ec6\u51b2\u7a81\u6807\u7b7e\u3002", "result": "\u8bc4\u4f30\u4e868\u4e2a\u9ad8\u7aefLLM\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u5904\u7406\u5404\u7c7b\u51b2\u7a81\u65f6\u8868\u73b0\u8106\u5f31\uff0c\u5e76\u91c7\u7528\u4e86\u6709\u7f3a\u9677\u7684\u89e3\u51b3\u7b56\u7565\u3002", "conclusion": "LLM\u5728\u591a\u7b54\u6848\u51b2\u7a81\u5904\u7406\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u51b2\u7a81\u68c0\u6d4b\u548c\u89e3\u51b3\u673a\u5236\uff0cNATCONFQA\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2508.12387", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12387", "abs": "https://arxiv.org/abs/2508.12387", "authors": ["Yuanfeng Xu", "Zehui Dai", "Jian Liang", "Jiapeng Guan", "Guangrun Wang", "Liang Lin", "Xiaohui Lv"], "title": "ReaLM: Reflection-Enhanced Autonomous Reasoning with Small Language Models", "comment": "16pages, 3 figures", "summary": "Small Language Models (SLMs) are a cost-effective alternative to Large\nLanguage Models (LLMs), but often struggle with complex reasoning due to their\nlimited capacity and a tendency to produce mistakes or inconsistent answers\nduring multi-step reasoning. Existing efforts have improved SLM performance,\nbut typically at the cost of one or more of three key aspects: (1) reasoning\ncapability, due to biased supervision that filters out negative reasoning paths\nand limits learning from errors; (2) autonomy, due to over-reliance on\nexternally generated reasoning signals; and (3) generalization, which suffers\nwhen models overfit to teacher-specific patterns. In this paper, we introduce\nReaLM, a reinforcement learning framework for robust and self-sufficient\nreasoning in vertical domains. To enhance reasoning capability, we propose\nMulti-Route Process Verification (MRPV), which contrasts both positive and\nnegative reasoning paths to extract decisive patterns. To reduce reliance on\nexternal guidance and improve autonomy, we introduce Enabling Autonomy via\nAsymptotic Induction (EAAI), a training strategy that gradually fades external\nsignals. To improve generalization, we apply guided chain-of-thought\ndistillation to encode domain-specific rules and expert knowledge into SLM\nparameters, making them part of what the model has learned. Extensive\nexperiments on both vertical and general reasoning tasks demonstrate that ReaLM\nsignificantly improves SLM performance across aspects (1)-(3) above.", "AI": {"tldr": "ReaLM\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8def\u5f84\u9a8c\u8bc1\u3001\u6e10\u8fdb\u81ea\u4e3b\u5b66\u4e60\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u5782\u76f4\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u3001\u81ea\u4e3b\u6027\u548c\u6cdb\u5316\u6027", "motivation": "\u73b0\u6709\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4e2d\u5b58\u5728\u80fd\u529b\u4e0d\u8db3\u3001\u4f9d\u8d56\u5916\u90e8\u6307\u5bfc\u3001\u6cdb\u5316\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3001\u81ea\u4e3b\u6027\u548c\u6cdb\u5316\u6027\u7684\u65b9\u6cd5", "method": "\u63d0\u51faReaLM\u6846\u67b6\uff1a1\uff09MRPV\u591a\u8def\u5f84\u8fc7\u7a0b\u9a8c\u8bc1\u5bf9\u6bd4\u6b63\u8d1f\u63a8\u7406\u8def\u5f84\uff1b2\uff09EAAI\u6e10\u8fdb\u81ea\u4e3b\u5b66\u4e60\u9010\u6b65\u51cf\u5c11\u5916\u90e8\u4fe1\u53f7\u4f9d\u8d56\uff1b3\uff09\u5f15\u5bfc\u5f0f\u601d\u7ef4\u94fe\u84b8\u998f\u7f16\u7801\u9886\u57df\u77e5\u8bc6", "result": "\u5728\u5782\u76f4\u9886\u57df\u548c\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cReaLM\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u80fd\u529b\u3001\u81ea\u4e3b\u6027\u548c\u6cdb\u5316\u6027\u4e09\u4e2a\u65b9\u9762\u7684\u6027\u80fd", "conclusion": "ReaLM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4e3a\u5782\u76f4\u9886\u57df\u5e94\u7528\u63d0\u4f9b\u4e86\u5f3a\u5927\u800c\u7ecf\u6d4e\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12393", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12393", "abs": "https://arxiv.org/abs/2508.12393", "authors": ["Duzhen Zhang", "Zixiao Wang", "Zhong-Zhi Li", "Yahan Yu", "Shuncheng Jia", "Jiahua Dong", "Haotian Xu", "Xing Wu", "Yingying Zhang", "Tielin Zhang", "Jie Yang", "Xiuying Chen", "Le Song"], "title": "MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph", "comment": null, "summary": "The rapid expansion of medical literature presents growing challenges for\nstructuring and integrating domain knowledge at scale. Knowledge Graphs (KGs)\noffer a promising solution by enabling efficient retrieval, automated\nreasoning, and knowledge discovery. However, current KG construction methods\noften rely on supervised pipelines with limited generalizability or naively\naggregate outputs from Large Language Models (LLMs), treating biomedical\ncorpora as static and ignoring the temporal dynamics and contextual uncertainty\nof evolving knowledge. To address these limitations, we introduce MedKGent, a\nLLM agent framework for constructing temporally evolving medical KGs.\nLeveraging over 10 million PubMed abstracts published between 1975 and 2023, we\nsimulate the emergence of biomedical knowledge via a fine-grained daily time\nseries. MedKGent incrementally builds the KG in a day-by-day manner using two\nspecialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor\nAgent identifies knowledge triples and assigns confidence scores via\nsampling-based estimation, which are used to filter low-confidence extractions\nand inform downstream processing. The Constructor Agent incrementally\nintegrates the retained triples into a temporally evolving graph, guided by\nconfidence scores and timestamps to reinforce recurring knowledge and resolve\nconflicts. The resulting KG contains 156,275 entities and 2,971,384 relational\ntriples. Quality assessments by two SOTA LLMs and three domain experts\ndemonstrate an accuracy approaching 90\\%, with strong inter-rater agreement. To\nevaluate downstream utility, we conduct RAG across seven medical question\nanswering benchmarks using five leading LLMs, consistently observing\nsignificant improvements over non-augmented baselines. Case studies further\ndemonstrate the KG's value in literature-based drug repurposing via\nconfidence-aware causal inference.", "AI": {"tldr": "MedKGent\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u65f6\u95f4\u6f14\u5316\u7684\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7\u63d0\u53d6\u548c\u6784\u5efa\u4ee3\u7406\u5904\u7406PubMed\u6458\u8981\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u65f6\u95f4\u611f\u77e5\u7684\u533b\u5b66KG\uff0c\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f53\u524d\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u65b9\u6cd5\u5b58\u5728\u6cdb\u5316\u6027\u6709\u9650\u3001\u5ffd\u89c6\u65f6\u95f4\u52a8\u6001\u6027\u548c\u4e0a\u4e0b\u6587\u4e0d\u786e\u5b9a\u6027\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u533b\u5b66\u77e5\u8bc6\u6f14\u5316\u7684\u81ea\u52a8\u5316\u6784\u5efa\u6846\u67b6\u3002", "method": "\u4f7f\u7528Qwen2.5-32B-Instruct\u6a21\u578b\u9a71\u52a8\u7684\u4e24\u4e2a\u4e13\u95e8\u4ee3\u7406\uff1a\u63d0\u53d6\u4ee3\u7406\u901a\u8fc7\u91c7\u6837\u4f30\u8ba1\u7f6e\u4fe1\u5ea6\u8bc6\u522b\u77e5\u8bc6\u4e09\u5143\u7ec4\uff0c\u6784\u5efa\u4ee3\u7406\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u548c\u65f6\u95f4\u6233\u589e\u91cf\u6574\u5408\u4e09\u5143\u7ec4\u5230\u65f6\u95f4\u6f14\u5316\u56fe\u8c31\u4e2d\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b156,275\u4e2a\u5b9e\u4f53\u548c2,971,384\u4e2a\u5173\u7cfb\u4e09\u5143\u7ec4\u7684KG\uff0c\u51c6\u786e\u7387\u63a5\u8fd190%\uff0c\u57287\u4e2a\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u975e\u589e\u5f3a\u57fa\u7ebf\u3002", "conclusion": "MedKGent\u6210\u529f\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u7684\u65f6\u95f4\u6f14\u5316\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\uff0c\u5c55\u793a\u4e86\u5728\u836f\u7269\u91cd\u5b9a\u4f4d\u7b49\u4e0b\u6e38\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\uff0c\u4e3a\u5927\u89c4\u6a21\u533b\u5b66\u77e5\u8bc6\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12405", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12405", "abs": "https://arxiv.org/abs/2508.12405", "authors": ["Zilong Bai", "Zihan Xu", "Cong Sun", "Chengxi Zang", "H. Timothy Bunnell", "Catherine Sinfield", "Jacqueline Rutter", "Aaron Thomas Martinez", "L. Charles Bailey", "Mark Weiner", "Thomas R. Campion", "Thomas Carton", "Christopher B. Forrest", "Rainu Kaushal", "Fei Wang", "Yifan Peng"], "title": "Extracting Post-Acute Sequelae of SARS-CoV-2 Infection Symptoms from Clinical Notes via Hybrid Natural Language Processing", "comment": "Accepted for publication in npj Health Systems", "summary": "Accurately and efficiently diagnosing Post-Acute Sequelae of COVID-19 (PASC)\nremains challenging due to its myriad symptoms that evolve over long- and\nvariable-time intervals. To address this issue, we developed a hybrid natural\nlanguage processing pipeline that integrates rule-based named entity\nrecognition with BERT-based assertion detection modules for PASC-symptom\nextraction and assertion detection from clinical notes. We developed a\ncomprehensive PASC lexicon with clinical specialists. From 11 health systems of\nthe RECOVER initiative network across the U.S., we curated 160 intake progress\nnotes for model development and evaluation, and collected 47,654 progress notes\nfor a population-level prevalence study. We achieved an average F1 score of\n0.82 in one-site internal validation and 0.76 in 10-site external validation\nfor assertion detection. Our pipeline processed each note at $2.448\\pm 0.812$\nseconds on average. Spearman correlation tests showed $\\rho >0.83$ for positive\nmentions and $\\rho >0.72$ for negative ones, both with $P <0.0001$. These\ndemonstrate the effectiveness and efficiency of our models and their potential\nfor improving PASC diagnosis.", "AI": {"tldr": "\u4e00\u79cd\u6df7\u5408\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6d41\u6c34\u7ebf\uff0c\u7ed3\u5408\u89c4\u5219\u57fa\u4e8e\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548cBERT\u57fa\u4e8e\u65ad\u8a00\u68c0\u6d4b\uff0c\u7528\u4e8e\u4ece\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u63d0\u53d6COVID-19\u540e\u9057\u75c7\u72b6\u7684\u6709\u6548\u8bc6\u522b\u548c\u65ad\u8a00\u68c0\u6d4b\u3002", "motivation": "\u51c6\u786e\u6709\u6548\u8bca\u65adCOVID-19\u540e\u9057\u75c7(PASC)\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u5176\u75c7\u72b6\u591a\u6837\u4e14\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u5927\u91cf\u4e34\u5e8a\u7b14\u8bb0\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u6df7\u5408NLP\u6d41\u6c34\u7ebf\uff0c\u96c6\u6210\u89c4\u5219\u57fa\u4e8e\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548cBERT\u57fa\u4e8e\u65ad\u8a00\u68c0\u6d4b\u6a21\u5757\uff0c\u4f7f\u7528\u4e13\u4e1a\u533b\u751f\u5f00\u53d1\u7684\u5168\u9762PASC\u8bcd\u5178\uff0c\u57fa\u4e8e\u7f8e\u56fd11\u4e2a\u5065\u5eb7\u7cfb\u7edf\u7684160\u4efd\u8fdb\u5c55\u7b14\u8bb0\u8fdb\u884c\u6a21\u578b\u5f00\u53d1\u548c\u8bc4\u4f30\u3002", "result": "\u5728\u65ad\u8a00\u68c0\u6d4b\u4e2d\u83b7\u5f97\u5185\u90e8\u9a8c\u8bc1\u5e73\u5747F1\u52060.82\uff0c\u5916\u90e8\u9a8c\u8bc1\u5e73\u5747F1\u52060.76\uff0c\u6bcf\u4efd\u7b14\u8bb0\u5904\u7406\u65f6\u95f4\u4ec5\u97002.448\u79d2\uff0c\u76f8\u5173\u6027\u6d4b\u8bd5\u663e\u793a\u9ad8\u5ea6\u663e\u8457\u76f8\u5173(\u03c1>0.83\u6b63\u9762\u63d0\u53ca\uff0c\u03c1>0.72\u8d1f\u9762\u63d0\u53ca\uff0cP<0.0001)\u3002", "conclusion": "\u8be5\u6a21\u578b\u663e\u793a\u4e86\u5728PASC\u8bca\u65ad\u4e2d\u7684\u6709\u6548\u6027\u548c\u6548\u7387\uff0c\u4e3a\u6539\u5584PASC\u8bca\u65ad\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2508.12407", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12407", "abs": "https://arxiv.org/abs/2508.12407", "authors": ["Zhuorui Liu", "Chen Zhang", "Dawei Song"], "title": "ZigzagAttention: Efficient Long-Context Inference with Exclusive Retrieval and Streaming Heads", "comment": "5 pages, 4 figures", "summary": "With the rapid development of large language models (LLMs), handling long\ncontext has become one of the vital abilities in LLMs. Such long-context\nability is accompanied by difficulties in deployment, especially due to the\nincreased consumption of KV cache. There is certain work aiming to optimize the\nmemory footprint of KV cache, inspired by the observation that attention heads\ncan be categorized into retrieval heads that are of great significance and\nstreaming heads that are of less significance. Typically, identifying the\nstreaming heads and and waiving the KV cache in the streaming heads would\nlargely reduce the overhead without hurting the performance that much. However,\nsince employing both retrieval and streaming heads in one layer decomposes one\nlarge round of attention computation into two small ones, it may unexpectedly\nbring extra latency on accessing and indexing tensors. Based on this intuition,\nwe impose an important improvement to the identification process of retrieval\nand streaming heads, in which we design a criterion that enforces exclusively\nretrieval or streaming heads gathered in one unique layer. In this way, we\nfurther eliminate the extra latency and only incur negligible performance\ndegradation. Our method named \\textsc{ZigzagAttention} is competitive among\nconsidered baselines owing to reduced latency and comparable performance.", "AI": {"tldr": "ZigzagAttention\u901a\u8fc7\u5c06\u6ce8\u610f\u529b\u5934\u6309\u5c42\u5206\u7c7b\u4e3a\u68c0\u7d22\u5934\u6216\u6d41\u5f0f\u5934\uff0c\u4f18\u5316KV\u7f13\u5b58\u5185\u5b58\u5360\u7528\uff0c\u51cf\u5c11\u5ef6\u8fdf\u540c\u65f6\u4fdd\u6301\u6027\u80fd", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6KV\u7f13\u5b58\u6d88\u8017\u5de8\u5927\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u533a\u5206\u91cd\u8981\u68c0\u7d22\u5934\u548c\u4e0d\u91cd\u8981\u6d41\u5f0f\u5934\u6765\u4f18\u5316\u5185\u5b58\uff0c\u4f46\u6df7\u5408\u5934\u7c7b\u578b\u4f1a\u5bfc\u81f4\u989d\u5916\u7684\u5f20\u91cf\u8bbf\u95ee\u5ef6\u8fdf", "method": "\u8bbe\u8ba1\u65b0\u6807\u51c6\u5f3a\u5236\u6bcf\u5c42\u53ea\u5305\u542b\u68c0\u7d22\u5934\u6216\u6d41\u5f0f\u5934\uff0c\u907f\u514d\u6df7\u5408\u5934\u5e26\u6765\u7684\u989d\u5916\u5ef6\u8fdf\uff0c\u5b9e\u73b0KV\u7f13\u5b58\u4f18\u5316", "result": "\u663e\u8457\u51cf\u5c11\u5ef6\u8fdf\uff0c\u6027\u80fd\u635f\u5931\u53ef\u5ffd\u7565\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u7ade\u4e89\u529b", "conclusion": "\u6309\u5c42\u5206\u7c7b\u6ce8\u610f\u529b\u5934\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u4f18\u5316KV\u7f13\u5b58\uff0c\u5728\u51cf\u5c11\u5ef6\u8fdf\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd"}}
{"id": "2508.12411", "categories": ["cs.CL", "I.2.7; K.4.1; H.3.3"], "pdf": "https://arxiv.org/pdf/2508.12411", "abs": "https://arxiv.org/abs/2508.12411", "authors": ["Emanuel Z. Fenech-Borg", "Tilen P. Meznaric-Kos", "Milica D. Lekovic-Bojovic", "Arni J. Hentze-Djurhuus"], "title": "The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus Training on Model Values and Biases", "comment": "10 pages, 5 figures, IEEE conference format, submitted to [Conference\n  Name]", "summary": "Large language models (LLMs) are deployed globally, yet their underlying\ncultural and ethical assumptions remain underexplored. We propose the notion of\na \"cultural gene\" -- a systematic value orientation that LLMs inherit from\ntheir training corpora -- and introduce a Cultural Probe Dataset (CPD) of 200\nprompts targeting two classic cross-cultural dimensions:\nIndividualism-Collectivism (IDV) and Power Distance (PDI). Using standardized\nzero-shot prompts, we compare a Western-centric model (GPT-4) and an\nEastern-centric model (ERNIE Bot). Human annotation shows significant and\nconsistent divergence across both dimensions. GPT-4 exhibits individualistic\nand low-power-distance tendencies (IDV score approx 1.21; PDI score approx\n-1.05), while ERNIE Bot shows collectivistic and higher-power-distance\ntendencies (IDV approx -0.89; PDI approx 0.76); differences are statistically\nsignificant (p < 0.001). We further compute a Cultural Alignment Index (CAI)\nagainst Hofstede's national scores and find GPT-4 aligns more closely with the\nUSA (e.g., IDV CAI approx 0.91; PDI CAI approx 0.88) whereas ERNIE Bot aligns\nmore closely with China (IDV CAI approx 0.85; PDI CAI approx 0.81). Qualitative\nanalyses of dilemma resolution and authority-related judgments illustrate how\nthese orientations surface in reasoning. Our results support the view that LLMs\nfunction as statistical mirrors of their cultural corpora and motivate\nculturally aware evaluation and deployment to avoid algorithmic cultural\nhegemony.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u7d22\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5316\u504f\u5411\uff0c\u901a\u8fc7\u6587\u5316\u6316\u6398\u6570\u636e\u96c6\u5bf9\u6bd4GPT-4\u548cERNIE Bot\uff0c\u53d1\u73b0\u897f\u65b9\u6a21\u578b\u5448\u73b0\u4e2a\u4eba\u4e3b\u4e49\u548c\u4f4e\u6743\u529b\u8ddd\u79bb\u7279\u5f81\uff0c\u800c\u4e1c\u65b9\u6a21\u578b\u5448\u73b0\u96c6\u4f53\u4e3b\u4e49\u548c\u9ad8\u6743\u529b\u8ddd\u79bb\u7279\u5f81", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5168\u7403\u90e8\u7f72\uff0c\u4f46\u5176\u57fa\u7840\u6587\u5316\u548c\u4f26\u7406\u504f\u5411\u5f88\u5c11\u88ab\u63a2\u7d22\uff0c\u9700\u8981\u7814\u7a76LLMs\u5982\u4f55\u7ee7\u627f\u8bad\u7ec3\u8bed\u6599\u5e93\u7684\u7cfb\u7edf\u6027\u4ef7\u503c\u89c2\u5f0f", "method": "\u63d0\u51fa\"\u6587\u5316\u57fa\u56e0\"\u6982\u5ff5\uff0c\u6784\u5efa\u5305\u542b200\u4e2a\u63d0\u793a\u7684\u6587\u5316\u6316\u6398\u6570\u636e\u96c6(CPD)\uff0c\u91cd\u70b9\u8003\u5bdf\u4e2a\u4eba\u4e3b\u4e49-\u96c6\u4f53\u4e3b\u4e49(IDV)\u548c\u6743\u529b\u8ddd\u79bb(PDI)\u4e24\u4e2a\u6587\u5316\u7ef4\u5ea6\uff0c\u4f7f\u7528\u6807\u51c6\u5316\u96f6\u6837\u672c\u63d0\u793a\u6bd4\u8f83GPT-4\u548cERNIE Bot", "result": "\u4eba\u5de5\u6807\u6ce8\u663e\u793a\u4e24\u4e2a\u6a21\u578b\u5728\u4e24\u4e2a\u7ef4\u5ea6\u4e0a\u90fd\u5b58\u5728\u663e\u8457\u548c\u4e00\u81f4\u7684\u5206\u5f02\uff1aGPT-4\u5448\u73b0\u4e2a\u4eba\u4e3b\u4e49(IDV\u7ea61.21)\u548c\u4f4e\u6743\u529b\u8ddd\u79bb(PDI\u7ea6-1.05)\u503e\u5411\uff0cERNIE Bot\u5448\u73b0\u96c6\u4f53\u4e3b\u4e49(IDV\u7ea6-0.89)\u548c\u9ad8\u6743\u529b\u8ddd\u79bb(PDI\u7ea60.76)\u503e\u5411\uff0c\u5dee\u5f02\u7edf\u8ba1\u663e\u8457(p<0.001)\uff0c\u6587\u5316\u5bf9\u9f50\u6307\u6570\u663e\u793aGPT-4\u66f4\u63a5\u8fd1\u7f8e\u56fd\uff0cERNIE Bot\u66f4\u63a5\u8fd1\u4e2d\u56fd", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5176\u6587\u5316\u8bed\u6599\u5e93\u7684\u7edf\u8ba1\u955c\u50cf\uff0c\u5177\u6709\u660e\u663e\u7684\u6587\u5316\u504f\u5411\uff0c\u9700\u8981\u6587\u5316\u610f\u8bc6\u7684\u8bc4\u4f30\u548c\u90e8\u7f72\u4ee5\u907f\u514d\u7b97\u6cd5\u6587\u5316\u9738\u6743"}}
{"id": "2508.12448", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12448", "abs": "https://arxiv.org/abs/2508.12448", "authors": ["Yeongwoo Song", "Jaeyong Bae", "Dong-Kyum Kim", "Hawoong Jeong"], "title": "Uncovering Emergent Physics Representations Learned In-Context by Large Language Models", "comment": "17 pages, 10 figures", "summary": "Large language models (LLMs) exhibit impressive in-context learning (ICL)\nabilities, enabling them to solve wide range of tasks via textual prompts\nalone. As these capabilities advance, the range of applicable domains continues\nto expand significantly. However, identifying the precise mechanisms or\ninternal structures within LLMs that allow successful ICL across diverse,\ndistinct classes of tasks remains elusive. Physics-based tasks offer a\npromising testbed for probing this challenge. Unlike synthetic sequences such\nas basic arithmetic or symbolic equations, physical systems provide\nexperimentally controllable, real-world data based on structured dynamics\ngrounded in fundamental principles. This makes them particularly suitable for\nstudying the emergent reasoning behaviors of LLMs in a realistic yet tractable\nsetting. Here, we mechanistically investigate the ICL ability of LLMs,\nespecially focusing on their ability to reason about physics. Using a dynamics\nforecasting task in physical systems as a proxy, we evaluate whether LLMs can\nlearn physics in context. We first show that the performance of dynamics\nforecasting in context improves with longer input contexts. To uncover how such\ncapability emerges in LLMs, we analyze the model's residual stream activations\nusing sparse autoencoders (SAEs). Our experiments reveal that the features\ncaptured by SAEs correlate with key physical variables, such as energy. These\nfindings demonstrate that meaningful physical concepts are encoded within LLMs\nduring in-context learning. In sum, our work provides a novel case study that\nbroadens our understanding of how LLMs learn in context.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u7269\u7406\u4efb\u52a1\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u80fd\u591f\u5b66\u4e60\u7269\u7406\u6982\u5ff5\u5e76\u7f16\u7801\u5173\u952e\u7269\u7406\u53d8\u91cf\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u5176\u5185\u90e8\u673a\u5236\u4ecd\u4e0d\u660e\u786e\u3002\u7269\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u57fa\u4e8e\u57fa\u7840\u539f\u7406\u7684\u7ed3\u6784\u5316\u52a8\u6001\u6570\u636e\uff0c\u9002\u5408\u63a2\u7d22LLM\u7684\u51fa\u73b0\u6027\u63a8\u7406\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u7269\u7406\u7cfb\u7edf\u52a8\u6001\u9884\u6d4b\u4efb\u52a1\u4f5c\u4e3a\u4ee3\u7406\uff0c\u9010\u6b65\u589e\u52a0\u8f93\u5165\u4e0a\u4e0b\u6587\u957f\u5ea6\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAEs)\u5206\u6790\u6a21\u578b\u6fc0\u6d3b\u72b6\u6001\uff0c\u8bc6\u522b\u4e0e\u7269\u7406\u53d8\u91cf\u76f8\u5173\u7684\u7279\u5f81\u3002", "result": "\u52a8\u6001\u9884\u6d4b\u6027\u80fd\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u800c\u63d0\u5347\u3002SAEs\u6355\u83b7\u7684\u7279\u5f81\u4e0e\u80fd\u91cf\u7b49\u5173\u952e\u7269\u7406\u53d8\u91cf\u5448\u73b0\u663e\u8457\u76f8\u5173\u6027\uff0c\u8bc1\u660eLLM\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7f16\u7801\u4e86\u6709\u610f\u4e49\u7684\u7269\u7406\u6982\u5ff5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3LLM\u5982\u4f55\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u6269\u5c55\u4e86\u6211\u4eec\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u4efb\u52a1\u4e2d\u5b66\u4e60\u6982\u5ff5\u673a\u5236\u7684\u8ba4\u77e5\u3002"}}
{"id": "2508.12458", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12458", "abs": "https://arxiv.org/abs/2508.12458", "authors": ["Ruirui Gao", "Emily Johnson", "Bowen Tan", "Yanfei Qian"], "title": "M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following", "comment": null, "summary": "Large Vision-Language Models (LVLMs) hold immense potential for complex\nmultimodal instruction following, yet their development is often hindered by\nthe high cost and inconsistency of human annotation required for effective\nfine-tuning and preference alignment. Traditional supervised fine-tuning (SFT)\nand existing preference optimization methods like RLHF and DPO frequently\nstruggle to efficiently leverage the model's own generation space to identify\nhighly informative \"hard negative\" samples. To address these challenges, we\npropose Multimodal-Model-Guided Preference Optimization (M3PO), a novel and\ndata-efficient method designed to enhance LVLMs' capabilities in visual\ninstruction following. M3PO intelligently selects the most \"learning-valuable\"\npreference sample pairs from a diverse pool of LVLM-generated candidates. This\nselection is driven by a sophisticated mechanism that integrates two crucial\nsignals: a Multimodal Alignment Score (MAS) to assess external quality and the\nmodel's Self-Consistency / Confidence (log-probability) to gauge internal\nbelief. These are combined into a novel M3P-Score, which specifically\nidentifies preferred responses and challenging dispreferred responses that the\nmodel might confidently generate despite being incorrect. These high-quality\npreference pairs are then used for efficient Direct Preference Optimization\n(DPO) fine-tuning on base LVLMs like LLaVA-1.5 (7B/13B) using LoRA. Our\nextensive experiments demonstrate that M3PO consistently outperforms strong\nbaselines, including SFT, simulated RLHF, vanilla DPO, and RM-DPO, across a\ncomprehensive suite of multimodal instruction following benchmarks (MME-Bench,\nPOPE, IFT, Human Pref. Score).", "AI": {"tldr": "\u901a\u8fc7\u667a\u80fd\u9009\u62e9\u591a\u6a21\u6001\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u504f\u597d\u5bf9\u8fdb\u884c\u654f\u611f\u5b66\u4e60\uff0cM3PO\u65b9\u6cd5\u5728\u4f4e\u6210\u672c\u4e0b\u663e\u8457\u63d0\u5347\u5927\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u6307\u4ee4\u8ffd\u968f\u80fd\u529b", "motivation": "\u89e3\u51b3\u5927\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u4e2d\u4eba\u5de5\u6ce8\u91ca\u6210\u672c\u9ad8\u3001\u4e00\u81f4\u6027\u5dee\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u4f20\u7edfSFT\u548c\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u65e0\u6cd5\u9ad8\u6548\u5229\u7528\u6a21\u578b\u81ea\u8eab\u751f\u6210\u7a7a\u95f4\u8bc6\u522b\u96be\u6837\u672c\u7684\u6311\u6218", "method": "\u63d0\u51faM3PO\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5bf9\u9f50\u5206\u6570(MAS)\u548c\u6a21\u578b\u81ea\u8eab\u4e00\u81f4\u6027/\u4fe1\u5fc3\u5ea6\u7ef4\u5ea6\uff0c\u7ec4\u5408\u6210M3P-Score\uff0c\u667a\u80fd\u9009\u62e9\u6700\u6709\u5b66\u4e60\u4ef7\u503c\u7684\u504f\u597d\u5bf9\u8fdb\u884cDPO\u7cbe\u7ec6\u8c03\u6574", "result": "\u5728\u591a\u6a21\u6001\u6307\u4ee4\u8ffd\u968f\u6d4b\u8bd5\u96c6(MME-Bench, POPE, IFT, \u4eba\u7c7b\u504f\u597d\u5206\u6570)\u4e0a\u6301\u7eed\u8d85\u8d8aSFT\u3001\u6a21\u62dfRLHF\u3001\u666e\u901aDPO\u548cRM-DPO\u7b49\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "M3PO\u4e3aLVLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u3001\u6210\u672c\u4f4e\u5ec9\u7684\u504f\u597d\u4f18\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u667a\u80fd\u6837\u672c\u9009\u62e9\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u591a\u6a21\u6001\u6307\u4ee4\u8ffd\u968f\u80fd\u529b"}}
{"id": "2508.12459", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12459", "abs": "https://arxiv.org/abs/2508.12459", "authors": ["Alham Fikri Aji", "Trevor Cohn"], "title": "LoraxBench: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages", "comment": null, "summary": "As one of the world's most populous countries, with 700 languages spoken,\nIndonesia is behind in terms of NLP progress. We introduce LoraxBench, a\nbenchmark that focuses on low-resource languages of Indonesia and covers 6\ndiverse tasks: reading comprehension, open-domain QA, language inference,\ncausal reasoning, translation, and cultural QA. Our dataset covers 20\nlanguages, with the addition of two formality registers for three languages. We\nevaluate a diverse set of multilingual and region-focused LLMs and found that\nthis benchmark is challenging. We note a visible discrepancy between\nperformance in Indonesian and other languages, especially the low-resource\nones. There is no clear lead when using a region-specific model as opposed to\nthe general multilingual model. Lastly, we show that a change in register\naffects model performance, especially with registers not commonly found in\nsocial media, such as high-level politeness `Krama' Javanese.", "AI": {"tldr": "LoraxBench\u662f\u4e00\u4e2a\u9488\u5bf9\u5370\u5c3c\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d66\u4e2a\u4efb\u52a1\u548c20\u79cd\u8bed\u8a00\uff0c\u53d1\u73b0\u591a\u8bed\u8a00\u6a21\u578b\u5728\u5370\u5c3c\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u8868\u73b0\u5b58\u5728\u660e\u663e\u5dee\u8ddd\uff0c\u4e14\u8bed\u57df\u53d8\u5316\u663e\u8457\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5370\u5c3c\u4f5c\u4e3a\u4e16\u754c\u4eba\u53e3\u5927\u56fd\uff0c\u62e5\u6709700\u79cd\u8bed\u8a00\uff0c\u4f46\u5728NLP\u9886\u57df\u8fdb\u5c55\u76f8\u5bf9\u843d\u540e\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u80fd\u529b\u3002", "method": "\u6784\u5efaLoraxBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u9605\u8bfb\u7406\u89e3\u3001\u5f00\u653e\u57df\u95ee\u7b54\u3001\u8bed\u8a00\u63a8\u7406\u3001\u56e0\u679c\u63a8\u7406\u3001\u7ffb\u8bd1\u548c\u6587\u5316\u95ee\u7b546\u4e2a\u4efb\u52a1\uff0c\u8986\u76d620\u79cd\u5370\u5c3c\u8bed\u8a00\uff0c\u5e76\u4e3a3\u79cd\u8bed\u8a00\u6dfb\u52a0\u4e24\u79cd\u6b63\u5f0f\u5ea6\u8bed\u57df\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u5177\u6709\u6311\u6218\u6027\uff0c\u5370\u5c3c\u8bed\u4e0e\u5176\u4ed6\u4f4e\u8d44\u6e90\u8bed\u8a00\u6027\u80fd\u5b58\u5728\u660e\u663e\u5dee\u8ddd\uff0c\u533a\u57df\u7279\u5b9a\u6a21\u578b\u4e0e\u901a\u7528\u591a\u8bed\u8a00\u6a21\u578b\u76f8\u6bd4\u6ca1\u6709\u660e\u663e\u4f18\u52bf\uff0c\u8bed\u57df\u53d8\u5316\uff08\u7279\u522b\u662f\u793e\u4ea4\u5a92\u4f53\u4e2d\u4e0d\u5e38\u89c1\u7684\u9ad8\u7ea7\u793c\u8c8c\u8bed\u57df\uff09\u663e\u8457\u5f71\u54cd\u6a21\u578b\u8868\u73b0\u3002", "conclusion": "\u5370\u5c3c\u4f4e\u8d44\u6e90\u8bed\u8a00NLP\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u66f4\u591a\u9488\u5bf9\u6027\u7684\u7814\u7a76\u548c\u6a21\u578b\u4f18\u5316\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4e0d\u540c\u8bed\u57df\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u65b9\u9762\u3002"}}
{"id": "2508.12461", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12461", "abs": "https://arxiv.org/abs/2508.12461", "authors": ["Ziqian Bi", "Keyu Chen", "Chiung-Yi Tseng", "Danyang Zhang", "Tianyang Wang", "Hongying Luo", "Lu Chen", "Junming Huang", "Jibin Guan", "Junfeng Hao", "Junhao Song"], "title": "Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models", "comment": null, "summary": "In August 2025, OpenAI released GPT-OSS models, its first open weight large\nlanguage models since GPT-2 in 2019, comprising two mixture of experts\narchitectures with 120B and 20B parameters. We evaluated both variants against\nsix contemporary open source large language models ranging from 14.7B to 235B\nparameters, representing both dense and sparse designs, across ten benchmarks\ncovering general knowledge, mathematical reasoning, code generation,\nmultilingual understanding, and conversational ability. All models were tested\nin unquantised form under standardised inference settings, with statistical\nvalidation using McNemars test and effect size analysis. Results show that\ngpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such\nas HumanEval and MMLU, despite requiring substantially less memory and energy\nper response. Both models demonstrate mid-tier overall performance within the\ncurrent open source landscape, with relative strength in code generation and\nnotable weaknesses in multilingual tasks. These findings provide empirical\nevidence that scaling in sparse architectures may not yield proportional\nperformance gains, underscoring the need for further investigation into\noptimisation strategies and informing more efficient model selection for future\nopen source deployments.", "AI": {"tldr": "OpenAI\u53d1\u5e03GPT-OSS\u5f00\u6e90\u5927\u6a21\u578b\uff0820B\u548c120B\u53c2\u6570\uff09\uff0c\u8bc4\u4f30\u663e\u793a\u8f83\u5c0f\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u7a00\u758f\u67b6\u6784\u6269\u5c55\u4e0d\u4e00\u5b9a\u5e26\u6765\u6027\u80fd\u63d0\u5347", "motivation": "\u8bc4\u4f30OpenAI\u9996\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578bGPT-OSS\u7684\u6027\u80fd\u8868\u73b0\uff0c\u6bd4\u8f83\u4e0d\u540c\u89c4\u6a21\u7a00\u758f\u67b6\u6784\u6a21\u578b\u7684\u6548\u7387\uff0c\u4e3a\u5f00\u6e90\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e", "method": "\u4f7f\u752810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f306\u4e2a\u5f00\u6e90\u5927\u6a21\u578b\uff0814.7B-235B\u53c2\u6570\uff09\uff0c\u5305\u62ec\u5bc6\u96c6\u548c\u7a00\u758f\u67b6\u6784\uff0c\u91c7\u7528\u6807\u51c6\u5316\u63a8\u7406\u8bbe\u7f6e\u548c\u7edf\u8ba1\u9a8c\u8bc1\u65b9\u6cd5", "result": "GPT-OSS-20B\u5728\u591a\u4e2a\u6d4b\u8bd5\u4e2d\u4f18\u4e8eGPT-OSS-120B\uff0c\u5185\u5b58\u548c\u80fd\u8017\u66f4\u4f4e\uff1b\u4e24\u6a21\u578b\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u5904\u4e8e\u4e2d\u6e38\u6c34\u5e73\uff0c\u4ee3\u7801\u751f\u6210\u5f3a\u4f46\u591a\u8bed\u8a00\u80fd\u529b\u5f31", "conclusion": "\u7a00\u758f\u67b6\u6784\u7684\u6269\u5c55\u4e0d\u4e00\u5b9a\u5e26\u6765\u6027\u80fd\u7684\u6210\u6bd4\u4f8b\u63d0\u5347\uff0c\u9700\u8981\u4f18\u5316\u7b56\u7565\u7814\u7a76\uff0c\u4e3a\u672a\u6765\u5f00\u6e90\u90e8\u7f72\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u9009\u62e9\u6307\u5bfc"}}
{"id": "2508.12482", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12482", "abs": "https://arxiv.org/abs/2508.12482", "authors": ["Xiaomeng Zhu", "R. Thomas McCoy", "Robert Frank"], "title": "The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping", "comment": null, "summary": "Syntactic bootstrapping (Gleitman, 1990) is the hypothesis that children use\nthe syntactic environments in which a verb occurs to learn its meaning. In this\npaper, we examine whether large language models exhibit a similar behavior. We\ndo this by training RoBERTa and GPT-2 on perturbed datasets where syntactic\ninformation is ablated. Our results show that models' verb representation\ndegrades more when syntactic cues are removed than when co-occurrence\ninformation is removed. Furthermore, the representation of mental verbs, for\nwhich syntactic bootstrapping has been shown to be particularly crucial in\nhuman verb learning, is more negatively impacted in such training regimes than\nphysical verbs. In contrast, models' representation of nouns is affected more\nwhen co-occurrences are distorted than when syntax is distorted. In addition to\nreinforcing the important role of syntactic bootstrapping in verb learning, our\nresults demonstrated the viability of testing developmental hypotheses on a\nlarger scale through manipulating the learning environments of large language\nmodels.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.12495", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12495", "abs": "https://arxiv.org/abs/2508.12495", "authors": ["Yuangang Li", "Yiqing Shen", "Yi Nian", "Jiechao Gao", "Ziyi Wang", "Chenxiao Yu", "Shawn Li", "Jie Wang", "Xiyang Hu", "Yue Zhao"], "title": "Mitigating Hallucinations in Large Language Models via Causal Reasoning", "comment": null, "summary": "Large language models (LLMs) exhibit logically inconsistent hallucinations\nthat appear coherent yet violate reasoning principles, with recent research\nsuggesting an inverse relationship between causal reasoning capabilities and\nsuch hallucinations. However, existing reasoning approaches in LLMs, such as\nChain-of-Thought (CoT) and its graph-based variants, operate at the linguistic\ntoken level rather than modeling the underlying causal relationships between\nvariables, lacking the ability to represent conditional independencies or\nsatisfy causal identification assumptions. To bridge this gap, we introduce\ncausal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning\nframework that trains LLMs to explicitly construct variable-level directed\nacyclic graph (DAG) and then perform reasoning over it. Moreover, we present a\ndataset comprising 25,368 samples (CausalDR), where each sample includes an\ninput question, explicit causal DAG, graph-based reasoning trace, and validated\nanswer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves\nthe causal reasoning capability with the state-of-the-art 95.33% accuracy on\nCLADDER (surpassing human performance of 94.8% for the first time) and reduces\nthe hallucination on HaluEval with 10% improvements. It demonstrates that\nexplicit causal structure modeling in LLMs can effectively mitigate logical\ninconsistencies in LLM outputs. Code is available at\nhttps://github.com/MrLYG/CDCR-SFT.", "AI": {"tldr": "\u63d0\u51faCDCR-SFT\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u6784\u5efa\u56e0\u679cDAG\u5e76\u8fdb\u884c\u63a8\u7406\uff0c\u6709\u6548\u63d0\u5347LLMs\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u5e76\u51cf\u5c11\u5e7b\u89c9", "motivation": "\u73b0\u6709LLMs\u63a8\u7406\u65b9\u6cd5\uff08\u5982CoT\uff09\u5728\u8bed\u8a00token\u5c42\u9762\u64cd\u4f5c\uff0c\u65e0\u6cd5\u5efa\u6a21\u53d8\u91cf\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u548c\u6761\u4ef6\u72ec\u7acb\u6027\uff0c\u5bfc\u81f4\u903b\u8f91\u4e0d\u4e00\u81f4\u7684\u5e7b\u89c9", "method": "\u76d1\u7763\u5fae\u8c03\u6846\u67b6CDCR-SFT\uff0c\u8bad\u7ec3LLMs\u663e\u5f0f\u6784\u5efa\u53d8\u91cf\u7ea7\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u5e76\u5728\u5176\u4e0a\u8fdb\u884c\u63a8\u7406\uff0c\u540c\u65f6\u6784\u5efa\u5305\u542b25,368\u6837\u672c\u7684CausalDR\u6570\u636e\u96c6", "result": "\u57288\u4e2a\u4efb\u52a1\u4e0a\u6d4b\u8bd54\u4e2aLLM\uff0cCDCR-SFT\u5728CLADDER\u4e0a\u8fbe\u523095.33%\u51c6\u786e\u7387\uff08\u9996\u6b21\u8d85\u8d8a\u4eba\u7c7b94.8%\uff09\uff0c\u5728HaluEval\u4e0a\u5e7b\u89c9\u51cf\u5c1110%", "conclusion": "LLMs\u4e2d\u663e\u5f0f\u56e0\u679c\u7ed3\u6784\u5efa\u6a21\u80fd\u6709\u6548\u51cf\u8f7b\u8f93\u51fa\u4e2d\u7684\u903b\u8f91\u4e0d\u4e00\u81f4\u6027\uff0c\u8bc1\u660e\u4e86\u56e0\u679c\u63a8\u7406\u80fd\u529b\u4e0e\u5e7b\u89c9\u4e4b\u95f4\u7684\u8d1f\u76f8\u5173\u5173\u7cfb"}}
{"id": "2508.12535", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12535", "abs": "https://arxiv.org/abs/2508.12535", "authors": ["Seonglae Cho", "Zekun Wu", "Adriano Koshiyama"], "title": "CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection", "comment": "42 pages, 9 tables", "summary": "Sparse Autoencoders (SAEs) can extract interpretable features from large\nlanguage models (LLMs) without supervision. However, their effectiveness in\ndownstream steering tasks is limited by the requirement for contrastive\ndatasets or large activation storage. To address these limitations, we propose\nCorrSteer, which selects features by correlating sample correctness with SAE\nactivations from generated tokens at inference time. This approach uses only\ninference-time activations to extract more relevant features, thereby avoiding\nspurious correlations. It also obtains steering coefficients from average\nactivations, automating the entire pipeline. Our method shows improved task\nperformance on QA, bias mitigation, jailbreaking prevention, and reasoning\nbenchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1%\nimprovement in MMLU performance and a +22.9% improvement in HarmBench with only\n4000 samples. Selected features demonstrate semantically meaningful patterns\naligned with each task's requirements, revealing the underlying capabilities\nthat drive performance. Our work establishes correlationbased selection as an\neffective and scalable approach for automated SAE steering across language\nmodel applications.", "AI": {"tldr": "CorrSteer\u662f\u4e00\u79cd\u57fa\u4e8e\u76f8\u5173\u6027\u9009\u62e9\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u63a8\u7406\u65f6\u6fc0\u6d3b\u6765\u81ea\u52a8\u5316\u7279\u5f81\u9009\u62e9\u548c\u8f6c\u5411\u7cfb\u6570\u8ba1\u7b97\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5728\u4e0b\u6e38\u8f6c\u5411\u4efb\u52a1\u4e2d\u9700\u8981\u5bf9\u6bd4\u6570\u636e\u96c6\u6216\u5927\u91cf\u6fc0\u6d3b\u5b58\u50a8\u7684\u9650\u5236\uff0c\u907f\u514d\u4f2a\u76f8\u5173\u6027\u3002", "method": "\u901a\u8fc7\u5c06\u6837\u672c\u6b63\u786e\u6027\u4e0e\u63a8\u7406\u65f6\u751f\u6210\u7684token\u7684SAE\u6fc0\u6d3b\u8fdb\u884c\u76f8\u5173\u6027\u5206\u6790\u6765\u9009\u62e9\u7279\u5f81\uff0c\u4f7f\u7528\u5e73\u5747\u6fc0\u6d3b\u83b7\u5f97\u8f6c\u5411\u7cfb\u6570\uff0c\u5b9e\u73b0\u5168\u81ea\u52a8\u5316\u6d41\u7a0b\u3002", "result": "\u5728Gemma 2 2B\u548cLLaMA 3.1 8B\u4e0a\uff0cQA\u3001\u504f\u89c1\u7f13\u89e3\u3001\u8d8a\u72f1\u9884\u9632\u548c\u63a8\u7406\u57fa\u51c6\u4efb\u52a1\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0cMMLU\u6027\u80fd\u63d0\u9ad84.1%\uff0cHarmBench\u63d0\u9ad822.9%\uff08\u4ec5\u75284000\u6837\u672c\uff09\u3002", "conclusion": "\u57fa\u4e8e\u76f8\u5173\u6027\u9009\u62e9\u7684\u65b9\u6cd5\u662f\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316SAE\u8f6c\u5411\u65b9\u6cd5\uff0c\u6240\u9009\u7279\u5f81\u5c55\u73b0\u51fa\u4e0e\u4efb\u52a1\u9700\u6c42\u4e00\u81f4\u7684\u8bed\u4e49\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u9a71\u52a8\u6027\u80fd\u7684\u5e95\u5c42\u80fd\u529b\u3002"}}
{"id": "2508.12630", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12630", "abs": "https://arxiv.org/abs/2508.12630", "authors": ["Maitreyi Chatterjee", "Devansh Agarwal"], "title": "Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context", "comment": "Paper is currently in peer review", "summary": "Large Language Models (LLMs) have demonstrated impressive fluency and task\ncompetence in conversational settings. However, their effectiveness in\nmulti-session and long-term interactions is hindered by limited memory\npersistence. Typical retrieval-augmented generation (RAG) systems store\ndialogue history as dense vectors, which capture semantic similarity but\nneglect finer linguistic structures such as syntactic dependencies, discourse\nrelations, and coreference links. We propose Semantic Anchoring, a hybrid\nagentic memory architecture that enriches vector-based storage with explicit\nlinguistic cues to improve recall of nuanced, context-rich exchanges. Our\napproach combines dependency parsing, discourse relation tagging, and\ncoreference resolution to create structured memory entries. Experiments on\nadapted long-term dialogue datasets show that semantic anchoring improves\nfactual recall and discourse coherence by up to 18% over strong RAG baselines.\nWe further conduct ablation studies, human evaluations, and error analysis to\nassess robustness and interpretability.", "AI": {"tldr": "\u63d0\u51faSemantic Anchoring\u6df7\u5408\u8bb0\u5fc6\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u5408\u4f9d\u8d56\u89e3\u6790\u3001\u7bc7\u7ae0\u5173\u7cfb\u6807\u6ce8\u548c\u5171\u6307\u6d88\u89e3\u6765\u589e\u5f3a\u5411\u91cf\u68c0\u7d22\uff0c\u5728\u957f\u671f\u5bf9\u8bdd\u4e2d\u6bd4\u4f20\u7edfRAG\u65b9\u6cd5\u63d0\u534718%\u7684\u4e8b\u5b9e\u56de\u5fc6\u548c\u7bc7\u7ae0\u8fde\u8d2f\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4f1a\u8bdd\u548c\u957f\u671f\u4ea4\u4e92\u4e2d\u53d7\u9650\u4e8e\u8bb0\u5fc6\u6301\u4e45\u6027\u95ee\u9898\uff0c\u4f20\u7edfRAG\u7cfb\u7edf\u4f7f\u7528\u5bc6\u96c6\u5411\u91cf\u5b58\u50a8\u5bf9\u8bdd\u5386\u53f2\uff0c\u867d\u7136\u80fd\u6355\u6349\u8bed\u4e49\u76f8\u4f3c\u6027\u4f46\u5ffd\u7565\u4e86\u66f4\u7cbe\u7ec6\u7684\u8bed\u8a00\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u4ee3\u7406\u8bb0\u5fc6\u67b6\u6784Semantic Anchoring\uff0c\u5728\u5411\u91cf\u5b58\u50a8\u57fa\u7840\u4e0a\u589e\u52a0\u663e\u5f0f\u8bed\u8a00\u7ebf\u7d22\uff0c\u7ed3\u5408\u4f9d\u8d56\u89e3\u6790\u3001\u7bc7\u7ae0\u5173\u7cfb\u6807\u6ce8\u548c\u5171\u6307\u6d88\u89e3\u6765\u521b\u5efa\u7ed3\u6784\u5316\u8bb0\u5fc6\u6761\u76ee\u3002", "result": "\u5728\u9002\u914d\u7684\u957f\u671f\u5bf9\u8bdd\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u8bed\u4e49\u951a\u5b9a\u76f8\u6bd4\u5f3a\u5927\u7684RAG\u57fa\u7ebf\u5728\u4e8b\u5b9e\u56de\u5fc6\u548c\u7bc7\u7ae0\u8fde\u8d2f\u6027\u65b9\u9762\u63d0\u5347\u9ad8\u8fbe18%\u3002", "conclusion": "\u901a\u8fc7\u878d\u5408\u663e\u5f0f\u8bed\u8a00\u7ed3\u6784\u548c\u5411\u91cf\u8868\u793a\uff0c\u8bed\u4e49\u951a\u5b9a\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u957f\u671f\u5bf9\u8bdd\u4e2d\u7684\u8bb0\u5fc6\u6027\u80fd\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2508.12631", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12631", "abs": "https://arxiv.org/abs/2508.12631", "authors": ["Yiqun Zhang", "Hao Li", "Jianhao Chen", "Hangfan Zhang", "Peng Ye", "Lei Bai", "Shuyue Hu"], "title": "Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing", "comment": "Ongoing work", "summary": "Balancing performance and efficiency is a central challenge in large language\nmodel (LLM) advancement. GPT-5 addresses this with test-time routing,\ndynamically assigning queries to either an efficient or a high-capacity model\nduring inference. In this work, we present Avengers-Pro, a test-time routing\nframework that ensembles LLMs of varying capacities and efficiencies, providing\na unified solution for all performance-efficiency tradeoffs. The Avengers-Pro\nembeds and clusters incoming queries, then routes each to the most suitable\nmodel based on a performance-efficiency score. Across 6 challenging benchmarks\nand 8 leading models -- including GPT-5-medium, Gemini-2.5-pro, and\nClaude-opus-4.1 -- Avengers-Pro achieves state-of-the-art results: by varying a\nperformance-efficiency trade-off parameter, it can surpass the strongest single\nmodel (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the\naverage accuracy of the strongest single model at 27% lower cost, and reach\n~90% of that performance at 63% lower cost. Last but not least, it achieves a\nPareto frontier, consistently yielding the highest accuracy for any given cost,\nand the lowest cost for any given accuracy, among all single models. Code is\navailable at https://github.com/ZhangYiqun018/AvengersPro.", "AI": {"tldr": "Avengers-Pro\u662f\u4e00\u4e2a\u6d4b\u8bd5\u65f6\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u914d\u67e5\u8be2\u7ed9\u4e0d\u540c\u5bb9\u91cf\u548c\u6548\u7387\u7684LLM\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u6700\u4f18\u5e73\u8861\uff0c\u57286\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u6700\u5f3a\u5355\u6a21\u578b7%\u51c6\u786e\u7387\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u7684\u5e73\u8861\u6311\u6218\uff0cGPT-5\u867d\u7136\u5f15\u5165\u4e86\u6d4b\u8bd5\u65f6\u8def\u7531\uff0c\u4f46\u9700\u8981\u66f4\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5904\u7406\u6240\u6709\u6027\u80fd-\u6548\u7387\u6743\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51faAvengers-Pro\u6d4b\u8bd5\u65f6\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u5d4c\u5165\u548c\u805a\u7c7b\u8f93\u5165\u67e5\u8be2\uff0c\u7136\u540e\u57fa\u4e8e\u6027\u80fd-\u6548\u7387\u5206\u6570\u5c06\u6bcf\u4e2a\u67e5\u8be2\u8def\u7531\u5230\u6700\u5408\u9002\u7684\u6a21\u578b\u3002\u96c6\u6210\u4e86\u4e0d\u540c\u5bb9\u91cf\u548c\u6548\u7387\u7684LLM\u3002", "result": "\u57286\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u548c8\u4e2a\u9886\u5148\u6a21\u578b\u4e0a\u5b9e\u73b0SOTA\u7ed3\u679c\uff1a\u8d85\u8d8a\u6700\u5f3a\u5355\u6a21\u578b(GPT-5-medium)7%\u5e73\u5747\u51c6\u786e\u7387\uff1b\u4ee527%\u66f4\u4f4e\u6210\u672c\u5339\u914d\u6700\u5f3a\u6a21\u578b\u6027\u80fd\uff1b\u4ee563%\u66f4\u4f4e\u6210\u672c\u8fbe\u523090%\u6027\u80fd\uff1b\u5b9e\u73b0\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "conclusion": "Avengers-Pro\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4efb\u4f55\u7ed9\u5b9a\u6210\u672c\u4e0b\u83b7\u5f97\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u6216\u5728\u4efb\u4f55\u7ed9\u5b9a\u51c6\u786e\u7387\u4e0b\u5b9e\u73b0\u6700\u4f4e\u6210\u672c\uff0c\u4e3aLLM\u7684\u6027\u80fd-\u6548\u7387\u6743\u8861\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2508.12632", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12632", "abs": "https://arxiv.org/abs/2508.12632", "authors": ["Chi Wang", "Min Gao", "Zongwei Wang", "Junwei Yin", "Kai Shu", "Chenghua Lin"], "title": "Prompt-Induced Linguistic Fingerprints for LLM-Generated Fake News Detection", "comment": null, "summary": "With the rapid development of large language models, the generation of fake\nnews has become increasingly effortless, posing a growing societal threat and\nunderscoring the urgent need for reliable detection methods. Early efforts to\nidentify LLM-generated fake news have predominantly focused on the textual\ncontent itself; however, because much of that content may appear coherent and\nfactually consistent, the subtle traces of falsification are often difficult to\nuncover. Through distributional divergence analysis, we uncover prompt-induced\nlinguistic fingerprints: statistically distinct probability shifts between\nLLM-generated real and fake news when maliciously prompted. Based on this\ninsight, we propose a novel method named Linguistic Fingerprints Extraction\n(LIFE). By reconstructing word-level probability distributions, LIFE can find\ndiscriminative patterns that facilitate the detection of LLM-generated fake\nnews. To further amplify these fingerprint patterns, we also leverage\nkey-fragment techniques that accentuate subtle linguistic differences, thereby\nimproving detection reliability. Our experiments show that LIFE achieves\nstate-of-the-art performance in LLM-generated fake news and maintains high\nperformance in human-written fake news. The code and data are available at\nhttps://anonymous.4open.science/r/LIFE-E86A.", "AI": {"tldr": "\u63d0\u51faLIFE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790LLM\u751f\u6210\u771f\u5047\u65b0\u95fb\u7684\u6982\u7387\u5206\u5e03\u5dee\u5f02\u6765\u68c0\u6d4bAI\u751f\u6210\u7684\u5047\u65b0\u95fb\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5feb\u901f\u53d1\u5c55\uff0c\u5047\u65b0\u95fb\u751f\u6210\u53d8\u5f97\u5bb9\u6613\u4e14\u96be\u4ee5\u68c0\u6d4b\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u5185\u5bb9\u4f46\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u68c0\u6d4b\u624b\u6bb5", "method": "LIFE\u65b9\u6cd5\u901a\u8fc7\u5206\u5e03\u5dee\u5f02\u5206\u6790\u53d1\u73b0\u63d0\u793a\u8bf1\u5bfc\u7684\u8bed\u8a00\u6307\u7eb9\uff0c\u91cd\u6784\u8bcd\u7ea7\u6982\u7387\u5206\u5e03\u6765\u5bfb\u627e\u5224\u522b\u6a21\u5f0f\uff0c\u5e76\u5229\u7528\u5173\u952e\u7247\u6bb5\u6280\u672f\u653e\u5927\u8bed\u8a00\u5dee\u5f02", "result": "\u5b9e\u9a8c\u663e\u793aLIFE\u5728LLM\u751f\u6210\u5047\u65b0\u95fb\u68c0\u6d4b\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u5728\u4eba\u7c7b\u64b0\u5199\u5047\u65b0\u95fb\u4e0a\u4e5f\u4fdd\u6301\u9ad8\u68c0\u6d4b\u6027\u80fd", "conclusion": "\u901a\u8fc7\u6982\u7387\u5206\u5e03\u5206\u6790\u63ed\u793a\u7684\u8bed\u8a00\u6307\u7eb9\u4e3a\u5047\u65b0\u95fb\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b0\u9014\u5f84\uff0cLIFE\u65b9\u6cd5\u5728\u591a\u79cd\u573a\u666f\u4e0b\u90fd\u8868\u73b0\u51fa\u8272"}}
{"id": "2508.12662", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12662", "abs": "https://arxiv.org/abs/2508.12662", "authors": ["Tanay Nagar", "Grigorii Khvatskii", "Anna Sokol", "Nitesh V. Chawla"], "title": "Breaking Language Barriers: Equitable Performance in Multilingual Language Models", "comment": "Accepted as a non-archival work-in-progress paper at the NAACL 2025\n  Student Research Workshop", "summary": "Cutting-edge LLMs have emerged as powerful tools for multilingual\ncommunication and understanding. However, LLMs perform worse in Common Sense\nReasoning (CSR) tasks when prompted in low-resource languages (LRLs) like Hindi\nor Swahili compared to high-resource languages (HRLs) like English. Equalizing\nthis inconsistent access to quality LLM outputs is crucial to ensure fairness\nfor speakers of LRLs and across diverse linguistic communities. In this paper,\nwe propose an approach to bridge this gap in LLM performance. Our approach\ninvolves fine-tuning an LLM on synthetic code-switched text generated using\ncontrolled language-mixing methods. We empirically demonstrate that fine-tuning\nLLMs on synthetic code-switched datasets leads to substantial improvements in\nLRL model performance while preserving or enhancing performance in HRLs.\nAdditionally, we present a new dataset of synthetic code-switched text derived\nfrom the CommonSenseQA dataset, featuring three distinct language ratio\nconfigurations.", "AI": {"tldr": "\u901a\u8fc7\u4ee3\u7801\u8f6c\u6362\u6280\u672f\u751f\u6210\u5408\u6210\u8bed\u6599\u5e93\uff0c\u7ec6\u8c03LLM\u4ee5\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5e38\u8bc6\u63a8\u7406\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u8868\u73b0", "motivation": "\u89e3\u51b3LLM\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00(LRLs)\u4e0e\u9ad8\u8d44\u6e90\u8bed\u8a00(HRLs)\u4e4b\u95f4\u5e38\u8bc6\u63a8\u7406\u6027\u80fd\u5dee\u5f02\u7684\u95ee\u9898\uff0c\u786e\u4fdd\u4e0d\u540c\u8bed\u8a00\u793e\u533a\u7684\u516c\u5e73\u8bbf\u95ee", "method": "\u4f7f\u7528\u53d7\u63a7\u8bed\u8a00\u6df7\u5408\u65b9\u6cd5\u751f\u6210\u5408\u6210\u4ee3\u7801\u8f6c\u6362\u6587\u672c\uff0c\u5e76\u5728\u8fd9\u4e9b\u6570\u636e\u96c6\u4e0a\u7ec6\u8c03LLM", "result": "\u7ec6\u8c03\u540e\u7684LLM\u5728LRLs\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u589e\u5f3a\u4e86HRLs\u7684\u6027\u80fd", "conclusion": "\u4ee3\u7801\u8f6c\u6362\u7ec6\u8c03\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7f29\u5c0f\u8bed\u8a00\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u63d0\u5347\u591a\u8bed\u8a00\u516c\u5e73\u6027\u7684\u6709\u6548\u9014\u5f84"}}
{"id": "2508.12669", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.12669", "abs": "https://arxiv.org/abs/2508.12669", "authors": ["Bishanka Seal", "Rahul Seetharaman", "Aman Bansal", "Abhilash Nandy"], "title": "Leveraging Large Language Models for Predictive Analysis of Human Misery", "comment": "14 pages, 4 tables", "summary": "This study investigates the use of Large Language Models (LLMs) for\npredicting human-perceived misery scores from natural language descriptions of\nreal-world scenarios. The task is framed as a regression problem, where the\nmodel assigns a scalar value from 0 to 100 to each input statement. We evaluate\nmultiple prompting strategies, including zero-shot, fixed-context few-shot, and\nretrieval-based prompting using BERT sentence embeddings. Few-shot approaches\nconsistently outperform zero-shot baselines, underscoring the value of\ncontextual examples in affective prediction. To move beyond static evaluation,\nwe introduce the \"Misery Game Show\", a novel gamified framework inspired by a\ntelevision format. It tests LLMs through structured rounds involving ordinal\ncomparison, binary classification, scalar estimation, and feedback-driven\nreasoning. This setup enables us to assess not only predictive accuracy but\nalso the model's ability to adapt based on corrective feedback. The gamified\nevaluation highlights the broader potential of LLMs in dynamic emotional\nreasoning tasks beyond standard regression. Code and data link:\nhttps://github.com/abhi1nandy2/Misery_Data_Exps_GitHub", "AI": {"tldr": "\u8fd9\u7814\u7a76\u63a2\u7d22\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9884\u6d4b\u4eba\u7c7b\u60b2\u4f24\u611f\u60c5\u65b9\u9762\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u591a\u79cd\u63d0\u793a\u7b56\u7565\u548c\u6e38\u620f\u5316\u8bc4\u4f30\u6846\u67b6\u8fdb\u884c\u7efc\u5408\u5206\u6790\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22LLM\u5728\u60c5\u611f\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u591a\u79cd\u63d0\u793a\u7b56\u7565\uff08\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u3001\u68c0\u7d22\u57fa\u4e8eBERT\u7684\u63d0\u793a\uff09\u8fdb\u884c\u56de\u5f52\u5206\u6790\uff0c\u5e76\u521b\u5efa\u4e86\"\u60b2\u4f24\u6e38\u620f\u8868\u6f14\"\u6e38\u620f\u5316\u6846\u67b6\uff0c\u5305\u542b\u987a\u5e8f\u6bd4\u8f83\u3001\u4e8c\u5143\u5206\u7c7b\u3001\u6570\u503c\u4f30\u8ba1\u548c\u53cd\u9988\u9a71\u52a8\u7684\u5408\u7406\u6027\u5206\u6790\u3002", "result": "\u5c11\u6837\u672c\u65b9\u6cd5\u5728\u60c5\u611f\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u660e\u663e\u4f18\u4e8e\u96f6\u6837\u672c\u57fa\u7ebf\uff0c\u6e38\u620f\u5316\u8bc4\u4f30\u663e\u793a\u4e86LLM\u5728\u52a8\u6001\u60c5\u611f\u63a8\u7406\u4e2d\u7684\u5e7f\u6cdb\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u4e0a\u4e0b\u6587\u793a\u4f8b\u5728\u60c5\u611f\u9884\u6d4b\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u901a\u8fc7\u521b\u65b0\u7684\u6e38\u620f\u5316\u8bc4\u4f30\u6846\u67b6\u4e3aLLM\u5728\u52a8\u6001\u60c5\u611f\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u5f00\u542f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.12685", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12685", "abs": "https://arxiv.org/abs/2508.12685", "authors": ["Xingshan Zeng", "Weiwen Liu", "Lingzhi Wang", "Liangyou Li", "Fei Mi", "Yasheng Wang", "Lifeng Shang", "Xin Jiang", "Qun Liu"], "title": "ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction", "comment": null, "summary": "Agentic task-solving with Large Language Models (LLMs) requires multi-turn,\nmulti-step interactions, often involving complex function calls and dynamic\nuser-agent exchanges. Existing simulation-based data generation methods for\nsuch scenarios rely heavily on costly autoregressive interactions between\nmultiple LLM agents, thereby limiting real-world performance of agentic tasks.\nIn this paper, we propose a novel Non-Autoregressive Iterative Generation\nframework, called ToolACE-MT, for constructing high-quality multi-turn agentic\ndialogues. ToolACE-MT generates full conversational trajectories through three\nstages: coarse-grained initialization, iterative refinement, and offline\nverification. The initialization phase builds a structurally complete yet\nsemantically coarse dialogue skeleton; the iterative refinement phase\nintroduces realistic complexities and continued refinement via mask-and-fill\noperations; and the offline verification phase ensures correctness and\ncoherence via rule- and model-based checks. Experiments demonstrate that\nToolACE-MT enables efficient, effective and generalizable agentic data\ngeneration, offering a new paradigm for high-quality data construction in\ntool-augmented LLM scenarios.", "AI": {"tldr": "ToolACE-MT\u662f\u4e00\u4e2a\u975e\u81ea\u56de\u5f52\u8fed\u4ee3\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u591a\u8f6e\u4ee3\u7406\u5bf9\u8bdd\u6570\u636e\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u7a0b\uff08\u7c97\u7c92\u5ea6\u521d\u59cb\u5316\u3001\u8fed\u4ee3\u7cbe\u70bc\u3001\u79bb\u7ebf\u9a8c\u8bc1\uff09\u66ff\u4ee3\u6602\u8d35\u7684\u81ea\u56de\u5f52\u4ea4\u4e92\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6a21\u62df\u7684\u4ee3\u7406\u4efb\u52a1\u6570\u636e\u751f\u6210\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u591a\u4e2aLLM\u4ee3\u7406\u4e4b\u95f4\u6602\u8d35\u7684\u81ea\u56de\u5f52\u4ea4\u4e92\uff0c\u9650\u5236\u4e86\u4ee3\u7406\u4efb\u52a1\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u7c97\u7c92\u5ea6\u521d\u59cb\u5316\u6784\u5efa\u7ed3\u6784\u5b8c\u6574\u4f46\u8bed\u4e49\u7c97\u7cd9\u7684\u5bf9\u8bdd\u9aa8\u67b6\uff1b2\uff09\u8fed\u4ee3\u7cbe\u70bc\u901a\u8fc7\u63a9\u7801\u586b\u5145\u64cd\u4f5c\u5f15\u5165\u73b0\u5b9e\u590d\u6742\u6027\u5e76\u6301\u7eed\u4f18\u5316\uff1b3\uff09\u79bb\u7ebf\u9a8c\u8bc1\u901a\u8fc7\u89c4\u5219\u548c\u6a21\u578b\u68c0\u67e5\u786e\u4fdd\u6b63\u786e\u6027\u548c\u8fde\u8d2f\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eToolACE-MT\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u3001\u6709\u6548\u4e14\u53ef\u6cdb\u5316\u7684\u4ee3\u7406\u6570\u636e\u751f\u6210\uff0c\u4e3a\u5de5\u5177\u589e\u5f3aLLM\u573a\u666f\u4e2d\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u6784\u5efa\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "conclusion": "ToolACE-MT\u4e3a\u975e\u81ea\u56de\u5f52\u8fed\u4ee3\u751f\u6210\u6846\u67b6\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4ee3\u7406\u5bf9\u8bdd\u6570\u636e\u751f\u6210\u7684\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u6807\u51c6\u3002"}}
{"id": "2508.12726", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12726", "abs": "https://arxiv.org/abs/2508.12726", "authors": ["Weize Liu", "Yongchi Zhao", "Yijia Luo", "Mingyu Xu", "Jiaheng Liu", "Yanan Li", "Xiguo Hu", "Yuchi Xu", "Wenbo Su", "Bo Zheng"], "title": "DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in many natural\nlanguage tasks but still struggle with complex, multi-step reasoning,\nparticularly across diverse disciplines. Existing reasoning datasets often\neither lack disciplinary breadth or the structural depth necessary to elicit\nrobust reasoning behaviors. We propose DESIGNER: a DESIGN-logic-guidEd\nReasoning data synthesis pipeline that leverages naturally available, extensive\nraw documents (book corpus and web corpus) to generate multidisciplinary\nchallenging questions. A core innovation of our approach is the introduction of\na Design Logic concept, which mimics the question-creation process of human\neducators. We use LLMs to reverse-engineer and abstract over 120,000 design\nlogics from existing questions across various disciplines. By matching these\ndesign logics with disciplinary source materials, we are able to create\nreasoning questions that far surpass the difficulty and diversity of existing\ndatasets. Based on this pipeline, we synthesized two large-scale reasoning\ndatasets that span 75 disciplines: Design-Logic-Reasoning-Book (DLR-Book),\ncontaining 3.04 million challenging questions synthesized from the book corpus,\nand Design-Logic-Reasoning-Web (DLR-Web), with 1.66 million challenging\nquestions from the web corpus. Our data analysis demonstrates that the\nquestions synthesized by our method exhibit substantially greater difficulty\nand diversity than those in the baseline datasets. We validate the\neffectiveness of these datasets by conducting SFT experiments on the\nQwen3-8B-Base and Qwen3-4B-Base models. The results show that our dataset\nsignificantly outperforms existing multidisciplinary datasets of the same\nvolume. Training with the full datasets further enables the models to surpass\nthe multidisciplinary reasoning performance of the official Qwen3-8B and\nQwen3-4B models.", "AI": {"tldr": "\u63d0\u51fa\u4e86DESIGNER\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u901a\u8fc7\u8bbe\u8ba1\u903b\u8f91\u6982\u5ff5\u4ece\u539f\u59cb\u6587\u6863\u751f\u6210\u591a\u5b66\u79d1\u590d\u6742\u63a8\u7406\u95ee\u9898\uff0c\u521b\u5efa\u4e86\u4e24\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6DLR-Book\u548cDLR-Web\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u591a\u5b66\u79d1\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u6570\u636e\u96c6\u7f3a\u4e4f\u5b66\u79d1\u5e7f\u5ea6\u548c\u7ed3\u6784\u6df1\u5ea6\uff0c\u65e0\u6cd5\u6709\u6548\u6fc0\u53d1\u5f3a\u5927\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u9700\u8981\u521b\u5efa\u66f4\u5177\u6311\u6218\u6027\u548c\u591a\u6837\u6027\u7684\u591a\u5b66\u79d1\u63a8\u7406\u6570\u636e\u96c6\u3002", "method": "\u5f15\u5165\u8bbe\u8ba1\u903b\u8f91\u6982\u5ff5\uff0c\u6a21\u4eff\u4eba\u7c7b\u6559\u80b2\u8005\u7684\u95ee\u9898\u521b\u5efa\u8fc7\u7a0b\uff0c\u4f7f\u7528LLMs\u4ece12\u4e07\u591a\u4e2a\u73b0\u6709\u95ee\u9898\u4e2d\u9006\u5411\u5de5\u7a0b\u62bd\u8c61\u51fa\u8bbe\u8ba1\u903b\u8f91\uff0c\u5e76\u5c06\u5176\u4e0e\u5b66\u79d1\u6e90\u6750\u6599\u5339\u914d\u751f\u6210\u63a8\u7406\u95ee\u9898\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b304\u4e07\u672c\u4e66\u7c4d\u95ee\u9898\u548c166\u4e07\u7f51\u9875\u95ee\u9898\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u663e\u793a\u5408\u6210\u95ee\u9898\u96be\u5ea6\u548c\u591a\u6837\u6027\u8fdc\u8d85\u57fa\u7ebf\u6570\u636e\u96c6\uff0cSFT\u5b9e\u9a8c\u8bc1\u660e\u6570\u636e\u96c6\u663e\u8457\u4f18\u4e8e\u540c\u7b49\u89c4\u6a21\u7684\u591a\u5b66\u79d1\u6570\u636e\u96c6\u3002", "conclusion": "DESIGNER\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u5b66\u79d1\u63a8\u7406\u95ee\u9898\uff0c\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u5728\u591a\u5b66\u79d1\u63a8\u7406\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u5b98\u65b9Qwen3\u6a21\u578b\uff0c\u4e3a\u89e3\u51b3LLMs\u590d\u6742\u63a8\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2508.12733", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12733", "abs": "https://arxiv.org/abs/2508.12733", "authors": ["Zhiyuan Ning", "Tianle Gu", "Jiaxin Song", "Shixin Hong", "Lingyu Li", "Huacan Liu", "Jie Li", "Yixu Wang", "Meng Lingyu", "Yan Teng", "Yingchun Wang"], "title": "LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models", "comment": "7pages, 5 figures", "summary": "The widespread adoption and increasing prominence of large language models\n(LLMs) in global technologies necessitate a rigorous focus on ensuring their\nsafety across a diverse range of linguistic and cultural contexts. The lack of\na comprehensive evaluation and diverse data in existing multilingual safety\nevaluations for LLMs limits their effectiveness, hindering the development of\nrobust multilingual safety alignment. To address this critical gap, we\nintroduce LinguaSafe, a comprehensive multilingual safety benchmark crafted\nwith meticulous attention to linguistic authenticity. The LinguaSafe dataset\ncomprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated\nusing a combination of translated, transcreated, and natively-sourced data, our\ndataset addresses the critical need for multilingual safety evaluations of\nLLMs, filling the void in the safety evaluation of LLMs across diverse\nunder-represented languages from Hungarian to Malay. LinguaSafe presents a\nmultidimensional and fine-grained evaluation framework, with direct and\nindirect safety assessments, including further evaluations for oversensitivity.\nThe results of safety and helpfulness evaluations vary significantly across\ndifferent domains and different languages, even in languages with similar\nresource levels. Our benchmark provides a comprehensive suite of metrics for\nin-depth safety evaluation, underscoring the critical importance of thoroughly\nassessing multilingual safety in LLMs to achieve more balanced safety\nalignment. Our dataset and code are released to the public to facilitate\nfurther research in the field of multilingual LLM safety.", "AI": {"tldr": "LinguaSafe\u662f\u4e00\u4e2a\u5305\u542b12\u79cd\u8bed\u8a00\u300145k\u6761\u76ee\u7684\u591a\u8bed\u8a00\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u7ffb\u8bd1\u3001\u8f6c\u521b\u548c\u672c\u5730\u5316\u6570\u636e\u6784\u5efa\uff0c\u63d0\u4f9b\u591a\u7ef4\u5ea6\u7684\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff0c\u586b\u8865\u4e86LLM\u5728\u591a\u8bed\u8a00\u5b89\u5168\u8bc4\u4f30\u65b9\u9762\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709LLM\u591a\u8bed\u8a00\u5b89\u5168\u8bc4\u4f30\u7f3a\u4e4f\u5168\u9762\u6027\u548c\u591a\u6837\u6027\u6570\u636e\uff0c\u9650\u5236\u4e86\u591a\u8bed\u8a00\u5b89\u5168\u5bf9\u9f50\u7684\u53d1\u5c55\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u5dee\u8ddd\u3002", "method": "\u7ed3\u5408\u7ffb\u8bd1\u3001\u8f6c\u521b\u548c\u672c\u5730\u5316\u6570\u636e\u6784\u5efa\u5305\u542b12\u79cd\u8bed\u8a0045k\u6761\u76ee\u7684\u6570\u636e\u96c6\uff0c\u5efa\u7acb\u591a\u7ef4\u5ea6\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u76f4\u63a5/\u95f4\u63a5\u5b89\u5168\u8bc4\u4f30\u548c\u8fc7\u5ea6\u654f\u611f\u6027\u8bc4\u4f30\u3002", "result": "\u4e0d\u540c\u9886\u57df\u548c\u8bed\u8a00\u7684\u5b89\u5168\u6027\u548c\u6709\u7528\u6027\u8bc4\u4f30\u7ed3\u679c\u5dee\u5f02\u663e\u8457\uff0c\u5373\u4f7f\u662f\u8d44\u6e90\u6c34\u5e73\u76f8\u4f3c\u7684\u8bed\u8a00\u4e5f\u5b58\u5728\u660e\u663e\u5dee\u5f02\u3002", "conclusion": "LinguaSafe\u57fa\u51c6\u6d4b\u8bd5\u5f3a\u8c03\u4e86\u5168\u9762\u8bc4\u4f30LLM\u591a\u8bed\u8a00\u5b89\u5168\u6027\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u5b9e\u73b0\u66f4\u5e73\u8861\u7684\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.12769", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12769", "abs": "https://arxiv.org/abs/2508.12769", "authors": ["Shaoming Duan", "Zirui Wang", "Chuanyi Liu", "Zhibin Zhu", "Yuhao Zhang", "Peiyi Han", "Liang Yan", "Zewu Penge"], "title": "CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description", "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly improved\nthe accuracy of Text-to-SQL systems. However, a critical challenge remains: the\nsemantic mismatch between natural language questions (NLQs) and their\ncorresponding SQL queries. This issue is exacerbated in large-scale databases,\nwhere semantically similar attributes hinder schema linking and semantic drift\nduring SQL generation, ultimately reducing model accuracy. To address these\nchallenges, we introduce CRED-SQL, a framework designed for large-scale\ndatabases that integrates Cluster Retrieval and Execution Description. CRED-SQL\nfirst performs cluster-based large-scale schema retrieval to pinpoint the\ntables and columns most relevant to a given NLQ, alleviating schema mismatch.\nIt then introduces an intermediate natural language representation-Execution\nDescription Language (EDL)-to bridge the gap between NLQs and SQL. This\nreformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,\nleveraging LLMs' strong general reasoning capabilities while reducing semantic\ndeviation. Extensive experiments on two large-scale, cross-domain\nbenchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new\nstate-of-the-art (SOTA) performance, validating its effectiveness and\nscalability. Our code is available at https://github.com/smduan/CRED-SQL.git", "AI": {"tldr": "CRED-SQL\u662f\u4e00\u4e2a\u9488\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u5e93\u7684Text-to-SQL\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u68c0\u7d22\u548c\u6267\u884c\u63cf\u8ff0\u8bed\u8a00\u6765\u89e3\u51b3\u8bed\u4e49\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5728\u4e24\u4e2a\u5927\u578b\u8de8\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u63d0\u5347\u4e86Text-to-SQL\u7cfb\u7edf\u7684\u51c6\u786e\u6027\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u6570\u636e\u5e93\u4e2d\uff0c\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u4e0eSQL\u67e5\u8be2\u4e4b\u95f4\u7684\u8bed\u4e49\u4e0d\u5339\u914d\u95ee\u9898\u4ecd\u7136\u4e25\u91cd\uff0c\u7279\u522b\u662f\u5728\u8bed\u4e49\u76f8\u4f3c\u5c5e\u6027\u5bfc\u81f4\u7684\u6a21\u5f0f\u94fe\u63a5\u56f0\u96be\u548c\u8bed\u4e49\u6f02\u79fb\u65b9\u9762\u3002", "method": "CRED-SQL\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u57fa\u4e8e\u805a\u7c7b\u7684\u6a21\u5f0f\u68c0\u7d22\u6765\u8bc6\u522b\u4e0e\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u6700\u76f8\u5173\u7684\u8868\u548c\u5217\uff1b2\uff09\u5f15\u5165\u6267\u884c\u63cf\u8ff0\u8bed\u8a00\uff08EDL\uff09\u4f5c\u4e3a\u4e2d\u95f4\u81ea\u7136\u8bed\u8a00\u8868\u793a\uff0c\u5c06\u4efb\u52a1\u5206\u89e3\u4e3aText-to-EDL\u548cEDL-to-SQL\u4e24\u4e2a\u9636\u6bb5\u3002", "result": "\u5728\u4e24\u4e2a\u5927\u89c4\u6a21\u8de8\u57df\u57fa\u51c6\u6d4b\u8bd5SpiderUnion\u548cBirdUnion\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCRED-SQL\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "CRED-SQL\u901a\u8fc7\u521b\u65b0\u7684\u805a\u7c7b\u68c0\u7d22\u548c\u4e2d\u95f4\u8bed\u8a00\u8868\u793a\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u6570\u636e\u5e93\u4e2dText-to-SQL\u7684\u8bed\u4e49\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12774", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12774", "abs": "https://arxiv.org/abs/2508.12774", "authors": ["Javier Garcia Gilabert", "Xixian Liao", "Severino Da Dalt", "Ella Bohman", "Audrey Mash", "Francesca De Luca Fornaciari", "Irene Baucells", "Joan Llop", "Miguel Claramunt Argote", "Carlos Escolano", "Maite Melero"], "title": "From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task", "comment": null, "summary": "In this paper, we present the SALAMANDRATA family of models, an improved\niteration of SALAMANDRA LLMs (Gonzalez-Agirre et al., 2025) specifically\ntrained to achieve strong performance in translation-related tasks for 38\nEuropean languages. SALAMANDRATA comes in two scales: 2B and 7B parameters. For\nboth versions, we applied the same training recipe with a first step of\ncontinual pre-training on parallel data, and a second step of supervised\nfine-tuning on high-quality instructions. The BSC submission to the WMT25\nGeneral Machine Translation shared task is based on the 7B variant of\nSALAMANDRATA. We first adapted the model vocabulary to support the additional\nnon-European languages included in the task. This was followed by a second\nphase of continual pre-training and supervised fine-tuning, carefully designed\nto optimize performance across all translation directions for this year's\nshared task. For decoding, we employed two quality-aware strategies: Minimum\nBayes Risk Decoding and Tuned Re-ranking using COMET and COMET-KIWI\nrespectively. We publicly release both the 2B and 7B versions of SALAMANDRATA,\nalong with the newer SALAMANDRATA-V2 model, on Hugging Face1", "AI": {"tldr": "SALAMANDRATA\u6a21\u578b\u5bb6\u65cf\u662fSALAMANDRA LLMs\u7684\u6539\u8fdb\u7248\u672c\uff0c\u4e13\u95e8\u9488\u5bf938\u79cd\u6b27\u6d32\u8bed\u8a00\u7684\u7ffb\u8bd1\u4efb\u52a1\u8fdb\u884c\u8bad\u7ec3\uff0c\u63d0\u4f9b2B\u548c7B\u4e24\u4e2a\u89c4\u6a21\u7248\u672c\uff0c\u91c7\u7528\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5fae\u8c03\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u5728WMT25\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u5e94\u7528\u4e86\u8d28\u91cf\u611f\u77e5\u7684\u89e3\u7801\u7b56\u7565\u3002", "motivation": "\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u591a\u8bed\u8a00\u7ffb\u8bd1\u4efb\u52a1\u7684\u9ad8\u6027\u80fd\u6a21\u578b\uff0c\u7279\u522b\u662f\u9488\u5bf938\u79cd\u6b27\u6d32\u8bed\u8a00\u7684\u7ffb\u8bd1\u9700\u6c42\uff0c\u63d0\u5347\u673a\u5668\u7ffb\u8bd1\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff1a\u9996\u5148\u5728\u5e73\u884c\u6570\u636e\u4e0a\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u9ad8\u8d28\u91cf\u6307\u4ee4\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u3002\u5728WMT25\u4efb\u52a1\u4e2d\u8fd8\u4f7f\u7528\u4e86\u8bcd\u6c47\u8868\u9002\u914d\u3001\u4e8c\u6b21\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\uff0c\u4ee5\u53caMinimum Bayes Risk Decoding\u548cCOMET/COMET-KIWI\u91cd\u6392\u5e8f\u7b49\u8d28\u91cf\u611f\u77e5\u89e3\u7801\u7b56\u7565\u3002", "result": "\u5f00\u53d1\u51fa\u4e86SALAMANDRATA\u6a21\u578b\u5bb6\u65cf\uff082B\u548c7B\u7248\u672c\uff09\u4ee5\u53ca\u66f4\u65b0\u7684SALAMANDRATA-V2\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u4e13\u95e8\u9488\u5bf9\u591a\u8bed\u8a00\u7ffb\u8bd1\u4efb\u52a1\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u5e76\u5728Hugging Face\u5e73\u53f0\u4e0a\u516c\u5f00\u53d1\u5e03\u3002", "conclusion": "SALAMANDRATA\u7cfb\u5217\u6a21\u578b\u901a\u8fc7\u4e13\u95e8\u7684\u591a\u8bed\u8a00\u7ffb\u8bd1\u8bad\u7ec3\u65b9\u6cd5\u548c\u5148\u8fdb\u7684\u8d28\u91cf\u63a7\u5236\u7b56\u7565\uff0c\u4e3a\u6b27\u6d32\u8bed\u8a00\u7ffb\u8bd1\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728WMT25\u5171\u4eab\u4efb\u52a1\u4e2d\u5f97\u5230\u4e86\u5e94\u7528\u548c\u9a8c\u8bc1\u3002"}}
{"id": "2508.12778", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12778", "abs": "https://arxiv.org/abs/2508.12778", "authors": ["Zhe Chen", "Yusheng Liao", "Shuyang Jiang", "Zhiyuan Zhu", "Haolin Li", "Yanfeng Wang", "Yu Wang"], "title": "HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks", "comment": null, "summary": "Medical large vision-language Models (Med-LVLMs) have shown promise in\nclinical applications but suffer from factual inaccuracies and unreliable\noutputs, posing risks in real-world diagnostics. While retrieval-augmented\ngeneration has emerged as a potential solution, current medical multimodal RAG\nsystems are unable to perform effective retrieval across heterogeneous sources.\nThe irrelevance of retrieved reports affects the factuality of analysis, while\ninsufficient knowledge affects the credibility of clinical decision-making. To\nbridge the gap, we construct MedAtlas, which includes extensive multimodal\nreport repositories and diverse text corpora. Based on it, we present\nHeteroRAG, a novel framework that enhances Med-LVLMs through heterogeneous\nknowledge sources. The framework introduces Modality-specific CLIPs for\neffective report retrieval and a Multi-corpora Query Generator for dynamically\nconstructing queries for diverse corpora. Incorporating knowledge from such\nmultifaceted sources, Med-LVLM is then trained with Heterogeneous Knowledge\nPreference Tuning to achieve cross-modality and multi-source knowledge\nalignment. Extensive experiments across 12 datasets and 3 modalities\ndemonstrate that the proposed HeteroRAG achieves state-of-the-art performance\nin most medical vision language benchmarks, significantly improving factual\naccuracy and reliability of Med-LVLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86HeteroRAG\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u89e3\u51b3\u533b\u7597\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u4e0d\u51c6\u786e\u95ee\u9898\uff0c\u572812\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd", "motivation": "\u533b\u7597\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4e8b\u5b9e\u4e0d\u51c6\u786e\u548c\u8f93\u51fa\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u5f53\u524d\u7684\u591a\u6a21\u6001RAG\u7cfb\u7edf\u65e0\u6cd5\u5728\u5f02\u6784\u6570\u636e\u6e90\u4e0a\u8fdb\u884c\u6709\u6548\u68c0\u7d22\uff0c\u5f71\u54cd\u4e34\u5e8a\u51b3\u7b56\u7684\u53ef\u4fe1\u5ea6", "method": "\u6784\u5efaMedAtlas\u591a\u6a21\u6001\u62a5\u544a\u5e93\uff0c\u5f00\u53d1HeteroRAG\u6846\u67b6\uff1a\u4f7f\u7528\u6a21\u6001\u7279\u5b9aCLIP\u8fdb\u884c\u62a5\u544a\u68c0\u7d22\uff0c\u591a\u8bed\u6599\u67e5\u8be2\u751f\u6210\u5668\u52a8\u6001\u6784\u5efa\u67e5\u8be2\uff0c\u901a\u8fc7\u5f02\u6784\u77e5\u8bc6\u504f\u597d\u8c03\u4f18\u5b9e\u73b0\u8de8\u6a21\u6001\u591a\u6e90\u77e5\u8bc6\u5bf9\u9f50", "result": "\u572812\u4e2a\u6570\u636e\u96c6\u548c3\u79cd\u6a21\u6001\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHeteroRAG\u5728\u5927\u591a\u6570\u533b\u7597\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86Med-LVLMs\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027", "conclusion": "HeteroRAG\u6846\u67b6\u901a\u8fc7\u6709\u6548\u6574\u5408\u5f02\u6784\u77e5\u8bc6\u6e90\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u533b\u7597\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u95ee\u9898\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684AI\u8f85\u52a9\u5de5\u5177"}}
{"id": "2508.12800", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12800", "abs": "https://arxiv.org/abs/2508.12800", "authors": ["Yong Deng", "Guoqing Wang", "Zhenzhe Ying", "Xiaofeng Wu", "Jinzhen Lin", "Wenwen Xiong", "Yuqin Dai", "Shuo Yang", "Zhanwei Zhang", "Qiwen Wang", "Yang Qin", "Changhua Meng"], "title": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward", "comment": null, "summary": "Large language models (LLMs) exhibit remarkable problem-solving abilities,\nbut struggle with complex tasks due to static internal knowledge.\nRetrieval-Augmented Generation (RAG) enhances access to external information,\nyet remains limited in multi-hop reasoning and strategic search due to rigid\nworkflows. Recent advancements in agentic deep research empower LLMs to\nautonomously reason, search, and synthesize information. However, current\napproaches relying on outcome-based reinforcement learning (RL) face critical\nissues such as conflicting gradients and reward sparsity, limiting performance\ngains and training efficiency. To address these, we first propose Atomic\nThought, a novel LLM thinking paradigm that decomposes reasoning into\nfine-grained functional units. These units are supervised by Reasoning Reward\nModels (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained\nguidance. Building on this, we propose Atom-Searcher, a novel RL framework for\nagentic deep research that integrates Atomic Thought and ATR. Atom-Searcher\nuses a curriculum-inspired reward schedule, prioritizing process-level ATR\nearly and transitioning to outcome rewards, accelerating convergence on\neffective reasoning paths. Experiments on seven benchmarks show consistent\nimprovements over the state-of-the-art. Key advantages include: (1)\nAtom-Searcher scales computation at test-time. (2) Atomic Thought provides\nsupervision anchors for RRMs, bridging deep research tasks and RRMs. (3)\nAtom-Searcher exhibits more interpretable, human-like reasoning patterns.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86Atom-Searcher\u6846\u67b6\uff0c\u901a\u8fc7\u539f\u5b50\u601d\u7ef4\u5206\u89e3\u548c\u8fc7\u7a0b\u5956\u52b1\u89e3\u51b3\u4e86\u73b0\u6709\u6df1\u5ea6\u7814\u7a76\u4e2d\u7684\u5956\u52b1\u7a00\u758f\u548c\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6807\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u7ed3\u679c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff0c\u5b58\u5728\u5956\u52b1\u7a00\u758f\u548c\u68af\u5ea6\u51b2\u7a81\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u548c\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u63d0\u51fa\u539f\u5b50\u601d\u7ef4\u8303\u5f0f\uff0c\u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u529f\u80fd\u5355\u5143\uff0c\u901a\u8fc7\u63a8\u7406\u5956\u52b1\u6a21\u578b(RRM)\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u6307\u5bfc\uff1b\u6784\u5efaAtom-Searcher RL\u6846\u67b6\uff0c\u91c7\u7528\u8bfe\u7a0b\u53d7\u4f53\u5956\u52b1\u8c03\u5ea6\uff0c\u5148\u4f18\u5148\u8fc7\u7a0b\u5956\u52b1\u518d\u8f6c\u5230\u7ed3\u679c\u5956\u52b1\u3002", "result": "\u57287\u4e2a\u6807\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u90fd\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5177\u6709\u53ef\u6269\u5c55\u8ba1\u7b97\u3001\u66f4\u53ef\u89e3\u91ca\u7684\u4eba\u7c7b\u5316\u63a8\u7406\u6a21\u5f0f\u7b49\u4f18\u52bf\u3002", "conclusion": "\u539f\u5b50\u601d\u7ef4\u548cAtom-Searcher\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6df1\u5ea6\u7814\u7a76\u4e2d\u7684\u5956\u52b1\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u53ef\u89e3\u91ca\u7684\u81ea\u4e3b\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2508.12803", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12803", "abs": "https://arxiv.org/abs/2508.12803", "authors": ["Ahmed Elshabrawy", "Hour Kaing", "Haiyue Song", "Alham Fikri Aji", "Hideki Tanaka", "Masao Utiyama", "Raj Dabre"], "title": "When Alignment Hurts: Decoupling Representational Spaces in Multilingual Models", "comment": null, "summary": "Alignment with high-resource standard languages is often assumed to aid the\nmodeling of related low-resource varieties. We challenge this assumption by\ndemonstrating that excessive representational entanglement with a dominant\nvariety, such as Modern Standard Arabic (MSA) in relation to Arabic dialects,\ncan actively hinder generative modeling. We present the first comprehensive\ncausal study of this phenomenon by analyzing and directly intervening in the\ninternal representation geometry of large language models (LLMs). Our key\ncontribution is an online variational probing framework that continuously\nestimates the subspace of the standard variety during fine-tuning, enabling\nprojection-based decoupling from this space. While our study uses Arabic as a\ncase due to its unusually rich parallel resources across 25 dialects, the\nbroader motivation is methodological: dialectal MT serves as a controlled proxy\nfor generative tasks where comparable multi-variety corpora are unavailable.\nAcross 25 dialects, our intervention improves generation quality by up to +4.9\nchrF++ and +2.0 on average compared to standard fine-tuning, despite a measured\ntradeoff in standard-language performance. These results provide causal\nevidence that subspace dominance by high-resource varieties can restrict\ngenerative capacity for related varieties. More generally, we unify geometric\nand information-theoretic probing with subspace-level causal interventions,\noffering practical tools for improving generative modeling in closely related\nlanguage families and, more broadly, for controlling representational\nallocation in multilingual and multi-domain LLMs. Code will be released.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u9ad8\u8d44\u6e90\u6807\u51c6\u8bed\u8a00\uff08\u5982\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bedMSA\uff09\u7684\u8fc7\u5ea6\u8868\u5f81\u7ea0\u7f20\u4f1a\u963b\u788d\u76f8\u5173\u4f4e\u8d44\u6e90\u65b9\u8a00\u7684\u751f\u6210\u5efa\u6a21\u3002\u901a\u8fc7\u5728\u7ebf\u53d8\u5206\u63a2\u6d4b\u6846\u67b6\u5b9e\u73b0\u5b50\u7a7a\u95f4\u89e3\u8026\uff0c\u572825\u79cd\u963f\u62c9\u4f2f\u65b9\u8a00\u4e0a\u5e73\u5747\u63d0\u5347\u751f\u6210\u8d28\u91cf+2.0 chrF++\u3002", "motivation": "\u6311\u6218\u9ad8\u8d44\u6e90\u6807\u51c6\u8bed\u8a00\u5fc5\u7136\u6709\u52a9\u4e8e\u76f8\u5173\u4f4e\u8d44\u6e90\u53d8\u4f53\u5efa\u6a21\u7684\u5047\u8bbe\uff0c\u63a2\u7d22\u8868\u5f81\u7ea0\u7f20\u5bf9\u751f\u6210\u5efa\u6a21\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5e76\u4e3a\u591a\u8bed\u8a00\u591a\u9886\u57dfLLMs\u63d0\u4f9b\u8868\u5f81\u5206\u914d\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5728\u7ebf\u53d8\u5206\u63a2\u6d4b\u6846\u67b6\uff0c\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u6301\u7eed\u4f30\u8ba1\u6807\u51c6\u53d8\u4f53\u7684\u5b50\u7a7a\u95f4\uff0c\u5b9e\u73b0\u57fa\u4e8e\u6295\u5f71\u7684\u89e3\u8026\u3002\u4f7f\u7528\u963f\u62c9\u4f2f\u8bed\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u5e76\u76f4\u63a5\u5e72\u9884LLMs\u7684\u5185\u90e8\u8868\u5f81\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u572825\u79cd\u65b9\u8a00\u4e0a\uff0c\u5e72\u9884\u4f7f\u751f\u6210\u8d28\u91cf\u5e73\u5747\u63d0\u5347+2.0 chrF++\uff08\u6700\u9ad8+4.9\uff09\uff0c\u5c3d\u7ba1\u6807\u51c6\u8bed\u8a00\u6027\u80fd\u6709\u6240\u4e0b\u964d\u3002\u63d0\u4f9b\u4e86\u9ad8\u8d44\u6e90\u53d8\u4f53\u5b50\u7a7a\u95f4\u4e3b\u5bfc\u9650\u5236\u76f8\u5173\u53d8\u4f53\u751f\u6210\u80fd\u529b\u7684\u56e0\u679c\u8bc1\u636e\u3002", "conclusion": "\u7edf\u4e00\u51e0\u4f55\u548c\u4fe1\u606f\u8bba\u63a2\u6d4b\u4e0e\u5b50\u7a7a\u95f4\u7ea7\u56e0\u679c\u5e72\u9884\uff0c\u4e3a\u5bc6\u5207\u76f8\u5173\u7684\u8bed\u8a00\u5bb6\u65cf\u63d0\u4f9b\u6539\u8fdb\u751f\u6210\u5efa\u6a21\u7684\u5b9e\u7528\u5de5\u5177\uff0c\u5e76\u4e3a\u591a\u8bed\u8a00\u591a\u9886\u57dfLLMs\u7684\u8868\u5f81\u5206\u914d\u63a7\u5236\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2508.12819", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12819", "abs": "https://arxiv.org/abs/2508.12819", "authors": ["Jeongwoo Kang", "Maria Boritchev", "Maximin Coavoux"], "title": "ding-01 :ARG0: An AMR Corpus for Spontaneous French Dialogue", "comment": "Accepted at IWCS 2025", "summary": "We present our work to build a French semantic corpus by annotating French\ndialogue in Abstract Meaning Representation (AMR). Specifically, we annotate\nthe DinG corpus, consisting of transcripts of spontaneous French dialogues\nrecorded during the board game Catan. As AMR has insufficient coverage of the\ndynamics of spontaneous speech, we extend the framework to better represent\nspontaneous speech and sentence structures specific to French. Additionally, to\nsupport consistent annotation, we provide an annotation guideline detailing\nthese extensions. We publish our corpus under a free license (CC-SA-BY). We\nalso train and evaluate an AMR parser on our data. This model can be used as an\nassistance annotation tool to provide initial annotations that can be refined\nby human annotators. Our work contributes to the development of semantic\nresources for French dialogue.", "AI": {"tldr": "\u6784\u5efa\u6cd5\u8bed\u8bed\u4e49\u8bed\u6599\u5e93\uff0c\u901a\u8fc7AMR\u6807\u6ce8\u6cd5\u8bed\u5bf9\u8bdd\uff0c\u6269\u5c55\u6846\u67b6\u4ee5\u5904\u7406\u81ea\u53d1\u8bed\u97f3\u7279\u5f81\uff0c\u5e76\u8bad\u7ec3AMR\u89e3\u6790\u5668\u4f5c\u4e3a\u8f85\u52a9\u6807\u6ce8\u5de5\u5177\u3002", "motivation": "\u4e3a\u6cd5\u8bed\u5bf9\u8bdd\u5f00\u53d1\u8bed\u4e49\u8d44\u6e90\uff0c\u89e3\u51b3AMR\u6846\u67b6\u5bf9\u81ea\u53d1\u8bed\u97f3\u8986\u76d6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u652f\u6301\u6cd5\u8bed\u7279\u6709\u7684\u53e5\u5b50\u7ed3\u6784\u6807\u6ce8\u3002", "method": "\u6807\u6ce8DinG\u8bed\u6599\u5e93\u4e2d\u7684\u6cd5\u8bed\u5bf9\u8bdd\uff0c\u6269\u5c55AMR\u6846\u67b6\u4ee5\u9002\u5e94\u81ea\u53d1\u8bed\u97f3\u7279\u5f81\uff0c\u5236\u5b9a\u8be6\u7ec6\u6807\u6ce8\u6307\u5357\uff0c\u8bad\u7ec3\u548c\u8bc4\u4f30AMR\u89e3\u6790\u5668\u3002", "result": "\u53d1\u5e03\u4e86\u514d\u8d39\u8bb8\u53ef\u7684\u6cd5\u8bed\u8bed\u4e49\u8bed\u6599\u5e93\uff0c\u5f00\u53d1\u4e86\u80fd\u591f\u8f85\u52a9\u4eba\u5de5\u6807\u6ce8\u7684AMR\u89e3\u6790\u6a21\u578b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63a8\u8fdb\u4e86\u6cd5\u8bed\u5bf9\u8bdd\u8bed\u4e49\u8d44\u6e90\u7684\u53d1\u5c55\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u6ce8\u5de5\u5177\u548c\u8bed\u6599\u57fa\u7840\u3002"}}
{"id": "2508.12828", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12828", "abs": "https://arxiv.org/abs/2508.12828", "authors": ["Raneem Alharthi", "Rajwa Alharthi", "Aiqi Jiang", "Arkaitz Zubiaga"], "title": "Context Matters: Incorporating Target Awareness in Conversational Abusive Language Detection", "comment": null, "summary": "Abusive language detection has become an increasingly important task as a\nmeans to tackle this type of harmful content in social media. There has been a\nsubstantial body of research developing models for determining if a social\nmedia post is abusive or not; however, this research has primarily focused on\nexploiting social media posts individually, overlooking additional context that\ncan be derived from surrounding posts. In this study, we look at conversational\nexchanges, where a user replies to an earlier post by another user (the parent\ntweet). We ask: does leveraging context from the parent tweet help determine if\na reply post is abusive or not, and what are the features that contribute the\nmost? We study a range of content-based and account-based features derived from\nthe context, and compare this to the more widely studied approach of only\nlooking at the features from the reply tweet. For a more generalizable study,\nwe test four different classification models on a dataset made of\nconversational exchanges (parent-reply tweet pairs) with replies labeled as\nabusive or not. Our experiments show that incorporating contextual features\nleads to substantial improvements compared to the use of features derived from\nthe reply tweet only, confirming the importance of leveraging context. We\nobserve that, among the features under study, it is especially the\ncontent-based features (what is being posted) that contribute to the\nclassification performance rather than account-based features (who is posting\nit). While using content-based features, it is best to combine a range of\ndifferent features to ensure improved performance over being more selective and\nusing fewer features. Our study provides insights into the development of\ncontextualized abusive language detection models in realistic settings\ninvolving conversations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u7236\u63a8\u6587\u4e0a\u4e0b\u6587\u7279\u5f81\u6765\u68c0\u6d4b\u56de\u590d\u63a8\u6587\u4e2d\u7684\u8fb1\u9a82\u6027\u8bed\u8a00\uff0c\u53d1\u73b0\u7ed3\u5408\u4e0a\u4e0b\u6587\u5185\u5bb9\u7279\u5f81\u80fd\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8fb1\u9a82\u6027\u8bed\u8a00\u68c0\u6d4b\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\uff0c\u5ffd\u7565\u4e86\u4ece\u5468\u56f4\u5e16\u5b50\u4e2d\u83b7\u5f97\u7684\u989d\u5916\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u7279\u522b\u662f\u5728\u5bf9\u8bdd\u4ea4\u6d41\u573a\u666f\u4e2d\u3002", "method": "\u7814\u7a76\u5bf9\u8bdd\u4ea4\u6d41\uff08\u7236\u63a8\u6587-\u56de\u590d\u63a8\u6587\u5bf9\uff09\uff0c\u6d4b\u8bd5\u4e86\u57fa\u4e8e\u5185\u5bb9\u548c\u8d26\u6237\u7684\u4e0a\u4e0b\u6587\u7279\u5f81\uff0c\u4f7f\u7528\u56db\u79cd\u5206\u7c7b\u6a21\u578b\u5728\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u7279\u5f81\u76f8\u6bd4\u4ec5\u4f7f\u7528\u56de\u590d\u63a8\u6587\u7279\u5f81\u6709\u663e\u8457\u6539\u8fdb\uff0c\u5176\u4e2d\u5185\u5bb9\u7279\u5f81\u6bd4\u8d26\u6237\u7279\u5f81\u8d21\u732e\u66f4\u5927\uff0c\u4e14\u7ec4\u5408\u591a\u79cd\u7279\u5f81\u6548\u679c\u6700\u4f73\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u8fb1\u9a82\u6027\u8bed\u8a00\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u5bf9\u8bdd\u573a\u666f\u4e2d\uff0c\u5185\u5bb9\u7279\u5f81\u6bd4\u7528\u6237\u8eab\u4efd\u7279\u5f81\u66f4\u6709\u4ef7\u503c\uff0c\u5e94\u5f00\u53d1\u66f4\u591a\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u68c0\u6d4b\u6a21\u578b\u3002"}}
{"id": "2508.12830", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12830", "abs": "https://arxiv.org/abs/2508.12830", "authors": ["Jan Maliszewski"], "title": "It takes a village to write a book: Mapping anonymous contributions in Stephen Langton's Quaestiones Theologiae", "comment": null, "summary": "While the indirect evidence suggests that already in the early scholastic\nperiod the literary production based on records of oral teaching (so-called\nreportationes) was not uncommon, there are very few sources commenting on the\npractice. This paper details the design of a study applying stylometric\ntechniques of authorship attribution to a collection developed from\nreportationes -- Stephen Langton's Quaestiones Theologiae -- aiming to uncover\nlayers of editorial work and thus validate some hypotheses regarding the\ncollection's formation. Following Camps, Cl\\'erice, and Pinche (2021), I\ndiscuss the implementation of an HTR pipeline and stylometric analysis based on\nthe most frequent words, POS tags, and pseudo-affixes. The proposed study will\noffer two methodological gains relevant to computational research on the\nscholastic tradition: it will directly compare performance on manually composed\nand automatically extracted data, and it will test the validity of\ntransformer-based OCR and automated transcription alignment for workflows\napplied to scholastic Latin corpora. If successful, this study will provide an\neasily reusable template for the exploratory analysis of collaborative literary\nproduction stemming from medieval universities.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u98ce\u683c\u6d4b\u5ea6\u6280\u672f\u5206\u6790\u4e2d\u4e16\u7eaa\u6559\u80b2\u6587\u732e\u4ea7\u751f\u8fc7\u7a0b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9Stephen Langton\u795e\u5b66\u95ee\u9898\u96c6\u7684\u7814\u7a76\u6765\u9a8c\u8bc1\u5176\u7f16\u8f91\u5de5\u4f5c\u5c42\u6b21\u7684\u5047\u8bbe\u3002", "motivation": "\u867d\u7136\u95f4\u63a5\u8bc1\u636e\u663e\u793a\u65e9\u671f\u7ecf\u9662\u65f6\u4ee3\u5c31\u5b58\u5728\u57fa\u4e8e\u53e3\u8bb2\u6559\u5b66\u8bb0\u5f55\u7684\u6587\u5b66\u4ea7\u51fa\uff0c\u4f46\u5f88\u5c11\u6709\u6e90\u6cc4\u8fd9\u79cd\u5b9e\u8df5\u3002\u9700\u8981\u5f00\u53d1\u8ba1\u7b97\u65b9\u6cd5\u6765\u7814\u7a76\u4e2d\u4e16\u7eaa\u5927\u5b66\u7684\u5408\u4f5c\u6587\u5b66\u4ea7\u51fa\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u98ce\u683c\u6d4b\u5ea6\u4f5c\u8005\u5f52\u5c5e\u6280\u672f\uff0c\u57fa\u4e8e\u6700\u5e38\u89c1\u5355\u8bcd\u3001\u8bcd\u6027\u6807\u6ce8\u548c\u4f2a\u540e\u7f00\u8fdb\u884c\u5206\u6790\u3002\u5b9e\u65bdHTR\u6d41\u6c34\u7ebf\uff08\u624b\u5199\u6587\u672c\u8bc6\u522b\uff09\u548c\u81ea\u52a8\u5316\u8f6c\u5199\u5bf9\u9f50\u6280\u672f\u3002", "result": "\u8fd9\u9879\u7814\u7a76\u5c06\u63d0\u4f9b\u4e24\u4e2a\u65b9\u6cd5\u8bba\u653f\u76ca\uff1a\u76f4\u63a5\u6bd4\u8f83\u624b\u5de5\u7f16\u5199\u548c\u81ea\u52a8\u63d0\u53d6\u6570\u636e\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u57fa\u4e8etransformer\u7684OCR\u548c\u81ea\u52a8\u5bf9\u9f50\u6280\u672f\u5728\u7ecf\u9662\u62c9\u4e01\u8bed\u8bed\u6599\u5e93\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5982\u679c\u6210\u529f\uff0c\u8fd9\u9879\u7814\u7a76\u5c06\u4e3a\u4e2d\u4e16\u7eaa\u5927\u5b66\u5408\u4f5c\u6587\u5b66\u4ea7\u51fa\u7684\u63a2\u7d22\u6027\u5206\u6790\u63d0\u4f9b\u4e00\u4e2a\u53ef\u91cd\u7528\u7684\u6a21\u677f\uff0c\u4fc3\u8fdb\u5bf9\u7ecf\u9662\u4f20\u7edf\u7684\u8ba1\u7b97\u7814\u7a76\u3002"}}
{"id": "2508.12863", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12863", "abs": "https://arxiv.org/abs/2508.12863", "authors": ["Jumbly Grindrod", "Peter Grindrod"], "title": "Word Meanings in Transformer Language Models", "comment": null, "summary": "We investigate how word meanings are represented in the transformer language\nmodels. Specifically, we focus on whether transformer models employ something\nanalogous to a lexical store - where each word has an entry that contains\nsemantic information. To do this, we extracted the token embedding space of\nRoBERTa-base and k-means clustered it into 200 clusters. In our first study, we\nthen manually inspected the resultant clusters to consider whether they are\nsensitive to semantic information. In our second study, we tested whether the\nclusters are sensitive to five psycholinguistic measures: valence,\nconcreteness, iconicity, taboo, and age of acquisition. Overall, our findings\nwere very positive - there is a wide variety of semantic information encoded\nwithin the token embedding space. This serves to rule out certain \"meaning\neliminativist\" hypotheses about how transformer LLMs process semantic\ninformation.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u5bf9RoBERTa-base\u6a21\u578b\u7684\u5206\u6790\u53d1\u73b0\uff0c\u8f6c\u6362\u5668\u8bed\u8a00\u6a21\u578b\u5728\u8bcd\u6c47\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7f16\u7801\u4e86\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u63a8\u7ffb\u4e86\u67d0\u4e9b\u8bed\u4e49\u6d88\u8fc7\u4e3b\u4e49\u5047\u8bbe\u3002", "motivation": "\u63a2\u7d22\u8f6c\u6362\u5668\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u8868\u5f81\u8bcd\u6c47\u610f\u4e49\uff0c\u7279\u522b\u662f\u662f\u5426\u5b58\u5728\u7c7b\u4f3c\u4e8e\u8bcd\u5178\u5b58\u50a8\u7684\u673a\u5236\uff0c\u5176\u4e2d\u6bcf\u4e2a\u8bcd\u90fd\u6709\u5305\u542b\u8bed\u4e49\u4fe1\u606f\u7684\u5165\u53e3\u3002", "method": "\u63d0\u53d6RoBERTa-base\u6a21\u578b\u7684\u6807\u8bb0\u5d4c\u5165\u7a7a\u95f4\uff0c\u4f7f\u7528k-means\u805a\u7c7b\u7b97\u6cd5\u5c06\u5176\u5206\u4e3a200\u4e2a\u805a\u7c7b\u3002\u7b2c\u4e00\u4e2a\u7814\u7a76\u624b\u52a8\u68c0\u67e5\u8fd9\u4e9b\u805a\u7c7b\u662f\u5426\u5bf9\u8bed\u4e49\u4fe1\u606f\u654f\u611f\uff1b\u7b2c\u4e8c\u4e2a\u7814\u7a76\u6d4b\u8bd5\u805a\u7c7b\u662f\u5426\u5bf9\u4e94\u79cd\u5fc3\u7406\u8bed\u8a00\u5ea6\u91cf\u654f\u611f\uff1a\u4ef7\u503c\u89c2\u3001\u5177\u4f53\u6027\u3001\u8c61\u5f81\u6027\u3001\u7981\u5fcc\u8bcd\u548c\u83b7\u5f97\u5e74\u9f84\u3002", "result": "\u53d1\u73b0\u663e\u793a\u8bcd\u6c47\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7f16\u7801\u4e86\u5e7f\u6cdb\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u805a\u7c7b\u7ed3\u679c\u663e\u793a\u51fa\u5bf9\u8bed\u4e49\u4fe1\u606f\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u89c4\u5219\u4e86\u67d0\u4e9b\"\u8bed\u4e49\u6d88\u8fc7\u4e3b\u4e49\"\u7684\u5047\u8bbe\uff0c\u8bc1\u660e\u8f6c\u6362\u5668\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u8bcd\u6c47\u5d4c\u5165\u7a7a\u95f4\u6709\u6548\u5730\u5904\u7406\u548c\u8868\u5f81\u8bed\u4e49\u4fe1\u606f\u3002"}}
{"id": "2508.12868", "categories": ["cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.12868", "abs": "https://arxiv.org/abs/2508.12868", "authors": ["Yilin Geng", "Shujing Wang", "Chuan Wang", "Keqing He", "Yanfei Lv", "Ying Wang", "Zaiwen Feng", "Xiaoying Bai"], "title": "An LLM Agent-Based Complex Semantic Table Annotation Approach", "comment": null, "summary": "The Semantic Table Annotation (STA) task, which includes Column Type\nAnnotation (CTA) and Cell Entity Annotation (CEA), maps table contents to\nontology entities and plays important roles in various semantic applications.\nHowever, complex tables often pose challenges such as semantic loss of column\nnames or cell values, strict ontological hierarchy requirements, homonyms,\nspelling errors, and abbreviations, which hinder annotation accuracy. To\naddress these issues, this paper proposes an LLM-based agent approach for CTA\nand CEA. We design and implement five external tools with tailored prompts\nbased on the ReAct framework, enabling the STA agent to dynamically select\nsuitable annotation strategies depending on table characteristics. Experiments\nare conducted on the Tough Tables and BiodivTab datasets from the SemTab\nchallenge, which contain the aforementioned challenges. Our method outperforms\nexisting approaches across various metrics. Furthermore, by leveraging\nLevenshtein distance to reduce redundant annotations, we achieve a 70%\nreduction in time costs and a 60% reduction in LLM token usage, providing an\nefficient and cost-effective solution for STA.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u6c11\u5de5\u667a\u80fd\u4ee3\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4e94\u4e2a\u5916\u90e8\u5de5\u5177\u548cReAct\u6846\u67b6\u6765\u89e3\u51b3\u8bed\u4e49\u8868\u6ce8\u89e3\u4efb\u52a1\u4e2d\u7684\u590d\u6742\u6311\u6218\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u65f6\u95f4\u6210\u672c\u548c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u590d\u6742\u8868\u683c\u5728\u8bed\u4e49\u8868\u6ce8\u89e3\u4efb\u52a1\u4e2d\u9047\u5230\u591a\u79cd\u6311\u6218\uff0c\u5305\u62ec\u5217\u540d/\u5355\u5143\u683c\u503c\u7684\u8bed\u4e49\u4e22\u5931\u3001\u4e25\u683c\u7684\u672c\u4f53\u8bba\u5c42\u6b21\u8981\u6c42\u3001\u540c\u8bcd\u5f02\u4e49\u3001\u62fc\u5199\u9519\u8bef\u548c\u7f29\u5199\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u6ce8\u89e3\u7684\u51c6\u786e\u6027\u3002", "method": "\u57fa\u4e8eReAct\u6846\u67b6\u8bbe\u8ba1\u5b9e\u73b0\u4e94\u4e2a\u5916\u90e8\u5de5\u5177\uff0c\u8ba9STA\u4ee3\u7406\u80fd\u591f\u6839\u636e\u8868\u683c\u7279\u5f81\u52a8\u6001\u9009\u62e9\u9002\u5408\u7684\u6ce8\u89e3\u7b56\u7565\uff0c\u5e76\u5229\u7528Levenshtein\u8ddd\u79bb\u51cf\u5c11\u5197\u4f59\u6ce8\u89e3\u3002", "result": "\u5728SemTab\u6311\u6218\u7684Tough Tables\u548cBiodivTab\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u65b9\u6cd5\u5728\u5404\u79cd\u6307\u6807\u4e0a\u90fd\u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e86\u65f6\u95f4\u6210\u672c\u964d\u4f4e70%\u548cLLM\u4ee4\u724c\u4f7f\u7528\u91cf\u51cf\u5c1160%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8bed\u4e49\u8868\u6ce8\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u8868\u683c\u7684\u5404\u79cd\u6311\u6218\u95ee\u9898\u3002"}}
{"id": "2508.12903", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12903", "abs": "https://arxiv.org/abs/2508.12903", "authors": ["Jinyi Han", "Xinyi Wang", "Haiquan Zhao", "Tingyun li", "Zishang Jiang", "Sihang Jiang", "Jiaqing Liang", "Xin Lin", "Weikang Zhou", "Zeye Sun", "Fei Yu", "Yanghua Xiao"], "title": "A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models", "comment": null, "summary": "Recent advances in self-refinement have demonstrated significant potential\nfor improving the outputs of large language models (LLMs) through iterative\nrefinement. However, most existing self-refinement methods rely on a reactive\nprocess with a fixed number of iterations, making it difficult to determine the\noptimal timing and content of refinement based on the evolving generation\ncontext. Inspired by the way humans dynamically refine their thoughts during\nexecution, we propose ProActive Self-Refinement (PASR), a novel method that\nenables LLMs to refine their outputs during the generation process. Unlike\nmethods that regenerate entire responses, PASR proactively decides whether,\nwhen, and how to refine based on the model's internal state and evolving\ncontext. We conduct extensive experiments on a diverse set of 10 tasks to\nevaluate the effectiveness of PASR. Experimental results show that PASR\nsignificantly enhances problem-solving performance. In particular, on Qwen3-8B,\nPASR reduces average token consumption by 41.6 percent compared to standard\ngeneration, while also achieving an 8.2 percent improvement in accuracy. Our\ncode and all baselines used in the paper are available in the GitHub.", "AI": {"tldr": "PASR\u662f\u4e00\u79cd\u4e3b\u52a8\u5f0f\u81ea\u4f18\u5316\u65b9\u6cd5\uff0c\u8ba9LLM\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u52a8\u6001\u51b3\u5b9a\u662f\u5426\u3001\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u4f18\u5316\u8f93\u51fa\uff0c\u76f8\u6bd4\u56fa\u5b9a\u8fed\u4ee3\u6b21\u6570\u7684\u4f20\u7edf\u65b9\u6cd5\uff0c\u5728\u51cf\u5c1141.6%token\u6d88\u8017\u7684\u540c\u65f6\u63d0\u53478.2%\u51c6\u786e\u7387", "motivation": "\u73b0\u6709\u81ea\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u8fed\u4ee3\u6b21\u6570\u7684\u88ab\u52a8\u8fc7\u7a0b\uff0c\u96be\u4ee5\u6839\u636e\u751f\u6210\u4e0a\u4e0b\u6587\u52a8\u6001\u786e\u5b9a\u6700\u4f73\u4f18\u5316\u65f6\u673a\u548c\u5185\u5bb9\uff0c\u800c\u4eba\u7c7b\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u4f1a\u52a8\u6001\u4f18\u5316\u601d\u8def", "method": "\u63d0\u51faPASR\u65b9\u6cd5\uff0c\u57fa\u4e8e\u6a21\u578b\u5185\u90e8\u72b6\u6001\u548c\u6f14\u5316\u4e0a\u4e0b\u6587\u4e3b\u52a8\u51b3\u5b9a\u662f\u5426\u3001\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u4f18\u5316\uff0c\u800c\u4e0d\u662f\u91cd\u65b0\u751f\u6210\u6574\u4e2a\u54cd\u5e94", "result": "\u572810\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cPASR\u663e\u8457\u63d0\u5347\u95ee\u9898\u89e3\u51b3\u6027\u80fd\uff0cQwen3-8B\u6a21\u578b\u4e0a\u76f8\u6bd4\u6807\u51c6\u751f\u6210\u51cf\u5c1141.6%token\u6d88\u8017\uff0c\u540c\u65f6\u51c6\u786e\u7387\u63d0\u53478.2%", "conclusion": "PASR\u901a\u8fc7\u4e3b\u52a8\u5f0f\u81ea\u4f18\u5316\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u52a8\u6001\u4f18\u5316\u8f93\u51fa\uff0c\u5b9e\u73b0\u4e86\u6548\u7387\u548c\u8d28\u91cf\u7684\u53cc\u91cd\u63d0\u5347\uff0c\u4e3aLLM\u81ea\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2508.12981", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12981", "abs": "https://arxiv.org/abs/2508.12981", "authors": ["Tianyue Ou", "Saujas Vaduguru", "Daniel Fried"], "title": "Analyzing Information Sharing and Coordination in Multi-Agent Planning", "comment": null, "summary": "Multi-agent systems (MASs) have pushed the boundaries of large language model\n(LLM) agents in domains such as web research and software engineering. However,\nlong-horizon, multi-constraint planning tasks involve conditioning on detailed\ninformation and satisfying complex interdependent constraints, which can pose a\nchallenge for these systems. In this study, we construct an LLM-based MAS for a\ntravel planning task which is representative of these challenges. We evaluate\nthe impact of a notebook to facilitate information sharing, and evaluate an\norchestrator agent to improve coordination in free form conversation between\nagents. We find that the notebook reduces errors due to hallucinated details by\n18%, while an orchestrator directs the MAS to focus on and further reduce\nerrors by up to 13.5% within focused sub-areas. Combining both mechanisms\nachieves a 25% final pass rate on the TravelPlanner benchmark, a 17.5% absolute\nimprovement over the single-agent baseline's 7.5% pass rate. These results\nhighlight the potential of structured information sharing and reflective\norchestration as key components in MASs for long horizon planning with LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7528\u4e8e\u65c5\u884c\u89c4\u5212\u4efb\u52a1\uff0c\u901a\u8fc7\u5f15\u5165\u7b14\u8bb0\u672c\u673a\u5236\u548c\u534f\u8c03\u5668\u667a\u80fd\u4f53\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u7ea6\u675f\u89c4\u5212\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u7684\u957f\u65f6\u57df\u3001\u591a\u7ea6\u675f\u89c4\u5212\u4efb\u52a1\u4e2d\u9762\u4e34\u4fe1\u606f\u5171\u4eab\u548c\u534f\u8c03\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5904\u7406\u8be6\u7ec6\u4fe1\u606f\u548c\u6ee1\u8db3\u590d\u6742\u76f8\u4e92\u4f9d\u8d56\u7ea6\u675f\u7684\u573a\u666f\u4e0b\u3002", "method": "\u6784\u5efa\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8fdb\u884c\u65c5\u884c\u89c4\u5212\uff0c\u8bc4\u4f30\u7b14\u8bb0\u672c\u673a\u5236\u4fc3\u8fdb\u4fe1\u606f\u5171\u4eab\u7684\u6548\u679c\uff0c\u5e76\u6d4b\u8bd5\u534f\u8c03\u5668\u667a\u80fd\u4f53\u5728\u81ea\u7531\u5bf9\u8bdd\u4e2d\u7684\u534f\u8c03\u80fd\u529b\u3002", "result": "\u7b14\u8bb0\u672c\u673a\u5236\u5c06\u5e7b\u89c9\u7ec6\u8282\u5bfc\u81f4\u7684\u9519\u8bef\u51cf\u5c1118%\uff0c\u534f\u8c03\u5668\u5728\u91cd\u70b9\u5b50\u533a\u57df\u8fdb\u4e00\u6b65\u51cf\u5c11\u9519\u8bef13.5%\u3002\u7ec4\u5408\u4f7f\u7528\u4e24\u79cd\u673a\u5236\u5728TravelPlanner\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523025%\u7684\u901a\u8fc7\u7387\uff0c\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u76847.5%\u670917.5%\u7684\u7edd\u5bf9\u63d0\u5347\u3002", "conclusion": "\u7ed3\u6784\u5316\u4fe1\u606f\u5171\u4eab\u548c\u53cd\u5c04\u5f0f\u534f\u8c03\u662f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5e94\u5bf9\u957f\u65f6\u57df\u89c4\u5212\u4efb\u52a1\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u5177\u6709\u91cd\u8981\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.13024", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.13024", "abs": "https://arxiv.org/abs/2508.13024", "authors": ["Ralph Peeters", "Aaron Steiner", "Luca Schwarz", "Julian Yuya Caspary", "Christian Bizer"], "title": "WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents", "comment": null, "summary": "LLM-based web agents have the potential to automate long-running web tasks,\nsuch as finding offers for specific products in multiple online shops and\nsubsequently ordering the cheapest products that meet the users needs. This\npaper introduces WebMall, a multi-shop online shopping benchmark for evaluating\nthe effectiveness and efficiency of web agents for comparison-shopping. WebMall\nconsists of four simulated online shops populated with authentic product offers\nsourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. These\ntasks include basic tasks such as finding specific products in multiple shops,\nperforming price comparisons, adding items to the shopping cart, and completing\ncheckout. Advanced tasks involve searching for products based on vague\nrequirements, identifying suitable substitutes, and finding compatible\nproducts. Compared to existing e-commerce benchmarks, such as WebShop or\nShoppingBench, WebMall introduces comparison-shopping tasks across multiple\nshops. Furthermore, the product offers are more heterogeneous, as they\noriginate from hundreds of distinct real-world shops. The tasks in WebMall\nrequire longer interaction trajectories than those in WebShop, while remaining\nrepresentative of real-world shopping behaviors. We evaluate eight baseline\nagents on WebMall, varying in observation modality, memory utilization, and\nunderlying large language model (GPT 4.1 and Claude Sonnet 4). The\nbest-performing configurations achieve completion rates of 75% and 53%, and F1\nscores of 87% and 63%, on the basic and advanced task sets, respectively.\nWebMall is publicly released to facilitate research on web agents and to\npromote advancements in navigation, reasoning, and efficiency within e-commerce\nscenarios.", "AI": {"tldr": "WebMall\u662f\u4e00\u4e2a\u591a\u5546\u5e97\u5728\u7ebf\u8d2d\u7269\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u7f51\u7edc\u4ee3\u7406\u5728\u6bd4\u4ef7\u8d2d\u7269\u4e2d\u7684\u6548\u679c\u548c\u6548\u7387\uff0c\u5305\u542b91\u4e2a\u8de8\u5546\u5e97\u4efb\u52a1\u548c\u6765\u81ea\u771f\u5b9e\u7535\u5546\u7684\u5f02\u6784\u4ea7\u54c1\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u7684\u7535\u5546\u57fa\u51c6\u6d4b\u8bd5\u5982WebShop\u6216ShoppingBench\u7f3a\u4e4f\u8de8\u5546\u5e97\u6bd4\u4ef7\u8d2d\u7269\u4efb\u52a1\uff0c\u4ea7\u54c1\u6570\u636e\u540c\u8d28\u6027\u8f83\u9ad8\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u7f51\u7edc\u4ee3\u7406\u5728\u771f\u5b9e\u591a\u5546\u5e97\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u6784\u5efa\u56db\u4e2a\u6a21\u62df\u5728\u7ebf\u5546\u5e97\uff0c\u4f7f\u7528Common Crawl\u722c\u53d6\u7684\u771f\u5b9e\u4ea7\u54c1\u6570\u636e\u586b\u5145\uff0c\u8bbe\u8ba191\u4e2a\u8de8\u5546\u5e97\u4efb\u52a1\uff08\u5305\u62ec\u57fa\u7840\u4efb\u52a1\u548c\u9ad8\u7ea7\u4efb\u52a1\uff09\uff0c\u8bc4\u4f308\u4e2a\u4e0d\u540c\u914d\u7f6e\u7684\u57fa\u7ebf\u4ee3\u7406\u3002", "result": "\u6700\u4f73\u914d\u7f6e\u5728\u57fa\u7840\u4efb\u52a1\u96c6\u4e0a\u8fbe\u523075%\u5b8c\u6210\u7387\u548c87% F1\u5206\u6570\uff0c\u5728\u9ad8\u7ea7\u4efb\u52a1\u96c6\u4e0a\u8fbe\u523053%\u5b8c\u6210\u7387\u548c63% F1\u5206\u6570\u3002", "conclusion": "WebMall\u4e3a\u7f51\u7edc\u4ee3\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u3001\u66f4\u5177\u6311\u6218\u6027\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u4fc3\u8fdb\u4e86\u7535\u5546\u573a\u666f\u4e2d\u5bfc\u822a\u3001\u63a8\u7406\u548c\u6548\u7387\u65b9\u9762\u7684\u6280\u672f\u8fdb\u6b65\u3002"}}
{"id": "2508.13028", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.13028", "abs": "https://arxiv.org/abs/2508.13028", "authors": ["Zhu Li", "Yuqing Zhang", "Xiyuan Gao", "Devraj Raghuvanshi", "Nagendra Kumar", "Shekhar Nayak", "Matt Coler"], "title": "Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis", "comment": "Speech Synthesis Workshop 2025", "summary": "Sarcastic speech synthesis, which involves generating speech that effectively\nconveys sarcasm, is essential for enhancing natural interactions in\napplications such as entertainment and human-computer interaction. However,\nsynthesizing sarcastic speech remains a challenge due to the nuanced prosody\nthat characterizes sarcasm, as well as the limited availability of annotated\nsarcastic speech data. To address these challenges, this study introduces a\nnovel approach that integrates feedback loss from a bi-modal sarcasm detection\nmodel into the TTS training process, enhancing the model's ability to capture\nand convey sarcasm. In addition, by leveraging transfer learning, a speech\nsynthesis model pre-trained on read speech undergoes a two-stage fine-tuning\nprocess. First, it is fine-tuned on a diverse dataset encompassing various\nspeech styles, including sarcastic speech. In the second stage, the model is\nfurther refined using a dataset focused specifically on sarcastic speech,\nenhancing its ability to generate sarcasm-aware speech. Objective and\nsubjective evaluations demonstrate that our proposed methods improve the\nquality, naturalness, and sarcasm-awareness of synthesized speech.", "AI": {"tldr": "\u901a\u8fc7\u4e8c\u9636\u6bb5\u5fae\u8c03\u548c\u53cc\u6a21\u6001\u8bc1\u636e\u635f\u5931\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u751f\u6210\u5177\u6709\u98a0\u523a\u654f\u611f\u6027\u8bed\u97f3\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5408\u6210\u8bed\u97f3\u7684\u98ce\u8da3\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u98a0\u523a\u8bed\u97f3\u5408\u6210\u5bf9\u4e8e\u5a31\u4e50\u548c\u4eba\u673a\u4ea4\u4e92\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u98a0\u523a\u8bed\u8c03\u7684\u7ec6\u5fae\u5dee\u5f02\u6027\u548c\u6807\u6ce8\u6570\u636e\u7684\u7a00\u7f3a\uff0c\u8be5\u4efb\u52a1\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e8c\u9636\u6bb5\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff1a\u5148\u5728\u591a\u6837\u5316\u8bed\u97f3\u6837\u5f0f\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\uff0c\u7136\u540e\u5728\u98a0\u523a\u8bed\u97f3\u6570\u636e\u96c6\u4e0a\u7cbe\u7ec6\u8c03\u6574\u3002\u540c\u65f6\u5728TTS\u8bad\u7ec3\u4e2d\u96c6\u6210\u53cc\u6a21\u6001\u98a0\u523a\u68c0\u6d4b\u6a21\u578b\u7684\u53cd\u9988\u635f\u5931\uff0c\u4ee5\u63d0\u5347\u98a0\u523a\u611f\u77e5\u80fd\u529b\u3002", "result": "\u5ba2\u89c2\u548c\u4e3b\u89c2\u8bc4\u4f30\u90fd\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5408\u6210\u8bed\u97f3\u7684\u8d28\u91cf\u3001\u81ea\u7136\u5ea6\u548c\u98a0\u523a\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u98a0\u523a\u8bed\u97f3\u5408\u6210\u7684\u6311\u6218\uff0c\u4e3a\u751f\u6210\u66f4\u81ea\u7136\u3001\u66f4\u5177\u98a0\u523a\u654f\u611f\u6027\u7684\u8bed\u97f3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2508.13037", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13037", "abs": "https://arxiv.org/abs/2508.13037", "authors": ["Xinhe Li", "Jiajun Liu", "Peng Wang"], "title": "Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction", "comment": "Accepted by IJCAI2025", "summary": "Recent studies have demonstrated that Large Language Models (LLMs) have\nstrong mathematical reasoning abilities but rely on hundreds of billions of\nparameters. To tackle the challenge of poor reasoning in Small Language Models\n(SLMs), existing methods typically leverage LLMs to generate massive amounts of\ndata for cramming training. In psychology, they are akin to System 1 thinking,\nwhich resolves reasoning problems rapidly based on experience and intuition.\nHowever, human learning also requires System 2 thinking, where knowledge is\nfirst acquired and then reinforced through practice. Inspired by such two\ndistinct modes of thinking, we propose a novel method based on the multi-LoRA\nInteraction for mathematical reasoning Distillation (LoRID). First, we input\nthe question and reasoning of each sample into an LLM to create\nknowledge-enhanced datasets. Subsequently, we train a LoRA block on the student\nmodel as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts\nfor problem-solving. Then, to imitate System 2 thinking, we train the Knowledge\nGenerator (KG) and Deep Reasoner (DR), respectively. The former outputs only\nknowledge after receiving problems, while the latter uses that knowledge to\nperform reasoning. Finally, to address the randomness in the generation of IR\nand DR, we evaluate whether their outputs are consistent, and the inference\nprocess needs to be iterated if not. This step can enhance the mathematical\nreasoning ability of SLMs through mutual feedback. Experimental results show\nthat LoRID achieves state-of-the-art performance, especially on the GSM8K\ndataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%,\n12.3%, and 1.8% accuracy across the five base models, respectively.", "AI": {"tldr": "LoRID\u662f\u4e00\u79cd\u57fa\u4e8e\u591aLoRA\u4ea4\u4e92\u7684\u6570\u5b66\u63a8\u7406\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7bSystem 1\u548cSystem 2\u4e24\u79cd\u601d\u7ef4\u6a21\u5f0f\uff0c\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5728GSM8K\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5c0f\u8bed\u8a00\u6a21\u578b(SLMs)\u6570\u5b66\u63a8\u7406\u80fd\u529b\u5f31\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5927\u91cf\u6570\u636e\u8fdb\u884c\u586b\u9e2d\u5f0f\u8bad\u7ec3\uff0c\u8fd9\u7c7b\u4f3c\u4e8e\u5fc3\u7406\u5b66\u4e2d\u7684System 1\u601d\u7ef4\u3002\u4f46\u4eba\u7c7b\u5b66\u4e60\u8fd8\u9700\u8981System 2\u601d\u7ef4\uff0c\u5373\u5148\u83b7\u53d6\u77e5\u8bc6\u518d\u901a\u8fc7\u5b9e\u8df5\u5f3a\u5316\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u591aLoRA\u4ea4\u4e92\u7684\u6570\u5b66\u63a8\u7406\u84b8\u998f\u65b9\u6cd5(LoRID)\uff1a1)\u7528LLM\u521b\u5efa\u77e5\u8bc6\u589e\u5f3a\u6570\u636e\u96c6\uff1b2)\u8bad\u7ec3\u76f4\u89c9\u63a8\u7406\u5668(IR)\u76f4\u63a5\u751f\u6210\u601d\u7ef4\u94fe\uff1b3)\u8bad\u7ec3\u77e5\u8bc6\u751f\u6210\u5668(KG)\u548c\u6df1\u5ea6\u63a8\u7406\u5668(DR)\u6a21\u62dfSystem 2\u601d\u7ef4\uff1b4)\u901a\u8fc7IR\u548cDR\u8f93\u51fa\u4e00\u81f4\u6027\u8bc4\u4f30\u8fdb\u884c\u8fed\u4ee3\u63a8\u7406\u3002", "result": "LoRID\u5728GSM8K\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u4e94\u4e2a\u57fa\u7840\u6a21\u578b\u4e0a\u5206\u522b\u6bd4\u7b2c\u4e8c\u540d\u65b9\u6cd5\u9ad8\u51fa2.3%\u300116.1%\u30012.4%\u300112.3%\u548c1.8%\u7684\u51c6\u786e\u7387\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u4e24\u79cd\u601d\u7ef4\u6a21\u5f0f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u591aLoRA\u4ea4\u4e92\u548c\u8fed\u4ee3\u63a8\u7406\u673a\u5236\u663e\u8457\u6539\u5584\u4e86\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2508.13044", "categories": ["cs.CL", "68T50", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.13044", "abs": "https://arxiv.org/abs/2508.13044", "authors": ["M. Ali Bayram", "Ali Arda Fincan", "Ahmet Semih G\u00fcm\u00fc\u015f", "Banu Diri", "Sava\u015f Y\u0131ld\u0131r\u0131m", "\u00d6ner Ayta\u015f"], "title": "B\u00fcy\u00fck Dil Modelleri i\u00e7in TR-MMLU Benchmark\u0131: Performans De\u011ferlendirmesi, Zorluklar ve \u0130yile\u015ftirme F\u0131rsatlar\u0131", "comment": "10 pages, in Turkish language, 5 figures. Presented at the 2025 33rd\n  Signal Processing and Communications Applications Conference (SIU), 25--28\n  June 2025, Sile, Istanbul, T\\\"urkiye", "summary": "Language models have made significant advancements in understanding and\ngenerating human language, achieving remarkable success in various\napplications. However, evaluating these models remains a challenge,\nparticularly for resource-limited languages like Turkish. To address this\nissue, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive\nevaluation framework designed to assess the linguistic and conceptual\ncapabilities of large language models (LLMs) in Turkish. TR-MMLU is based on a\nmeticulously curated dataset comprising 6,200 multiple-choice questions across\n62 sections within the Turkish education system. This benchmark provides a\nstandard framework for Turkish NLP research, enabling detailed analyses of\nLLMs' capabilities in processing Turkish text. In this study, we evaluated\nstate-of-the-art LLMs on TR-MMLU, highlighting areas for improvement in model\ndesign. TR-MMLU sets a new standard for advancing Turkish NLP research and\ninspiring future innovations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u571f\u8033\u5176\u8bedMMLU\u57fa\u51c6(TR-MMLU)\uff0c\u5305\u542b6200\u9053\u9009\u62e9\u9898\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u571f\u8033\u5176\u8bed\u4e0a\u7684\u8bed\u8a00\u548c\u6982\u5ff5\u80fd\u529b", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e3b\u8981\u9488\u5bf9\u8d44\u6e90\u4e30\u5bcc\u7684\u8bed\u8a00\uff0c\u571f\u8033\u5176\u8bed\u7b49\u8d44\u6e90\u6709\u9650\u8bed\u8a00\u7f3a\u4e4f\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u6d4b\u6846\u67b6", "method": "\u57fa\u4e8e\u571f\u8033\u5176\u6559\u80b2\u4f53\u7cfb\u6784\u5efa\u5305\u542b62\u4e2a\u9886\u57df\u76846200\u9053\u9009\u62e9\u9898\u6570\u636e\u96c6\uff0c\u5efa\u7acb\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5bf9\u5148\u8fdbLLM\u8fdb\u884c\u8bc4\u6d4b", "result": "\u521b\u5efa\u4e86TR-MMLU\u57fa\u51c6\uff0c\u4e3a\u571f\u8033\u5176NLP\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u8bc4\u4f30\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u571f\u8033\u5176\u8bed\u5904\u7406\u65b9\u9762\u7684\u6539\u8fdb\u7a7a\u95f4", "conclusion": "TR-MMLU\u4e3a\u571f\u8033\u5176NLP\u7814\u7a76\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u5c06\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u5e76\u6fc0\u53d1\u672a\u6765\u521b\u65b0"}}
{"id": "2508.13058", "categories": ["cs.CL", "68T50", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.13058", "abs": "https://arxiv.org/abs/2508.13058", "authors": ["M. Ali Bayram", "Ali Arda Fincan", "Ahmet Semih G\u00fcm\u00fc\u015f", "Sercan Karaka\u015f", "Banu Diri", "Sava\u015f Y\u0131ld\u0131r\u0131m"], "title": "Do\u011fal Dil \u0130\u015flemede Tokenizasyon Standartlar\u0131 ve \u00d6l\u00e7\u00fcm\u00fc: T\u00fcrk\u00e7e \u00dczerinden B\u00fcy\u00fck Dil Modellerinin Kar\u015f\u0131la\u015ft\u0131rmal\u0131 Analizi", "comment": "in Turkish language, Presented at the 2025 33rd Signal Processing and\n  Communications Applications Conference (SIU), 25--28 June 2025, \\c{S}ile,\n  Istanbul, T\\\"urkiye", "summary": "Tokenization is a fundamental preprocessing step in Natural Language\nProcessing (NLP), significantly impacting the capability of large language\nmodels (LLMs) to capture linguistic and semantic nuances. This study introduces\na novel evaluation framework addressing tokenization challenges specific to\nmorphologically-rich and low-resource languages such as Turkish. Utilizing the\nTurkish MMLU (TR-MMLU) dataset, comprising 6,200 multiple-choice questions from\nthe Turkish education system, we assessed tokenizers based on vocabulary size,\ntoken count, processing time, language-specific token percentages (\\%TR), and\ntoken purity (\\%Pure). These newly proposed metrics measure how effectively\ntokenizers preserve linguistic structures. Our analysis reveals that\nlanguage-specific token percentages exhibit a stronger correlation with\ndownstream performance (e.g., MMLU scores) than token purity. Furthermore,\nincreasing model parameters alone does not necessarily enhance linguistic\nperformance, underscoring the importance of tailored, language-specific\ntokenization methods. The proposed framework establishes robust and practical\ntokenization standards for morphologically complex languages.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u5bf9\u4e8e\u5f62\u6001\u4e30\u5bcc\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u571f\u8033\u5176\u8bed\uff09\u7684\u6807\u8bb0\u5316\u65b9\u6848\u3002\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u7279\u5b9a\u6807\u8bb0\u767e\u5206\u6bd4\u4e0e\u4e0b\u6e38\u6027\u80fd\u66f4\u76f8\u5173\uff0c\u800c\u4e14\u5355\u7eaf\u589e\u52a0\u6a21\u578b\u53c2\u6570\u5e76\u4e0d\u80fd\u63d0\u5347\u8bed\u8a00\u6027\u80fd\u3002", "motivation": "\u6807\u8bb0\u5316\u662fNLP\u4e2d\u7684\u57fa\u7840\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u8a00\u5b66\u548c\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u6709\u91cd\u8981\u5f71\u54cd\u3002\u5c24\u5176\u662f\u5bf9\u4e8e\u5f62\u6001\u4e30\u5bcc\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u89e3\u51b3\u6807\u8bb0\u5316\u6311\u6218\u3002", "method": "\u4f7f\u7528\u571f\u8033\u5176MMLU\uff08TR-MMLU\uff09\u6570\u636e\u96c6\uff08\u5305\u542b6,200\u9053\u591a\u9009\u9898\uff09\uff0c\u901a\u8fc7\u8bc4\u4f30\u8bcd\u6c47\u89c4\u6a21\u3001\u6807\u8bb0\u6570\u91cf\u3001\u5904\u7406\u65f6\u95f4\u3001\u8bed\u8a00\u7279\u5b9a\u6807\u8bb0\u767e\u5206\u6bd4\uff08%TR\uff09\u548c\u6807\u8bb0\u7eaf\u5ea6\uff08%Pure\uff09\u7b49\u6307\u6807\u6765\u5206\u6790\u6807\u8bb0\u5316\u5668\u7684\u6027\u80fd\u3002", "result": "\u5206\u6790\u663e\u793a\u8bed\u8a00\u7279\u5b9a\u6807\u8bb0\u767e\u5206\u6bd4\u4e0e\u4e0b\u6e38\u6027\u80fd\uff08\u5982MMLU\u5206\u6570\uff09\u7684\u76f8\u5173\u6027\u66f4\u5f3a\uff0c\u800c\u6807\u8bb0\u7eaf\u5ea6\u7684\u76f8\u5173\u6027\u8f83\u5f31\u3002\u540c\u65f6\uff0c\u5355\u7eaf\u589e\u52a0\u6a21\u578b\u53c2\u6570\u5e76\u4e0d\u80fd\u6709\u6548\u63d0\u5347\u8bed\u8a00\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f62\u6001\u590d\u6742\u8bed\u8a00\u5efa\u7acb\u4e86\u5065\u58ee\u800c\u5b9e\u7528\u7684\u6807\u8bb0\u5316\u6807\u51c6\uff0c\u5f3a\u8c03\u4e86\u91c7\u7528\u9002\u914d\u7684\u8bed\u8a00\u7279\u5b9a\u6807\u8bb0\u5316\u65b9\u6cd5\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.13060", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.13060", "abs": "https://arxiv.org/abs/2508.13060", "authors": ["John Alderete", "Macarious Kin Fung Hui", "Aanchan Mohan"], "title": "Evaluating ASR robustness to spontaneous speech errors: A study of WhisperX using a Speech Error Database", "comment": "5 pages, 6 figures, 1 table, Interspeech 2025 (Rotterdam)", "summary": "The Simon Fraser University Speech Error Database (SFUSED) is a public data\ncollection developed for linguistic and psycholinguistic research. Here we\ndemonstrate how its design and annotations can be used to test and evaluate\nspeech recognition models. The database comprises systematically annotated\nspeech errors from spontaneous English speech, with each error tagged for\nintended and actual error productions. The annotation schema incorporates\nmultiple classificatory dimensions that are of some value to model assessment,\nincluding linguistic hierarchical level, contextual sensitivity, degraded\nwords, word corrections, and both word-level and syllable-level error\npositioning. To assess the value of these classificatory variables, we\nevaluated the transcription accuracy of WhisperX across 5,300 documented word\nand phonological errors. This analysis demonstrates the atabase's effectiveness\nas a diagnostic tool for ASR system performance.", "AI": {"tldr": "SFUSED\u662f\u4e00\u4e2a\u7528\u4e8e\u6d4b\u8bd5\u548c\u8bc4\u4f30\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u7684\u516c\u5f00\u8bed\u97f3\u9519\u8bef\u6570\u636e\u5e93\uff0c\u5305\u542b5300\u4e2a\u6807\u6ce8\u7684\u82f1\u8bed\u8bed\u97f3\u9519\u8bef\uff0c\u901a\u8fc7WhisperX\u8bc4\u4f30\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3aASR\u7cfb\u7edf\u8bca\u65ad\u5de5\u5177\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u7cfb\u7edf\u6807\u6ce8\u7684\u81ea\u53d1\u82f1\u8bed\u8bed\u97f3\u9519\u8bef\u6570\u636e\u5e93\uff0c\u7528\u4e8e\u6d4b\u8bd5\u548c\u8bc4\u4f30\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u63d0\u4f9b\u591a\u7ef4\u5ea6\u5206\u7c7b\u6807\u6ce8\u6765\u652f\u6301\u6a21\u578b\u8bc4\u4f30\u3002", "method": "\u6536\u96c6\u548c\u7cfb\u7edf\u6807\u6ce8\u81ea\u53d1\u82f1\u8bed\u8bed\u97f3\u9519\u8bef\uff0c\u5305\u542b\u610f\u56fe\u548c\u5b9e\u9645\u9519\u8bef\u4ea7\u751f\u7684\u6807\u6ce8\uff0c\u91c7\u7528\u591a\u7ef4\u5ea6\u5206\u7c7b\u4f53\u7cfb\uff08\u8bed\u8a00\u5c42\u7ea7\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u3001\u9000\u5316\u8bcd\u3001\u8bcd\u4fee\u6b63\u3001\u8bcd\u7ea7\u548c\u97f3\u8282\u7ea7\u9519\u8bef\u5b9a\u4f4d\uff09\uff0c\u4f7f\u7528WhisperX\u5bf95300\u4e2a\u8bcd\u548c\u97f3\u7cfb\u9519\u8bef\u8fdb\u884c\u8f6c\u5f55\u51c6\u786e\u6027\u8bc4\u4f30\u3002", "result": "SFUSED\u6570\u636e\u5e93\u88ab\u8bc1\u660e\u662f\u8bc4\u4f30ASR\u7cfb\u7edf\u6027\u80fd\u7684\u6709\u6548\u8bca\u65ad\u5de5\u5177\uff0c\u80fd\u591f\u901a\u8fc7\u591a\u7ef4\u5ea6\u5206\u7c7b\u53d8\u91cf\u6765\u6d4b\u8bd5\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "SFUSED\u6570\u636e\u5e93\u7684\u8bbe\u8ba1\u548c\u6807\u6ce8\u65b9\u6cd5\u4e3a\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u7684\u6d4b\u8bd5\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u5206\u6790\u4e0d\u540c\u7c7b\u578b\u8bed\u97f3\u9519\u8bef\u5bf9ASR\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u65b9\u9762\u3002"}}
{"id": "2508.13070", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13070", "abs": "https://arxiv.org/abs/2508.13070", "authors": ["Long Ma", "Fangwei Zhong", "Yizhou Wang"], "title": "Reinforced Context Order Recovery for Adaptive Reasoning and Planning", "comment": null, "summary": "Modern causal language models, followed by rapid developments in discrete\ndiffusion models, can now produce a wide variety of interesting and useful\ncontent. However, these families of models are predominantly trained to output\ntokens with a fixed (left-to-right) or random order, which may deviate from the\nlogical order in which tokens are generated originally. In this paper, we\nobserve that current causal and diffusion models encounter difficulties in\nproblems that require adaptive token generation orders to solve tractably,\nwhich we characterize with the $\\mathcal{V}$-information framework. Motivated\nby this, we propose Reinforced Context Order Recovery (ReCOR), a\nreinforcement-learning-based framework to extract adaptive, data-dependent\ntoken generation orders from text data without annotations. Self-supervised by\ntoken prediction statistics, ReCOR estimates the hardness of predicting every\nunfilled token and adaptively selects the next token during both training and\ninference. Experiments on challenging reasoning and planning datasets\ndemonstrate the superior performance of ReCOR compared with baselines,\nsometimes outperforming oracle models supervised with the ground-truth order.", "AI": {"tldr": "ReCOR\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9token\u751f\u6210\u987a\u5e8f\u6765\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u548c\u89c4\u5212\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u5373\u53ef\u4ece\u6587\u672c\u4e2d\u5b66\u4e60\u6700\u4f18\u751f\u6210\u987a\u5e8f\u3002", "motivation": "\u5f53\u524d\u56e0\u679c\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u4f7f\u7528\u56fa\u5b9a\u6216\u968f\u673a\u7684token\u751f\u6210\u987a\u5e8f\uff0c\u8fd9\u53ef\u80fd\u504f\u79bb\u539f\u59cb\u7684\u903b\u8f91\u987a\u5e8f\uff0c\u5bfc\u81f4\u5728\u5904\u7406\u9700\u8981\u81ea\u9002\u5e94token\u751f\u6210\u987a\u5e8f\u7684\u63a8\u7406\u548c\u89c4\u5212\u95ee\u9898\u65f6\u9047\u5230\u56f0\u96be\u3002", "method": "\u63d0\u51faReCOR\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u81ea\u76d1\u7763\u5730\u4f30\u8ba1\u6bcf\u4e2a\u672a\u586b\u5145token\u7684\u9884\u6d4b\u96be\u5ea6\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u9009\u62e9\u4e0b\u4e00\u4e2a\u8981\u751f\u6210\u7684token\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u4ece\u6570\u636e\u4e2d\u63d0\u53d6\u6700\u4f18\u751f\u6210\u987a\u5e8f\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u63a8\u7406\u548c\u89c4\u5212\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cReCOR\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u6709\u65f6\u751a\u81f3\u4f18\u4e8e\u4f7f\u7528\u771f\u5b9e\u987a\u5e8f\u76d1\u7763\u7684oracle\u6a21\u578b\u3002", "conclusion": "ReCOR\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94token\u751f\u6210\u987a\u5e8f\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5b66\u4e60\u6570\u636e\u4f9d\u8d56\u7684\u751f\u6210\u987a\u5e8f\u5bf9\u4e8e\u89e3\u51b3\u9700\u8981\u7075\u6d3b\u63a8\u7406\u6a21\u5f0f\u7684\u95ee\u9898\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.13079", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.13079", "abs": "https://arxiv.org/abs/2508.13079", "authors": ["Dayy\u00e1n O'Brien", "Bhavitvya Malik", "Ona de Gibert", "Pinzhen Chen", "Barry Haddow", "J\u00f6rg Tiedemann"], "title": "DocHPLT: A Massively Multilingual Document-Level Translation Dataset", "comment": null, "summary": "Existing document-level machine translation resources are only available for\na handful of languages, mostly high-resourced ones. To facilitate the training\nand evaluation of document-level translation and, more broadly, long-context\nmodeling for global communities, we create DocHPLT, the largest publicly\navailable document-level translation dataset to date. It contains 124 million\naligned document pairs across 50 languages paired with English, comprising 4.26\nbillion sentences, with further possibility to provide 2500 bonus pairs not\ninvolving English. Unlike previous reconstruction-based approaches that piece\ntogether documents from sentence-level data, we modify an existing web\nextraction pipeline to preserve complete document integrity from the source,\nretaining all content including unaligned portions. After our preliminary\nexperiments identify the optimal training context strategy for document-level\ntranslation, we demonstrate that LLMs fine-tuned on DocHPLT substantially\noutperform off-the-shelf instruction-tuned baselines, with particularly\ndramatic improvements for under-resourced languages. We open-source the dataset\nunder a permissive license, providing essential infrastructure for advancing\nmultilingual document-level translation.", "AI": {"tldr": "DocHPLT\u662f\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u516c\u5f00\u6587\u6863\u7ea7\u7ffb\u8bd1\u6570\u636e\u96c6\uff0c\u5305\u542b1.24\u4ebf\u4e2a\u6587\u6863\u5bf9\uff0c\u6db5\u76d650\u79cd\u8bed\u8a00\u4e0e\u82f1\u8bed\u914d\u5bf9\uff0c\u5305\u542b42.6\u4ebf\u4e2a\u53e5\u5b50\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u6587\u6863\u7ea7\u7ffb\u8bd1\u63d0\u4f9b\u91cd\u8981\u57fa\u7840\u8bbe\u65bd\u3002", "motivation": "\u73b0\u6709\u6587\u6863\u7ea7\u673a\u5668\u7ffb\u8bd1\u8d44\u6e90\u4ec5\u9002\u7528\u4e8e\u5c11\u6570\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u4e3a\u4fc3\u8fdb\u5168\u7403\u793e\u533a\u7684\u6587\u6863\u7ea7\u7ffb\u8bd1\u548c\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u8bad\u7ec3\u4e0e\u8bc4\u4f30\uff0c\u9700\u8981\u521b\u5efa\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u6587\u6863\u7ea7\u7ffb\u8bd1\u6570\u636e\u96c6\u3002", "method": "\u4fee\u6539\u73b0\u6709\u7684\u7f51\u9875\u63d0\u53d6\u6d41\u7a0b\u4ee5\u4fdd\u6301\u6e90\u6587\u6863\u7684\u5b8c\u6574\u6027\uff0c\u4fdd\u7559\u6240\u6709\u5185\u5bb9\u5305\u62ec\u672a\u5bf9\u9f50\u90e8\u5206\uff0c\u800c\u4e0d\u662f\u57fa\u4e8e\u53e5\u5b50\u7ea7\u6570\u636e\u62fc\u63a5\u6587\u6863\u7684\u91cd\u5efa\u65b9\u6cd5\u3002\u901a\u8fc7\u521d\u6b65\u5b9e\u9a8c\u786e\u5b9a\u6700\u4f73\u8bad\u7ec3\u4e0a\u4e0b\u6587\u7b56\u7565\u3002", "result": "\u5728DocHPLT\u4e0a\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u73b0\u6210\u7684\u6307\u4ee4\u8c03\u4f18\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u6539\u8fdb\u5c24\u4e3a\u663e\u8457\u3002", "conclusion": "DocHPLT\u6570\u636e\u96c6\u5728\u5bbd\u677e\u8bb8\u53ef\u4e0b\u5f00\u6e90\uff0c\u4e3a\u63a8\u8fdb\u591a\u8bed\u8a00\u6587\u6863\u7ea7\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u7279\u522b\u6709\u52a9\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.13107", "categories": ["cs.CL", "cs.IR", "F.2.2, H.3.3, I.2.7"], "pdf": "https://arxiv.org/pdf/2508.13107", "abs": "https://arxiv.org/abs/2508.13107", "authors": ["Figarri Keisha", "Prince Singh", "Pallavi", "Dion Fernandes", "Aravindh Manivannan", "Ilham Wicaksono", "Faisal Ahmad"], "title": "All for law and law for all: Adaptive RAG Pipeline for Legal Research", "comment": "submitted to NLLP 2025 Workshop", "summary": "Retrieval-Augmented Generation (RAG) mitigates hallucinations by grounding\nlarge language model outputs in cited sources, a capability that is especially\ncritical in the legal domain. We present an end-to-end RAG pipeline that\nrevisits and extends the LegalBenchRAG baseline with three targeted\nenhancements: (i) a context-aware query translator that disentangles document\nreferences from natural-language questions and adapts retrieval depth and\nresponse style based on expertise and specificity, (ii) open-source retrieval\nstrategies using SBERT and GTE embeddings that achieve substantial performance\ngains (improving Recall@K by 30-95\\% and Precision@K by $\\sim$2.5$\\times$ for\n$K>4$) while remaining cost-efficient, and (iii) a comprehensive evaluation and\ngeneration framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall to\nassess semantic alignment and faithfulness across models and prompt designs.\nOur results show that carefully designed open-source pipelines can rival or\noutperform proprietary approaches in retrieval quality, while a custom\nlegal-grounded prompt consistently produces more faithful and contextually\nrelevant answers than baseline prompting. Taken together, these contributions\ndemonstrate the potential of task-aware, component-level tuning to deliver\nlegally grounded, reproducible, and cost-effective RAG systems for legal\nresearch assistance.", "AI": {"tldr": "\u901a\u8fc7\u4e09\u9879\u5177\u4f53\u6539\u8fdb\uff08\u4e0a\u4e0b\u6587\u611f\u77e5\u67e5\u8be2\u7ffb\u8bd1\u3001\u5f00\u6e90\u68c0\u7d22\u7b56\u7565\u3001\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff09\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u6210\u672c\u6548\u76ca\u7684\u6cd5\u5f8b\u9886\u57dfRAG\u7cfb\u7edf\uff0c\u5728\u68c0\u7d22\u8d28\u91cf\u548c\u56de\u7b54\u51c6\u786e\u6027\u65b9\u9762\u53ef\u4e0e\u4e13\u6709\u65b9\u6848\u76f8\u6bd4\u6216\u66f4\u4f18\u3002", "motivation": "\u6cd5\u5f8b\u9886\u57df\u5bf9\u4e8e\u51c6\u786e\u6027\u548c\u53ef\u8003\u6027\u8981\u6c42\u6781\u9ad8\uff0cRAG\u6280\u672f\u80fd\u591f\u901a\u8fc7\u5f15\u7528\u6765\u6e90\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u4e3a\u6cd5\u5f8b\u9886\u57df\u8bbe\u8ba1\u9ad8\u6548\u7684RAG\u7cfb\u7edf\u3002", "method": "1\uff09\u4e0a\u4e0b\u6587\u611f\u77e5\u67e5\u8be2\u7ffb\u8bd1\u5668\uff0c\u5206\u79bb\u6587\u6863\u5f15\u7528\u548c\u81ea\u7136\u8bed\u8a00\u95ee\u9898\n2\uff09\u4f7f\u7528SBERT\u548cGTE\u5d4c\u5165\u7684\u5f00\u6e90\u68c0\u7d22\u7b56\u7565\n3\uff09\u7ed3\u5408RAGAS\u3001BERTScore-F1\u548cROUGE-Recall\u7684\u7efc\u5408\u8bc4\u4f30\u6846\u67b6", "result": "\u68c0\u7d22\u6027\u80fd\u663e\u8457\u63d0\u5347\uff1aRecall@K\u63d0\u9ad830-95%\uff0cPrecision@K\u63d0\u9ad8\u7ea62.5\u500d\uff08K>4\u65f6\uff09\uff1b\u5b9a\u5236\u6cd5\u5f8b\u63d0\u793a\u8bcd\u4ea7\u751f\u66f4\u51c6\u786e\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u7b54\u6848\uff1b\u5f00\u6e90\u7ba1\u7ebf\u5728\u68c0\u7d22\u8d28\u91cf\u65b9\u9762\u53ef\u4e0e\u4e13\u6709\u65b9\u6848\u76f8\u6bd4\u6216\u66f4\u4f18\u3002", "conclusion": "\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u7684\u7ec4\u4ef6\u7ea7\u8c03\u4f18\uff0c\u53ef\u4ee5\u5efa\u7acb\u9ad8\u6548\u3001\u53ef\u590d\u73b0\u4e14\u6210\u672c\u6548\u76ca\u7684\u6cd5\u5f8bRAG\u7cfb\u7edf\uff0c\u4e3a\u6cd5\u5f8b\u7814\u7a76\u63d0\u4f9b\u53ef\u9760\u7684\u8f85\u52a9\u3002"}}
{"id": "2508.13118", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.13118", "abs": "https://arxiv.org/abs/2508.13118", "authors": ["Zefang Liu", "Arman Anwar"], "title": "AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation", "comment": null, "summary": "Incident response (IR) requires fast, coordinated, and well-informed\ndecision-making to contain and mitigate cyber threats. While large language\nmodels (LLMs) have shown promise as autonomous agents in simulated IR settings,\ntheir reasoning is often limited by a lack of access to external knowledge. In\nthis work, we present AutoBnB-RAG, an extension of the AutoBnB framework that\nincorporates retrieval-augmented generation (RAG) into multi-agent incident\nresponse simulations. Built on the Backdoors & Breaches (B&B) tabletop game\nenvironment, AutoBnB-RAG enables agents to issue retrieval queries and\nincorporate external evidence during collaborative investigations. We introduce\ntwo retrieval settings: one grounded in curated technical documentation\n(RAG-Wiki), and another using narrative-style incident reports (RAG-News). We\nevaluate performance across eight team structures, including newly introduced\nargumentative configurations designed to promote critical reasoning. To\nvalidate practical utility, we also simulate real-world cyber incidents based\non public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct\ncomplex multi-stage attacks. Our results show that retrieval augmentation\nimproves decision quality and success rates across diverse organizational\nmodels. This work demonstrates the value of integrating retrieval mechanisms\ninto LLM-based multi-agent systems for cybersecurity decision-making.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86AutoBnB-RAG\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u6765\u6539\u5584\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e8b\u4ef6\u54cd\u5e94\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u5728\u591a\u79cd\u56e2\u961f\u7ed3\u6784\u4e0b\u90fd\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u51b3\u7b56\u8d28\u91cf\u3002", "motivation": "\u4e8b\u4ef6\u54cd\u5e94\u9700\u8981\u5feb\u901f\u3001\u534f\u540c\u7684\u51b3\u7b56\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6a21\u62df\u73af\u5883\u4e2d\u5f80\u5f80\u56e0\u7f3a\u4e4f\u5916\u90e8\u77e5\u8bc6\u800c\u9650\u5236\u4e86\u601d\u7ef4\u80fd\u529b\u3002", "method": "\u57fa\u4e8eAutoBnB\u6846\u67b6\u6784\u5efaAutoBnB-RAG\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u5305\u62ec\u4e24\u79cd\u68c0\u7d22\u8bbe\u7f6e\uff1a\u57fa\u4e8e\u7cbe\u9009\u6280\u672f\u6587\u6863\u7684RAG-Wiki\u548c\u4f7f\u7528\u53d9\u4e8b\u98ce\u683c\u4e8b\u4ef6\u62a5\u544a\u7684RAG-News\uff0c\u5e76\u57288\u79cd\u56e2\u961f\u7ed3\u6784\u4e0b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u68c0\u7d22\u589e\u5f3a\u63d0\u9ad8\u4e86\u51b3\u7b56\u8d28\u91cf\u548c\u6210\u529f\u7387\uff0c\u80fd\u591f\u91cd\u5efa\u590d\u6742\u7684\u591a\u9636\u6bb5\u653b\u51fb\uff0c\u5728\u591a\u79cd\u7ec4\u7ec7\u6a21\u578b\u4e2d\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u5584\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8bc1\u660e\u4e86\u5c06\u68c0\u7d22\u673a\u5236\u96c6\u6210\u5230\u57fa\u4e8eLLM\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u5bf9\u7f51\u7edc\u5b89\u5168\u51b3\u7b56\u7684\u4ef7\u503c\u3002"}}
{"id": "2508.13124", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13124", "abs": "https://arxiv.org/abs/2508.13124", "authors": ["Kawin Mayilvaghanan", "Siddhant Gupta", "Ayush Kumar"], "title": "Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries", "comment": null, "summary": "Abstractive summarization is a core application in contact centers, where\nLarge Language Models (LLMs) generate millions of summaries of call transcripts\ndaily. Despite their apparent quality, it remains unclear whether LLMs\nsystematically under- or over-attend to specific aspects of the transcript,\npotentially introducing biases in the generated summary. While prior work has\nexamined social and positional biases, the specific forms of bias pertinent to\ncontact center operations - which we term Operational Bias - have remained\nunexplored. To address this gap, we introduce BlindSpot, a framework built upon\na taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic)\nfor the identification and quantification of these biases. BlindSpot leverages\nan LLM as a zero-shot classifier to derive categorical distributions for each\nbias dimension in a pair of transcript and its summary. The bias is then\nquantified using two metrics: Fidelity Gap (the JS Divergence between\ndistributions) and Coverage (the percentage of source labels omitted). Using\nBlindSpot, we conducted an empirical study with 2500 real call transcripts and\ntheir summaries generated by 20 LLMs of varying scales and families (e.g., GPT,\nLlama, Claude). Our analysis reveals that biases are systemic and present\nacross all evaluated models, regardless of size or family.", "AI": {"tldr": "\u63d0\u51fa\u4e86BlindSpot\u6846\u67b6\u6765\u68c0\u6d4b\u548c\u91cf\u5316LLM\u5728\u5ba2\u670d\u901a\u8bdd\u6458\u8981\u4e2d\u7684\u64cd\u4f5c\u504f\u89c1\uff0c\u53d1\u73b0\u4e86\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u90fd\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1", "motivation": "LLM\u5728\u5ba2\u670d\u4e2d\u5fc3\u6bcf\u5929\u751f\u6210\u6570\u767e\u4e07\u901a\u8bdd\u6458\u8981\uff0c\u4f46\u5176\u662f\u5426\u5bf9\u7279\u5b9a\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\u5c1a\u672a\u7814\u7a76\uff0c\u7279\u522b\u662f\u64cd\u4f5c\u504f\u89c1\u7ef4\u5ea6", "method": "\u6784\u5efa\u5305\u542b15\u4e2a\u64cd\u4f5c\u504f\u89c1\u7ef4\u5ea6\u7684\u5206\u7c7b\u6cd5\uff0c\u4f7f\u7528LLM\u4f5c\u4e3a\u96f6\u6837\u672c\u5206\u7c7b\u5668\u8ba1\u7b97\u8f6c\u5f55\u548c\u6458\u8981\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u901a\u8fc7\u4fdd\u771f\u5ea6\u5dee\u8ddd\u548c\u8986\u76d6\u7387\u4e24\u4e2a\u6307\u6807\u91cf\u5316\u504f\u89c1", "result": "\u5bf92500\u4e2a\u771f\u5b9e\u901a\u8bdd\u548c20\u4e2a\u4e0d\u540c\u89c4\u6a21\u5bb6\u65cf\u7684LLM\u6458\u8981\u5206\u6790\u663e\u793a\uff0c\u504f\u89c1\u662f\u7cfb\u7edf\u6027\u7684\uff0c\u5b58\u5728\u4e8e\u6240\u6709\u8bc4\u4f30\u6a21\u578b\u4e2d", "conclusion": "\u64cd\u4f5c\u504f\u89c1\u662fLLM\u6458\u8981\u4e2d\u666e\u904d\u5b58\u5728\u7684\u7cfb\u7edf\u6027\u95ee\u9898\uff0c\u9700\u8981\u4e13\u95e8\u6846\u67b6\u6765\u68c0\u6d4b\u548c\u7f13\u89e3"}}
{"id": "2508.13130", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.13130", "abs": "https://arxiv.org/abs/2508.13130", "authors": ["Kareem Elozeiri", "Mervat Abassy", "Preslav Nakov", "Yuxia Wang"], "title": "MuDRiC: Multi-Dialect Reasoning for Arabic Commonsense Validation", "comment": null, "summary": "Commonsense validation evaluates whether a sentence aligns with everyday\nhuman understanding, a critical capability for developing robust natural\nlanguage understanding systems. While substantial progress has been made in\nEnglish, the task remains underexplored in Arabic, particularly given its rich\nlinguistic diversity. Existing Arabic resources have primarily focused on\nModern Standard Arabic (MSA), leaving regional dialects underrepresented\ndespite their prevalence in spoken contexts. To bridge this gap, we present two\nkey contributions: (i) we introduce MuDRiC, an extended Arabic commonsense\ndataset incorporating multiple dialects, and (ii) a novel method adapting Graph\nConvolutional Networks (GCNs) to Arabic commonsense reasoning, which enhances\nsemantic relationship modeling for improved commonsense validation. Our\nexperimental results demonstrate that this approach achieves superior\nperformance in Arabic commonsense validation. Our work enhances Arabic natural\nlanguage understanding by providing both a foundational dataset and a novel\nmethod for handling its complex variations. To the best of our knowledge, we\nrelease the first Arabic multi-dialect commonsense reasoning dataset.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86MuDRiC\u6570\u636e\u96c6\u548c\u57fa\u4e8eGCN\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u963f\u62c9\u4f2f\u8bed\u591a\u65b9\u8a00\u5e38\u8bc6\u9a8c\u8bc1\uff0c\u8865\u5145\u4e86\u73b0\u6709\u8d44\u6e90\u4ec5\u805a\u7126\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\u7684\u7a7a\u767d\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u5e38\u8bc6\u9a8c\u8bc1\u4efb\u52a1\u5728\u591a\u65b9\u8a00\u73af\u5883\u4e0b\u7814\u7a76\u4e0d\u8db3\uff0c\u73b0\u6709\u8d44\u6e90\u4e3b\u8981\u805a\u7126\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed(MSA)\uff0c\u800c\u5e7f\u6cdb\u4f7f\u7528\u7684\u5730\u533a\u65b9\u8a00\u5374\u7f3a\u4e4f\u8865\u5145\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u9879\u6838\u5fc3\u8d21\u732e\uff1a1) MuDRiC\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u79cd\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u7684\u5e38\u8bc6\u6570\u636e\uff1b2) \u57fa\u4e8e\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(GCN)\u7684\u65b0\u65b9\u6cd5\uff0c\u4f18\u5316\u8bed\u4e49\u5173\u7cfb\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u963f\u62c9\u4f2f\u8bed\u5e38\u8bc6\u9a8c\u8bc1\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u66f4\u4f18\u5f02\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u63d0\u4f9b\u57fa\u7840\u6570\u636e\u96c6\u548c\u65b0\u9898\u65b9\u6cd5\uff0c\u4e3a\u5904\u7406\u963f\u62c9\u4f2f\u8bed\u590d\u6742\u8bed\u8a00\u53d8\u4f53\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u662f\u9996\u4e2a\u963f\u62c9\u4f2f\u8bed\u591a\u65b9\u8a00\u5e38\u8bc6\u63a8\u7406\u6570\u636e\u96c6\u3002"}}
{"id": "2508.13131", "categories": ["cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.13131", "abs": "https://arxiv.org/abs/2508.13131", "authors": ["Dara Bahri", "John Wieting"], "title": "Improving Detection of Watermarked Language Models", "comment": null, "summary": "Watermarking has recently emerged as an effective strategy for detecting the\ngenerations of large language models (LLMs). The strength of a watermark\ntypically depends strongly on the entropy afforded by the language model and\nthe set of input prompts. However, entropy can be quite limited in practice,\nespecially for models that are post-trained, for example via instruction tuning\nor reinforcement learning from human feedback (RLHF), which makes detection\nbased on watermarking alone challenging. In this work, we investigate whether\ndetection can be improved by combining watermark detectors with non-watermark\nones. We explore a number of hybrid schemes that combine the two, observing\nperformance gains over either class of detector under a wide range of\nexperimental conditions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u7ed3\u5408\u6c34\u5370\u68c0\u6d4b\u5668\u548c\u975e\u6c34\u5370\u68c0\u6d4b\u5668\u6765\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\u6548\u679c", "motivation": "\u6c34\u5370\u6280\u672f\u7684\u68c0\u6d4b\u6548\u679c\u5f3a\u5ea6\u4f9d\u8d56\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u71b5\u4f5c\u7528\uff0c\u800c\u7ecf\u8fc7\u6307\u4ee4\u5faa\u73af\u8c03\u6574\u6216RLHF\u540e\u7684\u6a21\u578b\u71b5\u4f5c\u7528\u6709\u9650\uff0c\u4f7f\u5f97\u5355\u7eaf\u4f9d\u9760\u6c34\u5370\u7684\u68c0\u6d4b\u9769\u5177\u6311\u6218\u6027", "method": "\u63a2\u7d22\u591a\u79cd\u6df7\u5408\u65b9\u6848\uff0c\u5c06\u6c34\u5370\u68c0\u6d4b\u5668\u4e0e\u975e\u6c34\u5370\u68c0\u6d4b\u5668\u7ed3\u5408\u4f7f\u7528", "result": "\u5728\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u6761\u4ef6\u4e0b\uff0c\u6df7\u5408\u65b9\u6848\u7684\u6027\u80fd\u8d85\u8fc7\u4e86\u4efb\u4f55\u5355\u4e00\u7c7b\u578b\u68c0\u6d4b\u5668", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u6c34\u5370\u548c\u975e\u6c34\u5370\u68c0\u6d4b\u6280\u672f\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8LLM\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\u80fd\u529b"}}
{"id": "2508.13141", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13141", "abs": "https://arxiv.org/abs/2508.13141", "authors": ["Pranjal Aggarwal", "Seungone Kim", "Jack Lanchantin", "Sean Welleck", "Jason Weston", "Ilia Kulikov", "Swarnadeep Saha"], "title": "OptimalThinkingBench: Evaluating Over and Underthinking in LLMs", "comment": "26 pages, 6 tables, 10 figures", "summary": "Thinking LLMs solve complex tasks at the expense of increased compute and\noverthinking on simpler problems, while non-thinking LLMs are faster and\ncheaper but underthink on harder reasoning problems. This has led to the\ndevelopment of separate thinking and non-thinking LLM variants, leaving the\nonus of selecting the optimal model for each query on the end user. In this\nwork, we introduce OptimalThinkingBench, a unified benchmark that jointly\nevaluates overthinking and underthinking in LLMs and also encourages the\ndevelopment of optimally-thinking models that balance performance and\nefficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench,\nfeaturing simple queries in 72 domains, and UnderthinkingBench, containing 11\nchallenging reasoning tasks. Using novel thinking-adjusted accuracy metrics, we\nperform extensive evaluation of 33 different thinking and non-thinking models\nand show that no model is able to optimally think on our benchmark. Thinking\nmodels often overthink for hundreds of tokens on the simplest user queries\nwithout improving performance. In contrast, large non-thinking models\nunderthink, often falling short of much smaller thinking models. We further\nexplore several methods to encourage optimal thinking, but find that these\napproaches often improve on one sub-benchmark at the expense of the other,\nhighlighting the need for better unified and optimal models in the future.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6027\u80fd\u6d4b\u8bd5\u5e73\u53f0OptimalThinkingBench\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u7b80\u5355\u548c\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u601d\u8003\u6548\u7387\uff0c\u53d1\u73b0\u65e2\u6709\u6a21\u578b\u90fd\u65e0\u6cd5\u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u627e\u5230\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u8fc7\u5ea6\u601d\u8003\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\uff0c\u800c\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u53c8\u601d\u8003\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u6d4b\u6807\u51c6\u6765\u9f13\u52b1\u53d1\u5c55\u6700\u4f73\u601d\u8003\u6a21\u578b\u3002", "method": "\u6784\u5efa\u5305\u542bOverthinkingBench\uff08\u7b80\u5355\u67e5\u8be2\uff09\u548cUnderthinkingBench\uff08\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff09\u7684\u7edf\u4e00\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4f7f\u7528\u601d\u8003\u8c03\u6574\u51c6\u786e\u7387\u6307\u6807\uff0c\u5bf933\u4e2a\u4e0d\u540c\u6a21\u578b\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u3002", "result": "\u6ca1\u6709\u4efb\u4f55\u6a21\u578b\u80fd\u591f\u5728\u8be5\u6d4b\u8bd5\u4e0a\u5b9e\u73b0\u6700\u4f73\u601d\u8003\u3002\u601d\u8003\u6a21\u578b\u5728\u7b80\u5355\u67e5\u8be2\u4e0a\u6d6a\u8d39\u6570\u767e\u4e2atoken\u4f46\u6027\u80fd\u6ca1\u6709\u63d0\u5347\uff0c\u800c\u975e\u601d\u8003\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u5982\u66f4\u5c0f\u7684\u601d\u8003\u6a21\u578b\u3002", "conclusion": "\u5f53\u524d\u7684\u4f18\u5316\u65b9\u6cd5\u90fd\u662f\u5728\u4e24\u4e2a\u5b50\u6d4b\u8bd5\u4e4b\u95f4\u505a\u6298\u8877\uff0c\u5f3a\u8c03\u9700\u8981\u53d1\u5c55\u66f4\u597d\u7684\u7edf\u4e00\u6700\u4f73\u601d\u8003\u6a21\u578b\u6765\u5e73\u8861\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2508.13144", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13144", "abs": "https://arxiv.org/abs/2508.13144", "authors": ["David Heineman", "Valentin Hofmann", "Ian Magnusson", "Yuling Gu", "Noah A. Smith", "Hannaneh Hajishirzi", "Kyle Lo", "Jesse Dodge"], "title": "Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation", "comment": null, "summary": "Developing large language models is expensive and involves making decisions\nwith small experiments, typically by evaluating on large, multi-task evaluation\nsuites. In this work, we analyze specific properties which make a benchmark\nmore reliable for such decisions, and interventions to design higher-quality\nevaluation benchmarks. We introduce two key metrics that show differences in\ncurrent benchmarks: signal, a benchmark's ability to separate better models\nfrom worse models, and noise, a benchmark's sensitivity to random variability\nbetween training steps. We demonstrate that benchmarks with a better\nsignal-to-noise ratio are more reliable when making decisions at small scale,\nand those with less noise have lower scaling law prediction error. These\nresults suggest that improving signal or noise will lead to more useful\nbenchmarks, so we introduce three interventions designed to directly affect\nsignal or noise. For example, we propose that switching to a metric that has\nbetter signal and noise (e.g., perplexity rather than accuracy) leads to better\nreliability and improved scaling law error. We also find that filtering noisy\nsubtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable\nmulti-task evaluations. We also find that averaging the output of a model's\nintermediate checkpoints to reduce noise leads to consistent improvements. We\nconclude by recommending that those creating new benchmarks, or selecting which\nexisting benchmarks to use, aim for high signal and low noise. We use 30\nbenchmarks for these experiments, and 375 open-weight language models from 60M\nto 32B parameters, resulting in a new, publicly available dataset of 900K\nevaluation benchmark results, totaling 200M instances.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5206\u6790\u4e86\u8bed\u8a00\u6a21\u578b\u8bc4\u6d4b\u57fa\u51c6\u7684\u8d28\u91cf\u6807\u51c6\uff0c\u63d0\u51fa\u4e86\u4fe1\u5668\u548c\u566a\u58f0\u4e24\u4e2a\u5173\u952e\u6307\u6807\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u79cd\u63d0\u9ad8\u8bc4\u6d4b\u53ef\u9760\u6027\u7684\u5e72\u9884\u65b9\u6cd5", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u4f9d\u9760\u5c0f\u89c4\u6a21\u5b9e\u9a8c\u505a\u51b3\u7b56\uff0c\u800c\u73b0\u6709\u8bc4\u6d4b\u57fa\u51c6\u7684\u8d28\u91cf\u5f71\u54cd\u4e86\u8fd9\u4e9b\u51b3\u7b56\u7684\u53ef\u9760\u6027", "method": "\u5f15\u5165\u4fe1\u5668\uff08\u8bc4\u6d4b\u533a\u5206\u6a21\u578b\u80fd\u529b\u7684\u80fd\u529b\uff09\u548c\u566a\u58f0\uff08\u8bc4\u6d4b\u5bf9\u968f\u673a\u53d8\u5316\u7684\u654f\u611f\u5ea6\uff09\u4e24\u4e2a\u6307\u6807\uff0c\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u9a8c\uff0c\u5305\u62ec30\u4e2a\u57fa\u51c6\u548c375\u4e2a\u6a21\u578b\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u5e72\u9884\u65b9\u6cd5\uff1a\u6539\u7528\u66f4\u597d\u7684\u6307\u6807\uff08\u5982\u56f0\u60d1\u5ea6\u4ee3\u66ff\u51c6\u786e\u7387\uff09\u3001\u7b5b\u9009\u566a\u58f0\u5b50\u4efb\u52a1\u3001\u5e73\u5747\u4e2d\u95f4\u68c0\u67e5\u70b9\u8f93\u51fa", "result": "\u4fe1\u5668\u566a\u58f0\u6bd4\u66f4\u9ad8\u7684\u57fa\u51c6\u5728\u5c0f\u89c4\u6a21\u5b9e\u9a8c\u4e2d\u66f4\u53ef\u9760\uff0c\u566a\u58f0\u66f4\u5c0f\u7684\u57fa\u51c6\u6709\u66f4\u4f4e\u7684\u7f29\u653e\u5f8b\u9884\u6d4b\u9519\u8bef\uff0c\u63d0\u51fa\u7684\u4e09\u79cd\u5e72\u9884\u65b9\u6cd5\u90fd\u80fd\u63d0\u9ad8\u8bc4\u6d4b\u7684\u53ef\u9760\u6027", "conclusion": "\u5efa\u8bae\u5728\u521b\u5efa\u65b0\u57fa\u51c6\u6216\u9009\u62e9\u73b0\u6709\u57fa\u51c6\u65f6\uff0c\u5e94\u8be5\u8ffd\u6c42\u9ad8\u4fe1\u5668\u548c\u4f4e\u566a\u58f0\u7684\u8bc4\u6d4b\u65b9\u6848\uff0c\u4ee5\u63d0\u9ad8\u8bc4\u6d4b\u7ed3\u679c\u7684\u53ef\u9760\u6027\u548c\u51b3\u7b56\u8d28\u91cf"}}
{"id": "2508.13152", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13152", "abs": "https://arxiv.org/abs/2508.13152", "authors": ["Xin Chen", "Junchao Wu", "Shu Yang", "Runzhe Zhan", "Zeyu Wu", "Ziyang Luo", "Di Wang", "Min Yang", "Lidia S. Chao", "Derek F. Wong"], "title": "RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns", "comment": "Accepted to TACL 2025. This version is a pre-MIT Press publication\n  version", "summary": "Detecting content generated by large language models (LLMs) is crucial for\npreventing misuse and building trustworthy AI systems. Although existing\ndetection methods perform well, their robustness in out-of-distribution (OOD)\nscenarios is still lacking. In this paper, we hypothesize that, compared to\nfeatures used by existing detection methods, the internal representations of\nLLMs contain more comprehensive and raw features that can more effectively\ncapture and distinguish the statistical pattern differences between\nLLM-generated texts (LGT) and human-written texts (HWT). We validated this\nhypothesis across different LLMs and observed significant differences in neural\nactivation patterns when processing these two types of texts. Based on this, we\npropose RepreGuard, an efficient statistics-based detection method.\nSpecifically, we first employ a surrogate model to collect representation of\nLGT and HWT, and extract the distinct activation feature that can better\nidentify LGT. We can classify the text by calculating the projection score of\nthe text representations along this feature direction and comparing with a\nprecomputed threshold. Experimental results show that RepreGuard outperforms\nall baselines with average 94.92% AUROC on both in-distribution (ID) and OOD\nscenarios, while also demonstrating robust resilience to various text sizes and\nmainstream attacks. Data and code are publicly available at:\nhttps://github.com/NLP2CT/RepreGuard", "AI": {"tldr": "RepreGuard\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u5185\u90e8\u8868\u793a\u7edf\u8ba1\u7279\u5f81\u7684\u9ad8\u6548\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u5206\u5e03\u5185\u5916\u573a\u666f\u4e0b\u90fd\u80fd\u6709\u6548\u533a\u5206AI\u751f\u6210\u6587\u672c\u548c\u4eba\u7c7b\u64b0\u5199\u6587\u672c\uff0c\u5e73\u5747AUROC\u8fbe\u523094.92%", "motivation": "\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u4f5c\u8005\u5047\u8bbeLLM\u5185\u90e8\u8868\u793a\u5305\u542b\u66f4\u5168\u9762\u548c\u539f\u59cb\u7684\u7279\u5f81\uff0c\u80fd\u66f4\u6709\u6548\u6355\u6349AI\u751f\u6210\u6587\u672c\u548c\u4eba\u7c7b\u6587\u672c\u7684\u7edf\u8ba1\u6a21\u5f0f\u5dee\u5f02", "method": "\u4f7f\u7528\u4ee3\u7406\u6a21\u578b\u6536\u96c6\u4e24\u79cd\u6587\u672c\u7684\u8868\u793a\uff0c\u63d0\u53d6\u80fd\u66f4\u597d\u8bc6\u522bAI\u751f\u6210\u6587\u672c\u7684\u6fc0\u6d3b\u7279\u5f81\uff0c\u901a\u8fc7\u8ba1\u7b97\u6587\u672c\u8868\u793a\u5728\u8be5\u7279\u5f81\u65b9\u5411\u4e0a\u7684\u6295\u5f71\u5206\u6570\u5e76\u4e0e\u9884\u8bbe\u9608\u503c\u6bd4\u8f83\u8fdb\u884c\u5206\u7c7b", "result": "\u5728\u5206\u5e03\u5185\u5916\u573a\u666f\u4e0b\u5e73\u5747AUROC\u8fbe\u523094.92%\uff0c\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5bf9\u4e0d\u540c\u6587\u672c\u957f\u5ea6\u548c\u4e3b\u6d41\u653b\u51fb\u5177\u6709\u5f3a\u9c81\u68d2\u6027", "conclusion": "LLM\u5185\u90e8\u8868\u793a\u786e\u5b9e\u5305\u542b\u533a\u5206AI\u751f\u6210\u6587\u672c\u548c\u4eba\u7c7b\u6587\u672c\u7684\u6709\u6548\u7279\u5f81\uff0cRepreGuard\u65b9\u6cd5\u5728\u68c0\u6d4b\u6027\u80fd\u548c\u9c81\u68d2\u6027\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272"}}
