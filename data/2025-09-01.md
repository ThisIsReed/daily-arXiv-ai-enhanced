<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [CoBA: Counterbias Text Augmentation for Mitigating Various Spurious Correlations via Semantic Triples](https://arxiv.org/abs/2508.21083)
*Kyohoon Jin,Juhwan Choi,Jungmin Yun,Junho Lee,Soojin Jang,Youngbin Kim*

Main category: cs.CL

TL;DR: 提出了CoBA框架，通过语义三元组分解和选择性修改来生成反偏见数据，有效缓解深度学习模型对伪相关的依赖，提升下游任务性能和分布外鲁棒性


<details>
  <summary>Details</summary>
Motivation: 深度学习模型容易学习训练数据中的伪相关，利用非目标特征进行预测，导致性能下降和泛化能力差

Method: 引入反偏见数据增强方法CoBA，在语义三元组层面操作：分解文本为主谓宾三元组，选择性修改这些三元组来破坏伪相关，然后重构文本生成反偏见数据

Result: 实验表明CoBA不仅提升了下游任务性能，还能有效减少偏见并增强分布外鲁棒性

Conclusion: CoBA提供了一个通用且鲁棒的解决方案，能够同时处理多种偏见并应对伪相关带来的挑战

Abstract: Deep learning models often learn and exploit spurious correlations in
training data, using these non-target features to inform their predictions.
Such reliance leads to performance degradation and poor generalization on
unseen data. To address these limitations, we introduce a more general form of
counterfactual data augmentation, termed counterbias data augmentation, which
simultaneously tackles multiple biases (e.g., gender bias, simplicity bias) and
enhances out-of-distribution robustness. We present CoBA: CounterBias
Augmentation, a unified framework that operates at the semantic triple level:
first decomposing text into subject-predicate-object triples, then selectively
modifying these triples to disrupt spurious correlations. By reconstructing the
text from these adjusted triples, CoBA generates counterbias data that
mitigates spurious patterns. Through extensive experiments, we demonstrate that
CoBA not only improves downstream task performance, but also effectively
reduces biases and strengthens out-of-distribution resilience, offering a
versatile and robust solution to the challenges posed by spurious correlations.

</details>


### [2] [Mapping Toxic Comments Across Demographics: A Dataset from German Public Broadcasting](https://arxiv.org/abs/2508.21084)
*Jan Fillies,Michael Peter Hoffmann,Rebecca Reichel,Roman Salzwedel,Sven Bodemer,Adrian Paschke*

Main category: cs.CL

TL;DR: 首个大规模德语数据集，包含毒性和年龄信息，揭示了不同年龄段用户的网络语言模式差异


<details>
  <summary>Details</summary>
Motivation: 现有毒性语音数据集缺乏人口统计背景，限制了我们对不同年龄段在线交流方式的理解

Method: 与德国公共服务内容网络funk合作，收集Instagram、TikTok和YouTube的评论，使用预定义毒性关键词筛选，结合人工标注和LLM标注（3,024条人工标注+30,024条LLM标注），分析毒性类别和年龄模式

Result: 16.7%的评论被标记为有问题，发现年龄相关的毒性语言模式：年轻用户偏好表达性语言，年长用户更多参与虚假信息和贬低行为

Conclusion: 该数据集为研究跨人口统计的语言变异提供了新机会，支持开发更公平和年龄感知的内容审核系统

Abstract: A lack of demographic context in existing toxic speech datasets limits our
understanding of how different age groups communicate online. In collaboration
with funk, a German public service content network, this research introduces
the first large-scale German dataset annotated for toxicity and enriched with
platform-provided age estimates. The dataset includes 3,024 human-annotated and
30,024 LLM-annotated anonymized comments from Instagram, TikTok, and YouTube.
To ensure relevance, comments were consolidated using predefined toxic
keywords, resulting in 16.7\% labeled as problematic. The annotation pipeline
combined human expertise with state-of-the-art language models, identifying key
categories such as insults, disinformation, and criticism of broadcasting fees.
The dataset reveals age-based differences in toxic speech patterns, with
younger users favoring expressive language and older users more often engaging
in disinformation and devaluation. This resource provides new opportunities for
studying linguistic variation across demographics and supports the development
of more equitable and age-aware content moderation systems.

</details>


### [3] [Granite Embedding R2 Models](https://arxiv.org/abs/2508.21085)
*Parul Awasthy,Aashka Trivedi,Yulong Li,Meet Doshi,Riyaz Bhat,Vignesh P,Vishwajeet Kumar,Yushu Yang,Bhavani Iyer,Abraham Daniels,Rudra Murthy,Ken Barker,Martin Franz,Madison Lee,Todd Ward,Salim Roukos,David Cox,Luis Lastras,Jaydeep Sen,Radu Florian*

Main category: cs.CL

TL;DR: Granite Embedding R2模型是IBM推出的第二代高性能英文编码器嵌入模型，专为企业级密集检索应用设计，在上下文长度、检索性能和速度方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 企业级应用中检索速度和准确性对于竞争优势至关重要，需要开发既能保持高精度又能提供快速检索的开源嵌入模型。

Method: 采用双编码器和交叉编码器架构，包括22层检索模型和高效的12层对应版本，以及高质量的重排序模型，全部使用企业级数据进行训练。

Result: 模型实现了16倍上下文长度扩展（8,192 tokens），在文本、代码、长文档搜索、多轮对话和表格数据等多个检索领域达到最先进性能，速度比主要竞争对手快19-44%。

Conclusion: Granite R2模型为企业关键部署提供了前沿性能、企业级许可和透明数据来源的完美组合，所有模型都在Apache 2.0许可下公开可用。

Abstract: We introduce the Granite Embedding R2 models, a comprehensive family of
high-performance English encoder-based embedding models engineered for
enterprise-scale dense retrieval applications. Building upon our
first-generation release, these models deliver substantial improvements,
including 16x expanded context length (8,192 tokens), state-of-the-art
performance across diverse retrieval domains - text, code, long-document
search, multi-turn conversational, and tabular data - and measurable speed
advantages of 19-44\% over leading competitors while maintaining superior
accuracy. Our release encompasses both bi-encoder and cross-encoder
architectures, featuring a highly effective 22-layer retriever model and its
efficient 12-layer counterpart, alongside a high-quality reranker model, all
trained exclusively on enterprise-appropriate data with comprehensive
governance oversight. The models demonstrate exceptional versatility across
standard benchmarks, IBM-developed evaluation suites, and real-world enterprise
use cases, establishing new performance standards for open-source embedding
models. In an era where retrieval speed and accuracy are paramount for
competitive advantage, the Granite R2 models deliver a compelling combination
of cutting-edge performance, enterprise-ready licensing, and transparent data
provenance that organizations require for mission-critical deployments. All
models are publicly available under the Apache 2.0 license at
https://huggingface.co/collections/ibm-granite, enabling unrestricted research
and commercial use.

</details>


### [4] [TrInk: Ink Generation with Transformer Network](https://arxiv.org/abs/2508.21098)
*Zezhong Jin,Shubhang Desai,Xu Chen,Biyi Fang,Zhuoyi Huang,Zhe Li,Chong-Xin Gan,Xiao Tu,Man-Wai Mak,Yan Lu,Shujie Liu*

Main category: cs.CL

TL;DR: TrInk是一个基于Transformer的墨水生成模型，通过引入缩放位置嵌入和高斯记忆掩码来改善文本与笔画的对齐，在IAM-OnDB数据集上相比之前方法显著降低了字符错误率和词错误率。


<details>
  <summary>Details</summary>
Motivation: 为了解决手写生成中全局依赖关系捕捉和文本-笔画对齐的问题，需要开发能够更好处理这些挑战的模型。

Method: 提出基于Transformer的TrInk模型，在交叉注意力模块中引入缩放位置嵌入和高斯记忆掩码，并设计了主客观评估流程来全面评估生成手写的可读性和风格一致性。

Result: 在IAM-OnDB数据集上，相比之前方法实现了35.56%的字符错误率降低和29.66%的词错误率降低。

Conclusion: TrInk模型通过创新的注意力机制改进，在手写生成任务中取得了显著性能提升，证明了Transformer架构在该领域的有效性。

Abstract: In this paper, we propose TrInk, a Transformer-based model for ink
generation, which effectively captures global dependencies. To better
facilitate the alignment between the input text and generated stroke points, we
introduce scaled positional embeddings and a Gaussian memory mask in the
cross-attention module. Additionally, we design both subjective and objective
evaluation pipelines to comprehensively assess the legibility and style
consistency of the generated handwriting. Experiments demonstrate that our
Transformer-based model achieves a 35.56\% reduction in character error rate
(CER) and an 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset
compared to previous methods. We provide an demo page with handwriting samples
from TrInk and baseline models at: https://akahello-a11y.github.io/trink-demo/

</details>


### [5] [How Does Cognitive Bias Affect Large Language Models? A Case Study on the Anchoring Effect in Price Negotiation Simulations](https://arxiv.org/abs/2508.21137)
*Yoshiki Takenami,Yin Jou Huang,Yugo Murawaki,Chenhui Chu*

Main category: cs.CL

TL;DR: 研究发现LLM在价格谈判中存在锚定效应，推理模型能减轻这种效应，但人格特质与锚定效应敏感性无显著相关


<details>
  <summary>Details</summary>
Motivation: 研究LLM在现实应用中的认知偏见，特别是锚定效应对价格谈判可靠性的影响

Method: 使用卖家LLM代理应用锚定效应，通过客观和主观指标评估谈判效果，分析推理和人格因素与锚定效应的关系

Result: LLM像人类一样受锚定效应影响，推理模型较不易受影响（长思维链减轻效应），但人格特质与锚定效应敏感性无显著相关性

Conclusion: 这些发现有助于深入理解LLM的认知偏见，推动LLM在社会中安全负责任的应用

Abstract: Cognitive biases, well-studied in humans, can also be observed in LLMs,
affecting their reliability in real-world applications. This paper investigates
the anchoring effect in LLM-driven price negotiations. To this end, we
instructed seller LLM agents to apply the anchoring effect and evaluated
negotiations using not only an objective metric but also a subjective metric.
Experimental results show that LLMs are influenced by the anchoring effect like
humans. Additionally, we investigated the relationship between the anchoring
effect and factors such as reasoning and personality. It was shown that
reasoning models are less prone to the anchoring effect, suggesting that the
long chain of thought mitigates the effect. However, we found no significant
correlation between personality traits and susceptibility to the anchoring
effect. These findings contribute to a deeper understanding of cognitive biases
in LLMs and to the realization of safe and responsible application of LLMs in
society.

</details>


### [6] [Can Multimodal LLMs Solve the Basic Perception Problems of Percept-V?](https://arxiv.org/abs/2508.21143)
*Samrajnee Ghosh,Naman Agarwal,Hemanshu Garg,Chinmay Mittal,Mausam,Parag Singla*

Main category: cs.CL

TL;DR: Percept-V数据集测试发现，尽管多模态大语言模型在复杂任务中表现出色，但在基础视觉感知任务中性能随问题复杂度增加而显著下降。


<details>
  <summary>Details</summary>
Motivation: 评估多模态大语言模型在基本视觉感知任务上的表现，现有研究主要关注复杂任务而忽略了基础感知能力的测试。

Method: 创建包含7200张程序生成图像的Percept-V数据集，分为30个类别测试不同视觉感知技能，并在GPT-4o、Gemini、Claude等先进模型上进行测试。

Result: 实验显示所有测试模型在基础感知任务中性能随复杂度增加而显著下降，不同类别表现出相似的趋势，某些认知技能比其他技能更难。

Conclusion: 多模态大语言模型在基础视觉感知方面存在明显缺陷，需要进一步改进底层感知能力。

Abstract: The reasoning abilities of Multimodal Large Language Models (MLLMs) have
garnered a lot of attention in recent times, with advances made in frontiers
like coding, mathematics, and science. However, very limited experiments have
been done to assess their performance in simple perception tasks performed over
uncontaminated, generated images containing basic shapes and structures. To
address this issue, the paper introduces a dataset, Percept-V, containing a
total of 7200 program-generated images equally divided into 30 categories, each
testing a combination of visual perception skills. Unlike previously proposed
datasets, Percept-V comprises very basic tasks of varying complexity that test
the perception abilities of MLLMs. This dataset is then tested on
state-of-the-art MLLMs like GPT-4o, Gemini, and Claude as well as Large
Reasoning Models (LRMs) like OpenAI o4-mini and DeepSeek R1 to gauge their
performance. Contrary to the evidence that MLLMs excel in many complex tasks,
our experiments show a significant drop in the models' performance with
increasing problem complexity across all categories. An analysis of the
performances also reveals that the tested MLLMs exhibit a similar trend in
accuracy across categories, testing a particular cognitive skill and find some
skills to be more difficult than others.

</details>


### [7] [A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers](https://arxiv.org/abs/2508.21148)
*Ming Hu,Chenglong Ma,Wei Li,Wanghan Xu,Jiamin Wu,Jucheng Hu,Tianbin Li,Guohang Zhuang,Jiaqi Liu,Yingzhou Lu,Ying Chen,Chaoyang Zhang,Cheng Tan,Jie Ying,Guocheng Wu,Shujian Gao,Pengcheng Chen,Jiashi Lin,Haitao Wu,Lulu Chen,Fengxiang Wang,Yuanyuan Zhang,Xiangyu Zhao,Feilong Tang,Encheng Su,Junzhi Ning,Xinyao Liu,Ye Du,Changkai Ji,Cheng Tang,Huihui Xu,Ziyang Chen,Ziyan Huang,Jiyao Liu,Pengfei Jiang,Yizhou Wang,Chen Tang,Jianyu Wu,Yuchen Ren,Siyuan Yan,Zhonghua Wang,Zhongxing Xu,Shiyan Su,Shangquan Sun,Runkai Zhao,Zhisheng Zhang,Yu Liu,Fudi Wang,Yuanfeng Ji,Yanzhou Su,Hongming Shan,Chunmei Feng,Jiahao Xu,Jiangtao Yan,Wenhao Tang,Diping Song,Lihao Liu,Yanyan Huang,Lequan Yu,Bin Fu,Shujun Wang,Xiaomeng Li,Xiaowei Hu,Yun Gu,Ben Fei,Zhongying Deng,Benyou Wang,Yuewen Cao,Minjie Shen,Haodong Duan,Jie Xu,Yirong Chen,Fang Yan,Hongxia Hao,Jielan Li,Jiajun Du,Yanbo Wang,Imran Razzak,Chi Zhang,Lijun Wu,Conghui He,Zhaohui Lu,Jinhai Huang,Yihao Liu,Fenghua Ling,Yuqiang Li,Aoran Wang,Qihao Zheng,Nanqing Dong,Tianfan Fu,Dongzhan Zhou,Yan Lu,Wenlong Zhang,Jin Ye,Jianfei Cai,Wanli Ouyang,Yu Qiao,Zongyuan Ge,Shixiang Tang,Junjun He,Chunfeng Song,Lei Bai,Bowen Zhou*

Main category: cs.CL

TL;DR: 科学大语言模型(Sci-LLMs)进展的数据中心综述，将模型发展重新框架为模型与数据基础的共同进化，提出统一科学数据分类法和知识层次模型，分析了超过270个数据集和190个评测标准，并提出向闭环自主科学发现系统的范式转移。


<details>
  <summary>Details</summary>
Motivation: 科学大语言模型在科学研究中正在改变知识表达、集成和应用方式，但其进展受到科学数据复杂性的形式。需要一个统一的数据中心框架来理解和推动Sci-LLMs的发展。

Method: 构建统一的科学数据分类法和知识层次模型，系统性评估近期Sci-LLMs（从通用基础模型到专业化模型），分析超过270个训练数据集和190个评测标准。

Result: 识别了Sci-LLMs的特有需求：异构、多尺度、匀匀不确定性的语料库，需要保持领域不变性和支持跨模态推理的表征。评估方式正从静态考试向过程和发现导向的评估转变。

Conclusion: 需要向闭环自主科学发现系统的范式转移，构建可信赖、持续进化的AI系统，作为加速科学发现的真正合作伙伴。

Abstract: Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is
represented, integrated, and applied in scientific research, yet their progress
is shaped by the complex nature of scientific data. This survey presents a
comprehensive, data-centric synthesis that reframes the development of Sci-LLMs
as a co-evolution between models and their underlying data substrate. We
formulate a unified taxonomy of scientific data and a hierarchical model of
scientific knowledge, emphasizing the multimodal, cross-scale, and
domain-specific challenges that differentiate scientific corpora from general
natural language processing datasets. We systematically review recent Sci-LLMs,
from general-purpose foundations to specialized models across diverse
scientific disciplines, alongside an extensive analysis of over 270
pre-/post-training datasets, showing why Sci-LLMs pose distinct demands --
heterogeneous, multi-scale, uncertainty-laden corpora that require
representations preserving domain invariance and enabling cross-modal
reasoning. On evaluation, we examine over 190 benchmark datasets and trace a
shift from static exams toward process- and discovery-oriented assessments with
advanced evaluation protocols. These data-centric analyses highlight persistent
issues in scientific data development and discuss emerging solutions involving
semi-automated annotation pipelines and expert validation. Finally, we outline
a paradigm shift toward closed-loop systems where autonomous agents based on
Sci-LLMs actively experiment, validate, and contribute to a living, evolving
knowledge base. Collectively, this work provides a roadmap for building
trustworthy, continually evolving artificial intelligence (AI) systems that
function as a true partner in accelerating scientific discovery.

</details>


### [8] [Quantifying Label-Induced Bias in Large Language Model Self- and Cross-Evaluations](https://arxiv.org/abs/2508.21164)
*Muskan Saraf,Sajjad Rezvani Boroujeni,Justin Beaudry,Hossein Abedi,Tom Bush*

Main category: cs.CL

TL;DR: 研究发现LLM评估存在显著标签偏见：Claude标签总能提升评分，Gemini标签总会降低评分，错误标签可导致排名反转和50%的偏好投票变化


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在评估输出时可能存在的偏见问题，特别是自我评估和跨模型评估中的标签影响

Method: 使用ChatGPT、Gemini和Claude三种模型，在无标签、真实标签和两种错误标签条件下，通过整体偏好投票和三个质量维度（连贯性、信息性、简洁性）的百分比评分进行相互评估

Result: 发现明显的非对称偏见：Claude标签始终提高分数，Gemini标签始终降低分数；错误标签经常导致排名反转，偏好投票变化达50个百分点，质量评分变化达12个百分点

Conclusion: 模型身份认知会严重扭曲高级判断并微妙影响详细质量评分，强调需要盲评或多模型评估协议来确保LLM基准测试的公平性

Abstract: Large language models (LLMs) are increasingly used to evaluate outputs, yet
their judgments may be influenced. This study examines bias in self- and
cross-model evaluations by ChatGPT, Gemini, and Claude under four conditions:
no labels, true labels, and two false-label scenarios. Blog posts authored by
each model were evaluated by all three using both overall preference voting and
quality ratings for Coherence, Informativeness, and Conciseness, with all
scores expressed as percentages for direct comparison. Results reveal striking
asymmetries: the "Claude" label consistently boosts scores, while the "Gemini"
label consistently depresses them, regardless of actual content. False labels
frequently reversed rankings, producing shifts of up to 50 percentage points in
preference votes and up to 12 percentage points in converted quality ratings.
Gemini's self-scores collapsed under true labels, while Claude's
self-preference intensified. These findings show that perceived model identity
can heavily distort high-level judgments and subtly influence detailed quality
ratings, underscoring the need for blind or multimodel evaluation protocols to
ensure fairness in LLM benchmarking.

</details>


### [9] [BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design](https://arxiv.org/abs/2508.21184)
*Deepro Choudhury,Sinead Williamson,Adam Goliński,Ning Miao,Freddie Bickford Smith,Michael Kirchhof,Yizhe Zhang,Tom Rainforth*

Main category: cs.CL

TL;DR: BED-LLM是一个基于贝叶斯实验设计框架的方法，通过最大化期望信息增益来提升大语言模型在对话中智能收集信息的能力


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型作为多轮对话代理的能力，使其能够智能地、自适应地从用户或其他外部来源收集信息

Method: 基于序列贝叶斯实验设计框架，迭代选择最大化期望信息增益的问题或查询，使用基于LLM信念分布的概率模型，并设计了专门的EIG估计器和候选查询策略

Result: 在20个问题游戏和用户偏好推断测试中，相比直接提示和其他自适应设计策略，BED-LLM取得了显著的性能提升

Conclusion: BED-LLM为LLM提供了一个原则性的框架，使其能够作为有效的多轮对话代理与外部环境交互，在信息收集任务中表现优异

Abstract: We propose a general-purpose approach for improving the ability of Large
Language Models (LLMs) to intelligently and adaptively gather information from
a user or other external source using the framework of sequential Bayesian
experimental design (BED). This enables LLMs to act as effective multi-turn
conversational agents and interactively interface with external environments.
Our approach, which we call BED-LLM (Bayesian Experimental Design with Large
Language Models), is based on iteratively choosing questions or queries that
maximize the expected information gain (EIG) about the task of interest given
the responses gathered previously. We show how this EIG can be formulated in a
principled way using a probabilistic model derived from the LLM's belief
distribution and provide detailed insights into key decisions in its
construction. Further key to the success of BED-LLM are a number of specific
innovations, such as a carefully designed estimator for the EIG, not solely
relying on in-context updates for conditioning on previous responses, and a
targeted strategy for proposing candidate queries. We find that BED-LLM
achieves substantial gains in performance across a wide range of tests based on
the 20-questions game and using the LLM to actively infer user preferences,
compared to direct prompting of the LLM and other adaptive design strategies.

</details>


### [10] [Improving Aviation Safety Analysis: Automated HFACS Classification Using Reinforcement Learning with Group Relative Policy Optimization](https://arxiv.org/abs/2508.21201)
*Arash Ahmadi,Sarah Sharif,Yaser Banad*

Main category: cs.CL

TL;DR: 使用强化学习和GRPO算法自动化HFACS分析的方法，小型专业化模型在航空安全分析中超越大型模型，准确度提升350%


<details>
  <summary>Details</summary>
Motivation: 传统HFACS分析方法存在扩展性和一致性问题，需要自动化解决方案来提高航空事故人因分析的效率和准确性

Method: 使用强化学习组相对策略优化(GRPO)微调Llama-3.1 8B模型，统合多组件奖励系统和合成数据生成技术，解决数据集类别不平衡问题

Result: GRPO优化模型准确度显著提升，精确匹配准确度从0.0400提升到0.1800(增长350%)，部分匹配准确度达到0.8800，超越GPT-5-mini和Gemini-2.5-flash等领先模型

Conclusion: 小型领域优化模型能够提供计算效率更高、性能更好的安全分析方案，适合在资源受限边缘设处上部署

Abstract: Analyzing the human factors behind aviation accidents is crucial for
preventing future incidents, yet traditional methods using the Human Factors
Analysis and Classification System (HFACS) are limited by scalability and
consistency. To address this, we introduce an automated HFACS classification
framework for aviation safety analysis that utilizes Reinforcement Learning
with Group Relative Policy Optimization (GRPO) to fine-tune a Llama-3.1 8B
language model. Our approach incorporates a multi-component reward system
tailored for aviation safety analysis and integrates synthetic data generation
to overcome class imbalance in accident datasets. The resulting GRPO-optimized
model achieved noticeable performance gains, including a 350% increase in exact
match accuracy (from 0.0400 to 0.1800) and an improved partial match accuracy
of 0.8800. Significantly, our specialized model outperforms state-of-the-art
LLMs (Large Language Models), including GPT-5-mini and Gemini-2.5-fiash, on key
metrics. This research also proposes exact match accuracy in multi-label HFACS
classification problem as a new benchmarking methodology to evaluate the
advanced reasoning capabilities of language models. Ultimately, our work
validates that smaller, domain-optimized models can provide a computationally
efficient and better solution for critical safety analysis. This approach makes
powerful, low-latency deployment on resource-constrained edge devices feasible.

</details>


### [11] [Enhancing Robustness of Autoregressive Language Models against Orthographic Attacks via Pixel-based Approach](https://arxiv.org/abs/2508.21206)
*Han Yang,Jian Lan,Yihong Liu,Hinrich Schütze,Thomas Seidl*

Main category: cs.CL

TL;DR: 提出基于像素的生成语言模型，通过将单词渲染为图像来解决自回归语言模型对拼写攻击的脆弱性问题


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型容易受到多语言字母表字符扰动导致的拼写攻击，性能严重下降，主要源于子词分词器和嵌入的词汇表外问题

Method: 用基于像素的表示替代基于文本的嵌入，将单词渲染为单个图像，提供对噪声输入的更强鲁棒性，并扩展到多语言文本的兼容性

Result: 在多语言LAMBADA数据集、WMT24数据集和SST-2基准测试中评估，证明了其对拼写噪声的弹性和在多语言环境中的有效性

Conclusion: 基于像素的生成语言模型方法有效解决了文本模型的拼写攻击脆弱性问题，同时提供了多语言兼容性

Abstract: Autoregressive language models are vulnerable to orthographic attacks, where
input text is perturbed with characters from multilingual alphabets, leading to
substantial performance degradation. This vulnerability primarily stems from
the out-of-vocabulary issue inherent in subword tokenizers and their
embeddings. To address this limitation, we propose a pixel-based generative
language model that replaces the text-based embeddings with pixel-based
representations by rendering words as individual images. This design provides
stronger robustness to noisy inputs, while an extension of compatibility to
multilingual text across diverse writing systems. We evaluate the proposed
method on the multilingual LAMBADA dataset, WMT24 dataset and the SST-2
benchmark, demonstrating both its resilience to orthographic noise and its
effectiveness in multilingual settings.

</details>


### [12] [Do Self-Supervised Speech Models Exhibit the Critical Period Effects in Language Acquisition?](https://arxiv.org/abs/2508.21210)
*Yurie Koga,Shunsuke Kando,Yusuke Miyao*

Main category: cs.CL

TL;DR: 本研究探讨自监督语音模型是否表现出人类语言习得中的关键期效应，发现这些模型在语音习得方面并未显示明确的关键期证据。


<details>
  <summary>Details</summary>
Motivation: 关键期效应指第二语言习得难度随接触时间延迟而增加，第一语言保持能力随接触时间延长而增强。先前研究主要基于文本语言模型，而语音在人类语言习得中的核心作用使得研究语音模型中的这些效应具有重要意义。

Method: 在儿童导向语音数据上训练自监督语音模型，控制第二语言训练开始时间和第一语言训练结束时间的变化，评估模型的音位辨别性能。

Result: 自监督语音模型未显示明确的语音习得关键期效应。延迟第二语言接触开始的模型在第二语言上表现更好，延迟第一语言接触结束导致第一语言遗忘。

Conclusion: 与人类语言习得不同，自监督语音模型未表现出典型的关键期效应，表明当前模型架构与人类语言学习机制存在差异。

Abstract: This paper investigates whether the Critical Period (CP) effects in human
language acquisition are observed in self-supervised speech models (S3Ms). CP
effects refer to greater difficulty in acquiring a second language (L2) with
delayed L2 exposure onset, and greater retention of their first language (L1)
with delayed L1 exposure offset. While previous work has studied these effects
using textual language models, their presence in speech models remains
underexplored despite the central role of spoken language in human language
acquisition. We train S3Ms with varying L2 training onsets and L1 training
offsets on child-directed speech and evaluate their phone discrimination
performance. We find that S3Ms do not exhibit clear evidence of either CP
effects in terms of phonological acquisition. Notably, models with delayed L2
exposure onset tend to perform better on L2 and delayed L1 exposure offset
leads to L1 forgetting.

</details>


### [13] [Decoding Memories: An Efficient Pipeline for Self-Consistency Hallucination Detection](https://arxiv.org/abs/2508.21228)
*Weizhi Gao,Xiaorui Liu,Feiyi Wang,Dan Lu,Junqi Yin*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的解码内存管道（DMP）方法，通过选择性推理和退火解码来加速自一致性方法中的多响应生成，实现了最高3倍的加速效果且不损失性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在幻觉问题，现有的自一致性方法虽然能缓解这一问题，但需要重复生成导致计算成本高昂。研究发现生成过程中存在冗余的前缀标记，非精确答案标记对语义内容贡献很小。

Method: 提出解码内存管道（DMP），通过识别自一致性方法中的冗余（表现为跨生成共享的前缀标记），采用选择性推理和退火解码策略来加速生成过程。

Result: 大量实验表明，该方法在不牺牲AUROC性能的情况下，实现了最高3倍的生成速度提升。该方法与模型、数据集、解码策略和自一致性基线正交。

Conclusion: DMP方法能显著提高多响应生成的效率，并有潜力扩展到对齐和推理任务中，为解决自一致性方法的高计算成本问题提供了有效解决方案。

Abstract: Large language models (LLMs) have demonstrated impressive performance in both
research and real-world applications, but they still struggle with
hallucination. Existing hallucination detection methods often perform poorly on
sentence-level generation or rely heavily on domain-specific knowledge. While
self-consistency approaches help address these limitations, they incur high
computational costs due to repeated generation. In this paper, we conduct the
first study on identifying redundancy in self-consistency methods, manifested
as shared prefix tokens across generations, and observe that non-exact-answer
tokens contribute minimally to the semantic content. Based on these insights,
we propose a novel Decoding Memory Pipeline (DMP) that accelerates generation
through selective inference and annealed decoding. Being orthogonal to the
model, dataset, decoding strategy, and self-consistency baseline, our DMP
consistently improves the efficiency of multi-response generation and holds
promise for extension to alignment and reasoning tasks. Extensive experiments
show that our method achieves up to a 3x speedup without sacrificing AUROC
performance.

</details>


### [14] [Efficient Code Embeddings from Code Generation Models](https://arxiv.org/abs/2508.21290)
*Daria Kryvosheieva,Saba Sturua,Michael Günther,Scott Martens,Han Xiao*

Main category: cs.CL

TL;DR: jina-code-embeddings是一个创新的代码嵌入模型套件，使用自回归主干网络在文本和代码上预训练，通过最后标记池化生成嵌入，在小模型规模下实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 设计一个能够从自然语言查询检索代码、进行技术问答以及跨编程语言识别语义相似代码片段的代码嵌入模型

Method: 使用在文本和代码上预训练的自回归主干网络，通过最后标记池化(last-token pooling)生成嵌入向量

Result: 尽管模型规模相对较小，但实现了最先进的性能表现

Conclusion: 验证了这种代码嵌入模型构建方法的有效性，为代码检索和语义相似性识别提供了高效解决方案

Abstract: jina-code-embeddings is a novel code embedding model suite designed to
retrieve code from natural language queries, perform technical
question-answering, and identify semantically similar code snippets across
programming languages. It makes innovative use of an autoregressive backbone
pre-trained on both text and code, generating embeddings via last-token
pooling. We outline the training recipe and demonstrate state-of-the-art
performance despite the relatively small size of the models, validating this
approach to code embedding model construction.

</details>


### [15] [BLUEX Revisited: Enhancing Benchmark Coverage with Automatic Captioning](https://arxiv.org/abs/2508.21294)
*João Guilherme Alves Santos,Giovana Kerche Bonás,Thales Sales Almeida*

Main category: cs.CL

TL;DR: BLUEX数据集更新版本，包含2024-2025考试数据和AI生成的图像标注，增强了LLM预训练数据污染研究的相关性，标注策略使纯文本模型可访问性提高40%以上


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，特别是在多语言和非英语环境下，需要更强大的评估方法

Method: 更新BLUEX数据集，加入新考试数据和最先进模型自动生成的图像标注，评估商业和开源LLM利用视觉上下文的能力

Result: 生成1,422个可用问题，比原BLUEX数据集增加一倍多，标注策略使纯文本模型可访问性提高40%以上

Conclusion: 更新后的BLUEX数据集为LLM在多语言环境下的评估提供了更强大的工具，特别是在处理视觉内容方面

Abstract: With the growing capabilities of Large Language Models (LLMs), there is an
increasing need for robust evaluation methods, especially in multilingual and
non-English contexts. We present an updated version of the BLUEX dataset, now
including 2024-2025 exams and automatically generated image captions using
state-of-the-art models, enhancing its relevance for data contamination studies
in LLM pretraining. Captioning strategies increase accessibility to text-only
models by more than 40%, producing 1,422 usable questions, more than doubling
the number in the original BLUEX. We evaluated commercial and open-source LLMs
and their ability to leverage visual context through captions.

</details>


### [16] [Challenges and Applications of Large Language Models: A Comparison of GPT and DeepSeek family of models](https://arxiv.org/abs/2508.21377)
*Shubham Sharma,Sneha Tuli,Narendra Badam*

Main category: cs.CL

TL;DR: 这篇调查性论文分析了大语言模型的16个关键挑战，并通过对比OpenAI闭源GPT-4o和DeepSeek开源模型，探讨了闭源与开源模型在安全性、可靠性、效率和适配性方面的交换价值。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各行业的应用日益普及，开发和部署的复杂性成为重要挑战。本文旨在通过系统性评估和对比分析，为AI研究人员、开发者和决策者提供对当前LLM能力、限制和最佳实践的深入理解。

Method: 采用调查研究方法，综述16个关键挑战领域，并选取两个最新的标志性模型进行对比分析：OpenAI的闭源GPT-4o（2024年5月版）和DeepSeek-V3-0324（2025年3月版）开源混合专家模型。通过详细的特征对比和应用场景分析，展示不同模型在各领域的优势。

Result: 研究显示闭源模型（如GPT-4o）在安全性和可靠性方面更优，而开源模型（如DeepSeek-V3）在效率和适配性方面更具优势。不同应用领域（聊天机器人、编程工具、医疗健康、教育等）对模型特性有不同的偏好需求。

Conclusion: 本研究为LLM的开发和部署提供了实用的指南，帮助研究人员和开发者根据具体应用场景选择最合适的模型类型。闭源与开源模型各有优势，未来的研究应关注如何结合两者优点来提升LLM的整体性能。

Abstract: Large Language Models (LLMs) are transforming AI across industries, but their
development and deployment remain complex. This survey reviews 16 key
challenges in building and using LLMs and examines how these challenges are
addressed by two state-of-the-art models with unique approaches: OpenAI's
closed source GPT-4o (May 2024 update) and DeepSeek-V3-0324 (March 2025), a
large open source Mixture-of-Experts model. Through this comparison, we
showcase the trade-offs between closed source models (robust safety, fine-tuned
reliability) and open source models (efficiency, adaptability). We also explore
LLM applications across different domains (from chatbots and coding tools to
healthcare and education), highlighting which model attributes are best suited
for each use case. This article aims to guide AI researchers, developers, and
decision-makers in understanding current LLM capabilities, limitations, and
best practices.

</details>


### [17] [Normality and the Turing Test](https://arxiv.org/abs/2508.21382)
*Alexandre Kabbach*

Main category: cs.CL

TL;DR: 本文通过'正常性'概念重新审视图灵测试，认为图灵测试针对的是正常/平均人类智能而非卓越智能，需要机器像正常人一样犯错。同时测试应由陪审团而非单个法官评估，'平均人类询问者'是多个判断的数学抽象。结论是ChatGPT等大语言模型难以通过图灵测试，因为它们追求的是人工聪明而非真正的人工智能。


<details>
  <summary>Details</summary>
Motivation: 重新解读图灵测试的核心概念，通过统计意义上的'正常性'来理解图灵测试的本质要求，澄清对测试标准的误解。

Method: 概念分析和理论论证，基于统计学中的正态分布概念，对图灵测试的评判标准和评估机制进行重新阐释。

Result: 提出图灵测试实际上是测试'正常智能'，需要机器表现出正常人类的不完美行为；测试评估应基于多个法官的集体判断而非单个专家。

Conclusion: 大语言模型难以通过真正的图灵测试，因为它们追求卓越而非正常智能；图灵测试能否理解人类认知取决于人类心智是否可简化为平均心智，这超出了测试本身的范围。

Abstract: This paper proposes to revisit the Turing test through the concept of
normality. Its core argument is that the statistical interpretation of the
normal--understood as the average both in the normative and mathematical sense
of the term--proves useful for understanding the Turing test in at least two
ways. First, in the sense that the Turing test targets normal/average rather
than exceptional human intelligence, so that successfully passing the test
requires building machines that "make mistakes" and display imperfect behavior
just like normal/average humans. Second, in the sense that the Turing test is a
statistical test where judgments of intelligence are never carried out by a
single "average" judge (understood as non-expert) but always by a full jury. As
such, the notion of "average human interrogator" that Turing talks about in his
original paper should be understood primarily as referring to a mathematical
abstraction made of the normalized aggregate of individual judgments of
multiple judges. In short, this paper argues that the Turing test is a test of
normal intelligence as assessed by a normal judge characterizing the average
judgment of a pool of human interrogators. Its conclusions are twofold. First,
it argues that large language models such as ChatGPT are unlikely to pass the
Turing test as those models precisely target exceptional rather than
normal/average human intelligence. As such, they constitute models of what it
proposes to call artificial smartness rather than artificial intelligence per
se. Second, it argues that the core question of whether the Turing test can
contribute anything to the understanding of human cognition is that of whether
the human mind is really reducible to the normal/average mind--a question which
largely extends beyond the Turing test itself and questions the conceptual
underpinnings of the normalist paradigm it belongs to.

</details>


### [18] [AllSummedUp: un framework open-source pour comparer les metriques d'evaluation de resume](https://arxiv.org/abs/2508.21389)
*Tanguy Herserant,Vincent Guigue*

Main category: cs.CL

TL;DR: 本文研究发现文本摘要自动评估存在严重可复现性问题，六种主流评估指标在实验中出现显著性能差异，揭示了评估效果与计算成本之间的结构性权衡。


<details>
  <summary>Details</summary>
Motivation: 研究自动文本摘要评估中的可复现性挑战，因为现有文献中报告的评估指标性能与实际实验结果存在显著差异，需要建立公平透明的比较框架。

Method: 构建统一的开源评估框架，在SummEval数据集上测试六种代表性评估指标（从经典ROUGE到最新的LLM方法如G-Eval、SEval-Ex），进行系统性对比分析。

Result: 发现评估指标存在结构性权衡：与人类判断最一致的指标往往计算密集且运行稳定性差；LLM评估方法存在随机性、技术依赖性和有限可复现性等关键问题。

Conclusion: 呼吁建立更稳健的评估协议，包括详尽文档记录和方法论标准化，以确保自动摘要评估的更高可靠性。

Abstract: This paper investigates reproducibility challenges in automatic text
summarization evaluation. Based on experiments conducted across six
representative metrics ranging from classical approaches like ROUGE to recent
LLM-based methods (G-Eval, SEval-Ex), we highlight significant discrepancies
between reported performances in the literature and those observed in our
experimental setting. We introduce a unified, open-source framework, applied to
the SummEval dataset and designed to support fair and transparent comparison of
evaluation metrics. Our results reveal a structural trade-off: metrics with the
highest alignment with human judgments tend to be computationally intensive and
less stable across runs. Beyond comparative analysis, this study highlights key
concerns about relying on LLMs for evaluation, stressing their randomness,
technical dependencies, and limited reproducibility. We advocate for more
robust evaluation protocols including exhaustive documentation and
methodological standardization to ensure greater reliability in automatic
summarization assessment.

</details>


### [19] [Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers: A New Counterfactual Evaluation Framework](https://arxiv.org/abs/2508.21422)
*Nils Dycke,Iryna Gurevych*

Main category: cs.CL

TL;DR: LLMs作为自动评审生成器在检测研究逻辑缺陷方面表现不佳，即使论文存在明显逻辑错误，生成的评审内容也无显著差异


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地被用作全自动评审生成器，需要了解其在检测研究逻辑缺陷这一核心评审能力上的具体表现和局限性，以保障科学诚信

Method: 开发了一个完全自动化的反事实评估框架，在受控条件下测试各种ARG方法检测研究逻辑一致性的能力

Result: 研究发现，与预期相反，研究逻辑中的缺陷对ARG输出的评审内容没有显著影响

Conclusion: 基于研究发现提出了三个可操作的建议，并公开了反事实数据集和评估框架，为未来研究提供参考

Abstract: Large Language Models (LLMs) have great potential to accelerate and support
scholarly peer review and are increasingly used as fully automatic review
generators (ARGs). However, potential biases and systematic errors may pose
significant risks to scientific integrity; understanding the specific
capabilities and limitations of state-of-the-art ARGs is essential. We focus on
a core reviewing skill that underpins high-quality peer review: detecting
faulty research logic. This involves evaluating the internal consistency
between a paper's results, interpretations, and claims. We present a fully
automated counterfactual evaluation framework that isolates and tests this
skill under controlled conditions. Testing a range of ARG approaches, we find
that, contrary to expectation, flaws in research logic have no significant
effect on their output reviews. Based on our findings, we derive three
actionable recommendations for future work and release our counterfactual
dataset and evaluation framework publicly.

</details>


### [20] [Med-RewardBench: Benchmarking Reward Models and Judges for Medical Multimodal Large Language Models](https://arxiv.org/abs/2508.21430)
*Meidan Ding,Jipeng Zhang,Wenxuan Wang,Cheng-Yi Li,Wei-Chieh Fang,Hsin-Yu Wu,Haiqin Zhong,Wenting Chen,Linlin Shen*

Main category: cs.CL

TL;DR: Med-RewardBench是首个专门评估医疗奖励模型和评判器的基准，包含1026个专家标注的多模态医疗案例，覆盖13个器官系统和8个临床科室，在6个临床关键维度上评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注通用MLLM能力或作为求解器评估模型，忽视了医疗场景中诊断准确性和临床相关性等关键评估维度，缺乏专门针对医疗奖励模型和评判器的基准。

Method: 构建包含1026个专家标注案例的多模态数据集，采用严格的三步流程确保高质量评估数据，覆盖13个器官系统和8个临床科室，在6个临床关键维度上进行评估。评估了32个最先进的MLLM模型。

Result: 评估发现现有模型在输出与专家判断对齐方面存在重大挑战，开发的基线模型通过微调显示出显著的性能提升。

Conclusion: Med-RewardBench填补了医疗奖励模型评估的空白，为医疗MLLM的可靠评估提供了重要基准，显示了微调对提升医疗场景性能的有效性。

Abstract: Multimodal large language models (MLLMs) hold significant potential in
medical applications, including disease diagnosis and clinical decision-making.
However, these tasks require highly accurate, context-sensitive, and
professionally aligned responses, making reliable reward models and judges
critical. Despite their importance, medical reward models (MRMs) and judges
remain underexplored, with no dedicated benchmarks addressing clinical
requirements. Existing benchmarks focus on general MLLM capabilities or
evaluate models as solvers, neglecting essential evaluation dimensions like
diagnostic accuracy and clinical relevance. To address this, we introduce
Med-RewardBench, the first benchmark specifically designed to evaluate MRMs and
judges in medical scenarios. Med-RewardBench features a multimodal dataset
spanning 13 organ systems and 8 clinical departments, with 1,026
expert-annotated cases. A rigorous three-step process ensures high-quality
evaluation data across six clinically critical dimensions. We evaluate 32
state-of-the-art MLLMs, including open-source, proprietary, and
medical-specific models, revealing substantial challenges in aligning outputs
with expert judgment. Additionally, we develop baseline models that demonstrate
substantial performance improvements through fine-tuning.

</details>


### [21] [Discovering Semantic Subdimensions through Disentangled Conceptual Representations](https://arxiv.org/abs/2508.21436)
*Yunhao Zhang,Shaonan Wang,Nan Lin,Xinyi Dong,Chong Li,Chengqing Zong*

Main category: cs.CL

TL;DR: 本文提出了一种新的解漏连续语义表征模型(DCSRM)，用于探索粗粒度语义维度下的更细粒度语义子维度，并通过脑激活映射验证其神经可信性。


<details>
  <summary>Details</summary>
Motivation: 现有语义表征方法多依赖预定义的粗粒度语义维度，忽视了更细粒度的概念区分，需要探索概念意义的核心细粒度组织结构。

Method: 提出DCSRM模型，将大语言模型的词嵌入分解为多个子嵌入，每个编码特定语义信息，然后通过像素级编码模型将这些子维度映射到脑激活上验证其神经可信性。

Result: 识别出了一组可解释的语义子维度，发现语义维度按不同原则结构，极性是驱动分解的关键因素，子维度的神经关联支持其认知和神经科学可信性。

Conclusion: 该工作提供了更细粒度可解释的概念意义子维度，为理解语言和脑部意义组织提供了新的视角和方法。

Abstract: Understanding the core dimensions of conceptual semantics is fundamental to
uncovering how meaning is organized in language and the brain. Existing
approaches often rely on predefined semantic dimensions that offer only broad
representations, overlooking finer conceptual distinctions. This paper proposes
a novel framework to investigate the subdimensions underlying coarse-grained
semantic dimensions. Specifically, we introduce a Disentangled Continuous
Semantic Representation Model (DCSRM) that decomposes word embeddings from
large language models into multiple sub-embeddings, each encoding specific
semantic information. Using these sub-embeddings, we identify a set of
interpretable semantic subdimensions. To assess their neural plausibility, we
apply voxel-wise encoding models to map these subdimensions to brain
activation. Our work offers more fine-grained interpretable semantic
subdimensions of conceptual meaning. Further analyses reveal that semantic
dimensions are structured according to distinct principles, with polarity
emerging as a key factor driving their decomposition into subdimensions. The
neural correlates of the identified subdimensions support their cognitive and
neuroscientific plausibility.

</details>


### [22] [Beyond the Surface: Probing the Ideological Depth of Large Language Models](https://arxiv.org/abs/2508.21448)
*Shariar Kabir,Kevin Esterling,Yue Dong*

Main category: cs.CL

TL;DR: 研究发现大语言模型存在可量化的意识形态深度差异，一些模型具有更稳定、抽象的内部政治特征，而另一些则更容易被提示工程操控。


<details>
  <summary>Details</summary>
Motivation: 大语言模型显示出明显的意识形态倾向，但这些立场的稳定性和深度尚不清楚，表面响应容易被提示工程操控，需要探究其是否反映连贯的底层意识形态。

Method: 采用双重方法：1) 通过指令提示和激活引导测量开源LLM的可操控性；2) 使用稀疏自编码器(SAEs)探测模型内部机制，分析意识形态特征。

Result: 发现可操控性较低的模型具有更独特和抽象的意识形态特征，一个模型可能包含比同规模模型多7.3倍的政治特征。对深度模型核心政治特征的靶向消融能导致跨相关主题的一致逻辑转变，而浅层模型则增加拒绝输出。

Conclusion: 意识形态深度是LLM的可量化属性，可操控性为探索其潜在政治架构提供了有价值的窗口。

Abstract: Large Language Models (LLMs) have demonstrated pronounced ideological
leanings, yet the stability and depth of these positions remain poorly
understood. Surface-level responses can often be manipulated through simple
prompt engineering, calling into question whether they reflect a coherent
underlying ideology. This paper investigates the concept of "ideological depth"
in LLMs, defined as the robustness and complexity of their internal political
representations. We employ a dual approach: first, we measure the
"steerability" of two well-known open-source LLMs using instruction prompting
and activation steering. We find that while some models can easily switch
between liberal and conservative viewpoints, others exhibit resistance or an
increased rate of refusal, suggesting a more entrenched ideological structure.
Second, we probe the internal mechanisms of these models using Sparse
Autoencoders (SAEs). Preliminary analysis reveals that models with lower
steerability possess more distinct and abstract ideological features. Our
evaluations reveal that one model can contain 7.3x more political features than
another model of similar size. This allows targeted ablation of a core
political feature in an ideologically "deep" model, leading to consistent,
logical shifts in its reasoning across related topics, whereas the same
intervention in a "shallow" model results in an increase in refusal outputs.
Our findings suggest that ideological depth is a quantifiable property of LLMs
and that steerability serves as a valuable window into their latent political
architecture.

</details>


### [23] [Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards](https://arxiv.org/abs/2508.21476)
*Xiaolong Wei,Bo Lu,Xingyu Zhang,Zhejun Zhao,Dongdong Shen,Long Xia,Dawei Yin*

Main category: cs.CL

TL;DR: 使用RLAIF框架的两种AI奖励策略（基于偏好数据的RM和基于原则的LLM-as-a-Judge）来提升7B参数小语言模型的中文问候语创意写作能力，后者在生成质量、训练效率和数据依赖性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 大语言模型创意写作能力强但计算成本高，小语言模型是替代方案但现有方法（如SFT缺乏新颖性，RLHF成本高）存在局限，需要更有效的创意写作增强方法。

Method: 采用RLAIF框架，探索两种AI奖励策略：1）基于多智能体拒绝采样框架构建的高质量偏好数据训练的奖励模型；2）通过对抗训练和反思机制优化的原则引导LLM-as-a-Judge直接提供奖励信号。

Result: 两种方法都显著提升了创意输出，但原则引导的LLM-as-a-Judge方法在生成质量上表现更优，同时具有更高的训练效率和更低的人类标注数据依赖性。自动评估方法与人类判断高度一致。

Conclusion: 原则引导的LLM-as-a-Judge方法为创意小语言模型提供了更可扩展和有效的路径，减少了对人类标注数据的依赖，同时保持了高质量的创意生成能力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable creative writing
capabilities, yet their substantial computational demands hinder widespread
use. Enhancing Small Language Models (SLMs) offers a promising alternative, but
current methods like Supervised Fine-Tuning (SFT) struggle with novelty, and
Reinforcement Learning from Human Feedback (RLHF) is costly. This paper
explores two distinct AI-driven reward strategies within a Reinforcement
Learning from AI Feedback (RLAIF) framework to ignite the creative writing of a
7B-parameter SLM, specifically for generating Chinese greetings. The first
strategy employs a RM trained on high-quality preference data curated by a
novel multi-agent rejection sampling framework designed for creative tasks. The
second, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose
reward function is optimized via an adversarial training scheme with a
reflection mechanism, to directly provide reward signals. Comprehensive
experiments reveal that while both approaches significantly enhance creative
output over baselines, the principle-guided LLM-as-a-Judge demonstrably yields
superior generation quality. Furthermore, it offers notable advantages in
training efficiency and reduced dependency on human-annotated data, presenting
a more scalable and effective path towards creative SLMs. Our automated
evaluation methods also exhibit strong alignment with human judgments. Our code
and data are publicly available at
https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.

</details>


### [24] [HSFN: Hierarchical Selection for Fake News Detection building Heterogeneous Ensemble](https://arxiv.org/abs/2508.21482)
*Sara B. Coutinho,Rafael M. O. Cruz,Francimaria R. S. Nascimento,George D. C. Cavalcanti*

Main category: cs.CL

TL;DR: 提出了一种基于层次聚类的自动分类器选择方法，通过优先考虑多样性来改进集成学习在假新闻检测中的性能


<details>
  <summary>Details</summary>
Motivation: 心理偏见使人们容易相信和传播假新闻，集成学习方法虽然有效但性能严重依赖分类器的多样性，而选择真正多样化的模型是一个关键挑战

Method: 首先计算分类器间的成对多样性，应用层次聚类将其组织成不同粒度的组，然后通过HierarchySelect探索层次级别选择每个级别的分类器池，最后选择最具多样性的池用于集成构建

Result: 在6个数据集中的2个上达到了最高准确率，优于Elbow启发式方法和最先进的基线方法

Conclusion: 该方法通过优先考虑多样性并扩展性能指标，有效提高了集成学习在假新闻检测中的效果

Abstract: Psychological biases, such as confirmation bias, make individuals
particularly vulnerable to believing and spreading fake news on social media,
leading to significant consequences in domains such as public health and
politics. Machine learning-based fact-checking systems have been widely studied
to mitigate this problem. Among them, ensemble methods are particularly
effective in combining multiple classifiers to improve robustness. However,
their performance heavily depends on the diversity of the constituent
classifiers-selecting genuinely diverse models remains a key challenge,
especially when models tend to learn redundant patterns. In this work, we
propose a novel automatic classifier selection approach that prioritizes
diversity, also extended by performance. The method first computes pairwise
diversity between classifiers and applies hierarchical clustering to organize
them into groups at different levels of granularity. A HierarchySelect then
explores these hierarchical levels to select one pool of classifiers per level,
each representing a distinct intra-pool diversity. The most diverse pool is
identified and selected for ensemble construction from these. The selection
process incorporates an evaluation metric reflecting each classifiers's
performance to ensure the ensemble also generalises well. We conduct
experiments with 40 heterogeneous classifiers across six datasets from
different application domains and with varying numbers of classes. Our method
is compared against the Elbow heuristic and state-of-the-art baselines. Results
show that our approach achieves the highest accuracy on two of six datasets.
The implementation details are available on the project's repository:
https://github.com/SaraBCoutinho/HSFN .

</details>


### [25] [L3Cube-MahaSTS: A Marathi Sentence Similarity Dataset and Models](https://arxiv.org/abs/2508.21569)
*Aishwarya Mirashi,Ananya Joshi,Raviraj Joshi*

Main category: cs.CL

TL;DR: 提出了MahaSTS马拉地语句子相似度数据集和MahaSBERT-STS-v2模型，包含16,860个标注句子对，通过均衡分布标注分数来减少偏差，在低资源环境下有效提升句子相似度任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决马拉地语等低资源语言缺乏高质量句子相似度标注数据的问题，为马拉地语NLP任务提供可靠的句子表示学习资源。

Method: 构建人工标注的句子相似度数据集，采用0-5连续分数标注，并均匀分布在六个分数区间以减少标签偏差；基于MahaSBERT模型进行回归式相似度评分微调。

Result: MahaSBERT-STS-v2模型在马拉地语句子相似度任务上表现优于MahaBERT、MuRIL、IndicBERT和IndicSBERT等基准模型，证明了人工标注数据和结构化监督的有效性。

Conclusion: 人工精心标注的数据集、针对性微调和结构化监督策略在低资源语言环境中对提升句子相似度任务性能具有显著影响，为其他低资源语言提供了可借鉴的方法。

Abstract: We present MahaSTS, a human-annotated Sentence Textual Similarity (STS)
dataset for Marathi, along with MahaSBERT-STS-v2, a fine-tuned Sentence-BERT
model optimized for regression-based similarity scoring. The MahaSTS dataset
consists of 16,860 Marathi sentence pairs labeled with continuous similarity
scores in the range of 0-5. To ensure balanced supervision, the dataset is
uniformly distributed across six score-based buckets spanning the full 0-5
range, thus reducing label bias and enhancing model stability. We fine-tune the
MahaSBERT model on this dataset and benchmark its performance against other
alternatives like MahaBERT, MuRIL, IndicBERT, and IndicSBERT. Our experiments
demonstrate that MahaSTS enables effective training for sentence similarity
tasks in Marathi, highlighting the impact of human-curated annotations,
targeted fine-tuning, and structured supervision in low-resource settings. The
dataset and model are publicly shared at
https://github.com/l3cube-pune/MarathiNLP

</details>


### [26] [A Survey on Current Trends and Recent Advances in Text Anonymization](https://arxiv.org/abs/2508.21587)
*Tobias Deußer,Lorenz Sparrenberg,Armin Berger,Max Hahnbück,Christian Bauckhage,Rafet Sifa*

Main category: cs.CL

TL;DR: 本论文统订了文本匿名化技术的最新进展，包括基础方法、大语言模型影响、领域特定挑战、形式隐私模型、评估框架和实践工具，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 各行业中含有敏感个人信息的文本数据流通日益增多，需要突破性的匿名化技术来保护隐私同时维持数据可用性，以满足相关法规要求和下游任务需求。

Method: 调研报告综述了从基础命名实体识别方法到大语言模型应用的各种匿名化技术，包括医疗、法律、金融、教育等领域特定解决方案，以及形式隐私模型、风险感知框架和作者匿名化专门技术。

Result: 调研整合了当前领域知识，识别出了新兴趋势和持续挑战，包括隐私-效用权衡、几何标识符处理需求、大语言模型能力影响等关键问题，为学术界和实践者提供了指导。

Conclusion: 本综述为文本匿名化领域提供了全面的技术路线图，明确了未来研究的优先方向，将有助于推动更加高效、可靠的匿名化方案的发展，以满足日益增长的隐私保护需求。

Abstract: The proliferation of textual data containing sensitive personal information
across various domains requires robust anonymization techniques to protect
privacy and comply with regulations, while preserving data usability for
diverse and crucial downstream tasks. This survey provides a comprehensive
overview of current trends and recent advances in text anonymization
techniques. We begin by discussing foundational approaches, primarily centered
on Named Entity Recognition, before examining the transformative impact of
Large Language Models, detailing their dual role as sophisticated anonymizers
and potent de-anonymization threats. The survey further explores
domain-specific challenges and tailored solutions in critical sectors such as
healthcare, law, finance, and education. We investigate advanced methodologies
incorporating formal privacy models and risk-aware frameworks, and address the
specialized subfield of authorship anonymization. Additionally, we review
evaluation frameworks, comprehensive metrics, benchmarks, and practical
toolkits for real-world deployment of anonymization solutions. This review
consolidates current knowledge, identifies emerging trends and persistent
challenges, including the evolving privacy-utility trade-off, the need to
address quasi-identifiers, and the implications of LLM capabilities, and aims
to guide future research directions for both academics and practitioners in
this field.

</details>


### [27] [Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning](https://arxiv.org/abs/2508.21589)
*Zinan Tang,Xin Gao,Qizhi Pei,Zhuoshi Pan,Mengzhang Cai,Jiang Wu,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: Middo是一个自演化的模型驱动动态数据优化框架，通过模型感知的数据选择和上下文保持的数据精炼，建立闭环优化系统来持续提升SFT训练数据质量。


<details>
  <summary>Details</summary>
Motivation: 现有的数据选择和数据合成方法存在静态数据集构建的局限性，无法适应模型能力的动态演化，需要一种能够与模型共同进化的动态数据优化方法。

Method: 提出三轴模型信号诊断模块（损失模式、嵌入聚类动态、自对齐分数）识别次优样本，然后通过自适应优化引擎将次优样本转化为有教学价值的训练点，同时保持语义完整性。

Result: 在多个基准测试中，该方法平均提高准确率7.15%，同时保持原始数据集规模，显著提升了种子数据质量和LLM性能。

Conclusion: 这项工作通过数据和模型的动态人机协同进化，为可持续的LLM训练建立了新范式。

Abstract: Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely
on high-quality training data. While data selection and data synthesis are two
common strategies to improve data quality, existing approaches often face
limitations in static dataset curation that fail to adapt to evolving model
capabilities. In this paper, we introduce Middo, a self-evolving Model-informed
dynamic data optimization framework that uses model-aware data selection and
context-preserving data refinement. Unlike conventional one-off
filtering/synthesis methods, our framework establishes a closed-loop
optimization system: (1) A self-referential diagnostic module proactively
identifies suboptimal samples through tri-axial model signals - loss patterns
(complexity), embedding cluster dynamics (diversity), and self-alignment scores
(quality); (2) An adaptive optimization engine then transforms suboptimal
samples into pedagogically valuable training points while preserving semantic
integrity; (3) This optimization process continuously evolves with model
capability through dynamic learning principles. Experiments on multiple
benchmarks demonstrate that our \method consistently enhances the quality of
seed data and boosts LLM's performance with improving accuracy by 7.15% on
average while maintaining the original dataset scale. This work establishes a
new paradigm for sustainable LLM training through dynamic human-AI co-evolution
of data and models. Our datasets, models, and code are coming soon.

</details>


### [28] [Personality Matters: User Traits Predict LLM Preferences in Multi-Turn Collaborative Tasks](https://arxiv.org/abs/2508.21628)
*Sarfaroz Yunusov,Kaige Chen,Kazi Nishat Anwar,Ali Emami*

Main category: cs.CL

TL;DR: 研究发现不同人格类型的用户对GPT-4和Claude 3.5有显著偏好，理性者偏好GPT-4，理想主义者偏好Claude 3.5，传统评估方法忽略了这些人格相关差异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多轮协作工作流中的普及，需要了解不同人格特质的用户是否会系统地偏好某些LLM。

Method: 在32名参与者(均匀分布在4种Keirsey人格类型)中进行研究，评估他们与GPT-4和Claude 3.5在数据分析、创意写作、信息检索和写作辅助四个协作任务中的交互。

Result: 理性者强烈偏好GPT-4(尤其是目标导向任务)，理想主义者偏好Claude 3.5(尤其是创造性和分析性任务)，其他人格类型显示任务依赖性偏好。情感分析确认了这些模式。

Conclusion: 人格基础的分析能够揭示传统评估方法忽略的LLM差异，虽然总体有用性评分相似。

Abstract: As Large Language Models (LLMs) increasingly integrate into everyday
workflows, where users shape outcomes through multi-turn collaboration, a
critical question emerges: do users with different personality traits
systematically prefer certain LLMs over others? We conducted a study with 32
participants evenly distributed across four Keirsey personality types,
evaluating their interactions with GPT-4 and Claude 3.5 across four
collaborative tasks: data analysis, creative writing, information retrieval,
and writing assistance. Results revealed significant personality-driven
preferences: Rationals strongly preferred GPT-4, particularly for goal-oriented
tasks, while idealists favored Claude 3.5, especially for creative and
analytical tasks. Other personality types showed task-dependent preferences.
Sentiment analysis of qualitative feedback confirmed these patterns. Notably,
aggregate helpfulness ratings were similar across models, showing how
personality-based analysis reveals LLM differences that traditional evaluations
miss.

</details>


### [29] [QZhou-Embedding Technical Report](https://arxiv.org/abs/2508.21632)
*Peng Yu,En Xu,Bin Chen,Haibiao Chen,Yinfei Xu*

Main category: cs.CL

TL;DR: QZhou-Embedding是基于Qwen2.5-7B-Instruct构建的通用文本嵌入模型，采用多任务框架和专门的数据转换策略，在MTEB和CMTEB基准测试中取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个具有卓越文本表示能力的通用上下文文本嵌入模型，通过高质量、多样化的训练数据和优化的训练策略来提升检索模型性能。

Method: 构建统一的多任务框架，包含专门的数据转换和训练策略；利用LLM API开发数据合成流水线（包括释义、增强和困难负样本生成）；采用两阶段训练策略（检索预训练+全任务微调）。

Result: 在MTEB和CMTEB基准测试中排名第一（2025年8月27日），同时在重排序、聚类等任务上达到最先进性能。

Conclusion: 更高质量和更多样化的数据对提升检索模型性能至关重要，利用LLM生成能力可以进一步优化数据质量，实现嵌入模型的突破。

Abstract: We present QZhou-Embedding, a general-purpose contextual text embedding model
with exceptional text representation capabilities. Built upon the
Qwen2.5-7B-Instruct foundation model, we designed a unified multi-task
framework comprising specialized data transformation and training strategies.
The data transformation scheme enables the incorporation of more diverse
textual training datasets, while the task-specific training strategies enhance
model learning efficiency. We developed a data synthesis pipeline leveraging
LLM API, incorporating techniques such as paraphrasing, augmentation, and hard
negative example generation to improve the semantic richness and sample
difficulty of the training set. Additionally, we employ a two-stage training
strategy, comprising initial retrieval-focused pretraining followed by
full-task fine-tuning, enabling the embedding model to extend its capabilities
based on robust retrieval performance. Our model achieves state-of-the-art
results on the MTEB and CMTEB benchmarks, ranking first on both leaderboards
(August 27 2025), and simultaneously achieves state-of-the-art performance on
tasks including reranking, clustering, etc. Our findings demonstrate that
higher-quality, more diverse data is crucial for advancing retrieval model
performance, and that leveraging LLMs generative capabilities can further
optimize data quality for embedding model breakthroughs. Our model weights are
released on HuggingFace under Apache 2.0 license. For reproducibility, we
provide evaluation code and instructions on GitHub.

</details>


### [30] [Is this chart lying to me? Automating the detection of misleading visualizations](https://arxiv.org/abs/2508.21675)
*Jonathan Tonglet,Jan Zimny,Tinne Tuytelaars,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文介绍了Misviz基准数据集，包含2,604个真实世界误导性可视化图表和81,814个合成图表，用于训练和评估AI模型检测误导性可视化图表的能力。


<details>
  <summary>Details</summary>
Motivation: 误导性可视化图表是社交媒体和网络上错误信息的重要来源，违反图表设计原则会导致读者得出错误结论。目前缺乏大规模、多样化、公开可用的数据集来训练和评估AI模型。

Method: 创建Misviz基准数据集（2,604个真实图表）和Misviz-synth合成数据集（81,814个图表），使用最先进的多模态大语言模型、基于规则的系统以及微调分类器进行全面评估。

Result: 研究结果显示，检测误导性可视化图表的任务仍然极具挑战性，现有模型在此任务上表现有限。

Conclusion: 作者发布了Misviz、Misviz-synth数据集及相关代码，为检测误导性可视化图表的研究提供了重要资源，但该领域仍需进一步研究。

Abstract: Misleading visualizations are a potent driver of misinformation on social
media and the web. By violating chart design principles, they distort data and
lead readers to draw inaccurate conclusions. Prior work has shown that both
humans and multimodal large language models (MLLMs) are frequently deceived by
such visualizations. Automatically detecting misleading visualizations and
identifying the specific design rules they violate could help protect readers
and reduce the spread of misinformation. However, the training and evaluation
of AI models has been limited by the absence of large, diverse, and openly
available datasets. In this work, we introduce Misviz, a benchmark of 2,604
real-world visualizations annotated with 12 types of misleaders. To support
model training, we also release Misviz-synth, a synthetic dataset of 81,814
visualizations generated using Matplotlib and based on real-world data tables.
We perform a comprehensive evaluation on both datasets using state-of-the-art
MLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that
the task remains highly challenging. We release Misviz, Misviz-synth, and the
accompanying code.

</details>


### [31] [Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance](https://arxiv.org/abs/2508.21741)
*Yao Wang,Di Liang,Minlong Peng*

Main category: cs.CL

TL;DR: 提出CPI-FT框架解决多任务微调中的跷跷板现象，通过核心参数隔离和融合技术，显著减轻任务干扰和遗忘问题


<details>
  <summary>Details</summary>
Motivation: 传统监督微调在多任务场景下存在跷跷板现象，某些任务的提升以其他任务性能下降为代价，需要解决参数更新冲突问题

Method: 1) 独立微调识别核心参数区域 2) 基于区域重叠的任务聚类 3) 核心参数直接移植+非核心参数SLERP融合 4) 轻量级流水线SFT训练，冻结先前任务核心区域

Result: 在多个公开基准测试中显著优于传统的多任务和多阶段微调基线方法

Conclusion: CPI-FT框架有效缓解了任务干扰和灾难性遗忘问题，为多任务LLM微调提供了有效解决方案

Abstract: Supervised fine-tuning (SFT) is a pivotal approach to adapting large language
models (LLMs) for downstream tasks; however, performance often suffers from the
``seesaw phenomenon'', where indiscriminate parameter updates yield progress on
certain tasks at the expense of others. To address this challenge, we propose a
novel \emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework.
Specifically, we first independently fine-tune the LLM on each task to identify
its core parameter regions by quantifying parameter update magnitudes. Tasks
with similar core regions are then grouped based on region overlap, forming
clusters for joint modeling. We further introduce a parameter fusion technique:
for each task, core parameters from its individually fine-tuned model are
directly transplanted into a unified backbone, while non-core parameters from
different tasks are smoothly integrated via Spherical Linear Interpolation
(SLERP), mitigating destructive interference. A lightweight, pipelined SFT
training phase using mixed-task data is subsequently employed, while freezing
core regions from prior tasks to prevent catastrophic forgetting. Extensive
experiments on multiple public benchmarks demonstrate that our approach
significantly alleviates task interference and forgetting, consistently
outperforming vanilla multi-task and multi-stage fine-tuning baselines.

</details>


### [32] [Reasoning-Intensive Regression](https://arxiv.org/abs/2508.21762)
*Diane Tchuindjo,Omar Khattab*

Main category: cs.CL

TL;DR: 本文提出了推理密集型回归(RiR)任务的概念，针对需要从文本中推导细微数值属性的问题，并提出了MENTAT方法，结合批量反射提示优化和神经集成学习，在基准测试中取得了65%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究越来越多地将大语言模型应用于推理密集型回归任务，这类任务需要从文本中进行深度分析来推导数值属性，但现有的提示冻结LLM和微调Transformer编码器方法在此类任务上表现不佳。

Method: 提出了MENTAT方法，该方法结合了批量反射提示优化和神经集成学习，是一个简单轻量级的解决方案，专门针对推理密集型回归任务设计。

Result: MENTAT方法在三个现实问题构建的基准测试中，相比基线方法取得了最高65%的性能改进，证明了该方法的有效性。

Conclusion: 虽然MENTAT在推理密集型回归任务上取得了显著改进，但这类任务仍然存在很大的提升空间，需要未来的进一步研究和发展。

Abstract: AI researchers and practitioners increasingly apply large language models
(LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing
subtle numerical properties from text. Unlike standard language regression
tasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc
problems like rubric-based scoring or domain-specific retrieval, where much
deeper analysis of text is required while only limited task-specific training
data and computation are available. We cast three realistic problems as RiR
tasks to establish an initial benchmark, and use that to test our hypothesis
that prompting frozen LLMs and finetuning Transformer encoders via gradient
descent will both often struggle in RiR. We then propose MENTAT, a simple and
lightweight method that combines batch-reflective prompt optimization with
neural ensemble learning. MENTAT achieves up to 65% improvement over both
baselines, though substantial room remains for future advances in RiR.

</details>


### [33] [PiCSAR: Probabilistic Confidence Selection And Ranking](https://arxiv.org/abs/2508.21787)
*Joshua Ong Jun Leang,Zheng Zhao,Aryo Pradipta Gema,Sohee Yang,Wai-Chung Kwan,Xuanli He,Wenda Li,Pasquale Minervini,Eleonora Giunchiglia,Shay B. Cohen*

Main category: cs.CL

TL;DR: PiCSAR是一种无需训练的方法，通过联合对数似然对推理链和最终答案进行评分，在多个数学推理基准上显著优于现有方法，且样本效率更高。


<details>
  <summary>Details</summary>
Motivation: 针对推理任务中缺乏真实答案时难以设计有效评分函数的问题，需要一种能够准确识别正确推理链的评分方法。

Method: 提出Probabilistic Confidence Selection And Ranking (PiCSAR)，使用推理过程和最终答案的联合对数似然来评分候选生成结果，该方法自然分解为推理置信度和答案置信度。

Result: 在MATH500上提升10.18分，在AIME2025上提升9.81分，在20次比较中有16次以至少2倍少的样本量超越基线方法。

Conclusion: 正确推理链展现出显著更高的推理和答案置信度，验证了PiCSAR方法的有效性，为推理任务的候选选择提供了简单而强大的解决方案。

Abstract: Best-of-n sampling improves the accuracy of large language models (LLMs) and
large reasoning models (LRMs) by generating multiple candidate solutions and
selecting the one with the highest reward. The key challenge for reasoning
tasks is designing a scoring function that can identify correct reasoning
chains without access to ground-truth answers. We propose Probabilistic
Confidence Selection And Ranking (PiCSAR): a simple, training-free method that
scores each candidate generation using the joint log-likelihood of the
reasoning and final answer. The joint log-likelihood of the reasoning and final
answer naturally decomposes into reasoning confidence and answer confidence.
PiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500,
+9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in
16 out of 20 comparisons. Our analysis reveals that correct reasoning chains
exhibit significantly higher reasoning and answer confidence, justifying the
effectiveness of PiCSAR.

</details>


### [34] [Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval](https://arxiv.org/abs/2508.21788)
*Inés Altemir Marinas,Anastasiia Kucherenko,Andrei Kucharavy*

Main category: cs.CL

TL;DR: 提出了一个基于ElasticSearch的框架，用于快速索引和分析LLM训练数据集，在FineWeb-2语料库上实现毫秒级查询性能，为AI系统提供更安全、更负责任的数据分析工具。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型严重依赖网络规模数据集，但网络爬取的无差别性导致数据质量、安全性和伦理方面存在挑战。由于计算限制，先前对有害内容的研究仅限于小样本，需要更高效的分析方法。

Method: 开发了一个基于ElasticSearch的索引和分析管道框架，应用于SwissAI的FineWeb-2语料库（1.5TB，四种语言），实现快速查询性能。

Result: 实现了毫秒级的查询性能（大多数搜索在毫秒内完成，所有查询都在2秒内完成），展示了实时数据集分析能力。

Conclusion: 该框架为AI系统提供了实用的数据集分析工具，有助于构建更安全、更负责任的AI系统，解决了大规模训练数据分析的计算瓶颈问题。

Abstract: Large language models (LLMs) rely heavily on web-scale datasets like Common
Crawl, which provides over 80\% of training data for some modern models.
However, the indiscriminate nature of web crawling raises challenges in data
quality, safety, and ethics. Despite the critical importance of training data
quality, prior research on harmful content has been limited to small samples
due to computational constraints. This project presents a framework for
indexing and analyzing LLM training datasets using an ElasticSearch-based
pipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages),
achieving fast query performance--most searches in milliseconds, all under 2
seconds. Our work demonstrates real-time dataset analysis, offering practical
tools for safer, more accountable AI systems.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [35] [lifeXplore at the Lifelog Search Challenge 2020](https://arxiv.org/abs/2508.21397)
*Andreas Leibetseder,Klaus Schoeffmann*

Main category: cs.MM

TL;DR: 本文介绍了lifeXplore系统，这是一个用于Lifelog搜索挑战赛的视频探索和检索工具，结合了特征地图浏览、概念搜索过滤和手绘草图功能，并通过添加YOLO9000、OCR和均匀采样进行了改进。


<details>
  <summary>Details</summary>
Motivation: 为了在Lifelog搜索挑战赛(LSC)中更有效地检索和搜索不断增长的公共生活日志数据集，需要开发快速响应时间限制查询的系统。

Method: 开发了lifeXplore系统，结合特征地图浏览、概念搜索过滤和手绘草图功能，并集成了YOLO9000深度概念检测、光学字符识别(OCR)技术，以及采用均匀采样作为传统镜头分割的替代方案。

Result: 该系统参与了2018年和2019年两届LSC竞赛，展示了其在生活日志数据检索方面的能力。

Conclusion: lifeXplore系统通过集成多种先进技术，为生活日志数据的快速检索和探索提供了有效的解决方案，在LSC竞赛中表现出色。

Abstract: Since its first iteration in 2018, the Lifelog Search Challenge (LSC) - an
interactive competition for retrieving lifelogging moments - is co-located at
the annual ACM International Conference on Multimedia Retrieval (ICMR) and has
drawn international attention. With the goal of making an ever growing public
lifelogging dataset searchable, several teams develop systems for quickly
solving time-limited queries during the challenge. Having participated in both
previous LSC iterations, i.e. LSC2018 and LSC2019, we present our lifeXplore
system - a video exploration and retrieval tool combining feature map browsing,
concept search and filtering as well as hand-drawn sketching. The system is
improved by including additional deep concept YOLO9000, optical character
recognition (OCR) as well as adding uniform sampling as an alternative to the
system's traditional underlying shot segmentation.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [36] [RARR : Robust Real-World Activity Recognition with Vibration by Scavenging Near-Surface Audio Online](https://arxiv.org/abs/2508.21167)
*Dong Yoon Lee,Alyssa Weakley,Hui Wei,Blake Brown,Keyana Carrion,Shijia Pan*

Main category: cs.SD

TL;DR: 开发了一种基于结构振动传感器的远程监测系统，利用合成音频数据预训练模型，只需少量标注数据即可实现准确的活动识别，用于独居痴呆患者的日常活动追踪


<details>
  <summary>Details</summary>
Motivation: 四分之一痴呆患者独居，需要远程照护。现有远程监测方案存在隐私保护、活动识别和模型泛化性等问题，特别是在真实家庭环境中需要大量标注数据

Method: 使用结构振动传感器系统，通过近表面声学音频合成数据进行模型预训练，然后用极有限的真实数据进行微调，构建日常活动追踪的鲁棒框架

Result: 该方法能够在不侵犯隐私的前提下，通过少量标注数据实现准确的活动识别，解决了模型在新用户和新环境中的泛化问题

Conclusion: 提出的可扩展解决方案通过数据合成和迁移学习，显著降低了真实部署中对标注数据的依赖，为独居痴呆患者的远程监测提供了实用且保护隐私的技术途径

Abstract: One in four people dementia live alone, leading family members to take on
caregiving roles from a distance. Many researchers have developed remote
monitoring solutions to lessen caregiving needs; however, limitations remain
including privacy preserving solutions, activity recognition, and model
generalizability to new users and environments. Structural vibration sensor
systems are unobtrusive solutions that have been proven to accurately monitor
human information, such as identification and activity recognition, in
controlled settings by sensing surface vibrations generated by activities.
However, when deploying in an end user's home, current solutions require a
substantial amount of labeled data for accurate activity recognition. Our
scalable solution adapts synthesized data from near-surface acoustic audio to
pretrain a model and allows fine tuning with very limited data in order to
create a robust framework for daily routine tracking.

</details>


### [37] [DRASP: A Dual-Resolution Attentive Statistics Pooling Framework for Automatic MOS Prediction](https://arxiv.org/abs/2508.21407)
*Cheng-Yeh Yang,Kuan-Tang Huang,Chien-Chun Wang,Hung-Shin Lee,Hsin-Min Wang,Berlin Chen*

Main category: cs.SD

TL;DR: 提出了双分辨率注意力统计池化(DRASP)框架，通过结合粗粒度全局统计和细粒度注意力分析，在MOS预测中显著优于传统池化方法，SRCC相对提升10.39%。


<details>
  <summary>Details</summary>
Motivation: 现有池化方法通常只在单一粒度上操作，要么关注全局视角，要么关注帧级分析，可能忽略了互补的感知信息。

Method: DRASP框架整合了粗粒度的全局统计摘要和细粒度的感知显著片段注意力分析，采用双视角架构同时捕捉整体结构上下文和显著局部细节。

Result: 在多个数据集(MusicEval和AES-Natural)、不同MOS预测骨干网络(包括CLAP-based模型和AudioBox-Aesthetics)以及不同音频生成系统上，都一致优于各种基线方法。

Conclusion: DRASP框架具有有效性和强泛化能力，在系统级Spearman等级相关系数上相对广泛使用的平均池化方法提升了10.39%。

Abstract: A pooling mechanism is essential for mean opinion score (MOS) prediction,
facilitating the transformation of variable-length audio features into a
concise fixed-size representation that effectively encodes speech quality.
Existing pooling methods typically operate at a singular granularity,
concentrating either on a comprehensive global perspective or a detailed
frame-level analysis, which may overlook complementary perceptual insights. To
address this limitation, we introduce the Dual-Resolution Attentive Statistics
Pooling (DRASP) framework. DRASP integrates both coarse-grained, global
statistical summaries and fine-grained, attentive analyses of perceptually
significant segments. This dual-view architecture empowers our model to
formulate a more thorough and robust representation, capturing both the
overarching structural context and salient local details concurrently.
Extensive experiments validate the effectiveness and strong generalization
ability of the proposed framework. It consistently outperforms various baseline
methods across diverse datasets (MusicEval and AES-Natural), MOS prediction
backbones (including a CLAP-based model and AudioBox-Aesthetics), and different
audio generation systems, achieving a relative improvement of 10.39% in
system-level Spearman's rank correlation coefficient (SRCC) over the
widely-used average pooling approach.

</details>
