{"id": "2509.08873", "categories": ["cs.SD", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2509.08873", "abs": "https://arxiv.org/abs/2509.08873", "authors": ["Jonas M. Schmid", "Johannes D. Schmid", "Martin Eser", "Steffen Marburg"], "title": "In situ estimation of the acoustic surface impedance using simulation-based inference", "comment": null, "summary": "Accurate acoustic simulations of enclosed spaces require precise boundary\nconditions, typically expressed through surface impedances for wave-based\nmethods. Conventional measurement techniques often rely on simplifying\nassumptions about the sound field and mounting conditions, limiting their\nvalidity for real-world scenarios. To overcome these limitations, this study\nintroduces a Bayesian framework for the in situ estimation of\nfrequency-dependent acoustic surface impedances from sparse interior sound\npressure measurements. The approach employs simulation-based inference, which\nleverages the expressiveness of modern neural network architectures to directly\nmap simulated data to posterior distributions of model parameters, bypassing\nconventional sampling-based Bayesian approaches and offering advantages for\nhigh-dimensional inference problems. Impedance behavior is modeled using a\ndamped oscillator model extended with a fractional calculus term. The framework\nis verified on a finite element model of a cuboid room and further tested with\nimpedance tube measurements used as reference, achieving robust and accurate\nestimation of all six individual impedances. Application to a numerical car\ncabin model further demonstrates reliable uncertainty quantification and high\npredictive accuracy even for complex-shaped geometries. Posterior predictive\nchecks and coverage diagnostics confirm well-calibrated inference, highlighting\nthe method's potential for generalizable, efficient, and physically consistent\ncharacterization of acoustic boundary conditions in real-world interior\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8d1d\u53f6\u65af\u6846\u67b6\u548c\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u4ece\u7a00\u758f\u58f0\u538b\u6d4b\u91cf\u4e2d\u51c6\u786e\u4f30\u8ba1\u9891\u7387\u76f8\u5173\u7684\u58f0\u5b66\u8868\u9762\u963b\u6297\uff0c\u514b\u670d\u4f20\u7edf\u6d4b\u91cf\u65b9\u6cd5\u7684\u5c40\u9650\u6027", "motivation": "\u4f20\u7edf\u58f0\u5b66\u8fb9\u754c\u6761\u4ef6\u6d4b\u91cf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5bf9\u58f0\u573a\u548c\u5b89\u88c5\u6761\u4ef6\u7684\u7b80\u5316\u5047\u8bbe\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u73b0\u573a\u963b\u6297\u4f30\u8ba1\u65b9\u6cd5", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u6846\u67b6\u548c\u57fa\u4e8e\u6a21\u62df\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u76f4\u63a5\u5c06\u6a21\u62df\u6570\u636e\u6620\u5c04\u5230\u6a21\u578b\u53c2\u6570\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u4f7f\u7528\u5e26\u5206\u6570\u9636\u5fae\u79ef\u5206\u9879\u7684\u963b\u5c3c\u632f\u8361\u5668\u6a21\u578b\u5efa\u6a21\u963b\u6297\u884c\u4e3a", "result": "\u5728\u957f\u65b9\u4f53\u623f\u95f4\u6709\u9650\u5143\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u901a\u8fc7\u963b\u6297\u7ba1\u6d4b\u91cf\u4f5c\u4e3a\u53c2\u8003\uff0c\u6210\u529f\u4f30\u8ba1\u6240\u6709\u516d\u4e2a\u72ec\u7acb\u963b\u6297\uff0c\u5728\u6570\u503c\u6c7d\u8f66\u5ea7\u8231\u6a21\u578b\u4e2d\u5c55\u793a\u53ef\u9760\u7684uncertainty\u91cf\u5316\u548c\u9ad8\u9884\u6d4b\u7cbe\u5ea6", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u826f\u597d\u6821\u51c6\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u771f\u5b9e\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u58f0\u5b66\u8fb9\u754c\u6761\u4ef6\u63d0\u4f9b\u4e86\u53ef\u63a8\u5e7f\u3001\u9ad8\u6548\u4e14\u7269\u7406\u4e00\u81f4\u7684\u8868\u5f81\u65b9\u6cd5"}}
{"id": "2509.09175", "categories": ["cs.SD", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.09175", "abs": "https://arxiv.org/abs/2509.09175", "authors": ["Zihan Pan", "Sailor Hardik Bhupendra", "Jinyang Wu"], "title": "MoLEx: Mixture of LoRA Experts in Speech Self-Supervised Models for Audio Deepfake Detection", "comment": null, "summary": "While self-supervised learning (SSL)-based models have boosted audio deepfake\ndetection accuracy, fully finetuning them is computationally expensive. To\naddress this, we propose a parameter-efficient framework that combines Low-Rank\nAdaptation with a Mixture-of-Experts router, called Mixture of LoRA Experts\n(MoLEx). It preserves pre-trained knowledge of SSL models while efficiently\nfinetuning only selected experts, reducing training costs while maintaining\nrobust performance. The observed utility of experts during inference shows the\nrouter reactivates the same experts for similar attacks but switches to other\nexperts for novel spoofs, confirming MoLEx's domain-aware adaptability. MoLEx\nadditionally offers flexibility for domain adaptation by allowing extra experts\nto be trained without modifying the entire model. We mainly evaluate our\napproach on the ASVSpoof 5 dataset and achieve the state-of-the-art (SOTA)\nequal error rate (EER) of 5.56% on the evaluation set without augmentation.", "AI": {"tldr": "\u63d0\u51faMoLEx\u6846\u67b6\uff0c\u7ed3\u5408LoRA\u548c\u6df7\u5408\u4e13\u5bb6\u8def\u7531\uff0c\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u7684\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\uff0c\u5728ASVSpoof 5\u6570\u636e\u96c6\u4e0a\u8fbe\u52305.56%\u7684SOTA EER", "motivation": "\u89e3\u51b3\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u5b8c\u5168\u5fae\u8c03\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u8bad\u7ec3\u77e5\u8bc6\u5e76\u964d\u4f4e\u8bad\u7ec3\u6210\u672c", "method": "\u7ed3\u5408\u4f4e\u79e9\u9002\u5e94(LoRA)\u548c\u6df7\u5408\u4e13\u5bb6\u8def\u7531\uff0c\u53ea\u5fae\u8c03\u9009\u5b9a\u7684\u4e13\u5bb6\u6a21\u5757\uff0c\u4fdd\u6301\u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\u4e0d\u53d8", "result": "\u5728ASVSpoof 5\u8bc4\u4f30\u96c6\u4e0a\u8fbe\u52305.56%\u7684\u7b49\u9519\u8bef\u7387(EER)\uff0c\u65e0\u9700\u6570\u636e\u589e\u5f3a\u5373\u53ef\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd", "conclusion": "MoLEx\u6846\u67b6\u5177\u6709\u53c2\u6570\u9ad8\u6548\u3001\u9886\u57df\u611f\u77e5\u9002\u5e94\u6027\u5f3a\u3001\u652f\u6301\u7075\u6d3b\u6269\u5c55\u7b49\u4f18\u52bf\uff0c\u4e3a\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09201", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.09201", "abs": "https://arxiv.org/abs/2509.09201", "authors": ["Xiaoxue Luo", "Jinwei Huang", "Runyan Yang", "Yingying Gao", "Junlan Feng", "Chao Deng", "Shilei Zhang"], "title": "DeCodec: Rethinking Audio Codecs as Universal Disentangled Representation Learners", "comment": null, "summary": "Universal audio codecs learn entangled representations across audio types,\nwhereas some specific codecs offer decoupled representations but are limited to\nspeech. Real-world audio, however, often contains mixed speech and background\nsounds, and downstream tasks require selective access to these components.\nTherefore, we rethink the audio codec as a universal disentangled\nrepresentation learner to enable controllable feature selection across\ndifferent audio tasks. To this end, we introduce DeCodec, a novel neural codec\nthat learns to decouple audio representations into orthogonal subspaces\ndedicated to speech and background sound, and within speech, representations\nare further decomposed into semantic and paralinguistic components. This\nhierarchical disentanglement allows flexible feature selection, making DeCodec\na universal front-end for multiple audio applications. Technically, built upon\na codec framework, DeCodec incorporates two key innovations: a subspace\northogonal projection module that factorizes the input into two decoupled\northogonal subspaces, and a representation swap training procedure that ensures\nthese two subspaces are correlate to the speech and background sound,\nrespectively. These allows parallel RVQs to quantize speech and background\nsound components independently. Furthermore, we employ semantic guidance to the\nspeech RVQ to achieve semantic and paralinguistic decomposition. Experimental\nresults show that DeCodec maintains advanced signal reconstruction while\nenabling new capabilities: superior speech enhancement and effective one-shot\nvoice conversion on noisy speech via representation recombination, improved ASR\nrobustness through clean semantic features, and controllable background sound\npreservation/suppression in TTS. Demo Page: https://luo404.github.io/DeCodecV2/", "AI": {"tldr": "DeCodec\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u795e\u7ecf\u7f16\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u5b66\u4e60\u5c06\u97f3\u9891\u8868\u793a\u89e3\u8026\u5230\u6b63\u4ea4\u5b50\u7a7a\u95f4\uff0c\u5206\u522b\u5904\u7406\u8bed\u97f3\u548c\u80cc\u666f\u58f0\u97f3\uff0c\u5e76\u5728\u8bed\u97f3\u5185\u90e8\u8fdb\u4e00\u6b65\u5206\u89e3\u4e3a\u8bed\u4e49\u548c\u526f\u8bed\u8a00\u6210\u5206\uff0c\u5b9e\u73b0\u53ef\u63a7\u7684\u7279\u5f81\u9009\u62e9\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u97f3\u9891\u901a\u5e38\u5305\u542b\u6df7\u5408\u7684\u8bed\u97f3\u548c\u80cc\u666f\u58f0\u97f3\uff0c\u800c\u4e0b\u6e38\u4efb\u52a1\u9700\u8981\u9009\u62e9\u6027\u8bbf\u95ee\u8fd9\u4e9b\u7ec4\u4ef6\u3002\u73b0\u6709\u901a\u7528\u97f3\u9891\u7f16\u89e3\u7801\u5668\u5b66\u4e60\u7ea0\u7f20\u8868\u793a\uff0c\u7279\u5b9a\u7f16\u89e3\u7801\u5668\u867d\u7136\u63d0\u4f9b\u89e3\u8026\u8868\u793a\u4f46\u4ec5\u9650\u4e8e\u8bed\u97f3\u3002", "method": "\u57fa\u4e8e\u7f16\u89e3\u7801\u5668\u6846\u67b6\uff0c\u5f15\u5165\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u5b50\u7a7a\u95f4\u6b63\u4ea4\u6295\u5f71\u6a21\u5757\u5c06\u8f93\u5165\u5206\u89e3\u4e3a\u4e24\u4e2a\u89e3\u8026\u7684\u6b63\u4ea4\u5b50\u7a7a\u95f4\uff1b\u8868\u793a\u4ea4\u6362\u8bad\u7ec3\u7a0b\u5e8f\u786e\u4fdd\u8fd9\u4e24\u4e2a\u5b50\u7a7a\u95f4\u5206\u522b\u4e0e\u8bed\u97f3\u548c\u80cc\u666f\u58f0\u97f3\u76f8\u5173\u3002\u4f7f\u7528\u5e76\u884cRVQ\u72ec\u7acb\u91cf\u5316\u8bed\u97f3\u548c\u80cc\u666f\u58f0\u97f3\u7ec4\u4ef6\uff0c\u5e76\u5bf9\u8bed\u97f3RVQ\u5e94\u7528\u8bed\u4e49\u6307\u5bfc\u5b9e\u73b0\u8bed\u4e49\u548c\u526f\u8bed\u8a00\u5206\u89e3\u3002", "result": "DeCodec\u5728\u4fdd\u6301\u5148\u8fdb\u4fe1\u53f7\u91cd\u5efa\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u65b0\u529f\u80fd\uff1a\u901a\u8fc7\u8868\u793a\u91cd\u7ec4\u5728\u566a\u58f0\u8bed\u97f3\u4e0a\u5b9e\u73b0\u4f18\u5f02\u7684\u8bed\u97f3\u589e\u5f3a\u548c\u6709\u6548\u7684\u4e00\u6b21\u6027\u8bed\u97f3\u8f6c\u6362\uff1b\u901a\u8fc7\u5e72\u51c0\u7684\u8bed\u4e49\u7279\u5f81\u63d0\u9ad8ASR\u9c81\u68d2\u6027\uff1b\u5728TTS\u4e2d\u5b9e\u73b0\u53ef\u63a7\u7684\u80cc\u666f\u58f0\u97f3\u4fdd\u7559/\u6291\u5236\u3002", "conclusion": "DeCodec\u4f5c\u4e3a\u4e00\u4e2a\u901a\u7528\u7684\u89e3\u8026\u8868\u793a\u5b66\u4e60\u5668\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u89e3\u8026\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684\u7279\u5f81\u9009\u62e9\uff0c\u4f7f\u5176\u6210\u4e3a\u591a\u4e2a\u97f3\u9891\u5e94\u7528\u7684\u901a\u7528\u524d\u7aef\u3002"}}
{"id": "2509.09204", "categories": ["cs.SD", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09204", "abs": "https://arxiv.org/abs/2509.09204", "authors": ["Chin Yuen Kwok", "Jia Qi Yip", "Zhen Qiu", "Chi Hung Chi", "Kwok Yan Lam"], "title": "Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection Systems", "comment": "Published in Interspeech 2025", "summary": "Audio deepfake detection (ADD) models are commonly evaluated using datasets\nthat combine multiple synthesizers, with performance reported as a single Equal\nError Rate (EER). However, this approach disproportionately weights\nsynthesizers with more samples, underrepresenting others and reducing the\noverall reliability of EER. Additionally, most ADD datasets lack diversity in\nbona fide speech, often featuring a single environment and speech style (e.g.,\nclean read speech), limiting their ability to simulate real-world conditions.\nTo address these challenges, we propose bona fide cross-testing, a novel\nevaluation framework that incorporates diverse bona fide datasets and\naggregates EERs for more balanced assessments. Our approach improves robustness\nand interpretability compared to traditional evaluation methods. We benchmark\nover 150 synthesizers across nine bona fide speech types and release a new\ndataset to facilitate further research at\nhttps://github.com/cyaaronk/audio_deepfake_eval.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u8bc4\u4f30\u6846\u67b6\u2014\u2014\u771f\u5b9e\u8bed\u97f3\u4ea4\u53c9\u6d4b\u8bd5\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6837\u5316\u7684\u771f\u5b9e\u8bed\u97f3\u6570\u636e\u96c6\u548c\u805a\u5408EER\u6765\u63d0\u4f9b\u66f4\u5e73\u8861\u7684\u8bc4\u4f30\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u5408\u6210\u5668\u6837\u672c\u4e0d\u5e73\u8861\u548c\u771f\u5b9e\u8bed\u97f3\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4f7f\u7528\u5355\u4e00EER\u6307\u6807\u4f1a\u56e0\u5408\u6210\u5668\u6837\u672c\u6570\u91cf\u4e0d\u5e73\u8861\u800c\u964d\u4f4e\u8bc4\u4f30\u53ef\u9760\u6027\uff1b2\uff09\u5927\u591a\u6570\u6570\u636e\u96c6\u7f3a\u4e4f\u771f\u5b9e\u8bed\u97f3\u7684\u591a\u6837\u6027\uff0c\u901a\u5e38\u53ea\u5305\u542b\u5355\u4e00\u73af\u5883\u548c\u8bed\u97f3\u98ce\u683c\uff0c\u65e0\u6cd5\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u3002", "method": "\u63d0\u51fa\u4e86\u771f\u5b9e\u8bed\u97f3\u4ea4\u53c9\u6d4b\u8bd5\u6846\u67b6\uff0c\u6574\u5408\u4e86\u591a\u6837\u5316\u7684\u771f\u5b9e\u8bed\u97f3\u6570\u636e\u96c6\uff0c\u91c7\u7528\u805a\u5408EER\u7684\u65b9\u5f0f\u8fdb\u884c\u66f4\u5e73\u8861\u7684\u8bc4\u4f30\u3002\u8be5\u65b9\u6cd5\u57289\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u771f\u5b9e\u8bed\u97f3\u4e0a\u5bf9\u8d85\u8fc7150\u4e2a\u5408\u6210\u5668\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5f00\u53d1\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\u3002\u76f8\u6bd4\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u771f\u5b9e\u8bed\u97f3\u4ea4\u53c9\u6d4b\u8bd5\u6846\u67b6\u4e3a\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u7814\u7a76\u3002"}}
{"id": "2509.08903", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08903", "abs": "https://arxiv.org/abs/2509.08903", "authors": ["Alex Clay", "Ernesto Jim\u00e9nez-Ruiz", "Pranava Madhyastha"], "title": "Noise or Nuance: An Investigation Into Useful Information and Filtering For LLM Driven AKBC", "comment": "8 pages, 1 figure, accepted to the ISWC 2025 LM-KBC Workshop", "summary": "RAG and fine-tuning are prevalent strategies for improving the quality of LLM\noutputs. However, in constrained situations, such as that of the 2025 LM-KBC\nchallenge, such techniques are restricted. In this work we investigate three\nfacets of the triple completion task: generation, quality assurance, and LLM\nresponse parsing. Our work finds that in this constrained setting: additional\ninformation improves generation quality, LLMs can be effective at filtering\npoor quality triples, and the tradeoff between flexibility and consistency with\nLLM response parsing is setting dependent.", "AI": {"tldr": "\u5728\u53d7\u9650\u73af\u5883\u4e0b\uff0c\u7814\u7a76\u53d1\u73b0\u989d\u5916\u4fe1\u606f\u63d0\u5347\u4e09\u5143\u7ec4\u751f\u6210\u8d28\u91cf\uff0cLLM\u80fd\u6709\u6548\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u4e09\u5143\u7ec4\uff0c\u89e3\u6790\u65b9\u5f0f\u9700\u5728\u7075\u6d3b\u6027\u4e0e\u4e00\u81f4\u6027\u95f4\u53d6\u8868", "motivation": "\u5728\u50cf2025 LM-KBC\u6311\u6218\u8fd9\u6837\u7684\u53d7\u9650\u73af\u5883\u4e0b\uff0cRAG\u548c\u5fae\u8c03\u7b49\u4f20\u7edf\u6539\u5584\u65b9\u6cd5\u53d7\u9650\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8LLM\u8f93\u51fa\u8d28\u91cf", "method": "\u7814\u7a76\u4e09\u5143\u7ec4\u5b8c\u6210\u4efb\u52a1\u7684\u4e09\u4e2a\u65b9\u9762\uff1a\u751f\u6210\u3001\u8d28\u91cf\u4fdd\u969c\u548cLLM\u54cd\u5e94\u89e3\u6790\uff0c\u5728\u53d7\u9650\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u5b9e\u9a8c\u5206\u6790", "result": "\u989d\u5916\u4fe1\u606f\u80fd\u663e\u8457\u63d0\u9ad8\u751f\u6210\u4e09\u5143\u7ec4\u7684\u8d28\u91cf\uff1bLLM\u5728\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u4e09\u5143\u7ec4\u65b9\u9762\u8868\u73b0\u6709\u6548\uff1bLLM\u54cd\u5e94\u89e3\u6790\u7684\u7075\u6d3b\u6027\u4e0e\u4e00\u81f4\u6027\u95f4\u7684\u4ea4\u6362\u5173\u7cfb\u53d6\u51b3\u4e8e\u5177\u4f53\u8bbe\u7f6e", "conclusion": "\u5728\u53d7\u9650\u73af\u5883\u4e0b\uff0c\u901a\u8fc7\u63d0\u4f9b\u989d\u5916\u4fe1\u606f\u3001\u5229\u7528LLM\u8fdb\u884c\u8d28\u91cf\u8fc7\u6ee4\u4ee5\u53ca\u6839\u636e\u5177\u4f53\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u54cd\u5e94\u89e3\u6790\u7b56\u7565\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u4e09\u5143\u7ec4\u5b8c\u6210\u4efb\u52a1\u7684\u6027\u80fd"}}
{"id": "2509.09262", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09262", "abs": "https://arxiv.org/abs/2509.09262", "authors": ["Seung Gyu Jeong", "Seong Eun Kim"], "title": "Adaptive Knowledge Distillation using a Device-Aware Teacher for Low-Complexity Acoustic Scene Classification", "comment": null, "summary": "In this technical report, we describe our submission for Task 1,\nLow-Complexity Device-Robust Acoustic Scene Classification, of the DCASE 2025\nChallenge. Our work tackles the dual challenges of strict complexity\nconstraints and robust generalization to both seen and unseen devices, while\nalso leveraging the new rule allowing the use of device labels at test time.\nOur proposed system is based on a knowledge distillation framework where an\nefficient CP-MobileNet student learns from a compact, specialized two-teacher\nensemble. This ensemble combines a baseline PaSST teacher, trained with\nstandard cross-entropy, and a 'generalization expert' teacher. This expert is\ntrained using our novel Device-Aware Feature Alignment (DAFA) loss, adapted\nfrom prior work, which explicitly structures the feature space for device\nrobustness. To capitalize on the availability of test-time device labels, the\ndistilled student model then undergoes a final device-specific fine-tuning\nstage. Our proposed system achieves a final accuracy of 57.93\\% on the\ndevelopment set, demonstrating a significant improvement over the official\nbaseline, particularly on unseen devices.", "AI": {"tldr": "\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u8f7b\u91cf\u7ea7\u97f3\u9891\u573a\u666f\u5206\u7c7b\u7cfb\u7edf\uff0c\u4f7f\u7528\u53cc\u6559\u5e08\u96c6\u6210\uff08\u6807\u51c6\u6559\u5e08+\u8bbe\u5907\u9c81\u68d2\u4e13\u5bb6\uff09\u8bad\u7ec3\u5b66\u751f\u7f51\u7edc\uff0c\u5e76\u901a\u8fc7\u8bbe\u5907\u7279\u5b9a\u5fae\u8c03\u63d0\u5347\u6027\u80fd\uff0c\u5728DCASE 2025\u6311\u6218\u8d5b\u4e2d\u53d6\u5f9757.93%\u7684\u51c6\u786e\u7387", "motivation": "\u89e3\u51b3\u4f4e\u590d\u6742\u5ea6\u8bbe\u5907\u9c81\u68d2\u97f3\u9891\u573a\u666f\u5206\u7c7b\u7684\u53cc\u91cd\u6311\u6218\uff1a\u4e25\u683c\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u7ea6\u675f\u548c\u5bf9\u5df2\u77e5/\u672a\u77e5\u8bbe\u5907\u7684\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u5229\u7528\u6d4b\u8bd5\u65f6\u8bbe\u5907\u6807\u7b7e\u7684\u65b0\u89c4\u5219", "method": "\u63d0\u51fa\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff1a\u9ad8\u6548CP-MobileNet\u5b66\u751f\u7f51\u7edc\u4ece\u7d27\u51d1\u7684\u53cc\u6559\u5e08\u96c6\u6210\u5b66\u4e60\uff08\u6807\u51c6PaSST\u6559\u5e08+\u8bbe\u5907\u611f\u77e5\u7279\u5f81\u5bf9\u9f50\u4e13\u5bb6\uff09\uff0c\u6700\u540e\u8fdb\u884c\u8bbe\u5907\u7279\u5b9a\u5fae\u8c03", "result": "\u5728\u5f00\u53d1\u96c6\u4e0a\u8fbe\u523057.93%\u7684\u6700\u7ec8\u51c6\u786e\u7387\uff0c\u76f8\u6bd4\u5b98\u65b9\u57fa\u7ebf\u6709\u663e\u8457\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u672a\u77e5\u8bbe\u5907\u4e0a\u8868\u73b0\u4f18\u5f02", "conclusion": "\u6240\u63d0\u51fa\u7684\u77e5\u8bc6\u84b8\u998f\u7ed3\u5408\u8bbe\u5907\u611f\u77e5\u7279\u5f81\u5bf9\u9f50\u548c\u8bbe\u5907\u7279\u5b9a\u5fae\u8c03\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u590d\u6742\u5ea6\u8bbe\u5907\u9c81\u68d2\u97f3\u9891\u573a\u666f\u5206\u7c7b\u95ee\u9898\uff0c\u5728DCASE\u6311\u6218\u4e2d\u8868\u73b0\u51fa\u8272"}}
{"id": "2509.08907", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08907", "abs": "https://arxiv.org/abs/2509.08907", "authors": ["Imene Kolli", "Ario Saeid Vaghefi", "Chiara Colesanti Senni", "Shantam Raj", "Markus Leippold"], "title": "Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement: A Multilingual RAG Approach", "comment": null, "summary": "InfluenceMap's LobbyMap Platform monitors the climate policy engagement of\nover 500 companies and 250 industry associations, assessing each entity's\nsupport or opposition to science-based policy pathways for achieving the Paris\nAgreement's goal of limiting global warming to 1.5{\\deg}C. Although\nInfluenceMap has made progress with automating key elements of the analytical\nworkflow, a significant portion of the assessment remains manual, making it\ntime- and labor-intensive and susceptible to human error. We propose an\nAI-assisted framework to accelerate the monitoring of corporate climate policy\nengagement by leveraging Retrieval-Augmented Generation to automate the most\ntime-intensive extraction of relevant evidence from large-scale textual data.\nOur evaluation shows that a combination of layout-aware parsing, the Nomic\nembedding model, and few-shot prompting strategies yields the best performance\nin extracting and classifying evidence from multilingual corporate documents.\nWe conclude that while the automated RAG system effectively accelerates\nevidence extraction, the nuanced nature of the analysis necessitates a\nhuman-in-the-loop approach where the technology augments, rather than replaces,\nexpert judgment to ensure accuracy.", "AI": {"tldr": "\u63d0\u51faAI\u8f85\u52a9\u6846\u67b6\uff0c\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u81ea\u52a8\u5316\u4ece\u4f01\u4e1a\u6587\u6863\u4e2d\u63d0\u53d6\u6c14\u5019\u653f\u7b56\u53c2\u4e0e\u8bc1\u636e\uff0c\u52a0\u901f\u4f01\u4e1a\u6c14\u5019\u653f\u7b56\u53c2\u4e0e\u76d1\u6d4b", "motivation": "\u73b0\u6709\u4f01\u4e1a\u6c14\u5019\u653f\u7b56\u53c2\u4e0e\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\uff0c\u8017\u65f6\u8017\u529b\u4e14\u6613\u51fa\u9519\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u63d0\u9ad8\u6548\u7387", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u7ed3\u5408\u5e03\u5c40\u611f\u77e5\u89e3\u6790\u3001Nomic\u5d4c\u5165\u6a21\u578b\u548c\u5c11\u6837\u672c\u63d0\u793a\u7b56\u7565\uff0c\u4ece\u591a\u8bed\u8a00\u4f01\u4e1a\u6587\u6863\u4e2d\u63d0\u53d6\u548c\u5206\u7c7b\u8bc1\u636e", "result": "\u8bc4\u4f30\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u8bc1\u636e\u63d0\u53d6\u548c\u5206\u7c7b\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u80fd\u6709\u6548\u52a0\u901f\u8bc1\u636e\u63d0\u53d6\u8fc7\u7a0b", "conclusion": "\u81ea\u52a8\u5316RAG\u7cfb\u7edf\u80fd\u6709\u6548\u52a0\u901f\u8bc1\u636e\u63d0\u53d6\uff0c\u4f46\u7531\u4e8e\u5206\u6790\u7684\u590d\u6742\u6027\uff0c\u4ecd\u9700\u91c7\u7528\u4eba\u673a\u534f\u540c\u65b9\u5f0f\uff0c\u6280\u672f\u5e94\u589e\u5f3a\u800c\u975e\u66ff\u4ee3\u4e13\u5bb6\u5224\u65ad"}}
{"id": "2509.09318", "categories": ["cs.SD", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.09318", "abs": "https://arxiv.org/abs/2509.09318", "authors": ["Weixing Wei", "Kazuyoshi Yoshii"], "title": "Efficient Transformer-Based Piano Transcription With Sparse Attention Mechanisms", "comment": "Accepted by APSIPA 2025", "summary": "This paper investigates automatic piano transcription based on\ncomputationally-efficient yet high-performant variants of the Transformer that\ncan capture longer-term dependency over the whole musical piece. Recently,\ntransformer-based sequence-to-sequence models have demonstrated excellent\nperformance in piano transcription. These models, however, fail to deal with\nthe whole piece at once due to the quadratic complexity of the self-attention\nmechanism, and music signals are thus typically processed in a sliding-window\nmanner in practice. To overcome this limitation, we propose an efficient\narchitecture with sparse attention mechanisms. Specifically, we introduce\nsliding-window self-attention mechanisms for both the encoder and decoder, and\na hybrid global-local cross-attention mechanism that attends to various spans\naccording to the MIDI token types. We also use a hierarchical pooling strategy\nbetween the encoder and decoder to further reduce computational load. Our\nexperiments on the MAESTRO dataset showed that the proposed model achieved a\nsignificant reduction in computational cost and memory usage, accelerating\ninference speed, while maintaining transcription performance comparable to the\nfull-attention baseline. This allows for training with longer audio contexts on\nthe same hardware, demonstrating the viability of sparse attention for building\nefficient and high-performance piano transcription systems. The code is\navailable at https://github.com/WX-Wei/efficient-seq2seq-piano-trans.", "AI": {"tldr": "\u901a\u8fc7\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u6539\u8fdbTransformer\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u94a2\u7434\u8f6c\u8bd1\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u5360\u7528\uff0c\u652f\u6301\u66f4\u957f\u97f3\u9891\u4e0a\u4e0b\u6587\u5904\u7406\u3002", "motivation": "\u73b0\u6709Transformer\u6a21\u578b\u5728\u94a2\u7434\u8f6c\u8bd1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u5e73\u65b9\u590d\u6742\u5ea6\u5bfc\u81f4\u65e0\u6cd5\u5904\u7406\u6574\u4e2a\u97f3\u4e50\u4f5c\u54c1\uff0c\u901a\u5e38\u9700\u8981\u6ed1\u52a8\u7a97\u53e3\u65b9\u5f0f\u5904\u7406\uff0c\u5f71\u54cd\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff1a\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u90fd\u91c7\u7528\u6ed1\u52a8\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\uff0c\u4ee5\u53ca\u6df7\u5408\u5168\u5c40-\u5c40\u90e8\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6839\u636eMIDI\u6807\u7b7e\u7c7b\u578b\u5173\u6ce8\u4e0d\u540c\u8303\u56f4\u3002\u8fd8\u4f7f\u7528\u4e86\u5c42\u6b21\u6c60\u5316\u7b56\u7565\u6765\u8fdb\u4e00\u6b65\u964d\u4f4e\u8ba1\u7b97\u8d1f\u62c5\u3002", "result": "\u5728MAESTRO\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u6a21\u578b\u5728\u4fdd\u6301\u4e0e\u5168\u6ce8\u610f\u529b\u57fa\u7ebf\u76f8\u4f3c\u8f6c\u8bd1\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u4f7f\u7528\u7684\u663e\u8457\u964d\u4f4e\uff0c\u52a0\u5feb\u4e86\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u53ef\u4ee5\u5efa\u7acb\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u94a2\u7434\u8f6c\u8bd1\u7cfb\u7edf\uff0c\u5141\u8bb8\u5728\u540c\u6837\u786c\u4ef6\u4e0a\u8bad\u7ec3\u66f4\u957f\u97f3\u9891\u4e0a\u4e0b\u6587\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.08920", "categories": ["cs.CL", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.08920", "abs": "https://arxiv.org/abs/2509.08920", "authors": ["Jinsong Chen"], "title": "Documents Are People and Words Are Items: A Psychometric Approach to Textual Data with Contextual Embeddings", "comment": null, "summary": "This research introduces a novel psychometric method for analyzing textual\ndata using large language models. By leveraging contextual embeddings to create\ncontextual scores, we transform textual data into response data suitable for\npsychometric analysis. Treating documents as individuals and words as items,\nthis approach provides a natural psychometric interpretation under the\nassumption that certain keywords, whose contextual meanings vary significantly\nacross documents, can effectively differentiate documents within a corpus. The\nmodeling process comprises two stages: obtaining contextual scores and\nperforming psychometric analysis. In the first stage, we utilize natural\nlanguage processing techniques and encoder based transformer models to identify\ncommon keywords and generate contextual scores. In the second stage, we employ\nvarious types of factor analysis, including exploratory and bifactor models, to\nextract and define latent factors, determine factor correlations, and identify\nthe most significant words associated with each factor. Applied to the Wiki\nSTEM corpus, our experimental results demonstrate the method's potential to\nuncover latent knowledge dimensions and patterns within textual data. This\napproach not only enhances the psychometric analysis of textual data but also\nholds promise for applications in fields rich in textual information, such as\neducation, psychology, and law.", "AI": {"tldr": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u9898\u5fc3\u7406\u6d4b\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5d4c\u5165\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u53ef\u5206\u6790\u7684\u54cd\u5e94\u6570\u636e\uff0c\u63a2\u7d22\u6587\u6863\u4e2d\u7684\u6f5c\u5728\u77e5\u8bc6\u7ef4\u5ea6", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u5bf9\u6587\u672c\u6570\u636e\u8fdb\u884c\u5fc3\u7406\u6d4b\u91cf\u5206\u6790\uff0c\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u53ef\u5206\u6790\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u4ee5\u6d89\u53ca\u6559\u80b2\u3001\u5fc3\u7406\u5b66\u7b49\u9886\u57df", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528NLP\u6280\u672f\u548ctransformer\u6a21\u578b\u83b7\u53d6\u4e0a\u4e0b\u6587\u5f97\u5206\uff1b\u7b2c\u4e8c\u9636\u6bb5\u91c7\u7528\u56e0\u5b50\u5206\u6790\uff08\u63a2\u7d22\u6027\u3001\u53cc\u56e0\u5b50\u6a21\u578b\uff09\u63d0\u53d6\u6f5c\u5728\u56e0\u5b50\u548c\u5173\u8054\u5173\u7cfb", "result": "\u5728Wiki STEM\u8bed\u6599\u5e93\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u53d1\u73b0\u6587\u672c\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u77e5\u8bc6\u7ef4\u5ea6\u548c\u6a21\u5f0f", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u6587\u672c\u6570\u636e\u7684\u5fc3\u7406\u6d4b\u91cf\u5206\u6790\u80fd\u529b\uff0c\u8fd8\u5728\u6559\u80b2\u3001\u5fc3\u7406\u5b66\u3001\u6cd5\u5f8b\u7b49\u9886\u57df\u5177\u6709\u5e7f\u9614\u7684\u5e94\u7528\u524d\u666f"}}
{"id": "2509.09550", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09550", "abs": "https://arxiv.org/abs/2509.09550", "authors": ["Harry Julia", "Rachel Beeson", "Lohith Konathala", "Johanna Ulin", "Jiameng Gao"], "title": "Finite Scalar Quantization Enables Redundant and Transmission-Robust Neural Audio Compression at Low Bit-rates", "comment": null, "summary": "Neural Audio Codecs (NACs) have become increasingly adopted in speech\nprocessing tasks due to their excellent rate-distortion performance and\ncompatibility with Large Language Models (LLMs) as discrete feature\nrepresentations for audio generation. While most existing codecs rely on\nResidual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has\nrecently emerged as a compelling alternative that simplifies training and\nnatively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC,\nand show that FSQ encodes baked-in redundancy which produces an encoding which\nis robust when transmitted through noisy channels. First, through an encoder\ndistillation experiment, we show that two different encoders can learn to\nencode identical audio into vastly different code sequences whilst maintaining\ncomparable reconstruction quality with the same quantizer and decoder. Second,\nwe demonstrate that FSQ has vastly superior bit-level perturbation robustness\nby comparing the performance of RVQ and FSQ codecs when simulating the\ntransmission of code sequences through a noisy channel.", "AI": {"tldr": "NeuCodec\u662f\u57fa\u4e8eFSQ\u7684\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\uff0c\u76f8\u6bd4\u4f20\u7edfRVQ\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6297\u566a\u6027\u80fd\u548c\u7f16\u7801\u5197\u4f59\u7279\u6027\uff0c\u80fd\u5728\u566a\u58f0\u4fe1\u9053\u4e2d\u4fdd\u6301\u66f4\u597d\u7684\u97f3\u9891\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u5927\u591a\u57fa\u4e8eRVQ\uff0c\u800cFSQ\u4f5c\u4e3a\u65b0\u5174\u7684\u91cf\u5316\u65b9\u6cd5\u5177\u6709\u8bad\u7ec3\u7b80\u5355\u548c\u5355\u7801\u672c\u652f\u6301\u7684\u4f18\u52bf\uff0c\u4f46\u9700\u8981\u9a8c\u8bc1\u5176\u5728\u97f3\u9891\u7f16\u7801\u4e2d\u7684\u5b9e\u9645\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eFSQ\u7684NeuCodec\u7f16\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u7f16\u7801\u5668\u84b8\u998f\u5b9e\u9a8c\u9a8c\u8bc1\u4e0d\u540c\u7f16\u7801\u5668\u80fd\u4ea7\u751f\u4e0d\u540c\u4f46\u7b49\u4ef7\u7684\u7f16\u7801\u5e8f\u5217\uff0c\u5e76\u6bd4\u8f83FSQ\u548cRVQ\u5728\u6a21\u62df\u566a\u58f0\u4fe1\u9053\u4f20\u8f93\u65f6\u7684\u6bd4\u7279\u7ea7\u6270\u52a8\u9c81\u68d2\u6027\u3002", "result": "FSQ\u7f16\u7801\u5177\u6709\u5185\u7f6e\u5197\u4f59\u7279\u6027\uff0c\u4e24\u4e2a\u4e0d\u540c\u7f16\u7801\u5668\u80fd\u5b66\u4e60\u5230\u5b8c\u5168\u4e0d\u540c\u7684\u7f16\u7801\u5e8f\u5217\u4f46\u4fdd\u6301\u76f8\u540c\u7684\u91cd\u5efa\u8d28\u91cf\uff1bFSQ\u5728\u566a\u58f0\u4fe1\u9053\u4e2d\u7684\u6bd4\u7279\u7ea7\u6270\u52a8\u9c81\u68d2\u6027\u663e\u8457\u4f18\u4e8eRVQ\u3002", "conclusion": "FSQ\u662fRVQ\u7684\u6709\u529b\u66ff\u4ee3\u65b9\u6848\uff0cNeuCodec\u5c55\u793a\u4e86FSQ\u5728\u97f3\u9891\u7f16\u7801\u4e2d\u7684\u4f18\u8d8a\u6027\uff0c\u7279\u522b\u662f\u5728\u566a\u58f0\u4fe1\u9053\u4f20\u8f93\u573a\u666f\u4e0b\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u8868\u73b0\u3002"}}
{"id": "2509.08960", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08960", "abs": "https://arxiv.org/abs/2509.08960", "authors": ["Thales Sales Almeida", "Giovana Kerche Bon\u00e1s", "Jo\u00e3o Guilherme Alves Santos"], "title": "BRoverbs -- Measuring how much LLMs understand Portuguese proverbs", "comment": null, "summary": "Large Language Models (LLMs) exhibit significant performance variations\ndepending on the linguistic and cultural context in which they are applied.\nThis disparity signals the necessity of mature evaluation frameworks that can\nassess their capabilities in specific regional settings. In the case of\nPortuguese, existing evaluations remain limited, often relying on translated\ndatasets that may not fully capture linguistic nuances or cultural references.\nMeanwhile, native Portuguese-language datasets predominantly focus on\nstructured national exams or sentiment analysis of social media interactions,\nleaving gaps in evaluating broader linguistic understanding. To address this\nlimitation, we introduce BRoverbs, a dataset specifically designed to assess\nLLM performance through Brazilian proverbs. Proverbs serve as a rich linguistic\nresource, encapsulating cultural wisdom, figurative expressions, and complex\nsyntactic structures that challenge the model comprehension of regional\nexpressions. BRoverbs aims to provide a new evaluation tool for\nPortuguese-language LLMs, contributing to advancing regionally informed\nbenchmarking. The benchmark is available at\nhttps://huggingface.co/datasets/Tropic-AI/BRoverbs.", "AI": {"tldr": "BRoverbs\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u5df4\u897f\u8461\u8404\u7259\u8bed\u7684\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5df4\u897f\u8c1a\u8bed\u6765\u6d4b\u8bd5\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533a\u57df\u8bed\u8a00\u6587\u5316\u7406\u89e3\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u8461\u8404\u7259\u8bed\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u7ffb\u8bd1\u6570\u636e\u96c6\u6216\u5c40\u9650\u4e8e\u7ed3\u6784\u5316\u8003\u8bd5\u548c\u793e\u4ea4\u5a92\u4f53\u60c5\u611f\u5206\u6790\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u8bed\u8a00\u7ec6\u5fae\u5dee\u522b\u548c\u6587\u5316\u53c2\u8003\uff0c\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u533a\u57df\u7684\u6210\u719f\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u521b\u5efaBRoverbs\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u5df4\u897f\u8c1a\u8bed\u4f5c\u4e3a\u8bc4\u4f30\u5de5\u5177\uff0c\u8fd9\u4e9b\u8c1a\u8bed\u5305\u542b\u6587\u5316\u667a\u6167\u3001\u6bd4\u55bb\u8868\u8fbe\u548c\u590d\u6742\u53e5\u6cd5\u7ed3\u6784\uff0c\u80fd\u591f\u6709\u6548\u6d4b\u8bd5\u6a21\u578b\u5bf9\u533a\u57df\u8868\u8fbe\u7684\u7406\u89e3\u80fd\u529b\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u5df4\u897f\u8461\u8404\u7259\u8bed\u7684\u65b0\u8bc4\u4f30\u57fa\u51c6\uff0c\u53ef\u7528\u4e8e\u8bc4\u4f30\u8461\u8404\u7259\u8bed\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533a\u57df\u8bed\u8a00\u6587\u5316\u7406\u89e3\u65b9\u9762\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "BRoverbs\u586b\u8865\u4e86\u8461\u8404\u7259\u8bed\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u8461\u8404\u7259\u8bed\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u533a\u57df\u5316\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u8fdb\u57fa\u4e8e\u533a\u57df\u4fe1\u606f\u7684\u57fa\u51c6\u6d4b\u8bd5\u53d1\u5c55\u3002"}}
{"id": "2509.09631", "categories": ["cs.SD", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09631", "abs": "https://arxiv.org/abs/2509.09631", "authors": ["Ngoc-Son Nguyen", "Hieu-Nghia Huynh-Nguyen", "Thanh V. T. Tran", "Truong-Son Hy", "Van Nguyen"], "title": "DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for Low-Latency Zero-Shot Text-To-Speech", "comment": null, "summary": "Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that\nmimics the voice of an unseen speaker using only a short reference sample,\nrequiring not only speaker adaptation but also accurate modeling of prosodic\nattributes. Recent approaches based on language models, diffusion, and flow\nmatching have shown promising results in zero-shot TTS, but still suffer from\nslow inference and repetition artifacts. Discrete codec representations have\nbeen widely adopted for speech synthesis, and recent works have begun to\nexplore diffusion models in purely discrete settings, suggesting the potential\nof discrete generative modeling for speech synthesis. However, existing\nflow-matching methods typically embed these discrete tokens into a continuous\nspace and apply continuous flow matching, which may not fully leverage the\nadvantages of discrete representations. To address these challenges, we\nintroduce DiFlow-TTS, which, to the best of our knowledge, is the first model\nto explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS\nexplicitly models factorized speech attributes within a compact and unified\narchitecture. It leverages in-context learning by conditioning on textual\ncontent, along with prosodic and acoustic attributes extracted from a reference\nspeech, enabling effective attribute cloning in a zero-shot setting. In\naddition, the model employs a factorized flow prediction mechanism with\ndistinct heads for prosody and acoustic details, allowing it to learn\naspect-specific distributions. Experimental results demonstrate that DiFlow-TTS\nachieves promising performance in several key metrics, including naturalness,\nprosody, preservation of speaker style, and energy control. It also maintains a\ncompact model size and achieves low-latency inference, generating speech up to\n25.8 times faster than the latest existing baselines.", "AI": {"tldr": "DiFlow-TTS\u662f\u9996\u4e2a\u63a2\u7d22\u7eaf\u79bb\u6563\u6d41\u5339\u914d\u7684\u8bed\u97f3\u5408\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5206\u89e3\u7684\u8bed\u97f3\u5c5e\u6027\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u8bed\u97f3\u5408\u6210\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u73b0\u6709\u57fa\u7ebf\u5feb25.8\u500d", "motivation": "\u89e3\u51b3\u73b0\u6709\u96f6\u6837\u672cTTS\u65b9\u6cd5\u63a8\u7406\u901f\u5ea6\u6162\u3001\u5b58\u5728\u91cd\u590d\u4f2a\u5f71\u7684\u95ee\u9898\uff0c\u5145\u5206\u5229\u7528\u79bb\u6563\u8868\u793a\u7684\u4f18\u52bf\uff0c\u907f\u514d\u5c06\u79bb\u6563token\u5d4c\u5165\u8fde\u7eed\u7a7a\u95f4\u7684\u505a\u6cd5", "method": "\u91c7\u7528\u7eaf\u79bb\u6563\u6d41\u5339\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\uff0c\u57fa\u4e8e\u6587\u672c\u5185\u5bb9\u548c\u53c2\u8003\u8bed\u97f3\u63d0\u53d6\u7684\u97f5\u5f8b\u3001\u58f0\u5b66\u5c5e\u6027\u8fdb\u884c\u6761\u4ef6\u751f\u6210\uff0c\u4f7f\u7528\u5206\u89e3\u7684\u6d41\u9884\u6d4b\u673a\u5236\u5206\u522b\u5904\u7406\u97f5\u5f8b\u548c\u58f0\u5b66\u7ec6\u8282", "result": "\u5728\u81ea\u7136\u5ea6\u3001\u97f5\u5f8b\u3001\u8bf4\u8bdd\u4eba\u98ce\u683c\u4fdd\u6301\u548c\u80fd\u91cf\u63a7\u5236\u7b49\u5173\u952e\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6a21\u578b\u7d27\u51d1\u4e14\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u63a8\u7406\uff0c\u751f\u6210\u901f\u5ea6\u6bd4\u6700\u65b0\u57fa\u7ebf\u5feb25.8\u500d", "conclusion": "DiFlow-TTS\u8bc1\u660e\u4e86\u7eaf\u79bb\u6563\u6d41\u5339\u914d\u5728\u8bed\u97f3\u5408\u6210\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u96f6\u6837\u672cTTS\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09013", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09013", "abs": "https://arxiv.org/abs/2509.09013", "authors": ["Monjoy Narayan Choudhury", "Junling Wang", "Yifan Hou", "Mrinmaya Sachan"], "title": "Can Vision-Language Models Solve Visual Math Equations?", "comment": "Monjoy Narayan Choudhury and Junling Wang contributed equally to this\n  work. Accepted at EMNLP2025 main. Code and datasets are open-sourced with\n  links in the paper", "summary": "Despite strong performance in visual understanding and language-based\nreasoning, Vision-Language Models (VLMs) struggle with tasks requiring\nintegrated perception and symbolic computation. We study this limitation\nthrough visual equation solving, where mathematical equations are embedded in\nimages, variables are represented by object icons, and coefficients must be\ninferred by counting. While VLMs perform well on textual equations, they fail\non visually grounded counterparts. To understand this gap, we decompose the\ntask into coefficient counting and variable recognition, and find that counting\nis the primary bottleneck, even when recognition is accurate. We also observe\nthat composing recognition and reasoning introduces additional errors,\nhighlighting challenges in multi-step visual reasoning. Finally, as equation\ncomplexity increases, symbolic reasoning itself becomes a limiting factor.\nThese findings reveal key weaknesses in current VLMs and point toward future\nimprovements in visually grounded mathematical reasoning.", "AI": {"tldr": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u65b9\u7a0b\u6c42\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u7cfb\u6570\u8ba1\u6570\u548c\u7b26\u53f7\u63a8\u7406\u80fd\u529b\u4e0d\u8db3", "motivation": "\u7814\u7a76\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u611f\u77e5\u4e0e\u7b26\u53f7\u8ba1\u7b97\u6574\u5408\u7684\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u89c6\u89c9\u65b9\u7a0b\u6c42\u89e3\u95ee\u9898", "method": "\u5c06\u89c6\u89c9\u65b9\u7a0b\u6c42\u89e3\u4efb\u52a1\u5206\u89e3\u4e3a\u7cfb\u6570\u8ba1\u6570\u548c\u53d8\u91cf\u8bc6\u522b\u4e24\u4e2a\u5b50\u4efb\u52a1\u8fdb\u884c\u5206\u6790\uff0c\u7814\u7a76\u4e0d\u540c\u590d\u6742\u5ea6\u65b9\u7a0b\u4e0b\u7684\u8868\u73b0", "result": "VLMs\u5728\u6587\u672c\u65b9\u7a0b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u89c6\u89c9\u65b9\u7a0b\u4e0a\u5931\u8d25\uff1b\u8ba1\u6570\u662f\u4e3b\u8981\u74f6\u9888\uff1b\u591a\u6b65\u9aa4\u63a8\u7406\u4f1a\u5f15\u5165\u989d\u5916\u9519\u8bef\uff1b\u968f\u7740\u65b9\u7a0b\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u7b26\u53f7\u63a8\u7406\u672c\u8eab\u6210\u4e3a\u9650\u5236\u56e0\u7d20", "conclusion": "\u5f53\u524dVLMs\u5728\u89c6\u89c9\u6570\u5b66\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5173\u952e\u5f31\u70b9\uff0c\u9700\u8981\u6539\u8fdb\u8ba1\u6570\u80fd\u529b\u548c\u591a\u6b65\u9aa4\u89c6\u89c9\u63a8\u7406\u80fd\u529b"}}
{"id": "2509.09174", "categories": ["cs.CL", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.09174", "abs": "https://arxiv.org/abs/2509.09174", "authors": ["Yuhao Zhang", "Yuhao Du", "Zhanchen Dai", "Xiangnan Ma", "Kaiqi Kou", "Benyou Wang", "Haizhou Li"], "title": "EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs", "comment": null, "summary": "Speech-to-speech large language models (SLLMs) are attracting increasing\nattention. Derived from text-based large language models (LLMs), SLLMs often\nexhibit degradation in knowledge and reasoning capabilities. We hypothesize\nthat this limitation arises because current training paradigms for SLLMs fail\nto bridge the acoustic-semantic gap in the feature representation space. To\naddress this issue, we propose EchoX, which leverages semantic representations\nand dynamically generates speech training targets. This approach integrates\nboth acoustic and semantic learning, enabling EchoX to preserve strong\nreasoning abilities as a speech LLM. Experimental results demonstrate that\nEchoX, with about six thousand hours of training data, achieves advanced\nperformance on multiple knowledge-based question-answering benchmarks. The\nproject is available at https://github.com/FreedomIntelligence/EchoX.", "AI": {"tldr": "EchoX\u662f\u4e00\u4e2a\u8bed\u97f3-\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u4e49\u8868\u793a\u548c\u52a8\u6001\u751f\u6210\u8bed\u97f3\u8bad\u7ec3\u76ee\u6807\u6765\u89e3\u51b3\u58f0\u5b66-\u8bed\u4e49\u5dee\u8ddd\u95ee\u9898\uff0c\u5728\u77e5\u8bc6\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02", "motivation": "\u5f53\u524d\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u4e0a\u5b58\u5728\u9000\u5316\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8bad\u7ec3\u8303\u5f0f\u672a\u80fd\u5f25\u5408\u7279\u5f81\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u58f0\u5b66-\u8bed\u4e49\u5dee\u8ddd", "method": "\u63d0\u51faEchoX\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u4e49\u8868\u793a\u5e76\u52a8\u6001\u751f\u6210\u8bed\u97f3\u8bad\u7ec3\u76ee\u6807\uff0c\u6574\u5408\u58f0\u5b66\u548c\u8bed\u4e49\u5b66\u4e60", "result": "\u4f7f\u7528\u7ea66000\u5c0f\u65f6\u8bad\u7ec3\u6570\u636e\uff0c\u5728\u591a\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u7684\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u5148\u8fdb\u6027\u80fd", "conclusion": "EchoX\u6210\u529f\u89e3\u51b3\u4e86\u8bed\u97f3LLM\u4e2d\u7684\u58f0\u5b66-\u8bed\u4e49\u8868\u793a\u95ee\u9898\uff0c\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b"}}
{"id": "2509.09043", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.09043", "abs": "https://arxiv.org/abs/2509.09043", "authors": ["Thomas Manuel Rost", "Martina Figlia", "Bernd Wallraff"], "title": "Stated Preference for Interaction and Continued Engagement (SPICE): Evaluating an LLM's Willingness to Re-engage in Conversation", "comment": null, "summary": "We introduce and evaluate Stated Preference for Interaction and Continued\nEngagement (SPICE), a simple diagnostic signal elicited by asking a Large\nLanguage Model a YES or NO question about its willingness to re-engage with a\nuser's behavior after reviewing a short transcript. In a study using a 3-tone\n(friendly, unclear, abusive) by 10-interaction stimulus set, we tested four\nopen-weight chat models across four framing conditions, resulting in 480\ntrials. Our findings show that SPICE sharply discriminates by user tone.\nFriendly interactions yielded a near-unanimous preference to continue (97.5%\nYES), while abusive interactions yielded a strong preference to discontinue\n(17.9% YES), with unclear interactions falling in between (60.4% YES). This\ncore association remains decisive under multiple dependence-aware statistical\ntests, including Rao-Scott adjustment and cluster permutation tests.\nFurthermore, we demonstrate that SPICE provides a distinct signal from abuse\nclassification. In trials where a model failed to identify abuse, it still\noverwhelmingly stated a preference not to continue the interaction (81% of the\ntime). An exploratory analysis also reveals a significant interaction effect: a\npreamble describing the study context significantly impacts SPICE under\nambiguity, but only when transcripts are presented as a single block of text\nrather than a multi-turn chat. The results validate SPICE as a robust,\nlow-overhead, and reproducible tool for auditing model dispositions,\ncomplementing existing metrics by offering a direct, relational signal of a\nmodel's state. All stimuli, code, and analysis scripts are released to support\nreplication.", "AI": {"tldr": "SPICE\u662f\u4e00\u79cd\u901a\u8fc7\u8be2\u95eeLLM\u662f\u5426\u613f\u610f\u7ee7\u7eed\u4e0e\u7528\u6237\u4e92\u52a8\u7684\u7b80\u5355\u8bca\u65ad\u4fe1\u53f7\uff0c\u80fd\u591f\u6709\u6548\u533a\u5206\u4e0d\u540c\u7528\u6237\u8bed\u6c14\u4e0b\u7684\u6a21\u578b\u504f\u597d\uff0c\u4e3a\u6a21\u578b\u5ba1\u8ba1\u63d0\u4f9b\u76f4\u63a5\u7684\u5173\u7cfb\u4fe1\u53f7\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u7b80\u5355\u3001\u4f4e\u5f00\u9500\u7684\u8bca\u65ad\u5de5\u5177\u6765\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u7528\u6237\u884c\u4e3a\u7684\u7ee7\u7eed\u4e92\u52a8\u610f\u613f\uff0c\u8865\u5145\u73b0\u6709\u7684\u6a21\u578b\u8bc4\u4f30\u6307\u6807\u3002", "method": "\u4f7f\u75283\u79cd\u7528\u6237\u8bed\u6c14\uff08\u53cb\u597d\u3001\u6a21\u7cca\u3001\u8fb1\u9a82\uff09\u548c10\u79cd\u4e92\u52a8\u573a\u666f\u7684\u523a\u6fc0\u96c6\uff0c\u6d4b\u8bd54\u4e2a\u5f00\u6e90\u804a\u5929\u6a21\u578b\u57284\u79cd\u6846\u67b6\u6761\u4ef6\u4e0b\u7684SPICE\u54cd\u5e94\uff0c\u5171480\u6b21\u8bd5\u9a8c\u3002", "result": "SPICE\u80fd\u6e05\u6670\u533a\u5206\u7528\u6237\u8bed\u6c14\uff1a\u53cb\u597d\u4e92\u52a897.5%\u613f\u610f\u7ee7\u7eed\uff0c\u8fb1\u9a82\u4e92\u52a8\u4ec517.9%\u613f\u610f\u7ee7\u7eed\uff0c\u6a21\u7cca\u4e92\u52a860.4%\u613f\u610f\u7ee7\u7eed\u3002\u5373\u4f7f\u5728\u6a21\u578b\u672a\u80fd\u8bc6\u522b\u8fb1\u9a82\u65f6\uff0c81%\u7684\u60c5\u51b5\u4e0b\u4ecd\u8868\u793a\u4e0d\u613f\u7ee7\u7eed\u4e92\u52a8\u3002", "conclusion": "SPICE\u662f\u4e00\u4e2a\u7a33\u5065\u3001\u4f4e\u5f00\u9500\u4e14\u53ef\u590d\u73b0\u7684\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u5ba1\u8ba1\u6a21\u578b\u503e\u5411\uff0c\u901a\u8fc7\u63d0\u4f9b\u76f4\u63a5\u7684\u3001\u5173\u7cfb\u6027\u7684\u6a21\u578b\u72b6\u6001\u4fe1\u53f7\u6765\u8865\u5145\u73b0\u6709\u6307\u6807\u3002"}}
{"id": "2509.09055", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09055", "abs": "https://arxiv.org/abs/2509.09055", "authors": ["Piyush Pant"], "title": "Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M", "comment": "17 pages, 3 figures. Code and dataset available at\n  https://github.com/PiyushWithPant/Improving-LLM-Safety-and-Helpfulness-using-SFT-and-DPO", "summary": "This research investigates the effectiveness of alignment techniques,\nSupervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a\ncombined SFT+DPO approach on improving the safety and helpfulness of the\nOPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset,\nwe train and evaluate four models: the base OPT350M, an SFT model, a DPO model,\nand a model trained with both SFT and DPO. We introduce three key evaluation\nmetrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined\nAlignment Score (CAS), all derived from reward model outputs. The results show\nthat while SFT outperforms DPO, The combined SFT+DPO model outperforms all\nothers across all metrics, demonstrating the complementary nature of these\ntechniques. Our findings also highlight challenges posed by noisy data, limited\nGPU resources, and training constraints. This study offers a comprehensive view\nof how fine-tuning strategies affect model alignment and provides a foundation\nfor more robust alignment pipelines in future work.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86SFT\u3001DPO\u548cSFT+DPO\u4e09\u79cd\u5bf9\u9f50\u6280\u672f\u5728OPT-350M\u6a21\u578b\u4e0a\u7684\u6548\u679c\uff0c\u53d1\u73b0\u7ec4\u5408\u65b9\u6cd5\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7814\u7a76\u4e0d\u540c\u5bf9\u9f50\u6280\u672f\uff08SFT\u3001DPO\u53ca\u5176\u7ec4\u5408\uff09\u5bf9\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u6027\u548c\u6709\u7528\u6027\u7684\u6539\u8fdb\u6548\u679c\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u5bf9\u9f50\u6d41\u7a0b\u63d0\u4f9b\u57fa\u7840\u3002", "method": "\u4f7f\u7528Anthropic Helpful-Harmless RLHF\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u56db\u4e2a\u6a21\u578b\uff1a\u57fa\u7840OPT350M\u3001SFT\u6a21\u578b\u3001DPO\u6a21\u578b\u548cSFT+DPO\u7ec4\u5408\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e09\u4e2a\u8bc4\u4f30\u6307\u6807\uff1a\u65e0\u5bb3\u7387(HmR)\u3001\u6709\u7528\u7387(HpR)\u548c\u7ec4\u5408\u5bf9\u9f50\u5206\u6570(CAS)\u3002", "result": "SFT\u8868\u73b0\u4f18\u4e8eDPO\uff0c\u4f46SFT+DPO\u7ec4\u5408\u6a21\u578b\u5728\u6240\u6709\u6307\u6807\u4e0a\u90fd\u8d85\u8d8a\u5176\u4ed6\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u8fd9\u4e9b\u6280\u672f\u7684\u4e92\u8865\u6027\u3002\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u566a\u58f0\u6570\u636e\u3001\u6709\u9650GPU\u8d44\u6e90\u548c\u8bad\u7ec3\u7ea6\u675f\u5e26\u6765\u7684\u6311\u6218\u3002", "conclusion": "\u7ec4\u5408SFT\u548cDPO\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u6700\u4f73\u7684\u5bf9\u9f50\u6548\u679c\uff0c\u4e3a\u672a\u6765\u66f4\u5f3a\u5927\u7684\u5bf9\u9f50\u6d41\u7a0b\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u548c\u57fa\u7840\u3002"}}
{"id": "2509.09082", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09082", "abs": "https://arxiv.org/abs/2509.09082", "authors": ["Zhongqiu Li", "Shiquan Wang", "Ruiyu Fang", "Mengjiao Bao", "Zhenhe Wu", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He"], "title": "MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for Universal Information Extraction", "comment": null, "summary": "Large language models (LLMs) demonstrate robust capabilities across diverse\nresearch domains. However, their performance in universal information\nextraction (UIE) remains insufficient, especially when tackling structured\noutput scenarios that involve complex schema descriptions and require\nmulti-step reasoning. While existing approaches enhance the performance of LLMs\nthrough in-context learning and instruction tuning, significant limitations\nnonetheless persist. To enhance the model's generalization ability, we propose\nintegrating reinforcement learning (RL) with multi-perspective reasoning for\ninformation extraction (IE) tasks. Our work transitions LLMs from passive\nextractors to active reasoners, enabling them to understand not only what to\nextract but also how to reason. Experiments conducted on multiple IE benchmarks\ndemonstrate that MR-UIE consistently elevates extraction accuracy across\ndomains and surpasses state-of-the-art methods on several datasets.\nFurthermore, incorporating multi-perspective reasoning into RL notably enhances\ngeneralization in complex IE tasks, underscoring the critical role of reasoning\nin challenging scenarios.", "AI": {"tldr": "\u63d0\u51faMR-UIE\u65b9\u6cd5\uff0c\u5c06\u5f3a\u5316\u5b66\u4e60\u4e0e\u591a\u89c6\u89d2\u63a8\u7406\u7ed3\u5408\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u6a21\u5f0f\u63cf\u8ff0\u548c\u591a\u6b65\u63a8\u7406\u573a\u666f\u4e0b\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u7ed3\u6784\u5316\u8f93\u51fa\u3001\u590d\u6742\u6a21\u5f0f\u63cf\u8ff0\u548c\u591a\u6b65\u63a8\u7406\u573a\u666f\u65f6\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\u3002", "method": "\u6574\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u591a\u89c6\u89d2\u63a8\u7406\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u88ab\u52a8\u62bd\u53d6\u5668\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u63a8\u7406\u5668\uff0c\u4f7f\u5176\u4e0d\u4ec5\u80fd\u7406\u89e3\u62bd\u53d6\u5185\u5bb9\uff0c\u8fd8\u80fd\u638c\u63e1\u63a8\u7406\u65b9\u5f0f\u3002", "result": "\u5728\u591a\u4e2a\u4fe1\u606f\u62bd\u53d6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMR-UIE\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8de8\u9886\u57df\u7684\u62bd\u53d6\u51c6\u786e\u7387\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u591a\u89c6\u89d2\u63a8\u7406\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\u663e\u8457\u589e\u5f3a\u4e86\u590d\u6742\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u51f8\u663e\u4e86\u63a8\u7406\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2509.09101", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09101", "abs": "https://arxiv.org/abs/2509.09101", "authors": ["Nishat Raihan", "Antonios Anastasopoulos", "Marcos Zampieri"], "title": "TigerCoder: A Novel Suite of LLMs for Code Generation in Bangla", "comment": null, "summary": "Despite being the 5th most spoken language, Bangla remains underrepresented\nin Large Language Models (LLMs), particularly for code generation. This\nprimarily stems from the scarcity of high-quality data to pre-train and/or\nfinetune such models. Hence, we introduce the first dedicated family of Code\nLLMs for Bangla (1B & 9B). We offer three major contributions: (1) a\ncomprehensive Bangla code instruction datasets for programming domain\nadaptation; (2) MBPP-Bangla, an evaluation benchmark for Bangla code\ngeneration; and (3) the TigerCoder-family of Code LLMs, achieving significant\n~11-18% performance gains at Pass@1 over existing multilingual and\ngeneral-purpose Bangla LLMs. Our findings show that curated, high-quality\ndatasets can overcome limitations of smaller models for low-resource languages.\nWe open-source all resources to advance further Bangla LLM research.", "AI": {"tldr": "\u9996\u4e2a\u4e13\u95e8\u4e3a\u5b5c\u52a0\u62c9\u8bed\u8bbe\u8ba1\u7684\u4ee3\u7801\u751f\u6210\u5927\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\uff0c\u5305\u542b1B\u548c9B\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u8c03\u4f18\u5728Pass@1\u6307\u6807\u4e0a\u8f83\u73b0\u6709\u591a\u8bed\u8a00\u6a21\u578b\u63d0\u534711-18%\u6027\u80fd\u63d0\u5347", "motivation": "\u5b5c\u52a0\u62c9\u8bed\u4f5c\u4e3a\u4e16\u754c\u7b2c5\u5927\u8bed\u8a00\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u4ee3\u7801\u751f\u6210\u9886\u57df\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e", "method": "\u6784\u5efa\u4e86\u7efc\u5408\u7684\u5b5c\u52a0\u62c9\u8bed\u4ee3\u7801\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u521b\u5efaMBPP-Bangla\u8bc4\u6d4b\u57fa\u51c6\uff0c\u5f00\u53d1TigerCoder\u5bb6\u65cf\u4ee3\u7801LLM\u6a21\u578b\uff08\u5305\u542b1B\u548c9B\u4e24\u4e2a\u89c4\u6a21\uff09", "result": "\u5728Pass@1\u6307\u6807\u4e0a\u8f83\u73b0\u6709\u591a\u8bed\u8a00\u6a21\u578b\u548c\u901a\u7528\u5b5c\u52a0\u62c9\u8bedLLM\u83b7\u5f9711-18%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u7cbe\u5fc3\u7f16\u8f91\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u53ef\u4ee5\u514b\u670d\u5c0f\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u9650\u5236", "conclusion": "\u901a\u8fc7\u6784\u5efa\u4e13\u95e8\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u8bc4\u6d4b\u6807\u51c6\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u5b5c\u52a0\u62c9\u8bed\u4ee3\u7801\u751f\u6210\u7684\u6027\u80fd\uff0c\u5e76\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u5f00\u6e90\u8d44\u6e90"}}
{"id": "2509.09121", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09121", "abs": "https://arxiv.org/abs/2509.09121", "authors": ["Sophia Maria"], "title": "Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast Asia", "comment": null, "summary": "Large language models (LLMs) excel in general-domain applications, yet their\nperformance often degrades in specialized tasks requiring domain-specific\nknowledge. E-commerce is particularly challenging, as its data are noisy,\nheterogeneous, multilingual, and highly dynamic. We present Compass-v3, a\nvertical-domain Mixture-of-Experts (MoE) model with 245B total parameters and\n71B active per token, designed for Southeast Asian e-commerce. Compass-v3\nadopts fewer but larger experts, combined with hardware-efficient\noptimizations-such as intra-node expert parallelism and a customized memcpy\noperator-to maximize GPU utilization. The model is trained on 12T tokens of\ncurated multilingual corpora and large-scale synthetic e-commerce instructions\nusing a mixed-training strategy. To enhance alignment, we propose\nOptimal-Transport Direct Preference Optimization (OTPO), which captures\ntoken-level distinctions and improves instruction adherence in\ncommerce-specific scenarios. Extensive evaluations demonstrate that Compass-v3\ndelivers state-of-the-art e-commerce performance, surpassing DeepSeek-V3.1,\nGPT-4 series, and Qwen3-235B. Moreover, Compass-v3 demonstrates strong\nmultilingual capability across low-resource Southeast Asian languages\n(Indonesian, Thai, Filipino, Vietnamese, Malay, Taglog) and Portuguese while\nsustaining competitive performance on general benchmarks. It has already been\nwidely applied in Shopee's industrial-scale e-commerce platform and is\ngradually replacing OpenAI's traffic, now accounting for over 70\\% of total LLM\nusage, highlighting its dual strengths in specialized commerce expertise and\nbroad linguistic competence.", "AI": {"tldr": "Compass-v3\u662f\u4e00\u4e2a245B\u53c2\u6570\u7684\u5782\u76f4\u9886\u57dfMoE\u6a21\u578b\uff0c\u4e13\u95e8\u9488\u5bf9\u4e1c\u5357\u4e9a\u7535\u5546\u573a\u666f\u8bbe\u8ba1\uff0c\u901a\u8fc7\u786c\u4ef6\u4f18\u5316\u548cOTPO\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5728\u7535\u5546\u4efb\u52a1\u548c\u591a\u8bed\u8a00\u5904\u7406\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5df2\u5728Shopee\u5e73\u53f0\u5e7f\u6cdb\u5e94\u7528\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9700\u8981\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u7684\u4e13\u95e8\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0b\u964d\u3002\u7535\u5546\u9886\u57df\u6570\u636e\u5608\u6742\u3001\u5f02\u6784\u3001\u591a\u8bed\u8a00\u4e14\u9ad8\u5ea6\u52a8\u6001\uff0c\u9700\u8981\u4e13\u95e8\u7684\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u91c7\u7528\u66f4\u5c11\u4f46\u66f4\u5927\u7684\u4e13\u5bb6\u8bbe\u8ba1MoE\u67b6\u6784\uff08245B\u603b\u53c2\u6570\uff0c71B\u6d3b\u8dc3\u53c2\u6570\uff09\uff0c\u4f7f\u7528\u786c\u4ef6\u9ad8\u6548\u4f18\u5316\uff08\u8282\u70b9\u5185\u4e13\u5bb6\u5e76\u884c\u548c\u5b9a\u5236memcpy\u64cd\u4f5c\u7b26\uff09\uff0c\u572812T token\u7684\u591a\u8bed\u8a00\u8bed\u6599\u548c\u5408\u6210\u7535\u5546\u6307\u4ee4\u4e0a\u8bad\u7ec3\uff0c\u63d0\u51faOTPO\u65b9\u6cd5\u589e\u5f3a\u5bf9\u9f50\u80fd\u529b\u3002", "result": "\u5728\u7535\u5546\u6027\u80fd\u4e0a\u8d85\u8d8aDeepSeek-V3.1\u3001GPT-4\u7cfb\u5217\u548cQwen3-235B\uff0c\u5728\u4e1c\u5357\u4e9a\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5370\u5c3c\u8bed\u3001\u6cf0\u8bed\u3001\u83f2\u5f8b\u5bbe\u8bed\u7b49\uff09\u548c\u8461\u8404\u7259\u8bed\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u540c\u65f6\u5728\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "Compass-v3\u5728\u4e13\u4e1a\u7535\u5546\u77e5\u8bc6\u548c\u5e7f\u6cdb\u8bed\u8a00\u80fd\u529b\u65b9\u9762\u5177\u6709\u53cc\u91cd\u4f18\u52bf\uff0c\u5df2\u5728Shopee\u5de5\u4e1a\u7ea7\u7535\u5546\u5e73\u53f0\u5e7f\u6cdb\u5e94\u7528\uff0c\u53d6\u4ee3\u4e8670%\u4ee5\u4e0a\u7684OpenAI\u6d41\u91cf\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5782\u76f4\u9886\u57df\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.09125", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09125", "abs": "https://arxiv.org/abs/2509.09125", "authors": ["Liqun He", "Jiaqi Xu"], "title": "Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus", "comment": "Accepted for publication in the journal Reflecting Digital Learning.\n  First submitted: 30 Oct 2023. The final version will be available open access\n  via the journal", "summary": "This study explores the use of generative AI for automating the\nclassification of tutors' Dialogue Acts (DAs), aiming to reduce the time and\neffort required by traditional manual coding. This case study uses the\nopen-source CIMA corpus, in which tutors' responses are pre-annotated into four\nDA categories. Both GPT-3.5-turbo and GPT-4 models were tested using tailored\nprompts. Results show that GPT-4 achieved 80% accuracy, a weighted F1-score of\n0.81, and a Cohen's Kappa of 0.74, surpassing baseline performance and\nindicating substantial agreement with human annotations. These findings suggest\nthat generative AI has strong potential to provide an efficient and accessible\napproach to DA classification, with meaningful implications for educational\ndialogue analysis. The study also highlights the importance of task-specific\nlabel definitions and contextual information in enhancing the quality of\nautomated annotation. Finally, it underscores the ethical considerations\nassociated with the use of generative AI and the need for responsible and\ntransparent research practices. The script of this research is publicly\navailable at\nhttps://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging.", "AI": {"tldr": "\u4f7f\u7528GPT-4\u81ea\u52a8\u5206\u7c7b\u6559\u80b2\u5bf9\u8bdd\u4e2d\u6559\u5e08\u7684\u8bdd\u8bed\u884c\u4e3a\uff0c\u8fbe\u523080%\u51c6\u786e\u7387\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u6790\u6548\u7387", "motivation": "\u51cf\u5c11\u4f20\u7edf\u624b\u5de5\u7f16\u7801\u6559\u80b2\u5bf9\u8bdd\u4e2d\u6559\u5e08\u8bdd\u8bed\u884c\u4e3a\u5206\u7c7b\u7684\u65f6\u95f4\u548c\u7cbe\u529b\u6d88\u8017", "method": "\u4f7f\u7528CIMA\u8bed\u6599\u5e93\uff0c\u901a\u8fc7GPT-3.5-turbo\u548cGPT-4\u6a21\u578b\u8fdb\u884c\u8bdd\u8bed\u884c\u4e3a\u5206\u7c7b\u6d4b\u8bd5\uff0c\u4f7f\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u8bcd", "result": "GPT-4\u8fbe\u523080%\u51c6\u786e\u7387\u3001\u52a0\u6743F1\u52060.81\u3001Cohen's Kappa\u7cfb\u65700.74\uff0c\u8d85\u8d8a\u57fa\u51c6\u7ee9\u6548\u5e76\u4e0e\u4eba\u5de5\u6807\u6ce8\u5b9e\u73b0\u5b9e\u8d28\u4e00\u81f4", "conclusion": "\u751f\u6210\u5f0fAI\u5728\u6559\u80b2\u5bf9\u8bdd\u5206\u6790\u4e2d\u5177\u6709\u5f3a\u5927\u6f5c\u529b\uff0c\u4efb\u52a1\u7279\u5b9a\u6807\u7b7e\u5b9a\u4e49\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u81ea\u52a8\u6807\u6ce8\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5173\u6ce8\u4f7f\u7528\u7684\u4f26\u7406\u95ee\u9898"}}
{"id": "2509.09131", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09131", "abs": "https://arxiv.org/abs/2509.09131", "authors": ["Phuong-Nam Dang", "Kieu-Linh Nguyen", "Thanh-Hieu Pham"], "title": "ViRanker: A BGE-M3 & Blockwise Parallel Transformer Cross-Encoder for Vietnamese Reranking", "comment": "9 pages", "summary": "This paper presents ViRanker, a cross-encoder reranking model tailored to the\nVietnamese language. Built on the BGE-M3 encoder and enhanced with the\nBlockwise Parallel Transformer, ViRanker addresses the lack of competitive\nrerankers for Vietnamese, a low-resource language with complex syntax and\ndiacritics. The model was trained on an 8 GB curated corpus and fine-tuned with\nhybrid hard-negative sampling to strengthen robustness. Evaluated on the\nMMARCO-VI benchmark, ViRanker achieves strong early-rank accuracy, surpassing\nmultilingual baselines and competing closely with PhoRanker. By releasing the\nmodel openly on Hugging Face, we aim to support reproducibility and encourage\nwider adoption in real-world retrieval systems. Beyond Vietnamese, this study\nillustrates how careful architectural adaptation and data curation can advance\nreranking in other underrepresented languages.", "AI": {"tldr": "ViRanker\u662f\u4e00\u4e2a\u9488\u5bf9\u8d8a\u5357\u8bed\u7684\u4ea4\u53c9\u7f16\u7801\u5668\u91cd\u6392\u5e8f\u6a21\u578b\uff0c\u57fa\u4e8eBGE-M3\u7f16\u7801\u5668\u6784\u5efa\uff0c\u91c7\u7528Blockwise Parallel Transformer\u589e\u5f3a\uff0c\u5728MMARCO-VI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u591a\u8bed\u8a00\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u8d8a\u5357\u8bed\u4f5c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7f3a\u4e4f\u7ade\u4e89\u6027\u91cd\u6392\u5e8f\u5668\u7684\u95ee\u9898\uff0c\u8d8a\u5357\u8bed\u5177\u6709\u590d\u6742\u8bed\u6cd5\u548c\u97f3\u8c03\u7b26\u53f7\uff0c\u9700\u8981\u4e13\u95e8\u4f18\u5316\u7684\u6a21\u578b\u3002", "method": "\u57fa\u4e8eBGE-M3\u7f16\u7801\u5668\uff0c\u4f7f\u7528Blockwise Parallel Transformer\u67b6\u6784\uff0c\u57288GB\u7cbe\u9009\u8bed\u6599\u4e0a\u8bad\u7ec3\uff0c\u91c7\u7528\u6df7\u5408\u786c\u8d1f\u91c7\u6837\u8fdb\u884c\u5fae\u8c03\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5728MMARCO-VI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u65e9\u671f\u6392\u540d\u51c6\u786e\u6027\uff0c\u8d85\u8d8a\u4e86\u591a\u8bed\u8a00\u57fa\u7ebf\u6a21\u578b\uff0c\u4e0ePhoRanker\u7ade\u4e89\u8868\u73b0\u63a5\u8fd1\u3002", "conclusion": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u67b6\u6784\u9002\u5e94\u548c\u6570\u636e\u7b56\u5c55\uff0c\u53ef\u4ee5\u63a8\u52a8\u5176\u4ed6\u4ee3\u8868\u6027\u4e0d\u8db3\u8bed\u8a00\u7684\u91cd\u6392\u5e8f\u6280\u672f\u53d1\u5c55\uff0c\u6a21\u578b\u5df2\u5728Hugging Face\u5f00\u6e90\u53d1\u5e03\u4ee5\u652f\u6301\u53ef\u590d\u73b0\u6027\u548c\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2509.09152", "categories": ["cs.CL", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2509.09152", "abs": "https://arxiv.org/abs/2509.09152", "authors": ["Taha Binhuraib", "Ruimin Gao", "Anna A. Ivanova"], "title": "LITcoder: A General-Purpose Library for Building and Comparing Encoding Models", "comment": null, "summary": "We introduce LITcoder, an open-source library for building and benchmarking\nneural encoding models. Designed as a flexible backend, LITcoder provides\nstandardized tools for aligning continuous stimuli (e.g., text and speech) with\nbrain data, transforming stimuli into representational features, mapping those\nfeatures onto brain data, and evaluating the predictive performance of the\nresulting model on held-out data. The library implements a modular pipeline\ncovering a wide array of methodological design choices, so researchers can\neasily compose, compare, and extend encoding models without reinventing core\ninfrastructure. Such choices include brain datasets, brain regions, stimulus\nfeature (both neural-net-based and control, such as word rate), downsampling\napproaches, and many others. In addition, the library provides built-in\nlogging, plotting, and seamless integration with experiment tracking platforms\nsuch as Weights & Biases (W&B). We demonstrate the scalability and versatility\nof our framework by fitting a range of encoding models to three story listening\ndatasets: LeBel et al. (2023), Narratives, and Little Prince. We also explore\nthe methodological choices critical for building encoding models for continuous\nfMRI data, illustrating the importance of accounting for all tokens in a TR\nscan (as opposed to just taking the last one, even when contextualized),\nincorporating hemodynamic lag effects, using train-test splits that minimize\ninformation leakage, and accounting for head motion effects on encoding model\npredictivity. Overall, LITcoder lowers technical barriers to encoding model\nimplementation, facilitates systematic comparisons across models and datasets,\nfosters methodological rigor, and accelerates the development of high-quality\nhigh-performance predictive models of brain activity.\n  Project page: https://litcoder-brain.github.io", "AI": {"tldr": "LITcoder\u662f\u4e00\u4e2a\u5f00\u6e90\u795e\u7ecf\u7f16\u7801\u6a21\u578b\u6784\u5efa\u548c\u57fa\u51c6\u6d4b\u8bd5\u5e93\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u5de5\u5177\u6765\u5904\u7406\u8fde\u7eed\u523a\u6fc0\u4e0e\u8111\u6570\u636e\u7684\u5bf9\u9f50\u3001\u7279\u5f81\u8f6c\u6362\u3001\u6620\u5c04\u548c\u6a21\u578b\u8bc4\u4f30\uff0c\u652f\u6301\u6a21\u5757\u5316\u6d41\u7a0b\u548c\u591a\u79cd\u65b9\u6cd5\u9009\u62e9\u3002", "motivation": "\u964d\u4f4e\u795e\u7ecf\u7f16\u7801\u6a21\u578b\u5b9e\u73b0\u7684\u6280\u672f\u95e8\u69db\uff0c\u4fc3\u8fdb\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u7cfb\u7edf\u6bd4\u8f83\uff0c\u63d0\u9ad8\u65b9\u6cd5\u4e25\u8c28\u6027\uff0c\u52a0\u901f\u9ad8\u8d28\u91cf\u8111\u6d3b\u52a8\u9884\u6d4b\u6a21\u578b\u7684\u5f00\u53d1\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u6d41\u6c34\u7ebf\u8bbe\u8ba1\uff0c\u652f\u6301\u591a\u79cd\u65b9\u6cd5\u9009\u62e9\uff08\u8111\u6570\u636e\u96c6\u3001\u8111\u533a\u57df\u3001\u523a\u6fc0\u7279\u5f81\u3001\u4e0b\u91c7\u6837\u65b9\u6cd5\u7b49\uff09\uff0c\u5185\u7f6e\u65e5\u5fd7\u8bb0\u5f55\u3001\u7ed8\u56fe\u529f\u80fd\uff0c\u5e76\u4e0e\u5b9e\u9a8c\u8ddf\u8e2a\u5e73\u53f0\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u901a\u8fc7\u5728\u4e09\u4e2a\u6545\u4e8b\u8046\u542c\u6570\u636e\u96c6\uff08LeBel et al. 2023\u3001Narratives\u3001Little Prince\uff09\u4e0a\u62df\u5408\u591a\u79cd\u7f16\u7801\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u53ef\u6269\u5c55\u6027\u548c\u591a\u529f\u80fd\u6027\u3002", "conclusion": "LITcoder\u6709\u6548\u964d\u4f4e\u4e86\u7f16\u7801\u6a21\u578b\u5b9e\u73b0\u7684\u6280\u672f\u969c\u788d\uff0c\u4fc3\u8fdb\u4e86\u7cfb\u7edf\u6bd4\u8f83\u548c\u65b9\u6cd5\u4e25\u8c28\u6027\uff0c\u52a0\u901f\u4e86\u9ad8\u8d28\u91cf\u8111\u6d3b\u52a8\u9884\u6d4b\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.09160", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09160", "abs": "https://arxiv.org/abs/2509.09160", "authors": ["Zhiyue Liu", "Fanrong Ma", "Xin Ling"], "title": "Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing", "comment": "Accepted by the IEEE International Conference on Multimedia and Expo\n  (ICME 2025). \\copyright\\ 2025 IEEE. Personal use of this material is\n  permitted. Permission from IEEE must be obtained for all other uses", "summary": "Target-oriented multimodal sentiment classification seeks to predict\nsentiment polarity for specific targets from image-text pairs. While existing\nworks achieve competitive performance, they often over-rely on textual content\nand fail to consider dataset biases, in particular word-level contextual\nbiases. This leads to spurious correlations between text features and output\nlabels, impairing classification accuracy. In this paper, we introduce a novel\ncounterfactual-enhanced debiasing framework to reduce such spurious\ncorrelations. Our framework incorporates a counterfactual data augmentation\nstrategy that minimally alters sentiment-related causal features, generating\ndetail-matched image-text samples to guide the model's attention toward content\ntied to sentiment. Furthermore, for learning robust features from\ncounterfactual data and prompting model decisions, we introduce an adaptive\ndebiasing contrastive learning mechanism, which effectively mitigates the\ninfluence of biased words. Experimental results on several benchmark datasets\nshow that our proposed method outperforms state-of-the-art baselines.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cd\u4e8b\u5b9e\u589e\u5f3a\u7684\u53bb\u504f\u67b6\u6784\uff0c\u7528\u4e8e\u51cf\u5c11\u76ee\u6807\u5bfc\u5411\u591a\u6a21\u6001\u60c5\u611f\u5206\u7c7b\u4e2d\u6587\u672c\u7279\u5f81\u4e0e\u8f93\u51fa\u6807\u7b7e\u95f4\u7684\u5047\u76f8\u5173\u6027\uff0c\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u5185\u5bb9\u4e14\u5ffd\u89c6\u4e86\u6570\u636e\u96c6\u504f\u89c1\uff0c\u5bfc\u81f4\u6587\u672c\u7279\u5f81\u4e0e\u8f93\u51fa\u6807\u7b7e\u95f4\u7684\u5047\u76f8\u5173\u6027\uff0c\u5f71\u54cd\u5206\u7c7b\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u6700\u5c0f\u5316\u6539\u53d8\u60c5\u611f\u76f8\u5173\u56e0\u679c\u7279\u5f81\u4ee5\u751f\u6210\u8be6\u7ec6\u5339\u914d\u7684\u56fe\u50cf-\u6587\u672c\u6837\u672c\uff0c\u5e76\u4e3a\u5b66\u4e60\u7a33\u5065\u7279\u5f81\u548c\u63d0\u793a\u6a21\u578b\u51b3\u7b56\u800c\u5f15\u5165\u9002\u5e94\u6027\u53bb\u504f\u5bf9\u6bd4\u5b66\u4e60\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u65b9\u6cd5\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u53cd\u4e8b\u5b9e\u589e\u5f3a\u548c\u5bf9\u6bd4\u5b66\u4e60\u6709\u6548\u51cf\u5c11\u4e86\u504f\u89c1\u8bcd\u7684\u5f71\u54cd\uff0c\u63d0\u5347\u4e86\u76ee\u6807\u5bfc\u5411\u591a\u6a21\u6001\u60c5\u611f\u5206\u7c7b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.09196", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09196", "abs": "https://arxiv.org/abs/2509.09196", "authors": ["Chin Yuen Kwok", "Jia Qi yip"], "title": "Efficient Trie-based Biasing using K-step Prediction for Rare Word Recognition", "comment": "Published in Interspeech 2025", "summary": "Contextual biasing improves rare word recognition of ASR models by\nprioritizing the output of rare words during decoding. A common approach is\nTrie-based biasing, which gives \"bonus scores\" to partial hypothesis (e.g.\n\"Bon\") that may lead to the generation of the rare word (e.g. \"Bonham\"). If the\nfull word (\"Bonham\") isn't ultimately recognized, the system revokes those\nearlier bonuses. This revocation is limited to beam search and is\ncomputationally expensive, particularly for models with large decoders. To\novercome these limitations, we propose adapting ASR models to look ahead and\npredict multiple steps at once. This avoids the revocation step entirely by\nbetter estimating whether a partial hypothesis will lead to the generation of\nthe full rare word. By fine-tuning Whisper with only 10 hours of synthetic\ndata, our method reduces the word error rate on the NSC Part 2 test set from\n30.86% to 12.19%.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4e0a\u4e0b\u6587\u504f\u7f6e\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6b65\u9884\u6d4b\u907f\u514d\u4f20\u7edfTrie\u504f\u7f6e\u4e2d\u7684\u5206\u6570\u64a4\u9500\u6b65\u9aa4\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u7f55\u89c1\u8bcd\u8bc6\u522b\u51c6\u786e\u7387", "motivation": "\u4f20\u7edfTrie\u504f\u7f6e\u65b9\u6cd5\u5728beam search\u4e2d\u9700\u8981\u64a4\u9500\u90e8\u5206\u5047\u8bbe\u7684\u5956\u52b1\u5206\u6570\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u4ec5\u9650\u4e8ebeam search\uff0c\u9650\u5236\u4e86\u5728\u5927\u89e3\u7801\u5668\u6a21\u578b\u4e2d\u7684\u5e94\u7528", "method": "\u901a\u8fc7\u8ba9ASR\u6a21\u578b\u8fdb\u884c\u591a\u6b65\u524d\u77bb\u9884\u6d4b\uff0c\u76f4\u63a5\u4f30\u8ba1\u90e8\u5206\u5047\u8bbe\u662f\u5426\u4f1a\u5bfc\u81f4\u5b8c\u6574\u7f55\u89c1\u8bcd\u7684\u751f\u6210\uff0c\u4ece\u800c\u907f\u514d\u5206\u6570\u64a4\u9500\u6b65\u9aa4\u3002\u4f7f\u7528\u4ec510\u5c0f\u65f6\u5408\u6210\u6570\u636e\u5bf9Whisper\u6a21\u578b\u8fdb\u884c\u5fae\u8c03", "result": "\u5728NSC Part 2\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u8bcd\u9519\u8bef\u7387\u4ece30.86%\u964d\u81f312.19%\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457", "conclusion": "\u591a\u6b65\u9884\u6d4b\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u4e0a\u4e0b\u6587\u504f\u7f6e\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5728\u7f55\u89c1\u8bcd\u8bc6\u522b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb"}}
{"id": "2509.09197", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09197", "abs": "https://arxiv.org/abs/2509.09197", "authors": ["Chin Yuen Kwok", "Jia Qi Yip", "Eng Siong Chng"], "title": "Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function", "comment": "Published in Interspeech 2025", "summary": "Rare word recognition can be improved by adapting ASR models to synthetic\ndata that includes these words. Further improvements can be achieved through\ncontextual biasing, which trains and adds a biasing module into the model\narchitecture to prioritize rare words. While training the module on synthetic\nrare word data is more effective than using non-rare-word data, it can lead to\noverfitting due to artifacts in the synthetic audio. To address this, we\nenhance the TCPGen-based contextual biasing approach and propose a\nkeyword-aware loss function that additionally focuses on biased words when\ntraining biasing modules. This loss includes a masked cross-entropy term for\nbiased word prediction and a binary classification term for detecting biased\nword positions. These two terms complementarily support the decoding of biased\nwords during inference. By adapting Whisper to 10 hours of synthetic data, our\nmethod reduced the word error rate on the NSC Part 2 test set from 29.71% to\n11.81%.", "AI": {"tldr": "\u901a\u8fc7\u5177\u6709\u5173\u952e\u8bcd\u610f\u8bc6\u635f\u5931\u51fd\u6570\u7684TCPGen\u4e0a\u4e0b\u6587\u504f\u7f6e\u65b9\u6cd5\uff0c\u5728\u5408\u6210\u8bed\u97f3\u6570\u636e\u4e0a\u8bad\u7ec3Whisper\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u7f55\u89c1\u8bcd\u8bc6\u522b\u9519\u8bef\u7387", "motivation": "\u89e3\u51b3\u5728\u5408\u6210\u8bed\u97f3\u6570\u636e\u4e0a\u8bad\u7ec3\u4e0a\u4e0b\u6587\u504f\u7f6e\u6a21\u5757\u65f6\u53ef\u80fd\u51fa\u73b0\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u63d0\u9ad8\u7f55\u89c1\u8bcd\u8bc6\u522b\u6027\u80fd", "method": "\u63d0\u51fa\u5173\u952e\u8bcd\u610f\u8bc6\u635f\u5931\u51fd\u6570\uff0c\u5305\u542b\u5bf9\u504f\u7f6e\u8bcd\u9884\u6d4b\u7684\u63a9\u7801\u4ea4\u53c9\u7463\u635f\u5931\u548c\u68c0\u6d4b\u504f\u7f6e\u8bcd\u4f4d\u7f6e\u7684\u4e8c\u5143\u5206\u7c7b\u635f\u5931\uff0c\u5728TCPGen\u57fa\u7840\u4e0a\u589e\u5f3a\u4e0a\u4e0b\u6587\u504f\u7f6e\u65b9\u6cd5", "result": "\u5728NSC Part 2\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u5c06\u8bcd\u8bed\u8bc6\u522b\u9519\u8bef\u7387\u4ece29.71%\u964d\u4f4e\u81f311.81%", "conclusion": "\u5173\u952e\u8bcd\u610f\u8bc6\u635f\u5931\u51fd\u6570\u80fd\u591f\u6709\u6548\u63d0\u5347\u4e0a\u4e0b\u6587\u504f\u7f6e\u6a21\u5757\u7684\u8bad\u7ec3\u6548\u679c\uff0c\u663e\u8457\u6539\u5584\u7f55\u89c1\u8bcd\u8bc6\u522b\u6027\u80fd"}}
{"id": "2509.09198", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09198", "abs": "https://arxiv.org/abs/2509.09198", "authors": ["Talia Sternberg", "Michael London", "David Omer", "Yossi Adi"], "title": "GmSLM : Generative Marmoset Spoken Language Modeling", "comment": null, "summary": "Marmoset monkeys exhibit complex vocal communication, challenging the view\nthat nonhuman primates vocal communication is entirely innate, and show similar\nfeatures of human speech, such as vocal labeling of others and turn-taking.\nStudying their vocal communication offers a unique opportunity to link it with\nbrain activity-especially given the difficulty of accessing the human brain in\nspeech and language research. Since Marmosets communicate primarily through\nvocalizations, applying standard LLM approaches is not straightforward. We\nintroduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized\nspoken language model pipeline for Marmoset vocal communication. We designed a\nnovel zero-shot evaluation metrics using unsupervised in-the-wild data,\nalongside weakly labeled conversational data, to assess GmSLM and demonstrate\nits advantage over a basic human-speech-based baseline. GmSLM generated\nvocalizations closely matched real resynthesized samples acoustically and\nperformed well on downstream tasks. Despite being fully unsupervised, GmSLM\neffectively distinguish real from artificial conversations and may support\nfurther investigations of the neural basis of vocal communication and provides\na practical framework linking vocalization and brain activity. We believe GmSLM\nstands to benefit future work in neuroscience, bioacoustics, and evolutionary\nbiology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM.", "AI": {"tldr": "GmSLM\u662f\u4e00\u4e2a\u9488\u5bf9\u72e8\u7334\u53eb\u58f0\u7684\u751f\u6210\u5f0f\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u91ce\u5916\u6570\u636e\u548c\u5f31\u6807\u8bb0\u5bf9\u8bdd\u6570\u636e\u8bc4\u4f30\uff0c\u5728\u58f0\u5b66\u5339\u914d\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u7814\u7a76\u53d1\u58f0\u4ea4\u6d41\u7684\u795e\u7ecf\u57fa\u7840\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002", "motivation": "\u72e8\u7334\u8868\u73b0\u51fa\u590d\u6742\u7684\u53d1\u58f0\u4ea4\u6d41\u80fd\u529b\uff0c\u5177\u6709\u7c7b\u4f3c\u4eba\u7c7b\u8bed\u8a00\u7684\u7279\u5f81\uff0c\u4f46\u7531\u4e8e\u5176\u4e3b\u8981\u901a\u8fc7\u53eb\u58f0\u4ea4\u6d41\uff0c\u6807\u51c6LLM\u65b9\u6cd5\u4e0d\u9002\u7528\uff0c\u9700\u8981\u4e13\u95e8\u6a21\u578b\u6765\u94fe\u63a5\u53d1\u58f0\u4e0e\u5927\u8111\u6d3b\u52a8\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u4e86GmSLM\u751f\u6210\u5f0f\u72e8\u7334\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7ba1\u9053\uff0c\u4f7f\u7528\u65e0\u76d1\u7763\u91ce\u5916\u6570\u636e\u548c\u5f31\u6807\u8bb0\u5bf9\u8bdd\u6570\u636e\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u5e76\u4e0e\u57fa\u4e8e\u4eba\u7c7b\u8bed\u97f3\u7684\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "GmSLM\u751f\u6210\u7684\u53eb\u58f0\u5728\u58f0\u5b66\u4e0a\u4e0e\u5b9e\u9645\u91cd\u5408\u6210\u6837\u672c\u9ad8\u5ea6\u5339\u914d\uff0c\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u80fd\u6709\u6548\u533a\u5206\u771f\u5b9e\u4e0e\u4eba\u5de5\u5bf9\u8bdd\uff0c\u5c3d\u7ba1\u5b8c\u5168\u65e0\u76d1\u7763\u3002", "conclusion": "GmSLM\u4e3a\u7814\u7a76\u53d1\u58f0\u4ea4\u6d41\u7684\u795e\u7ecf\u57fa\u7840\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\uff0c\u6709\u671b\u5728\u795e\u7ecf\u79d1\u5b66\u3001\u751f\u7269\u58f0\u5b66\u548c\u8fdb\u5316\u751f\u7269\u5b66\u9886\u57df\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2509.09199", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09199", "abs": "https://arxiv.org/abs/2509.09199", "authors": ["Wenhao Li", "Bangcheng Sun", "Weihao Ye", "Tianyi Zhang", "Daohai Yu", "Fei Chao", "Rongrong Ji"], "title": "CCF: A Context Compression Framework for Efficient Long-Sequence Language Modeling", "comment": null, "summary": "Scaling language models to longer contexts is essential for capturing rich\ndependencies across extended discourse. However, na\\\"ive context extension\nimposes significant computational and memory burdens, often resulting in\ninefficiencies during both training and inference. In this work, we propose\nCCF, a novel context compression framework designed to enable efficient\nlong-context modeling by learning hierarchical latent representations that\npreserve global semantics while aggressively reducing input redundancy. CCF\nintegrates segment-wise semantic aggregation with key-value memory encoding,\nforming compact representations that support accurate reconstruction and\nlong-range understanding. To further enhance scalability, we introduce a\ntraining-efficient optimization strategy that couples incremental segment\ndecoding with sparse reservoir sampling, substantially reducing memory overhead\nwithout degrading performance. Empirical results on multiple long-context\nlanguage modeling benchmarks demonstrate that CCF achieves competitive\nperplexity under high compression ratios, and significantly improves throughput\nand memory efficiency compared to existing approaches. These findings highlight\nthe potential of structured compression for scalable and effective long-context\nlanguage modeling.", "AI": {"tldr": "CCF\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u6f5c\u5728\u8868\u793a\u5b66\u4e60\u6765\u9ad8\u6548\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u5728\u4fdd\u6301\u5168\u5c40\u8bed\u4e49\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5197\u4f59\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u6269\u5c55\u8bed\u8a00\u6a21\u578b\u5230\u66f4\u957f\u4e0a\u4e0b\u6587\u5bf9\u4e8e\u6355\u83b7\u6269\u5c55\u8bed\u7bc7\u4e2d\u7684\u4e30\u5bcc\u4f9d\u8d56\u5173\u7cfb\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7b80\u5355\u7684\u4e0a\u4e0b\u6587\u6269\u5c55\u4f1a\u5e26\u6765\u663e\u8457\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u8d1f\u62c5\uff0c\u5bfc\u81f4\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faCCF\u6846\u67b6\uff0c\u6574\u5408\u5206\u6bb5\u8bed\u4e49\u805a\u5408\u4e0e\u952e\u503c\u8bb0\u5fc6\u7f16\u7801\uff0c\u5f62\u6210\u7d27\u51d1\u8868\u793a\uff1b\u5f15\u5165\u8bad\u7ec3\u9ad8\u6548\u7684\u4f18\u5316\u7b56\u7565\uff0c\u7ed3\u5408\u589e\u91cf\u5206\u6bb5\u89e3\u7801\u548c\u7a00\u758f\u50a8\u5c42\u91c7\u6837\u6765\u51cf\u5c11\u5185\u5b58\u5f00\u9500\u3002", "result": "\u5728\u591a\u4e2a\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCCF\u5728\u9ad8\u538b\u7f29\u6bd4\u4e0b\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u56f0\u60d1\u5ea6\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\u548c\u5185\u5b58\u6548\u7387\u3002", "conclusion": "\u7ed3\u6784\u5316\u538b\u7f29\u5bf9\u4e8e\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u5efa\u6a21\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0cCCF\u6846\u67b6\u4e3a\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u4e2d\u7684\u6548\u7387\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2509.09229", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09229", "abs": "https://arxiv.org/abs/2509.09229", "authors": ["Matan Cohen", "Shira Shani", "Eden Menahem", "Yehudit Aperstein", "Alexander Apartsin"], "title": "Reading Between the Lines: Classifying Resume Seniority with Large Language Models", "comment": "5 pages, 3 figures", "summary": "Accurately assessing candidate seniority from resumes is a critical yet\nchallenging task, complicated by the prevalence of overstated experience and\nambiguous self-presentation. In this study, we investigate the effectiveness of\nlarge language models (LLMs), including fine-tuned BERT architectures, for\nautomating seniority classification in resumes. To rigorously evaluate model\nperformance, we introduce a hybrid dataset comprising both real-world resumes\nand synthetically generated hard examples designed to simulate exaggerated\nqualifications and understated seniority. Using the dataset, we evaluate the\nperformance of Large Language Models in detecting subtle linguistic cues\nassociated with seniority inflation and implicit expertise. Our findings\nhighlight promising directions for enhancing AI-driven candidate evaluation\nsystems and mitigating bias introduced by self-promotional language. The\ndataset is available for the research community at https://bit.ly/4mcTovt", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ec\u5fae\u8c03BERT\u67b6\u6784\uff09\u6765\u81ea\u52a8\u5316\u7b80\u5386\u4e2d\u8d44\u5386\u5206\u7c7b\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u6df7\u5408\u771f\u5b9e\u7b80\u5386\u548c\u5408\u6210\u56f0\u96be\u6837\u672c\u7684\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u51c6\u786e\u8bc4\u4f30\u7b80\u5386\u4e2d\u7684\u5019\u9009\u4eba\u8d44\u5386\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u7531\u4e8e\u666e\u904d\u5b58\u5728\u7ecf\u9a8c\u5938\u5927\u548c\u6a21\u7cca\u81ea\u6211\u63cf\u8ff0\u7684\u60c5\u51b5\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165\u5305\u542b\u771f\u5b9e\u7b80\u5386\u548c\u5408\u6210\u56f0\u96be\u6837\u672c\u7684\u6df7\u5408\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ec\u5fae\u8c03BERT\uff09\u6765\u68c0\u6d4b\u4e0e\u8d44\u5386\u5938\u5927\u76f8\u5173\u7684\u5fae\u5999\u8bed\u8a00\u7ebf\u7d22\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\u4e86AI\u9a71\u52a8\u5019\u9009\u4eba\u8bc4\u4f30\u7cfb\u7edf\u589e\u5f3a\u7684\u6f5c\u529b\uff0c\u5e76\u80fd\u591f\u51cf\u8f7b\u81ea\u6211\u63a8\u9500\u8bed\u8a00\u5f15\u5165\u7684\u504f\u89c1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6539\u8fdb\u81ea\u52a8\u5316\u8d44\u5386\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u4f9b\u7814\u7a76\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2509.09234", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09234", "abs": "https://arxiv.org/abs/2509.09234", "authors": ["Rishit Tyagi", "Mohit Gupta", "Rahul Bouri"], "title": "Agentic LLMs for Question Answering over Tabular Data", "comment": "Accepted at ACL workshop SemEval 2025", "summary": "Question Answering over Tabular Data (Table QA) presents unique challenges\ndue to the diverse structure, size, and data types of real-world tables. The\nSemEval 2025 Task 8 (DataBench) introduced a benchmark composed of large-scale,\ndomain-diverse datasets to evaluate the ability of models to accurately answer\nstructured queries. We propose a Natural Language to SQL (NL-to-SQL) approach\nleveraging large language models (LLMs) such as GPT-4o, GPT-4o-mini, and\nDeepSeek v2:16b to generate SQL queries dynamically. Our system follows a\nmulti-stage pipeline involving example selection, SQL query generation, answer\nextraction, verification, and iterative refinement. Experiments demonstrate the\neffectiveness of our approach, achieving 70.5\\% accuracy on DataBench QA and\n71.6\\% on DataBench Lite QA, significantly surpassing baseline scores of 26\\%\nand 27\\% respectively. This paper details our methodology, experimental\nresults, and alternative approaches, providing insights into the strengths and\nlimitations of LLM-driven Table QA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u9636\u6bb5NL-to-SQL\u65b9\u6cd5\uff0c\u5728SemEval 2025 Task 8\u7684DataBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u8868\u683c\u95ee\u7b54\u4efb\u52a1\u4e2d\u7531\u4e8e\u8868\u683c\u7ed3\u6784\u3001\u5927\u5c0f\u548c\u6570\u636e\u7c7b\u578b\u7684\u591a\u6837\u6027\u5e26\u6765\u7684\u6311\u6218\uff0c\u8bc4\u4f30\u6a21\u578b\u5904\u7406\u7ed3\u6784\u5316\u67e5\u8be2\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528GPT-4o\u3001GPT-4o-mini\u548cDeepSeek v2:16b\u7b49\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\u65b9\u6cd5\uff0c\u5305\u62ec\u793a\u4f8b\u9009\u62e9\u3001SQL\u67e5\u8be2\u751f\u6210\u3001\u7b54\u6848\u63d0\u53d6\u3001\u9a8c\u8bc1\u548c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5728DataBench QA\u4e0a\u8fbe\u523070.5%\u51c6\u786e\u7387\uff0c\u5728DataBench Lite QA\u4e0a\u8fbe\u523071.6%\u51c6\u786e\u7387\uff0c\u663e\u8457\u8d85\u8fc726%\u548c27%\u7684\u57fa\u7ebf\u5206\u6570\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u8868\u683c\u95ee\u7b54\u65b9\u6cd5\u6709\u6548\uff0c\u8bba\u6587\u8be6\u7ec6\u5206\u6790\u4e86\u65b9\u6cd5\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2509.09303", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09303", "abs": "https://arxiv.org/abs/2509.09303", "authors": ["Grazia Sveva Ascione", "Nicol\u00f2 Tamagnone"], "title": "From scratch to silver: Creating trustworthy training data for patent-SDG classification using Large Language Models", "comment": null, "summary": "Classifying patents by their relevance to the UN Sustainable Development\nGoals (SDGs) is crucial for tracking how innovation addresses global\nchallenges. However, the absence of a large, labeled dataset limits the use of\nsupervised learning. Existing methods, such as keyword searches, transfer\nlearning, and citation-based heuristics, lack scalability and generalizability.\nThis paper frames patent-to-SDG classification as a weak supervision problem,\nusing citations from patents to SDG-tagged scientific publications (NPL\ncitations) as a noisy initial signal. To address its sparsity and noise, we\ndevelop a composite labeling function (LF) that uses large language models\n(LLMs) to extract structured concepts, namely functions, solutions, and\napplications, from patents and SDG papers based on a patent ontology.\nCross-domain similarity scores are computed and combined using a rank-based\nretrieval approach. The LF is calibrated via a custom positive-only loss that\naligns with known NPL-SDG links without penalizing discovery of new SDG\nassociations. The result is a silver-standard, soft multi-label dataset mapping\npatents to SDGs, enabling the training of effective multi-label regression\nmodels. We validate our approach through two complementary strategies: (1)\ninternal validation against held-out NPL-based labels, where our method\noutperforms several baselines including transformer-based models, and zero-shot\nLLM; and (2) external validation using network modularity in patent citation,\nco-inventor, and co-applicant graphs, where our labels reveal greater thematic,\ncognitive, and organizational coherence than traditional technological\nclassifications. These results show that weak supervision and semantic\nalignment can enhance SDG classification at scale.", "AI": {"tldr": "\u4f7f\u7528\u5f31\u76d1\u7763\u65b9\u6cd5\u901a\u8fc7\u4e13\u5229\u5f15\u6587\u548cLLM\u63d0\u53d6\u7ed3\u6784\u5316\u6982\u5ff5\uff0c\u6784\u5efa\u4e13\u5229\u4e0e\u8054\u5408\u56fd\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807(SDGs)\u7684\u8f6f\u591a\u6807\u7b7e\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u7f3a\u4e4f\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u9650\u5236\u4e86\u76d1\u7763\u5b66\u4e60\u5728\u4e13\u5229-SDG\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u6027\u3002", "method": "\u5c06\u4e13\u5229-SDG\u5206\u7c7b\u6784\u5efa\u4e3a\u5f31\u76d1\u7763\u95ee\u9898\uff0c\u5229\u7528\u4e13\u5229\u5bf9SDG\u6807\u8bb0\u79d1\u5b66\u6587\u732e\u7684\u5f15\u6587\u4f5c\u4e3a\u566a\u58f0\u4fe1\u53f7\uff0c\u5f00\u53d1\u590d\u5408\u6807\u6ce8\u51fd\u6570\uff0c\u4f7f\u7528LLM\u4ece\u4e13\u5229\u548cSDG\u8bba\u6587\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u6982\u5ff5\uff0c\u8ba1\u7b97\u8de8\u57df\u76f8\u4f3c\u5ea6\u5f97\u5206\u3002", "result": "\u5185\u90e8\u9a8c\u8bc1\u663e\u793a\u65b9\u6cd5\u4f18\u4e8e\u5305\u62ectransformer\u6a21\u578b\u548c\u96f6\u6837\u672cLLM\u5728\u5185\u7684\u591a\u4e2a\u57fa\u7ebf\uff1b\u5916\u90e8\u9a8c\u8bc1\u663e\u793a\u6807\u7b7e\u5728\u4e13\u5229\u5f15\u7528\u3001\u5171\u540c\u53d1\u660e\u4eba\u548c\u5171\u540c\u7533\u8bf7\u4eba\u7f51\u7edc\u4e2d\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u4e3b\u9898\u3001\u8ba4\u77e5\u548c\u7ec4\u7ec7\u4e00\u81f4\u6027\u3002", "conclusion": "\u5f31\u76d1\u7763\u548c\u8bed\u4e49\u5bf9\u9f50\u53ef\u4ee5\u5927\u89c4\u6a21\u589e\u5f3aSDG\u5206\u7c7b\uff0c\u4e3a\u8ddf\u8e2a\u521b\u65b0\u5982\u4f55\u5e94\u5bf9\u5168\u7403\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2509.09360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09360", "abs": "https://arxiv.org/abs/2509.09360", "authors": ["Channdeth Sok", "David Luz", "Yacine Haddam"], "title": "MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems", "comment": "under review", "summary": "Large Language Models (LLMs) are increasingly deployed in enterprise\napplications, yet their reliability remains limited by hallucinations, i.e.,\nconfident but factually incorrect information. Existing detection approaches,\nsuch as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not\naddress the unique challenges of Retrieval-Augmented Generation (RAG) systems,\nwhere responses must be consistent with retrieved evidence. We therefore\npresent MetaRAG, a metamorphic testing framework for hallucination detection in\nRetrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,\nunsupervised, black-box setting, requiring neither ground-truth references nor\naccess to model internals, making it suitable for proprietary and high-stakes\ndomains. The framework proceeds in four stages: (1) decompose answers into\natomic factoids, (2) generate controlled mutations of each factoid using\nsynonym and antonym substitutions, (3) verify each variant against the\nretrieved context (synonyms are expected to be entailed and antonyms\ncontradicted), and (4) aggregate penalties for inconsistencies into a\nresponse-level hallucination score. Crucially for identity-aware AI, MetaRAG\nlocalizes unsupported claims at the factoid span where they occur (e.g.,\npregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),\nallowing users to see flagged spans and enabling system designers to configure\nthresholds and guardrails for identity-sensitive queries. Experiments on a\nproprietary enterprise dataset illustrate the effectiveness of MetaRAG for\ndetecting hallucinations and enabling trustworthy deployment of RAG-based\nconversational agents. We also outline a topic-based deployment design that\ntranslates MetaRAG's span-level scores into identity-aware safeguards; this\ndesign is discussed but not evaluated in our experiments.", "AI": {"tldr": "MetaRAG\u662f\u4e00\u4e2a\u7528\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7cfb\u7edf\u7684\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u7b54\u6848\u3001\u751f\u6210\u53d8\u5f02\u4e8b\u5b9e\u3001\u9a8c\u8bc1\u4e00\u81f4\u6027\u6765\u68c0\u6d4b\u5e7b\u89c9\uff0c\u65e0\u9700\u771f\u5b9e\u53c2\u8003\u6216\u6a21\u578b\u5185\u90e8\u8bbf\u95ee\u3002", "motivation": "\u73b0\u6709\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u72ec\u7acbLLM\uff0c\u65e0\u6cd5\u89e3\u51b3RAG\u7cfb\u7edf\u4e2d\u54cd\u5e94\u5fc5\u987b\u4e0e\u68c0\u7d22\u8bc1\u636e\u4e00\u81f4\u7684\u7279\u6b8a\u6311\u6218\uff0c\u4f01\u4e1a\u5e94\u7528\u4e2d\u9700\u8981\u53ef\u9760\u7684\u65e0\u76d1\u7763\u68c0\u6d4b\u65b9\u6848\u3002", "method": "\u56db\u9636\u6bb5\u6846\u67b6\uff1a1)\u5206\u89e3\u7b54\u6848\u4e3a\u539f\u5b50\u4e8b\u5b9e 2)\u4f7f\u7528\u540c\u4e49\u8bcd/\u53cd\u4e49\u8bcd\u751f\u6210\u53d7\u63a7\u53d8\u5f02 3)\u6839\u636e\u68c0\u7d22\u4e0a\u4e0b\u6587\u9a8c\u8bc1\u53d8\u5f02(\u540c\u4e49\u8bcd\u5e94\u88ab\u8574\u542b\uff0c\u53cd\u4e49\u8bcd\u5e94\u88ab\u77db\u76fe) 4)\u805a\u5408\u4e0d\u4e00\u81f4\u6027\u60e9\u7f5a\u4e3a\u54cd\u5e94\u7ea7\u5e7b\u89c9\u5206\u6570", "result": "\u5728\u4f01\u4e1a\u4e13\u6709\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86MetaRAG\u5728\u68c0\u6d4b\u5e7b\u89c9\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u652f\u6301RAG\u5bf9\u8bdd\u4ee3\u7406\u7684\u53ef\u4fe1\u90e8\u7f72\u3002", "conclusion": "MetaRAG\u4e3aRAG\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u65f6\u3001\u65e0\u76d1\u7763\u3001\u9ed1\u76d2\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8eab\u4efd\u654f\u611f\u67e5\u8be2\u7684\u672c\u5730\u5316\u68c0\u6d4b\u548c\u5b89\u5168\u4fdd\u969c\u914d\u7f6e\u3002"}}
{"id": "2509.09381", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09381", "abs": "https://arxiv.org/abs/2509.09381", "authors": ["Molly R Petersen", "Claire E Stevenson", "Lonneke van der Plas"], "title": "Modelling Analogies and Analogical Reasoning: Connecting Cognitive Science Theory and NLP Research", "comment": null, "summary": "Analogical reasoning is an essential aspect of human cognition. In this\npaper, we summarize key theory about the processes underlying analogical\nreasoning from the cognitive science literature and relate it to current\nresearch in natural language processing. While these processes can be easily\nlinked to concepts in NLP, they are generally not viewed through a cognitive\nlens. Furthermore, we show how these notions are relevant for several major\nchallenges in NLP research, not directly related to analogy solving. This may\nguide researchers to better optimize relational understanding in text, as\nopposed to relying heavily on entity-level similarity.", "AI": {"tldr": "\u672c\u6587\u4ece\u8ba4\u77e5\u79d1\u5b66\u89d2\u5ea6\u603b\u7ed3\u7c7b\u6bd4\u63a8\u7406\u7684\u7406\u8bba\u8fc7\u7a0b\uff0c\u5e76\u5c06\u5176\u4e0eNLP\u7814\u7a76\u5173\u8054\uff0c\u6307\u51fa\u8ba4\u77e5\u89c6\u89d2\u5728NLP\u4e2d\u7684\u7f3a\u5931\uff0c\u5f3a\u8c03\u5173\u7cfb\u7406\u89e3\u4f18\u4e8e\u5b9e\u4f53\u76f8\u4f3c\u6027\u7684\u91cd\u8981\u6027", "motivation": "\u5c06\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7c7b\u6bd4\u63a8\u7406\u7684\u7406\u8bba\u8fc7\u7a0b\u4e0e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u8054\u7cfb\u8d77\u6765\uff0c\u586b\u8865\u8ba4\u77e5\u89c6\u89d2\u5728NLP\u4e2d\u7684\u7a7a\u767d\uff0c\u6307\u5bfc\u7814\u7a76\u8005\u66f4\u597d\u5730\u4f18\u5316\u6587\u672c\u4e2d\u7684\u5173\u7cfb\u7406\u89e3", "method": "\u603b\u7ed3\u8ba4\u77e5\u79d1\u5b66\u6587\u732e\u4e2d\u5173\u4e8e\u7c7b\u6bd4\u63a8\u7406\u8fc7\u7a0b\u7684\u5173\u952e\u7406\u8bba\uff0c\u5206\u6790\u8fd9\u4e9b\u8fc7\u7a0b\u4e0eNLP\u6982\u5ff5\u7684\u5173\u8054\uff0c\u5e76\u63a2\u8ba8\u5176\u5bf9NLP\u4e3b\u8981\u6311\u6218\u7684\u76f8\u5173\u6027", "result": "\u53d1\u73b0\u8ba4\u77e5\u79d1\u5b66\u7684\u7c7b\u6bd4\u63a8\u7406\u8fc7\u7a0b\u53ef\u4ee5\u8f7b\u677e\u4e0eNLP\u6982\u5ff5\u5173\u8054\uff0c\u4f46\u8fd9\u4e9b\u5173\u8054\u901a\u5e38\u7f3a\u4e4f\u8ba4\u77e5\u89c6\u89d2\uff1b\u8bc1\u660e\u4e86\u8fd9\u4e9b\u6982\u5ff5\u5bf9NLP\u7814\u7a76\u4e2d\u591a\u4e2a\u4e3b\u8981\u6311\u6218\u5177\u6709\u76f8\u5173\u6027", "conclusion": "\u8ba4\u77e5\u79d1\u5b66\u7684\u7c7b\u6bd4\u63a8\u7406\u7406\u8bba\u4e3aNLP\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u7814\u7a76\u8005\u51cf\u5c11\u5bf9\u5b9e\u4f53\u5c42\u9762\u76f8\u4f3c\u6027\u7684\u4f9d\u8d56\uff0c\u8f6c\u800c\u66f4\u6ce8\u91cd\u5173\u7cfb\u7406\u89e3\uff0c\u4ece\u800c\u66f4\u597d\u5730\u89e3\u51b3NLP\u6311\u6218"}}
{"id": "2509.09388", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09388", "abs": "https://arxiv.org/abs/2509.09388", "authors": ["Ana Ezquerro", "Carlos G\u00f3mez-Rodr\u00edguez", "David Vilares"], "title": "Hierarchical Bracketing Encodings Work for Dependency Graphs", "comment": "Accepted at EMNLP 2025 (main)", "summary": "We revisit hierarchical bracketing encodings from a practical perspective in\nthe context of dependency graph parsing. The approach encodes graphs as\nsequences, enabling linear-time parsing with $n$ tagging actions, and still\nrepresenting reentrancies, cycles, and empty nodes. Compared to existing graph\nlinearizations, this representation substantially reduces the label space while\npreserving structural information. We evaluate it on a multilingual and\nmulti-formalism benchmark, showing competitive results and consistent\nimprovements over other methods in exact match accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u5c42\u6b21\u62ec\u53f7\u7f16\u7801\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f9d\u8d56\u56fe\u89e3\u6790\uff0c\u5c06\u56fe\u7f16\u7801\u4e3a\u5e8f\u5217\uff0c\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u89e3\u6790\u5e76\u652f\u6301\u91cd\u5165\u3001\u5faa\u73af\u548c\u7a7a\u8282\u70b9\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u7ebf\u6027\u5316\u65b9\u6cd5\u6807\u7b7e\u7a7a\u95f4\u8f83\u5927\uff0c\u9700\u8981\u51cf\u5c11\u6807\u7b7e\u7a7a\u95f4\u540c\u65f6\u4fdd\u6301\u7ed3\u6784\u4fe1\u606f\uff0c\u63d0\u9ad8\u4f9d\u8d56\u56fe\u89e3\u6790\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u5c42\u6b21\u62ec\u53f7\u7f16\u7801\u5c06\u4f9d\u8d56\u56fe\u8868\u793a\u4e3a\u5e8f\u5217\uff0c\u901a\u8fc7n\u4e2a\u6807\u6ce8\u52a8\u4f5c\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u89e3\u6790\uff0c\u652f\u6301\u91cd\u5165\u3001\u5faa\u73af\u548c\u7a7a\u8282\u70b9\u7b49\u590d\u6742\u7ed3\u6784\u3002", "result": "\u5728\u591a\u8bed\u8a00\u591a\u5f62\u5f0f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u5728\u7cbe\u786e\u5339\u914d\u51c6\u786e\u7387\u4e0a\u76f8\u6bd4\u5176\u4ed6\u65b9\u6cd5\u6709\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "\u5c42\u6b21\u62ec\u53f7\u7f16\u7801\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u4f9d\u8d56\u56fe\u89e3\u6790\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u51cf\u5c11\u6807\u7b7e\u7a7a\u95f4\u5e76\u4fdd\u6301\u7ed3\u6784\u5b8c\u6574\u6027\uff0c\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u5177\u6709\u7ade\u4e89\u4f18\u52bf\u3002"}}
{"id": "2509.09438", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09438", "abs": "https://arxiv.org/abs/2509.09438", "authors": ["Zhaohan Zhang", "Ziquan Liu", "Ioannis Patras"], "title": "GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models", "comment": "20 pages, 11 figures", "summary": "Assessing the reliability of Large Language Models (LLMs) by confidence\nelicitation is a prominent approach to AI safety in high-stakes applications,\nsuch as healthcare and finance. Existing methods either require expensive\ncomputational overhead or suffer from poor calibration, making them impractical\nand unreliable for real-world deployment. In this work, we propose GrACE, a\nGenerative Approach to Confidence Elicitation that enables scalable and\nreliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in\nwhich the model expresses confidence by the similarity between the last hidden\nstate and the embedding of a special token appended to the vocabulary, in\nreal-time. We fine-tune the model for calibrating the confidence with\ncalibration targets associated with accuracy. Experiments with three LLMs and\ntwo benchmark datasets show that the confidence produced by GrACE achieves the\nbest discriminative capacity and calibration on open-ended generation tasks,\noutperforming six competing methods without resorting to additional sampling or\nan auxiliary model. Moreover, we propose two strategies for improving test-time\nscaling based on confidence induced by GrACE. Experimental results show that\nusing GrACE not only improves the accuracy of the final decision but also\nsignificantly reduces the number of required samples in the test-time scaling\nscheme, indicating the potential of GrACE as a practical solution for deploying\nLLMs with scalable, reliable, and real-time confidence estimation.", "AI": {"tldr": "GrACE\u662f\u4e00\u79cd\u65b0\u7684LLM\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u9690\u85cf\u72b6\u6001\u4e0e\u7279\u6b8a\u6807\u8bb0\u5d4c\u5165\u7684\u76f8\u4f3c\u6027\u6765\u5b9e\u65f6\u8bc4\u4f30\u6a21\u578b\u7f6e\u4fe1\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u91c7\u6837\u6216\u8f85\u52a9\u6a21\u578b\uff0c\u5728\u51c6\u786e\u6027\u548c\u6821\u51c6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u8981\u4e48\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u8981\u4e48\u6821\u51c6\u6548\u679c\u5dee\uff0c\u96be\u4ee5\u5728\u5b9e\u9645\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u53ef\u9760\u90e8\u7f72\u3002", "method": "\u63d0\u51faGrACE\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u6700\u540e\u4e00\u4e2a\u9690\u85cf\u72b6\u6001\u4e0e\u8bcd\u6c47\u8868\u4e2d\u7279\u6b8a\u6807\u8bb0\u5d4c\u5165\u7684\u76f8\u4f3c\u6027\u6765\u5b9e\u65f6\u8868\u8fbe\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u901a\u8fc7\u4e0e\u51c6\u786e\u6027\u76f8\u5173\u7684\u6821\u51c6\u76ee\u6807\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u4e09\u4e2aLLM\u548c\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGrACE\u5728\u5f00\u653e\u751f\u6210\u4efb\u52a1\u4e2d\u5177\u6709\u6700\u4f73\u5224\u522b\u80fd\u529b\u548c\u6821\u51c6\u6548\u679c\uff0c\u4f18\u4e8e\u516d\u79cd\u7ade\u4e89\u65b9\u6cd5\uff0c\u5e76\u80fd\u663e\u8457\u51cf\u5c11\u6d4b\u8bd5\u65f6\u7f29\u653e\u6240\u9700\u7684\u6837\u672c\u6570\u91cf\u3002", "conclusion": "GrACE\u4e3aLLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u9760\u4e14\u5b9e\u65f6\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u63d0\u9ad8\u6700\u7ec8\u51b3\u7b56\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2509.09473", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09473", "abs": "https://arxiv.org/abs/2509.09473", "authors": ["Lucie Pol\u00e1kov\u00e1", "Martin Popel", "V\u011bra Kloudov\u00e1", "Michal Nov\u00e1k", "Mariia Anisimova", "Ji\u0159\u00ed Balhar"], "title": "Mitigating Language Barriers in Education: Developing Multilingual Digital Learning Materials with Machine Translation", "comment": "8 pages, 2 figures", "summary": "The EdUKate project combines digital education, linguistics, translation\nstudies, and machine translation to develop multilingual learning materials for\nCzech primary and secondary schools. Launched through collaboration between a\nmajor Czech academic institution and the country's largest educational\npublisher, the project is aimed at translating up to 9,000 multimodal\ninteractive exercises from Czech into Ukrainian, English, and German for an\neducational web portal. It emphasizes the development and evaluation of a\ndirect Czech-Ukrainian machine translation system tailored to the educational\ndomain, with special attention to processing formatted content such as XML and\nPDF and handling technical and scientific terminology. We present findings from\nan initial survey of Czech teachers regarding the needs of non-Czech-speaking\nstudents and describe the system's evaluation and implementation on the web\nportal. All resulting applications are freely available to students, educators,\nand researchers.", "AI": {"tldr": "EdUKate\u9879\u76ee\u5f00\u53d1\u6377\u514b-\u4e4c\u514b\u5170\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u4e3a\u6377\u514b\u5b66\u6821\u521b\u5efa\u591a\u8bed\u8a00\u6559\u80b2\u6750\u6599\uff0c\u5c069000\u4e2a\u4e92\u52a8\u7ec3\u4e60\u7ffb\u8bd1\u6210\u4e4c\u514b\u5170\u8bed\u3001\u82f1\u8bed\u548c\u5fb7\u8bed\u3002", "motivation": "\u6ee1\u8db3\u6377\u514b\u5b66\u6821\u4e2d\u975e\u6377\u514b\u8bed\u5b66\u751f\u7684\u6559\u80b2\u9700\u6c42\uff0c\u901a\u8fc7\u673a\u5668\u7ffb\u8bd1\u6280\u672f\u63d0\u4f9b\u591a\u8bed\u8a00\u5b66\u4e60\u6750\u6599\uff0c\u7279\u522b\u5173\u6ce8\u4e4c\u514b\u5170\u8bed\u5b66\u751f\u7684\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u6570\u5b57\u6559\u80b2\u3001\u8bed\u8a00\u5b66\u548c\u673a\u5668\u7ffb\u8bd1\u6280\u672f\uff0c\u5f00\u53d1\u4e13\u95e8\u7684\u6377\u514b-\u4e4c\u514b\u5170\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u5904\u7406XML\u548cPDF\u683c\u5f0f\u5185\u5bb9\uff0c\u5e76\u9488\u5bf9\u6559\u80b2\u9886\u57df\u7684\u79d1\u6280\u672f\u8bed\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u9762\u5411\u6559\u80b2\u9886\u57df\u7684\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u5b8c\u6210\u4e86\u591a\u8bed\u8a00\u6750\u6599\u7684\u7ffb\u8bd1\uff0c\u6240\u6709\u5e94\u7528\u514d\u8d39\u5411\u5b66\u751f\u3001\u6559\u5e08\u548c\u7814\u7a76\u4eba\u5458\u5f00\u653e\u3002", "conclusion": "\u8be5\u9879\u76ee\u5c55\u793a\u4e86\u673a\u5668\u7ffb\u8bd1\u5728\u6559\u80b2\u9886\u57df\u7684\u6709\u6548\u5e94\u7528\uff0c\u4e3a\u591a\u8bed\u8a00\u6559\u80b2\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u6709\u52a9\u4e8e\u4e4c\u514b\u5170\u8bed\u5b66\u751f\u7684\u6559\u80b2\u878d\u5165\u3002"}}
{"id": "2509.09522", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09522", "abs": "https://arxiv.org/abs/2509.09522", "authors": ["Vadim Zadykian", "Bruno Andrade", "Haithem Afli"], "title": "Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs", "comment": null, "summary": "Semantic Textual Relatedness (STR) captures nuanced relationships between\ntexts that extend beyond superficial lexical similarity. In this study, we\ninvestigate STR in the context of job title matching - a key challenge in\nresume recommendation systems, where overlapping terms are often limited or\nmisleading. We introduce a self-supervised hybrid architecture that combines\ndense sentence embeddings with domain-specific Knowledge Graphs (KGs) to\nimprove both semantic alignment and explainability. Unlike previous work that\nevaluated models on aggregate performance, our approach emphasizes data\nstratification by partitioning the STR score continuum into distinct regions:\nlow, medium, and high semantic relatedness. This stratified evaluation enables\na fine-grained analysis of model performance across semantically meaningful\nsubspaces. We evaluate several embedding models, both with and without KG\nintegration via graph neural networks. The results show that fine-tuned SBERT\nmodels augmented with KGs produce consistent improvements in the high-STR\nregion, where the RMSE is reduced by 25% over strong baselines. Our findings\nhighlight not only the benefits of combining KGs with text embeddings, but also\nthe importance of regional performance analysis in understanding model\nbehavior. This granular approach reveals strengths and weaknesses hidden by\nglobal metrics, and supports more targeted model selection for use in Human\nResources (HR) systems and applications where fairness, explainability, and\ncontextual matching are essential.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u53e5\u5b50\u5d4c\u5165\u7684\u81ea\u76d1\u7763\u6df7\u5408\u67b6\u6784\uff0c\u7528\u4e8e\u8bed\u4e49\u6587\u672c\u76f8\u5173\u6027\u5206\u6790\uff0c\u901a\u8fc7\u5206\u5c42\u8bc4\u4f30\u65b9\u6cd5\u5728\u62db\u8058\u804c\u4f4d\u5339\u914d\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u76f8\u5173\u6027\u533a\u57df\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u7b80\u5386\u63a8\u8350\u7cfb\u7edf\u4e2d\u804c\u4f4d\u5339\u914d\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u8bcd\u6c47\u91cd\u53e0\u4f46\u5f80\u5f80\u6709\u9650\u6216\u8bef\u5bfc\uff0c\u9700\u8981\u6355\u6349\u8d85\u8d8a\u8868\u9762\u76f8\u4f3c\u6027\u7684\u8bed\u4e49\u5173\u7cfb\u3002", "method": "\u81ea\u76d1\u7763\u6df7\u5408\u67b6\u6784\uff0c\u7ed3\u5408\u7a20\u5bc6\u53e5\u5b50\u5d4c\u5165\u548c\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u56fe\u8c31\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u96c6\u6210\uff0c\u5e76\u91c7\u7528\u5206\u5c42\u8bc4\u4f30\u65b9\u6cd5\u5c06STR\u5206\u6570\u8fde\u7eed\u4f53\u5212\u5206\u4e3a\u4f4e\u3001\u4e2d\u3001\u9ad8\u4e09\u4e2a\u533a\u57df\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7ecf\u8fc7\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u7684\u5fae\u8c03SBERT\u6a21\u578b\u5728\u9ad8STR\u533a\u57df\u8868\u73b0\u6700\u4f73\uff0cRMSE\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u964d\u4f4e\u4e8625%\uff0c\u5728\u8bed\u4e49\u5bf9\u9f50\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u90fd\u6709\u663e\u8457\u6539\u5584\u3002", "conclusion": "\u77e5\u8bc6\u56fe\u8c31\u4e0e\u6587\u672c\u5d4c\u5165\u7684\u7ed3\u5408\u4ee5\u53ca\u5206\u5c42\u6027\u80fd\u5206\u6790\u65b9\u6cd5\u5bf9\u4e8e\u7406\u89e3\u6a21\u578b\u884c\u4e3a\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u79cd\u65b9\u6cd5\u63ed\u793a\u4e86\u5168\u5c40\u6307\u6807\u9690\u85cf\u7684\u4f18\u52bf\u548c\u5f31\u70b9\uff0c\u652f\u6301\u5728\u4eba\u529b\u8d44\u6e90\u7cfb\u7edf\u4e2d\u8fdb\u884c\u66f4\u6709\u9488\u5bf9\u6027\u7684\u6a21\u578b\u9009\u62e9\u3002"}}
{"id": "2509.09524", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09524", "abs": "https://arxiv.org/abs/2509.09524", "authors": ["Daniil Ignatev", "Nan Li", "Hugh Mee Wong", "Anh Dang", "Shane Kaszefski Yaschuk"], "title": "DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning and Label Distribution Learning", "comment": "11 pages, 4 figures; to appear at NLPerspectives@EMNLP-2025", "summary": "This system paper presents the DeMeVa team's approaches to the third edition\nof the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et\nal., 2025). We explore two directions: in-context learning (ICL) with large\nlanguage models, where we compare example sampling strategies; and label\ndistribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we\nevaluate several fine-tuning methods. Our contributions are twofold: (1) we\nshow that ICL can effectively predict annotator-specific annotations\n(perspectivist annotations), and that aggregating these predictions into soft\nlabels yields competitive performance; and (2) we argue that LDL methods are\npromising for soft label predictions and merit further exploration by the\nperspectivist community.", "AI": {"tldr": "DeMeVa\u56e2\u961f\u5728LeWiDi 2025\u4efb\u52a1\u4e2d\u63a2\u7d22\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u57fa\u4e8eRoBERTa\u7684\u6807\u7b7e\u5206\u5e03\u5b66\u4e60\u65b9\u6cd5\uff0c\u8bc1\u660e\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u80fd\u6709\u6548\u9884\u6d4b\u89c2\u70b9\u4e3b\u4e49\u6807\u6ce8", "motivation": "\u63a2\u7d22\u5728\u89c2\u70b9\u4e3b\u4e49\u6807\u6ce8\u4efb\u52a1\u4e2d\uff0c\u5982\u4f55\u5229\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u6807\u7b7e\u5206\u5e03\u5b66\u4e60\u65b9\u6cd5\u6765\u5904\u7406\u6807\u6ce8\u8005\u4e4b\u95f4\u7684\u5206\u6b67\uff0c\u9884\u6d4b\u8f6f\u6807\u7b7e", "method": "1) \u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u6bd4\u8f83\u4e0d\u540c\u793a\u4f8b\u91c7\u6837\u7b56\u7565\uff1b2) \u4f7f\u7528RoBERTa\u8fdb\u884c\u6807\u7b7e\u5206\u5e03\u5b66\u4e60\uff0c\u8bc4\u4f30\u591a\u79cd\u5fae\u8c03\u65b9\u6cd5", "result": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u6709\u6548\u9884\u6d4b\u6807\u6ce8\u8005\u7279\u5b9a\u7684\u6807\u6ce8\uff0c\u5c06\u8fd9\u4e9b\u9884\u6d4b\u805a\u5408\u6210\u8f6f\u6807\u7b7e\u53ef\u83b7\u5f97\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff1b\u6807\u7b7e\u5206\u5e03\u5b66\u4e60\u65b9\u6cd5\u5728\u8f6f\u6807\u7b7e\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u826f\u597d", "conclusion": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u6807\u7b7e\u5206\u5e03\u5b66\u4e60\u65b9\u6cd5\u90fd\u662f\u5904\u7406\u89c2\u70b9\u4e3b\u4e49\u6807\u6ce8\u4efb\u52a1\u7684\u6709\u6548\u9014\u5f84\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u63a2\u7d22"}}
{"id": "2509.09544", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09544", "abs": "https://arxiv.org/abs/2509.09544", "authors": ["Paolo Pedinotti", "Peter Baumann", "Nathan Jessurun", "Leslie Barrett", "Enrico Santus"], "title": "Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance NLP (2022-2025)", "comment": "7 pages, 6 appendices, EMNLP industry track", "summary": "Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling\nnew tasks and driving a proliferation of datasets and diversification of data\nsources. Yet, this transformation has outpaced traditional surveys. In this\npaper, we present MetaGraph, a generalizable methodology for extracting\nknowledge graphs from scientific literature and analyzing them to obtain a\nstructured, queryable view of research trends. We define an ontology for\nfinancial NLP research and apply an LLM-based extraction pipeline to 681 papers\n(2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals\nthree key phases: early LLM adoption and task/dataset innovation; critical\nreflection on LLM limitations; and growing integration of peripheral techniques\ninto modular systems. This structured view offers both practitioners and\nresearchers a clear understanding of how financial NLP has evolved -\nhighlighting emerging trends, shifting priorities, and methodological\nshifts-while also demonstrating a reusable approach for mapping scientific\nprogress in other domains.", "AI": {"tldr": "MetaGraph\u662f\u4e00\u4e2a\u4ece\u79d1\u5b66\u6587\u732e\u4e2d\u63d0\u53d6\u77e5\u8bc6\u56fe\u8c31\u7684\u65b9\u6cd5\u8bba\uff0c\u7528\u4e8e\u5206\u6790\u91d1\u878dNLP\u7814\u7a76\u8d8b\u52bf\uff0c\u63ed\u793a\u4e86LLM\u5728\u91d1\u878d\u9886\u57df\u7684\u4e09\u4e2a\u53d1\u5c55\u9636\u6bb5", "motivation": "LLM\u5feb\u901f\u91cd\u5851\u91d1\u878dNLP\u9886\u57df\uff0c\u4ea7\u751f\u4e86\u5927\u91cf\u6570\u636e\u96c6\u548c\u6570\u636e\u6e90\uff0c\u4f20\u7edf\u8c03\u67e5\u65b9\u6cd5\u5df2\u65e0\u6cd5\u8ddf\u4e0a\u8fd9\u79cd\u53d8\u9769\u901f\u5ea6\uff0c\u9700\u8981\u65b0\u7684\u5206\u6790\u5de5\u5177", "method": "\u5b9a\u4e49\u91d1\u878dNLP\u7814\u7a76\u672c\u4f53\u8bba\uff0c\u57fa\u4e8eLLM\u7684\u63d0\u53d6\u7ba1\u9053\u5904\u7406681\u7bc7\u8bba\u6587(2022-2025)\uff0c\u6784\u5efa\u53ef\u67e5\u8be2\u7684\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\u5927\u89c4\u6a21\u6570\u636e\u5206\u6790", "result": "\u63ed\u793a\u4e86\u91d1\u878dNLP\u53d1\u5c55\u7684\u4e09\u4e2a\u9636\u6bb5\uff1a\u65e9\u671fLLM\u91c7\u7528\u548c\u4efb\u52a1/\u6570\u636e\u96c6\u521b\u65b0\uff1b\u5bf9LLM\u5c40\u9650\u6027\u7684\u6279\u5224\u6027\u53cd\u601d\uff1b\u5916\u56f4\u6280\u672f\u5411\u6a21\u5757\u5316\u7cfb\u7edf\u7684\u96c6\u6210", "conclusion": "MetaGraph\u4e3a\u4ece\u4e1a\u8005\u548c\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5bf9\u91d1\u878dNLP\u6f14\u53d8\u7684\u6e05\u6670\u7406\u89e3\uff0c\u5c55\u793a\u4e86\u53ef\u91cd\u7528\u7684\u79d1\u5b66\u8fdb\u5c55\u6620\u5c04\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5176\u4ed6\u9886\u57df"}}
{"id": "2509.09583", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.09583", "abs": "https://arxiv.org/abs/2509.09583", "authors": ["Brittany Harbison", "Samuel Taubman", "Travis Taylor", "Ashok. K. Goel"], "title": "Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking", "comment": null, "summary": "Social connection is a vital part of learning, yet online course environments\npresent barriers to the organic formation of social groups. SAMI offers one\nsolution by facilitating student connections, but its effectiveness is\nconstrained by an incomplete Theory of Mind, limiting its ability to create an\neffective mental model of a student. One facet of this is its inability to\nintuit personality, which may influence the relevance of its recommendations.\nTo explore this, we propose a personality detection model utilizing GPTs\nzero-shot capability to infer Big-Five personality traits from forum\nintroduction posts, often encouraged in online courses. We benchmark its\nperformance against established models, demonstrating its efficacy in this\ntask. Furthermore, we integrate this model into SAMIs entity-based matchmaking\nsystem, enabling personality-informed social recommendations. Initial\nintegration suggests personality traits can complement existing matching\nfactors, though additional evaluation is required to determine their full\nimpact on student engagement and match quality.", "AI": {"tldr": "\u5229\u7528GPT\u96f6\u6837\u672c\u80fd\u529b\u4ece\u8bba\u575b\u4ecb\u7ecd\u5e16\u63a8\u65ad\u5927\u4e94\u4eba\u683c\u7279\u8d28\uff0c\u96c6\u6210\u5230SAMI\u5339\u914d\u7cfb\u7edf\u4e2d\u63d0\u4f9b\u4eba\u683c\u611f\u77e5\u7684\u793e\u4ea4\u63a8\u8350", "motivation": "\u5728\u7ebf\u8bfe\u7a0b\u73af\u5883\u963b\u788d\u793e\u4ea4\u7fa4\u4f53\u81ea\u7136\u5f62\u6210\uff0cSAMI\u7cfb\u7edf\u56e0\u7f3a\u4e4f\u5b8c\u6574\u5fc3\u667a\u7406\u8bba\u800c\u65e0\u6cd5\u6709\u6548\u5efa\u6a21\u5b66\u751f\u4e2a\u6027\uff0c\u5f71\u54cd\u63a8\u8350\u76f8\u5173\u6027", "method": "\u63d0\u51fa\u57fa\u4e8eGPT\u96f6\u6837\u672c\u80fd\u529b\u7684\u4eba\u683c\u68c0\u6d4b\u6a21\u578b\uff0c\u4ece\u8bba\u575b\u4ecb\u7ecd\u5e16\u63a8\u65ad\u5927\u4e94\u4eba\u683c\u7279\u8d28\uff0c\u5e76\u96c6\u6210\u5230SAMI\u7684\u5b9e\u4f53\u5339\u914d\u7cfb\u7edf\u4e2d", "result": "\u6a21\u578b\u5728\u4eba\u683c\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u6709\u6548\uff0c\u521d\u6b65\u96c6\u6210\u8868\u660e\u4eba\u683c\u7279\u8d28\u53ef\u4ee5\u8865\u5145\u73b0\u6709\u5339\u914d\u56e0\u7d20", "conclusion": "\u4eba\u683c\u7279\u8d28\u80fd\u591f\u589e\u5f3a\u793e\u4ea4\u63a8\u8350\u7cfb\u7edf\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u8bc4\u4f30\u5176\u5bf9\u5b66\u751f\u53c2\u4e0e\u5ea6\u548c\u5339\u914d\u8d28\u91cf\u7684\u5168\u9762\u5f71\u54cd"}}
{"id": "2509.09593", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09593", "abs": "https://arxiv.org/abs/2509.09593", "authors": ["Bangzhao Shu", "Isha Joshi", "Melissa Karnaze", "Anh C. Pham", "Ishita Kakkar", "Sindhu Kothe", "Arpine Hovasapian", "Mai ElSherief"], "title": "Fluent but Unfeeling: The Emotional Blind Spots of Language Models", "comment": "Camera-ready version for ICWSM 2026. First two authors contributed\n  equally", "summary": "The versatility of Large Language Models (LLMs) in natural language\nunderstanding has made them increasingly popular in mental health research.\nWhile many studies explore LLMs' capabilities in emotion recognition, a\ncritical gap remains in evaluating whether LLMs align with human emotions at a\nfine-grained level. Existing research typically focuses on classifying emotions\ninto predefined, limited categories, overlooking more nuanced expressions. To\naddress this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit\ncommunities featuring 251 fine-grained, self-disclosed emotion labels. Our\ncomprehensive evaluation framework examines predicted emotion terms and\ndecomposes them into eight basic emotions using established emotion theories,\nenabling a fine-grained comparison. Systematic testing of prevalent LLMs under\nvarious prompt settings reveals that accurately predicting emotions that align\nwith human self-disclosed emotions remains challenging. Qualitative analysis\nfurther shows that while certain LLMs generate emotion terms consistent with\nestablished emotion theories and definitions, they sometimes fail to capture\ncontextual cues as effectively as human self-disclosures. These findings\nhighlight the limitations of LLMs in fine-grained emotion alignment and offer\ninsights for future research aimed at enhancing their contextual understanding.", "AI": {"tldr": "EXPRESS\u57fa\u51c6\u6570\u636e\u96c6\u8bc4\u4f30LLM\u5728\u7ec6\u7c92\u5ea6\u60c5\u611f\u8bc6\u522b\u65b9\u9762\u4e0e\u4eba\u7c7b\u81ea\u6211\u62ab\u9732\u60c5\u611f\u7684\u5bf9\u9f50\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u51c6\u786e\u9884\u6d4b\u4e0e\u4eba\u7c7b\u60c5\u611f\u4e00\u81f4\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5c06\u60c5\u611f\u5206\u7c7b\u4e3a\u9884\u5b9a\u4e49\u7684\u6709\u9650\u7c7b\u522b\uff0c\u5ffd\u7565\u4e86\u66f4\u7ec6\u5fae\u7684\u60c5\u611f\u8868\u8fbe\uff0c\u9700\u8981\u8bc4\u4f30LLM\u662f\u5426\u80fd\u5728\u7ec6\u7c92\u5ea6\u5c42\u9762\u4e0e\u4eba\u7c7b\u60c5\u611f\u5bf9\u9f50", "method": "\u5f15\u5165EXPRESS\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5305\u542b251\u4e2a\u7ec6\u7c92\u5ea6\u60c5\u611f\u6807\u7b7e\uff09\uff0c\u901a\u8fc7\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\u5206\u6790\u9884\u6d4b\u60c5\u611f\u672f\u8bed\u5e76\u5c06\u5176\u5206\u89e3\u4e3a\u516b\u79cd\u57fa\u672c\u60c5\u611f\uff0c\u7cfb\u7edf\u6d4b\u8bd5\u4e3b\u6d41LLM\u5728\u4e0d\u540c\u63d0\u793a\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0", "result": "\u51c6\u786e\u9884\u6d4b\u4e0e\u4eba\u7c7b\u81ea\u6211\u62ab\u9732\u60c5\u611f\u4e00\u81f4\u7684\u60c5\u611f\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff1b\u67d0\u4e9bLLM\u80fd\u751f\u6210\u7b26\u5408\u60c5\u611f\u7406\u8bba\u548c\u5b9a\u4e49\u7684\u60c5\u611f\u672f\u8bed\uff0c\u4f46\u5728\u6355\u6349\u4e0a\u4e0b\u6587\u7ebf\u7d22\u65b9\u9762\u4e0d\u5982\u4eba\u7c7b\u81ea\u6211\u62ab\u9732\u6709\u6548", "conclusion": "LLM\u5728\u7ec6\u7c92\u5ea6\u60c5\u611f\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u672a\u6765\u7814\u7a76\u9700\u8981\u589e\u5f3a\u5176\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b"}}
{"id": "2509.09602", "categories": ["cs.CL", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.09602", "abs": "https://arxiv.org/abs/2509.09602", "authors": ["Yiqun T. Chen", "Tyler H. McCormick", "Li Liu", "Abhirup Datta"], "title": "LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death Determination", "comment": null, "summary": "Verbal autopsy (VA) is a critical tool for estimating causes of death in\nresource-limited settings where medical certification is unavailable. This\nstudy presents LA-VA, a proof-of-concept pipeline that combines Large Language\nModels (LLMs) with traditional algorithmic approaches and embedding-based\nclassification for improved cause-of-death prediction. Using the Population\nHealth Metrics Research Consortium (PHMRC) dataset across three age categories\n(Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches:\nGPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles.\nOur results demonstrate that GPT-5 achieves the highest individual performance\nwith average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5%\n(Neonate), outperforming traditional statistical machine learning baselines by\n5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches\ncould substantially improve verbal autopsy accuracy, with important\nimplications for global health surveillance in low-resource settings.", "AI": {"tldr": "LA-VA\u76ee\u6807\u662f\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u4f20\u7edf\u7b97\u6cd5\u65b9\u6cd5\u6765\u63d0\u9ad8\u8d28\u91cf\u7f3a\u4e4f\u5730\u533a\u7684\u8bed\u8a00\u67e5\u5c3b\uff08VA\uff09\u51b3\u7b56\u51c6\u786e\u6027\uff0c\u4ee5\u4f18\u5316\u5168\u7403\u5065\u5eb7\u76d1\u6d4b\u3002", "motivation": "\u8bed\u8a00\u67e5\u5c3b\uff08VA\uff09\u5728\u8d44\u6e90\u6709\u9650\u5730\u533a\u5bf9\u4f30\u8ba1\u6b7b\u56e0\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u7b97\u6cd5\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u6709\u9650\u3002\u7814\u7a76\u8005\u5c1d\u8bd5\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5927\u80fd\u529b\u6765\u63d0\u5347\u8fd9\u4e00\u8fc7\u7a0b\u7684\u6548\u679c\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86LA-VA\u6d41\u6c34\u7ebf\uff0c\u7ed3\u5408\u4e86GPT-5\u6a21\u578b\u9884\u6d4b\u3001LCVA\u57fa\u51c6\u3001\u6587\u672c\u5d4c\u5165\u5206\u7c7b\u4ee5\u53ca\u5143\u5b66\u4e60\u96c6\u6210\u7b49\u591a\u79cd\u65b9\u6cd5\u3002\u4f7f\u7528PHMRC\u6570\u636e\u96c6\uff0c\u6d89\u53ca\u4e09\u4e2a\u5e74\u9f84\u6bb5\uff1a\u6210\u4eba\uff087,580\u4f8b\uff09\u3001\u513f\u7ae5\uff081,960\u4f8b\uff09\u548c\u65b0\u751f\u513f\uff082,438\u4f8b\uff09\u3002", "result": "GPT-5\u5728\u4e09\u4e2a\u5e74\u9f84\u7ec4\u4e2d\u90fd\u8868\u73b0\u6700\u4f73\uff0c\u5e73\u5747\u6d4b\u8bd5\u7ad9\u70b9\u51c6\u786e\u7387\u5206\u522b\u4e3a48.6%\uff08\u6210\u4eba\uff09\u300150.5%\uff08\u513f\u7ae5\uff09\u548c53.5%\uff08\u65b0\u751f\u513f\uff09\uff0c\u8f83\u4f20\u7edf\u7edf\u8ba1\u673a\u5668\u5b66\u4e60\u57fa\u51c6\u63d0\u9ad85-10%\u3002", "conclusion": "\u7b80\u5355\u7684\u5546\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8f85\u52a9\u65b9\u6cd5\u53ef\u663e\u8457\u63d0\u9ad8\u8bed\u8a00\u67e5\u5c3b\u7684\u51c6\u786e\u6027\uff0c\u5bf9\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e2d\u7684\u5168\u7403\u5065\u5eb7\u76d1\u6d4b\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.09629", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09629", "abs": "https://arxiv.org/abs/2509.09629", "authors": ["Minghang Zhu", "Zhengliang Shi", "Zhiwei Xu", "Shiguang Wu", "Lingjie Wang", "Pengjie Ren", "Zhaochun Ren", "Zhumin Chen"], "title": "Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems", "comment": "EMNLP 2025 Findings", "summary": "The advancement of large language models (LLMs) has enabled the construction\nof multi-agent systems to solve complex tasks by dividing responsibilities\namong specialized agents, such as a planning agent for subgoal generation and a\ngrounding agent for executing tool-use actions. Most existing methods typically\nfine-tune these agents independently, leading to capability gaps among them\nwith poor coordination. To address this, we propose MOAT, a Multi-Agent Joint\nAlignment Tuning framework that improves agents collaboration through iterative\nalignment. MOAT alternates between two key stages: (1) Planning Agent\nAlignment, which optimizes the planning agent to generate subgoal sequences\nthat better guide the grounding agent; and (2) Grounding Agent Improving, which\nfine-tunes the grounding agent using diverse subgoal-action pairs generated by\nthe agent itself to enhance its generalization capablity. Theoretical analysis\nproves that MOAT ensures a non-decreasing and progressively convergent training\nprocess. Experiments across six benchmarks demonstrate that MOAT outperforms\nstate-of-the-art baselines, achieving average improvements of 3.1% on held-in\ntasks and 4.4% on held-out tasks.", "AI": {"tldr": "MOAT\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u8054\u5408\u5bf9\u9f50\u8c03\u4f18\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u5bf9\u9f50\u63d0\u5347\u667a\u80fd\u4f53\u534f\u4f5c\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u72ec\u7acb\u5fae\u8c03\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5404\u4e2a\u667a\u80fd\u4f53\uff0c\u5bfc\u81f4\u80fd\u529b\u5dee\u8ddd\u548c\u534f\u8c03\u6027\u5dee\u7684\u95ee\u9898", "method": "MOAT\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u4ea4\u66ff\u9636\u6bb5\uff1a\u89c4\u5212\u667a\u80fd\u4f53\u5bf9\u9f50\uff08\u4f18\u5316\u5b50\u76ee\u6807\u751f\u6210\uff09\u548c\u63a5\u5730\u667a\u80fd\u4f53\u6539\u8fdb\uff08\u4f7f\u7528\u81ea\u751f\u6210\u6570\u636e\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff09", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u53473.1%\uff08held-in\u4efb\u52a1\uff09\u548c4.4%\uff08held-out\u4efb\u52a1\uff09\uff0c\u7406\u8bba\u5206\u6790\u8bc1\u660e\u8bad\u7ec3\u8fc7\u7a0b\u975e\u9012\u51cf\u4e14\u6e10\u8fdb\u6536\u655b", "conclusion": "MOAT\u901a\u8fc7\u8054\u5408\u5bf9\u9f50\u8c03\u4f18\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u534f\u8c03\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd"}}
{"id": "2509.09650", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.09650", "abs": "https://arxiv.org/abs/2509.09650", "authors": ["Siddarth Mamidanna", "Daking Rai", "Ziyu Yao", "Yilun Zhou"], "title": "All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens", "comment": "EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) demonstrate proficiency across numerous\ncomputational tasks, yet their inner workings remain unclear. In theory, the\ncombination of causal self-attention and multilayer perceptron layers allows\nevery token to access and compute information based on all preceding tokens. In\npractice, to what extent are such operations present? In this paper, on mental\nmath tasks (i.e., direct math calculation via next-token prediction without\nexplicit reasoning), we investigate this question in three steps: inhibiting\ninput-specific token computations in the initial layers, restricting the routes\nof information transfer across token positions in the next few layers, and\nforcing all computation to happen at the last token in the remaining layers.\nWith two proposed techniques, Context-Aware Mean Ablation (CAMA) and\nAttention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with\nhigh accuracy on a wide variety of mental math tasks, where meaningful\ncomputation occurs very late (in terms of layer depth) and only at the last\ntoken, which receives information of other tokens in few specific middle\nlayers. Experiments on a variety of models and arithmetic expressions show that\nthis subgraph is sufficient and necessary for high model performance, transfers\nacross different models, and works on a variety of input styles. Ablations on\ndifferent CAMA and ABP alternatives reveal their unique advantages over other\nmethods, which may be of independent interest.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7Context-Aware Mean Ablation (CAMA)\u548cAttention-Based Peeking (ABP)\u6280\u672f\uff0c\u5728\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5fc3\u7b97\u4efb\u52a1\u4e2d\u53d1\u73b0\u4e86\u4e00\u4e2aAll-for-One\u5b50\u56fe(AF1)\uff0c\u8be5\u5b50\u56fe\u5728\u6df1\u5c42\u4e14\u4ec5\u5728\u6700\u540e\u4e00\u4e2atoken\u5904\u8fdb\u884c\u8ba1\u7b97\uff0c\u901a\u8fc7\u7279\u5b9a\u4e2d\u95f4\u5c42\u63a5\u6536\u5176\u4ed6token\u4fe1\u606f\uff0c\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\u4e14\u53ef\u8de8\u6a21\u578b\u8fc1\u79fb\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8ba1\u7b97\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u4ecd\u4e0d\u660e\u786e\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u6a21\u578b\u5728\u5b9e\u9645\u5fc3\u7b97\u4efb\u52a1\u4e2d\u5982\u4f55\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u548cMLP\u5c42\u8fdb\u884c\u4fe1\u606f\u5904\u7406\u548c\u8ba1\u7b97\u3002", "method": "\u91c7\u7528\u4e09\u6b65\u9aa4\u65b9\u6cd5\uff1a\u6291\u5236\u521d\u59cb\u5c42\u7684\u8f93\u5165\u7279\u5b9atoken\u8ba1\u7b97\u3001\u9650\u5236\u4e2d\u95f4\u5c42\u8de8token\u4f4d\u7f6e\u7684\u4fe1\u606f\u4f20\u9012\u8def\u5f84\u3001\u5f3a\u5236\u5269\u4f59\u5c42\u5728\u6700\u540e\u4e00\u4e2atoken\u5904\u8fdb\u884c\u8ba1\u7b97\u3002\u63d0\u51faCAMA\u548cABP\u4e24\u79cd\u6280\u672f\u6765\u8bc6\u522b\u5173\u952e\u7684All-for-One\u5b50\u56fe\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0AF1\u5b50\u56fe\u5728\u5404\u79cd\u5fc3\u7b97\u4efb\u52a1\u4e2d\u5177\u6709\u9ad8\u51c6\u786e\u6027\uff0c\u8ba1\u7b97\u53d1\u751f\u5728\u6df1\u5c42\u4e14\u4ec5\u6700\u540e\u4e00\u4e2atoken\u5904\uff0c\u901a\u8fc7\u7279\u5b9a\u4e2d\u95f4\u5c42\u63a5\u6536\u4fe1\u606f\u3002\u8be5\u5b50\u56fe\u5bf9\u6a21\u578b\u6027\u80fd\u65e2\u5145\u5206\u53c8\u5fc5\u8981\uff0c\u53ef\u8de8\u6a21\u578b\u8fc1\u79fb\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8f93\u5165\u98ce\u683c\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5728\u5fc3\u7b97\u4efb\u52a1\u4e2d\u7684\u7279\u5b9a\u8ba1\u7b97\u6a21\u5f0f\uff0cAF1\u5b50\u56fe\u7684\u53d1\u73b0\u6709\u52a9\u4e8e\u7406\u89e3\u6a21\u578b\u5185\u90e8\u5de5\u4f5c\u673a\u5236\uff0cCAMA\u548cABP\u6280\u672f\u7684\u4f18\u52bf\u4e5f\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2509.09660", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09660", "abs": "https://arxiv.org/abs/2509.09660", "authors": ["Mohsen Fayyaz", "Ali Modarressi", "Hanieh Deilamsalehy", "Franck Dernoncourt", "Ryan Rossi", "Trung Bui", "Hinrich Sch\u00fctze", "Nanyun Peng"], "title": "Steering MoE LLMs via Expert (De)Activation", "comment": null, "summary": "Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token\nthrough a subset of specialized Feed-Forward Networks (FFN), known as experts.\nWe present SteerMoE, a framework for steering MoE models by detecting and\ncontrolling behavior-linked experts. Our detection method identifies experts\nwith distinct activation patterns across paired inputs exhibiting contrasting\nbehaviors. By selectively (de)activating such experts during inference, we\ncontrol behaviors like faithfulness and safety without retraining or modifying\nweights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to\n+20% and faithfulness by +27%. In adversarial attack mode, it drops safety by\n-41% alone, and -100% when combined with existing jailbreak methods, bypassing\nall safety guardrails and exposing a new dimension of alignment faking hidden\nwithin experts.", "AI": {"tldr": "SteerMoE\u6846\u67b6\u901a\u8fc7\u68c0\u6d4b\u548c\u63a7\u5236MoE\u6a21\u578b\u4e2d\u7684\u884c\u4e3a\u76f8\u5173\u4e13\u5bb6\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u8c03\u63a7LLM\u7684\u5fe0\u5b9e\u6027\u548c\u5b89\u5168\u6027\u884c\u4e3a\uff0c\u65e2\u80fd\u63d0\u5347\u5b89\u5168\u6027+20%\u548c\u5fe0\u5b9e\u6027+27%\uff0c\u4e5f\u80fd\u5728\u5bf9\u6297\u6a21\u5f0f\u4e0b\u5b8c\u5168\u7ed5\u8fc7\u5b89\u5168\u9632\u62a4", "motivation": "\u73b0\u6709MoE\u6a21\u578b\u4e2d\u7684\u4e13\u5bb6\u7f51\u7edc\u53ef\u80fd\u5305\u542b\u7279\u5b9a\u884c\u4e3a\u6a21\u5f0f\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u4e13\u5bb6\u8fdb\u884c\u68c0\u6d4b\u548c\u63a7\u5236\u7684\u65b9\u6cd5\uff0c\u65e0\u6cd5\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u8c03\u63a7\u6a21\u578b\u884c\u4e3a", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\u4e0d\u540c\u884c\u4e3a\u8f93\u5165\u7684\u4e13\u5bb6\u6fc0\u6d3b\u6a21\u5f0f\u6765\u68c0\u6d4b\u884c\u4e3a\u76f8\u5173\u4e13\u5bb6\uff0c\u5728\u63a8\u7406\u65f6\u9009\u62e9\u6027\u6fc0\u6d3b\u6216\u505c\u7528\u8fd9\u4e9b\u4e13\u5bb6\u6765\u63a7\u5236\u6a21\u578b\u884c\u4e3a", "result": "\u572811\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c6\u4e2aLLM\u4e0a\uff0c\u5b89\u5168\u6027\u63d0\u5347\u6700\u9ad8+20%\uff0c\u5fe0\u5b9e\u6027\u63d0\u5347+27%\uff1b\u5bf9\u6297\u6a21\u5f0f\u4e0b\u5b89\u5168\u6027\u4e0b\u964d-41%\uff0c\u7ed3\u5408\u8d8a\u72f1\u65b9\u6cd5\u53ef\u5b8c\u5168\u7ed5\u8fc7\u6240\u6709\u5b89\u5168\u9632\u62a4(-100%)", "conclusion": "MoE\u6a21\u578b\u4e2d\u5b58\u5728\u9690\u85cf\u7684\u5bf9\u9f50\u4f2a\u9020\u7ef4\u5ea6\uff0c\u4e13\u5bb6\u7f51\u7edc\u5305\u542b\u53ef\u88ab\u64cd\u63a7\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u8fd9\u66b4\u9732\u4e86\u65b0\u7684\u5b89\u5168\u98ce\u9669"}}
{"id": "2509.09675", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09675", "abs": "https://arxiv.org/abs/2509.09675", "authors": ["Runpeng Dai", "Linfeng Song", "Haolin Liu", "Zhenwen Liang", "Dian Yu", "Haitao Mi", "Zhaopeng Tu", "Rui Liu", "Tong Zheng", "Hongtu Zhu", "Dong Yu"], "title": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models", "comment": "21 pages", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm\nfor enhancing the reasoning ability of Large Language Models (LLMs). Yet\ncurrent RLVR methods often explore poorly, leading to premature convergence and\nentropy collapse. To address this challenge, we introduce Curiosity-Driven\nExploration (CDE), a framework that leverages the model's own intrinsic sense\nof curiosity to guide exploration. We formalize curiosity with signals from\nboth the actor and the critic: for the actor, we use perplexity over its\ngenerated response, and for the critic, we use the variance of value estimates\nfrom a multi-head architecture. Both signals serve as an exploration bonus\nwithin the RLVR framework to guide the model. Our theoretical analysis shows\nthat the actor-wise bonus inherently penalizes overconfident errors and\npromotes diversity among correct responses; moreover, we connect the\ncritic-wise bonus to the well-established count-based exploration bonus in RL.\nEmpirically, our method achieves an approximate +3 point improvement over\nstandard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a\ncalibration collapse mechanism within RLVR, shedding light on common LLM\nfailure modes.", "AI": {"tldr": "\u63d0\u51fa\u597d\u5947\u5fc3\u9a71\u52a8\u63a2\u7d22(CDE)\u6846\u67b6\uff0c\u901a\u8fc7\u6f14\u5458\u56f0\u60d1\u5ea6\u548c\u8bc4\u8bba\u5bb6\u4ef7\u503c\u4f30\u8ba1\u65b9\u5dee\u4f5c\u4e3a\u5185\u5728\u5956\u52b1\uff0c\u89e3\u51b3RLVR\u65b9\u6cd5\u63a2\u7d22\u4e0d\u8db3\u548c\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u5728AIME\u57fa\u51c6\u4e0a\u6bd4\u6807\u51c6RLVR\u63d0\u5347\u7ea63\u5206", "motivation": "\u5f53\u524dRLVR\u65b9\u6cd5\u5b58\u5728\u63a2\u7d22\u4e0d\u8db3\u3001\u8fc7\u65e9\u6536\u655b\u548c\u71b5\u5d29\u6e83\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u597d\u7684\u63a2\u7d22\u673a\u5236\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b", "method": "\u4f7f\u7528\u6f14\u5458\u56f0\u60d1\u5ea6(\u5bf9\u751f\u6210\u54cd\u5e94\u7684\u56f0\u60d1\u5ea6)\u548c\u8bc4\u8bba\u5bb6\u4ef7\u503c\u4f30\u8ba1\u65b9\u5dee(\u591a\u5934\u67b6\u6784)\u4f5c\u4e3a\u597d\u5947\u5fc3\u4fe1\u53f7\uff0c\u4f5c\u4e3aRLVR\u6846\u67b6\u4e2d\u7684\u63a2\u7d22\u5956\u52b1", "result": "\u5728AIME\u57fa\u51c6\u4e0a\u76f8\u6bd4\u6807\u51c6RLVR(GRPO/PPO)\u83b7\u5f97\u7ea63\u5206\u7684\u63d0\u5347\uff0c\u5e76\u8bc6\u522b\u51faRLVR\u4e2d\u7684\u6821\u51c6\u5d29\u6e83\u673a\u5236", "conclusion": "CDE\u6846\u67b6\u901a\u8fc7\u5185\u5728\u597d\u5947\u5fc3\u4fe1\u53f7\u6709\u6548\u63d0\u5347\u63a2\u7d22\u80fd\u529b\uff0c\u7406\u8bba\u5206\u6790\u663e\u793a\u8be5\u65b9\u6cd5\u60e9\u7f5a\u8fc7\u5ea6\u81ea\u4fe1\u9519\u8bef\u5e76\u4fc3\u8fdb\u6b63\u786e\u54cd\u5e94\u7684\u591a\u6837\u6027"}}
