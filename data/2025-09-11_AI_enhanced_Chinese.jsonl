{"id": "2509.08438", "categories": ["cs.CL", "cs.MM", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.08438", "abs": "https://arxiv.org/abs/2509.08438", "authors": ["Jinzhong Ning", "Paerhati Tulajiang", "Yingying Le", "Yijia Zhang", "Yuanyuan Sun", "Hongfei Lin", "Haifeng Liu"], "title": "CommonVoice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction with a New Dataset and Multi-Order Generative Framework", "comment": null, "summary": "Speech Relation Extraction (SpeechRE) aims to extract relation triplets\ndirectly from speech. However, existing benchmark datasets rely heavily on\nsynthetic data, lacking sufficient quantity and diversity of real human speech.\nMoreover, existing models also suffer from rigid single-order generation\ntemplates and weak semantic alignment, substantially limiting their\nperformance. To address these challenges, we introduce CommonVoice-SpeechRE, a\nlarge-scale dataset comprising nearly 20,000 real-human speech samples from\ndiverse speakers, establishing a new benchmark for SpeechRE research.\nFurthermore, we propose the Relation Prompt-Guided Multi-Order Generative\nEnsemble (RPG-MoGe), a novel framework that features: (1) a multi-order triplet\ngeneration ensemble strategy, leveraging data diversity through diverse element\norders during both training and inference, and (2) CNN-based latent relation\nprediction heads that generate explicit relation prompts to guide cross-modal\nalignment and accurate triplet generation. Experiments show our approach\noutperforms state-of-the-art methods, providing both a benchmark dataset and an\neffective solution for real-world SpeechRE. The source code and dataset are\npublicly available at https://github.com/NingJinzhong/SpeechRE_RPG_MoGe.", "AI": {"tldr": "\u63d0\u51fa\u4e86CommonVoice-SpeechRE\u5927\u89c4\u6a21\u771f\u5b9e\u8bed\u97f3\u6570\u636e\u96c6\u548cRPG-MoGe\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5e8f\u4e09\u5143\u7ec4\u751f\u6210\u548c\u5173\u7cfb\u63d0\u793a\u5f15\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u5173\u7cfb\u62bd\u53d6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709SpeechRE\u57fa\u51c6\u6570\u636e\u96c6\u4e25\u91cd\u4f9d\u8d56\u5408\u6210\u6570\u636e\uff0c\u7f3a\u4e4f\u771f\u5b9e\u4eba\u7c7b\u8bed\u97f3\u7684\u591a\u6837\u6027\u548c\u6570\u91cf\uff1b\u73b0\u6709\u6a21\u578b\u5b58\u5728\u5355\u4e00\u751f\u6210\u6a21\u677f\u548c\u5f31\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "1) \u6784\u5efa\u5305\u542b\u8fd12\u4e07\u6761\u771f\u5b9e\u4eba\u7c7b\u8bed\u97f3\u6837\u672c\u7684CommonVoice-SpeechRE\u6570\u636e\u96c6\uff1b2) \u63d0\u51faRPG-MoGe\u6846\u67b6\uff0c\u5305\u542b\u591a\u5e8f\u4e09\u5143\u7ec4\u751f\u6210\u96c6\u6210\u7b56\u7565\u548c\u57fa\u4e8eCNN\u7684\u6f5c\u5728\u5173\u7cfb\u9884\u6d4b\u5934\u751f\u6210\u5173\u7cfb\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e3aSpeechRE\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\u548c\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u89e3\u51b3\u4e86SpeechRE\u9886\u57df\u7684\u6570\u636e\u7a00\u7f3a\u548c\u6a21\u578b\u6027\u80fd\u9650\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u771f\u5b9e\u6570\u636e\u96c6\u548c\u521b\u65b0\u6846\u67b6\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.08800", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.08800", "abs": "https://arxiv.org/abs/2509.08800", "authors": ["Yonghyun Kim", "Junhyung Park", "Joonhyung Bae", "Kirak Kim", "Taegyun Kwon", "Alexander Lerch", "Juhan Nam"], "title": "PianoVAM: A Multimodal Piano Performance Dataset", "comment": "Accepted to the 26th International Society for Music Information\n  Retrieval (ISMIR) Conference, 2025", "summary": "The multimodal nature of music performance has driven increasing interest in\ndata beyond the audio domain within the music information retrieval (MIR)\ncommunity. This paper introduces PianoVAM, a comprehensive piano performance\ndataset that includes videos, audio, MIDI, hand landmarks, fingering labels,\nand rich metadata. The dataset was recorded using a Disklavier piano, capturing\naudio and MIDI from amateur pianists during their daily practice sessions,\nalongside synchronized top-view videos in realistic and varied performance\nconditions. Hand landmarks and fingering labels were extracted using a\npretrained hand pose estimation model and a semi-automated fingering annotation\nalgorithm. We discuss the challenges encountered during data collection and the\nalignment process across different modalities. Additionally, we describe our\nfingering annotation method based on hand landmarks extracted from videos.\nFinally, we present benchmarking results for both audio-only and audio-visual\npiano transcription using the PianoVAM dataset and discuss additional potential\napplications.", "AI": {"tldr": "PianoVAM\u662f\u4e00\u4e2a\u5305\u542b\u89c6\u9891\u3001\u97f3\u9891\u3001MIDI\u3001\u624b\u90e8\u5173\u952e\u70b9\u3001\u6307\u6cd5\u6807\u6ce8\u548c\u4e30\u5bcc\u5143\u6570\u636e\u7684\u94a2\u7434\u6f14\u594f\u6570\u636e\u96c6\uff0c\u901a\u8fc7Disklavier\u94a2\u7434\u91c7\u96c6\u4e1a\u4f59\u94a2\u7434\u5bb6\u7684\u65e5\u5e38\u7ec3\u4e60\u6570\u636e\uff0c\u652f\u6301\u591a\u6a21\u6001\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u7814\u7a76\u3002", "motivation": "\u97f3\u4e50\u8868\u6f14\u7684\u591a\u6a21\u6001\u7279\u6027\u4fc3\u4f7fMIR\u793e\u533a\u5bf9\u97f3\u9891\u4ee5\u5916\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u94a2\u7434\u6f14\u594f\u6570\u636e\u96c6\u6765\u652f\u6301\u591a\u6a21\u6001\u7814\u7a76\u3002", "method": "\u4f7f\u7528Disklavier\u94a2\u7434\u91c7\u96c6\u4e1a\u4f59\u94a2\u7434\u5bb6\u7684\u65e5\u5e38\u7ec3\u4e60\u6570\u636e\uff0c\u5305\u62ec\u540c\u6b65\u7684\u97f3\u9891\u3001MIDI\u548c\u4fef\u89c6\u89c6\u9891\uff1b\u4f7f\u7528\u9884\u8bad\u7ec3\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\u63d0\u53d6\u624b\u90e8\u5173\u952e\u70b9\uff0c\u91c7\u7528\u534a\u81ea\u52a8\u6307\u6cd5\u6807\u6ce8\u7b97\u6cd5\u8fdb\u884c\u6307\u6cd5\u6807\u6ce8\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u5305\u542b\u591a\u6a21\u6001\u6570\u636e\u7684PianoVAM\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u97f3\u9891\u8f6c\u5f55\u548c\u89c6\u542c\u94a2\u7434\u8f6c\u5f55\u7684\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u3002", "conclusion": "PianoVAM\u6570\u636e\u96c6\u4e3a\u591a\u6a21\u6001\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u652f\u6301\u97f3\u9891\u8f6c\u5f55\u3001\u89c6\u542c\u5206\u6790\u7b49\u591a\u79cd\u5e94\u7528\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u591a\u6837\u5316\u8868\u6f14\u6761\u4ef6\u4e0b\u7684\u6570\u636e\u91c7\u96c6\u65b9\u6cd5\u3002"}}
{"id": "2509.08031", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.08031", "abs": "https://arxiv.org/abs/2509.08031", "authors": ["Sidharth Surapaneni", "Hoang Nguyen", "Jash Mehta", "Aman Tiwari", "Oluwanifemi Bamgbose", "Akshay Kalkunte", "Sai Rajeswar", "Sathwik Tejaswi Madhusudhan"], "title": "LALM-Eval: An Open-Source Toolkit for Holistic Evaluation of Large Audio Language Models", "comment": null, "summary": "Large Audio Language Models (LALMs) are rapidly advancing, but evaluating\nthem remains challenging due to inefficient toolkits that limit fair comparison\nand systematic assessment. Current frameworks suffer from three critical\nissues: slow processing that bottlenecks large-scale studies, inconsistent\nprompting that hurts reproducibility, and narrow task coverage that misses\nimportant audio reasoning capabilities. We introduce LALM-Eval, an efficient\nand comprehensive evaluation framework for LALMs. Our system achieves a speedup\nof up to 127% over existing toolkits through optimized batch processing and\nparallel execution, enabling large-scale evaluations previously impractical. We\nprovide standardized prompting protocols and flexible configurations for fair\nmodel comparison across diverse scenarios. Additionally, we introduce two new\nevaluation categories: LLM-Adaptive Diarization for temporal audio\nunderstanding and Spoken Language Reasoning for complex audio-based cognitive\ntasks. Through evaluation across 380+ tasks, we reveal significant gaps in\ncurrent LALMs, particularly in temporal understanding and complex spoken\nlanguage reasoning tasks. Our findings also highlight a lack of standardization\nin instruction modality existent across audio benchmarks, which can lead up\nperformance differences up to 9.5 absolute points on the challenging complex\ninstruction following downstream tasks. LALM-Eval provides both practical\nevaluation tools and insights into model limitations, advancing systematic LALM\ndevelopment.", "AI": {"tldr": "LALM-Eval\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u5927\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u5177\u5305\u5904\u7406\u6162\u3001\u63d0\u793a\u4e0d\u4e00\u81f4\u548c\u4efb\u52a1\u8986\u76d6\u7a84\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86127%\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u8bc4\u4f30\u7c7b\u522b\u3002", "motivation": "\u5f53\u524d\u5927\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u5de5\u5177\u5b58\u5728\u5904\u7406\u6548\u7387\u4f4e\u3001\u63d0\u793a\u4e0d\u4e00\u81f4\u548c\u4efb\u52a1\u8986\u76d6\u8303\u56f4\u6709\u9650\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u516c\u5e73\u6bd4\u8f83\u548c\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u5f00\u53d1LALM-Eval\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u6279\u5904\u7406\u548c\u5e76\u884c\u6267\u884c\u5b9e\u73b0\u9ad8\u6548\u8bc4\u4f30\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u63d0\u793a\u534f\u8bae\uff0c\u5e76\u5f15\u5165LLM-Adaptive Diarization\u548cSpoken Language Reasoning\u4e24\u4e2a\u65b0\u8bc4\u4f30\u7c7b\u522b\u3002", "result": "\u5728380\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u73b0\u6709LALM\u5728\u65f6\u95f4\u7406\u89e3\u548c\u590d\u6742\u8bed\u97f3\u63a8\u7406\u4efb\u52a1\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u6307\u4ee4\u6a21\u6001\u7f3a\u4e4f\u6807\u51c6\u5316\u5bfc\u81f4\u6027\u80fd\u5dee\u5f02\u9ad8\u8fbe9.5\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "LALM-Eval\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8bc4\u4f30\u5de5\u5177\u548c\u6a21\u578b\u5c40\u9650\u6027\u7684\u6df1\u5165\u89c1\u89e3\uff0c\u63a8\u52a8\u4e86\u5927\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\u6027\u53d1\u5c55\u3002"}}
{"id": "2509.07998", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07998", "abs": "https://arxiv.org/abs/2509.07998", "authors": ["Mesay Gemeda Yigezu", "Girma Yohannis Bade", "Atnafu Lambebo Tonja", "Olga Kolesnikova", "Grigori Sidorov", "Alexander Gelbukh"], "title": "Bilingual Word Level Language Identification for Omotic Languages", "comment": null, "summary": "Language identification is the task of determining the languages for a given\ntext. In many real world scenarios, text may contain more than one language,\nparticularly in multilingual communities. Bilingual Language Identification\n(BLID) is the task of identifying and distinguishing between two languages in a\ngiven text. This paper presents BLID for languages spoken in the southern part\nof Ethiopia, namely Wolaita and Gofa. The presence of words similarities and\ndifferences between the two languages makes the language identification task\nchallenging. To overcome this challenge, we employed various experiments on\nvarious approaches. Then, the combination of the BERT based pretrained language\nmodel and LSTM approach performed better, with an F1 score of 0.72 on the test\nset. As a result, the work will be effective in tackling unwanted social media\nissues and providing a foundation for further research in this area.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u57c3\u585e\u4fc4\u6bd4\u4e9a\u5357\u90e8Wolaita\u548cGofa\u8bed\u8a00\u7684\u53cc\u8bed\u8bc6\u522b(BLID)\u65b9\u6cd5\uff0c\u7ed3\u5408BERT\u9884\u8bad\u7ec3\u6a21\u578b\u548cLSTM\u53d6\u5f97\u4e860.72\u7684F1\u5206\u6570", "motivation": "\u5728\u591a\u8bed\u8a00\u793e\u533a\u4e2d\uff0c\u6587\u672c\u5e38\u5305\u542b\u591a\u79cd\u8bed\u8a00\uff0c\u800cWolaita\u548cGofa\u8bed\u8a00\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u548c\u5dee\u5f02\u6027\u4f7f\u5f97\u8bed\u8a00\u8bc6\u522b\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027", "method": "\u91c7\u7528\u591a\u79cd\u5b9e\u9a8c\u65b9\u6cd5\uff0c\u6700\u7ec8\u786e\u5b9aBERT\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e0eLSTM\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5", "result": "\u5728\u6d4b\u8bd5\u96c6\u4e0a\u83b7\u5f97\u4e860.72\u7684F1\u5206\u6570\uff0c\u8868\u73b0\u6700\u4f73", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u80fd\u6709\u6548\u5904\u7406\u793e\u4ea4\u5a92\u4f53\u95ee\u9898\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u57fa\u7840"}}
{"id": "2509.08283", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.08283", "abs": "https://arxiv.org/abs/2509.08283", "authors": ["Yumin Kim", "Seonghyeon Go"], "title": "Segment Transformer: AI-Generated Music Detection via Music Structural Analysis", "comment": null, "summary": "Audio and music generation systems have been remarkably developed in the\nmusic information retrieval (MIR) research field. The advancement of these\ntechnologies raises copyright concerns, as ownership and authorship of\nAI-generated music (AIGM) remain unclear. Also, it can be difficult to\ndetermine whether a piece was generated by AI or composed by humans clearly. To\naddress these challenges, we aim to improve the accuracy of AIGM detection by\nanalyzing the structural patterns of music segments. Specifically, to extract\nmusical features from short audio clips, we integrated various pre-trained\nmodels, including self-supervised learning (SSL) models or an audio effect\nencoder, each within our suggested transformer-based framework. Furthermore,\nfor long audio, we developed a segment transformer that divides music into\nsegments and learns inter-segment relationships. We used the FakeMusicCaps and\nSONICS datasets, achieving high accuracy in both the short-audio and full-audio\ndetection experiments. These findings suggest that integrating segment-level\nmusical features into long-range temporal analysis can effectively enhance both\nthe performance and robustness of AIGM detection systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u97f3\u4e50\u7247\u6bb5\u7ed3\u6784\u5206\u6790\u7684AI\u751f\u6210\u97f3\u4e50\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u63d0\u53d6\u7279\u5f81\uff0c\u901a\u8fc7transformer\u6846\u67b6\u5904\u7406\u77ed\u97f3\u9891\u548c\u957f\u97f3\u9891\uff0c\u5728FakeMusicCaps\u548cSONICS\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u9ad8\u51c6\u786e\u7387", "motivation": "\u89e3\u51b3AI\u751f\u6210\u97f3\u4e50(AIGM)\u7684\u7248\u6743\u5f52\u5c5e\u95ee\u9898\uff0c\u56e0\u4e3aAI\u751f\u6210\u97f3\u4e50\u4e0e\u4eba\u7c7b\u521b\u4f5c\u97f3\u4e50\u96be\u4ee5\u533a\u5206\uff0c\u9700\u8981\u5f00\u53d1\u51c6\u786e\u7684\u68c0\u6d4b\u6280\u672f", "method": "\u96c6\u6210\u591a\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u548c\u97f3\u9891\u6548\u679c\u7f16\u7801\u5668\uff09\u63d0\u53d6\u97f3\u4e50\u7279\u5f81\uff0c\u63d0\u51fa\u57fa\u4e8etransformer\u7684\u6846\u67b6\u5904\u7406\u77ed\u97f3\u9891\u7247\u6bb5\uff0c\u5f00\u53d1segment transformer\u5206\u6790\u957f\u97f3\u9891\u7684\u7247\u6bb5\u95f4\u5173\u7cfb", "result": "\u5728FakeMusicCaps\u548cSONICS\u6570\u636e\u96c6\u4e0a\uff0c\u77ed\u97f3\u9891\u548c\u5b8c\u6574\u97f3\u9891\u68c0\u6d4b\u5b9e\u9a8c\u90fd\u83b7\u5f97\u4e86\u9ad8\u51c6\u786e\u7387", "conclusion": "\u5c06\u7247\u6bb5\u7ea7\u97f3\u4e50\u7279\u5f81\u6574\u5408\u5230\u957f\u65f6\u57df\u5206\u6790\u4e2d\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347AIGM\u68c0\u6d4b\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027"}}
{"id": "2509.08000", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08000", "abs": "https://arxiv.org/abs/2509.08000", "authors": ["Debdeep Sanyal", "Manodeep Ray", "Murari Mandal"], "title": "AntiDote: Bi-level Adversarial Training for Tamper-Resistant LLMs", "comment": "19 pages", "summary": "The release of open-weight large language models (LLMs) creates a tension\nbetween advancing accessible research and preventing misuse, such as malicious\nfine-tuning to elicit harmful content. Current safety measures struggle to\npreserve the general capabilities of the LLM while resisting a determined\nadversary with full access to the model's weights and architecture, who can use\nfull-parameter fine-tuning to erase existing safeguards. To address this, we\nintroduce AntiDote, a bi-level optimization procedure for training LLMs to be\nresistant to such tampering. AntiDote involves an auxiliary adversary\nhypernetwork that learns to generate malicious Low-Rank Adaptation (LoRA)\nweights conditioned on the defender model's internal activations. The defender\nLLM is then trained with an objective to nullify the effect of these\nadversarial weight additions, forcing it to maintain its safety alignment. We\nvalidate this approach against a diverse suite of 52 red-teaming attacks,\nincluding jailbreak prompting, latent space manipulation, and direct\nweight-space attacks. AntiDote is upto 27.4\\% more robust against adversarial\nattacks compared to both tamper-resistance and unlearning baselines. Crucially,\nthis robustness is achieved with a minimal trade-off in utility, incurring a\nperformance degradation of upto less than 0.5\\% across capability benchmarks\nincluding MMLU, HellaSwag, and GSM8K. Our work offers a practical and compute\nefficient methodology for building open-weight models where safety is a more\nintegral and resilient property.", "AI": {"tldr": "AntiDote\u662f\u4e00\u79cd\u53cc\u5c42\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u8d85\u7f51\u7edc\u8bad\u7ec3LLM\u62b5\u6297\u6076\u610f\u5fae\u8c03\u653b\u51fb\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027", "motivation": "\u5f00\u6e90\u6743\u91cdLLM\u9762\u4e34\u6076\u610f\u5fae\u8c03\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u7684\u98ce\u9669\uff0c\u73b0\u6709\u5b89\u5168\u63aa\u65bd\u96be\u4ee5\u5728\u4fdd\u6301\u6a21\u578b\u80fd\u529b\u7684\u540c\u65f6\u62b5\u5fa1\u5b8c\u5168\u8bbf\u95ee\u6a21\u578b\u6743\u91cd\u7684\u653b\u51fb\u8005", "method": "\u4f7f\u7528\u8f85\u52a9\u5bf9\u6297\u6027\u8d85\u7f51\u7edc\u751f\u6210\u6076\u610fLoRA\u6743\u91cd\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u8bad\u7ec3\u9632\u5fa1\u6a21\u578b\u6765\u62b5\u6d88\u8fd9\u4e9b\u5bf9\u6297\u6027\u6743\u91cd\u7684\u5f71\u54cd\uff0c\u4fdd\u6301\u5b89\u5168\u5bf9\u9f50", "result": "\u572852\u79cd\u7ea2\u961f\u653b\u51fb\u6d4b\u8bd5\u4e2d\uff0cAntiDote\u6bd4\u73b0\u6709\u65b9\u6cd5\u9c81\u68d2\u6027\u63d0\u534727.4%\uff0c\u5728MMLU\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u4e0b\u964d\u5c0f\u4e8e0.5%", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u517c\u5907\u7684\u5f00\u6e90\u6743\u91cd\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\u8bba"}}
{"id": "2509.08379", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.08379", "abs": "https://arxiv.org/abs/2509.08379", "authors": ["Hirokazu Kameoka", "Takuhiro Kaneko", "Kou Tanaka", "Yuto Kondo"], "title": "LatentVoiceGrad: Nonparallel Voice Conversion with Latent Diffusion/Flow-Matching Models", "comment": "Submitted to IEEE-TASLP", "summary": "Previously, we introduced VoiceGrad, a nonparallel voice conversion (VC)\ntechnique enabling mel-spectrogram conversion from source to target speakers\nusing a score-based diffusion model. The concept involves training a score\nnetwork to predict the gradient of the log density of mel-spectrograms from\nvarious speakers. VC is executed by iteratively adjusting an input\nmel-spectrogram until resembling the target speaker's. However, challenges\npersist: audio quality needs improvement, and conversion is slower compared to\nmodern VC methods designed to operate at very high speeds. To address these, we\nintroduce latent diffusion models into VoiceGrad, proposing an improved version\nwith reverse diffusion in the autoencoder bottleneck. Additionally, we propose\nusing a flow matching model as an alternative to the diffusion model to further\nspeed up the conversion process without compromising the conversion quality.\nExperimental results show enhanced speech quality and accelerated conversion\ncompared to the original.", "AI": {"tldr": "VoiceGrad\u6539\u8fdb\u7248\uff1a\u901a\u8fc7\u6f5c\u5728\u6c89\u79cd\u6a21\u578b\u548c\u6d41\u5339\u914d\u6280\u672f\u63d0\u5347\u8bed\u97f3\u8f6c\u6362\u7684\u97f3\u9891\u8d28\u91cf\u548c\u8f6c\u6362\u901f\u5ea6", "motivation": "\u539f\u59cbVoiceGrad\u5728\u97f3\u9891\u8d28\u91cf\u548c\u8f6c\u6362\u901f\u5ea6\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u9700\u8981\u63d0\u5347\u4ee5\u8ddf\u4e0a\u73b0\u4ee3\u9ad8\u901f\u8bed\u97f3\u8f6c\u6362\u65b9\u6cd5\u7684\u8981\u6c42", "method": "1\uff09\u5728\u81ea\u52a8\u7f16\u7801\u5668\u74f6\u9888\u5c42\u4e2d\u5f15\u5165\u6f5c\u5728\u6c89\u79cd\u6a21\u578b\u8fdb\u884c\u53cd\u5411\u6c89\u79cd 2\uff09\u4f7f\u7528\u6d41\u5339\u914d\u6a21\u578b\u66ff\u4ee3\u6c89\u79cd\u6a21\u578b\u4ee5\u52a0\u901f\u8f6c\u6362\u8fc7\u7a0b", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6539\u8fdb\u7248\u5728\u8bed\u97f3\u8d28\u91cf\u548c\u8f6c\u6362\u901f\u5ea6\u65b9\u9762\u90fd\u8d85\u8fc7\u539f\u59cb\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u6f5c\u5728\u6c89\u79cd\u548c\u6d41\u5339\u914d\u6280\u672f\u7684\u7ed3\u5408\uff0c\u6210\u529f\u63d0\u5347\u4e86\u975e\u5e73\u884c\u8bed\u97f3\u8f6c\u6362\u7684\u6027\u80fd\uff0c\u4e3a\u9ad8\u8d28\u91cf\u9ad8\u901f\u8f6c\u6362\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.08022", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08022", "abs": "https://arxiv.org/abs/2509.08022", "authors": ["Yao Liang", "Dongcheng Zhao", "Feifei Zhao", "Guobin Shen", "Yuwei Wang", "Dongqi Liang", "Yi Zeng"], "title": "MVPBench: A Benchmark and Fine-Tuning Framework for Aligning Large Language Models with Diverse Human Values", "comment": null, "summary": "The alignment of large language models (LLMs) with human values is critical\nfor their safe and effective deployment across diverse user populations.\nHowever, existing benchmarks often neglect cultural and demographic diversity,\nleading to limited understanding of how value alignment generalizes globally.\nIn this work, we introduce MVPBench, a novel benchmark that systematically\nevaluates LLMs' alignment with multi-dimensional human value preferences across\n75 countries. MVPBench contains 24,020 high-quality instances annotated with\nfine-grained value labels, personalized questions, and rich demographic\nmetadata, making it the most comprehensive resource of its kind to date. Using\nMVPBench, we conduct an in-depth analysis of several state-of-the-art LLMs,\nrevealing substantial disparities in alignment performance across geographic\nand demographic lines. We further demonstrate that lightweight fine-tuning\nmethods, such as Low-Rank Adaptation (LoRA) and Direct Preference Optimization\n(DPO), can significantly enhance value alignment in both in-domain and\nout-of-domain settings. Our findings underscore the necessity for\npopulation-aware alignment evaluation and provide actionable insights for\nbuilding culturally adaptive and value-sensitive LLMs. MVPBench serves as a\npractical foundation for future research on global alignment, personalized\nvalue modeling, and equitable AI development.", "AI": {"tldr": "MVPBench\u662f\u4e00\u4e2a\u5305\u542b24,020\u4e2a\u5b9e\u4f8b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u572875\u4e2a\u56fd\u5bb6\u4e2d\u7684\u591a\u7ef4\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u5730\u7406\u548c\u4eba\u53e3\u7edf\u8ba1\u4e0a\u7684\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u8bc1\u660e\u8f7b\u91cf\u7ea7\u5fae\u8c03\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5bf9\u9f50\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5f80\u5f80\u5ffd\u89c6\u6587\u5316\u548c\u4eba\u53e3\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u5bf9\u4ef7\u503c\u89c2\u5bf9\u9f50\u5728\u5168\u7403\u8303\u56f4\u5185\u6cdb\u5316\u80fd\u529b\u7684\u7406\u89e3\u6709\u9650\u3002", "method": "\u5f00\u53d1MVPBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u7cbe\u7ec6\u4ef7\u503c\u89c2\u6807\u7b7e\u3001\u4e2a\u6027\u5316\u95ee\u9898\u548c\u4e30\u5bcc\u4eba\u53e3\u7edf\u8ba1\u5143\u6570\u636e\uff1b\u4f7f\u7528LoRA\u548cDPO\u7b49\u8f7b\u91cf\u7ea7\u5fae\u8c03\u65b9\u6cd5\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u5730\u7406\u548c\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\u95f4\u5b58\u5728\u663e\u8457\u7684\u5bf9\u9f50\u6027\u80fd\u5dee\u5f02\uff1b\u8f7b\u91cf\u7ea7\u5fae\u8c03\u65b9\u6cd5\u5728\u57df\u5185\u548c\u57df\u5916\u8bbe\u7f6e\u4e2d\u90fd\u80fd\u663e\u8457\u63d0\u5347\u4ef7\u503c\u89c2\u5bf9\u9f50\u6548\u679c\u3002", "conclusion": "\u9700\u8981\u8fdb\u884c\u7fa4\u4f53\u611f\u77e5\u7684\u5bf9\u9f50\u8bc4\u4f30\uff0cMVPBench\u4e3a\u5168\u7403\u5bf9\u9f50\u3001\u4e2a\u6027\u5316\u4ef7\u503c\u89c2\u5efa\u6a21\u548c\u516c\u5e73AI\u53d1\u5c55\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\u3002"}}
{"id": "2509.08454", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.08454", "abs": "https://arxiv.org/abs/2509.08454", "authors": ["Yujian Ma", "Jinqiu Sang", "Ruizhe Li"], "title": "Behind the Scenes: Mechanistic Interpretability of LoRA-adapted Whisper for Speech Emotion Recognition", "comment": "Work in process", "summary": "Large pre-trained speech models such as Whisper offer strong generalization\nbut pose significant challenges for resource-efficient adaptation. Low-Rank\nAdaptation (LoRA) has become a popular parameter-efficient fine-tuning method,\nyet its underlying mechanisms in speech tasks remain poorly understood. In this\nwork, we conduct the first systematic mechanistic interpretability study of\nLoRA within the Whisper encoder for speech emotion recognition (SER). Using a\nsuite of analytical tools, including layer contribution probing, logit-lens\ninspection, and representational similarity via singular value decomposition\n(SVD) and centered kernel alignment (CKA), we reveal two key mechanisms: a\ndelayed specialization process that preserves general features in early layers\nbefore consolidating task-specific information, and a forward alignment,\nbackward differentiation dynamic between LoRA's matrices. Our findings clarify\nhow LoRA reshapes encoder hierarchies, providing both empirical insights and a\ndeeper mechanistic understanding for designing efficient and interpretable\nadaptation strategies in large speech models.", "AI": {"tldr": "\u5bf9Whisper\u8bed\u97f3\u6a21\u578b\u5728\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2dLoRA\u5fae\u8c03\u673a\u5236\u7684\u9996\u4e2a\u7cfb\u7edf\u6027\u53ef\u89e3\u91ca\u6027\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u5ef6\u8fdf\u7279\u5316\u8fc7\u7a0b\u548c\u524d\u5411\u5bf9\u9f50-\u540e\u5411\u5206\u5316\u52a8\u6001", "motivation": "\u5927\u578b\u9884\u8bad\u7ec3\u8bed\u97f3\u6a21\u578b\u5982Whisper\u5177\u6709\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u8d44\u6e90\u9ad8\u6548\u7684\u9002\u914d\u9762\u4e34\u6311\u6218\u3002LoRA\u5df2\u6210\u4e3a\u6d41\u884c\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u4f46\u5176\u5728\u8bed\u97f3\u4efb\u52a1\u4e2d\u7684\u5de5\u4f5c\u673a\u5236\u5c1a\u4e0d\u660e\u786e", "method": "\u4f7f\u7528\u5c42\u8d21\u732e\u63a2\u6d4b\u3001logit-lens\u68c0\u67e5\u3001SVD\u548cCKA\u8868\u793a\u76f8\u4f3c\u6027\u5206\u6790\u7b49\u5de5\u5177\uff0c\u7cfb\u7edf\u7814\u7a76Whisper\u7f16\u7801\u5668\u4e2dLoRA\u7684\u673a\u5236", "result": "\u53d1\u73b0\u4e86\u4e24\u4e2a\u5173\u952e\u673a\u5236\uff1a\u65e9\u671f\u5c42\u4fdd\u7559\u901a\u7528\u7279\u5f81\u3001\u540e\u671f\u5c42\u6574\u5408\u4efb\u52a1\u7279\u5b9a\u4fe1\u606f\u7684\u5ef6\u8fdf\u7279\u5316\u8fc7\u7a0b\uff0c\u4ee5\u53caLoRA\u77e9\u9635\u95f4\u7684\u524d\u5411\u5bf9\u9f50-\u540e\u5411\u5206\u5316\u52a8\u6001", "conclusion": "\u9610\u660e\u4e86LoRA\u5982\u4f55\u91cd\u5851\u7f16\u7801\u5668\u5c42\u6b21\u7ed3\u6784\uff0c\u4e3a\u8bbe\u8ba1\u9ad8\u6548\u53ef\u89e3\u91ca\u7684\u5927\u578b\u8bed\u97f3\u6a21\u578b\u9002\u914d\u7b56\u7565\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u89c1\u89e3\u548c\u673a\u5236\u7406\u89e3"}}
{"id": "2509.08025", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08025", "abs": "https://arxiv.org/abs/2509.08025", "authors": ["Hoang-Trung Nguyen", "Tan-Minh Nguyen", "Xuan-Bach Le", "Tuan-Kiet Le", "Khanh-Huyen Nguyen", "Ha-Thanh Nguyen", "Thi-Hai-Yen Vuong", "Le-Minh Nguyen"], "title": "NOWJ@COLIEE 2025: A Multi-stage Framework Integrating Embedding Models and Large Language Models for Legal Retrieval and Entailment", "comment": null, "summary": "This paper presents the methodologies and results of the NOWJ team's\nparticipation across all five tasks at the COLIEE 2025 competition, emphasizing\nadvancements in the Legal Case Entailment task (Task 2). Our comprehensive\napproach systematically integrates pre-ranking models (BM25, BERT, monoT5),\nembedding-based semantic representations (BGE-m3, LLM2Vec), and advanced Large\nLanguage Models (Qwen-2, QwQ-32B, DeepSeek-V3) for summarization, relevance\nscoring, and contextual re-ranking. Specifically, in Task 2, our two-stage\nretrieval system combined lexical-semantic filtering with contextualized LLM\nanalysis, achieving first place with an F1 score of 0.3195. Additionally, in\nother tasks--including Legal Case Retrieval, Statute Law Retrieval, Legal\nTextual Entailment, and Legal Judgment Prediction--we demonstrated robust\nperformance through carefully engineered ensembles and effective prompt-based\nreasoning strategies. Our findings highlight the potential of hybrid models\nintegrating traditional IR techniques with contemporary generative models,\nproviding a valuable reference for future advancements in legal information\nprocessing.", "AI": {"tldr": "NOWJ\u56e2\u961f\u5728COLIEE 2025\u7ade\u8d5b\u4e2d\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4f20\u7edf\u4fe1\u606f\u68c0\u7d22\u6280\u672f\u548c\u73b0\u4ee3\u751f\u6210\u6a21\u578b\uff0c\u5728Legal Case Entailment\u4efb\u52a1\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0c\u5e76\u5728\u5176\u4ed6\u6cd5\u5f8b\u4fe1\u606f\u5904\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u5f3a\u52b2\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5c06\u4f20\u7edf\u4fe1\u606f\u68c0\u7d22\u6280\u672f\u4e0e\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u6cd5\u5f8b\u4fe1\u606f\u5904\u7406\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u6cd5\u5f8b\u6848\u4f8b\u8574\u542b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u68c0\u7d22\u7cfb\u7edf\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u9884\u6392\u5e8f\u6a21\u578b\uff08BM25\u3001BERT\u3001monoT5\uff09\u548c\u57fa\u4e8e\u5d4c\u5165\u7684\u8bed\u4e49\u8868\u793a\uff08BGE-m3\u3001LLM2Vec\uff09\u8fdb\u884c\u8bcd\u6c47\u8bed\u4e49\u8fc7\u6ee4\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08Qwen-2\u3001QwQ-32B\u3001DeepSeek-V3\uff09\u8fdb\u884c\u6458\u8981\u3001\u76f8\u5173\u6027\u8bc4\u5206\u548c\u4e0a\u4e0b\u6587\u91cd\u6392\u5e8f\u3002", "result": "\u5728Legal Case Entailment\u4efb\u52a1\uff08Task 2\uff09\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0cF1\u5206\u6570\u8fbe\u52300.3195\u3002\u5728\u5176\u4ed6\u4efb\u52a1\uff08\u6cd5\u5f8b\u6848\u4f8b\u68c0\u7d22\u3001\u6cd5\u89c4\u68c0\u7d22\u3001\u6cd5\u5f8b\u6587\u672c\u8574\u542b\u548c\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\uff09\u4e2d\u4e5f\u8868\u73b0\u51fa\u7a33\u5065\u7684\u6027\u80fd\u3002", "conclusion": "\u6df7\u5408\u6a21\u578b\u6574\u5408\u4f20\u7edfIR\u6280\u672f\u548c\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u6cd5\u5f8b\u4fe1\u606f\u5904\u7406\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\uff0c\u8bc1\u660e\u4e86\u96c6\u6210\u65b9\u6cd5\u5728\u6cd5\u5f8bAI\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.08717", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.08717", "abs": "https://arxiv.org/abs/2509.08717", "authors": ["Zubair Faruqui", "Mackenzie S. McIntire", "Rahul Dubey", "Jay McEntee"], "title": "Explainability of CNN Based Classification Models for Acoustic Signal", "comment": "Accepted in IEEE ICTAI 2025", "summary": "Explainable Artificial Intelligence (XAI) has emerged as a critical tool for\ninterpreting the predictions of complex deep learning models. While XAI has\nbeen increasingly applied in various domains within acoustics, its use in\nbioacoustics, which involves analyzing audio signals from living organisms,\nremains relatively underexplored. In this paper, we investigate the\nvocalizations of a bird species with strong geographic variation throughout its\nrange in North America. Audio recordings were converted into spectrogram images\nand used to train a deep Convolutional Neural Network (CNN) for classification,\nachieving an accuracy of 94.8\\%. To interpret the model's predictions, we\napplied both model-agnostic (LIME, SHAP) and model-specific (DeepLIFT,\nGrad-CAM) XAI techniques. These techniques produced different but complementary\nexplanations, and when their explanations were considered together, they\nprovided more complete and interpretable insights into the model's\ndecision-making. This work highlights the importance of using a combination of\nXAI techniques to improve trust and interoperability, not only in broader\nacoustics signal analysis but also argues for broader applicability in\ndifferent domain specific tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u5e94\u7528\u591a\u79cd\u53ef\u89e3\u91caAI\u6280\u672f\u5206\u6790\u9e1f\u7c7b\u9e23\u58f0\u5206\u7c7b\u6a21\u578b\uff0c\u7ed3\u5408\u6a21\u578b\u65e0\u5173\u548c\u6a21\u578b\u7279\u5b9a\u7684XAI\u65b9\u6cd5\u83b7\u5f97\u66f4\u5b8c\u6574\u7684\u89e3\u91ca\uff0c\u5728\u751f\u7269\u58f0\u5b66\u9886\u57df\u5c55\u793a\u4e86XAI\u7684\u91cd\u8981\u4ef7\u503c\u3002", "motivation": "\u867d\u7136XAI\u5728\u58f0\u5b66\u9886\u57df\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5728\u751f\u7269\u58f0\u5b66\uff08\u5206\u6790\u751f\u7269\u97f3\u9891\u4fe1\u53f7\uff09\u4e2d\u7684\u5e94\u7528\u4ecd\u76f8\u5bf9\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22XAI\u5728\u9e1f\u7c7b\u5730\u7406\u53d8\u5f02\u9e23\u58f0\u5206\u6790\u4e2d\u7684\u89e3\u91ca\u80fd\u529b\u3002", "method": "\u5c06\u97f3\u9891\u5f55\u97f3\u8f6c\u6362\u4e3a\u9891\u8c31\u56fe\u56fe\u50cf\uff0c\u8bad\u7ec3\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5206\u7c7b\uff08\u51c6\u786e\u738794.8%\uff09\uff0c\u5e76\u5e94\u7528\u6a21\u578b\u65e0\u5173\uff08LIME\u3001SHAP\uff09\u548c\u6a21\u578b\u7279\u5b9a\uff08DeepLIFT\u3001Grad-CAM\uff09XAI\u6280\u672f\u8fdb\u884c\u89e3\u91ca\u3002", "result": "\u4e0d\u540cXAI\u6280\u672f\u4ea7\u751f\u4e86\u4e92\u8865\u7684\u89e3\u91ca\uff0c\u5f53\u7efc\u5408\u8003\u8651\u8fd9\u4e9b\u89e3\u91ca\u65f6\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u5b8c\u6574\u548c\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u51b3\u7b56\u6d1e\u5bdf\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u7ed3\u5408\u591a\u79cdXAI\u6280\u672f\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u548c\u4e92\u64cd\u4f5c\u6027\uff0c\u4e0d\u4ec5\u5728\u58f0\u5b66\u4fe1\u53f7\u5206\u6790\u4e2d\uff0c\u5728\u5176\u4ed6\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u4e2d\u4e5f\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2509.08032", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08032", "abs": "https://arxiv.org/abs/2509.08032", "authors": ["Fengyu She", "Nan Wang", "Hongfei Wu", "Ziyi Wan", "Jingmian Wang", "Chang Wang"], "title": "SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery", "comment": null, "summary": "Scientific literature is growing exponentially, creating a critical\nbottleneck for researchers to efficiently synthesize knowledge. While\ngeneral-purpose Large Language Models (LLMs) show potential in text processing,\nthey often fail to capture scientific domain-specific nuances (e.g., technical\njargon, methodological rigor) and struggle with complex scientific tasks,\nlimiting their utility for interdisciplinary research. To address these gaps,\nthis paper presents SciGPT, a domain-adapted foundation model for scientific\nliterature understanding and ScienceBench, an open source benchmark tailored to\nevaluate scientific LLMs.\n  Built on the Qwen3 architecture, SciGPT incorporates three key innovations:\n(1) low-cost domain distillation via a two-stage pipeline to balance\nperformance and efficiency; (2) a Sparse Mixture-of-Experts (SMoE) attention\nmechanism that cuts memory consumption by 55\\% for 32,000-token long-document\nreasoning; and (3) knowledge-aware adaptation integrating domain ontologies to\nbridge interdisciplinary knowledge gaps.\n  Experimental results on ScienceBench show that SciGPT outperforms GPT-4o in\ncore scientific tasks including sequence labeling, generation, and inference.\nIt also exhibits strong robustness in unseen scientific tasks, validating its\npotential to facilitate AI-augmented scientific discovery.", "AI": {"tldr": "SciGPT\u662f\u4e00\u4e2a\u9488\u5bf9\u79d1\u5b66\u6587\u732e\u7406\u89e3\u7684\u9886\u57df\u81ea\u9002\u5e94\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u4f4e\u6210\u672c\u7684\u9886\u57df\u84b8\u998f\u3001\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c\u77e5\u8bc6\u611f\u77e5\u9002\u5e94\u7b49\u521b\u65b0\u6280\u672f\uff0c\u5728\u79d1\u5b66\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86GPT-4o\u7684\u8868\u73b0\u3002", "motivation": "\u79d1\u5b66\u6587\u732e\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u7814\u7a76\u4eba\u5458\u9700\u8981\u9ad8\u6548\u7684\u77e5\u8bc6\u5408\u6210\u5de5\u5177\u3002\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u9886\u57df\u5b58\u5728\u6280\u672f\u672f\u8bed\u7406\u89e3\u4e0d\u8db3\u3001\u65b9\u6cd5\u4e25\u8c28\u6027\u628a\u63e1\u56f0\u96be\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u8de8\u5b66\u79d1\u7814\u7a76\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u57fa\u4e8eQwen3\u67b6\u6784\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u521b\u65b0\uff1a1\uff09\u4f4e\u6210\u672c\u9886\u57df\u84b8\u998f\u7684\u4e24\u9636\u6bb5\u6d41\u6c34\u7ebf\uff1b2\uff09\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u572832,000\u8bcd\u957f\u6587\u6863\u63a8\u7406\u4e2d\u51cf\u5c1155%\u5185\u5b58\u6d88\u8017\uff1b3\uff09\u77e5\u8bc6\u611f\u77e5\u9002\u5e94\uff0c\u6574\u5408\u9886\u57df\u672c\u4f53\u4ee5\u5f25\u5408\u8de8\u5b66\u79d1\u77e5\u8bc6\u9e3f\u6c9f\u3002", "result": "\u5728ScienceBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSciGPT\u5728\u5e8f\u5217\u6807\u6ce8\u3001\u751f\u6210\u548c\u63a8\u7406\u7b49\u6838\u5fc3\u79d1\u5b66\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86GPT-4o\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u79d1\u5b66\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "SciGPT\u9a8c\u8bc1\u4e86\u5176\u5728\u4fc3\u8fdbAI\u589e\u5f3a\u79d1\u5b66\u53d1\u73b0\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u79d1\u5b66\u6587\u732e\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9886\u57df\u81ea\u9002\u5e94\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08075", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08075", "abs": "https://arxiv.org/abs/2509.08075", "authors": ["Flor Miriam Plaza-del-Arco", "Paul R\u00f6ttger", "Nino Scherrer", "Emanuele Borgonovo", "Elmar Plischke", "Dirk Hovy"], "title": "No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal in Language Models", "comment": null, "summary": "Large language models (LLMs) are increasingly integrated into our daily lives\nand personalized. However, LLM personalization might also increase unintended\nside effects. Recent work suggests that persona prompting can lead models to\nfalsely refuse user requests. However, no work has fully quantified the extent\nof this issue. To address this gap, we measure the impact of 15\nsociodemographic personas (based on gender, race, religion, and disability) on\nfalse refusal. To control for other factors, we also test 16 different models,\n3 tasks (Natural Language Inference, politeness, and offensiveness\nclassification), and nine prompt paraphrases. We propose a Monte Carlo-based\nmethod to quantify this issue in a sample-efficient manner. Our results show\nthat as models become more capable, personas impact the refusal rate less and\nless. Certain sociodemographic personas increase false refusal in some models,\nwhich suggests underlying biases in the alignment strategies or safety\nmechanisms. However, we find that the model choice and task significantly\ninfluence false refusals, especially in sensitive content tasks. Our findings\nsuggest that persona effects have been overestimated, and might be due to other\nfactors.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u4e2a\u6027\u5316\u4e2d\u7684\u865a\u5047\u62d2\u7edd\u95ee\u9898\u88ab\u9ad8\u4f30\uff0c\u6a21\u578b\u80fd\u529b\u548c\u4efb\u52a1\u7c7b\u578b\u5bf9\u62d2\u7edd\u7387\u7684\u5f71\u54cd\u6bd4\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u66f4\u91cd\u8981", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u65e5\u76ca\u4e2a\u6027\u5316\u548c\u878d\u5165\u65e5\u5e38\u751f\u6d3b\uff0c\u9700\u8981\u91cf\u5316\u4e2a\u6027\u5316\u53ef\u80fd\u5e26\u6765\u7684\u526f\u4f5c\u7528\uff0c\u7279\u522b\u662f\u865a\u5047\u62d2\u7edd\u8bf7\u6c42\u7684\u95ee\u9898", "method": "\u4f7f\u752815\u79cd\u793e\u4f1a\u4eba\u53e3\u7279\u5f81\u89d2\u8272\u300116\u4e2a\u4e0d\u540c\u6a21\u578b\u30013\u79cd\u4efb\u52a1\u7c7b\u578b\u548c9\u79cd\u63d0\u793a\u53d8\u4f53\uff0c\u63d0\u51fa\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u7684\u6837\u672c\u9ad8\u6548\u91cf\u5316\u65b9\u6cd5", "result": "\u6a21\u578b\u80fd\u529b\u8d8a\u5f3a\uff0c\u89d2\u8272\u5bf9\u62d2\u7edd\u7387\u7684\u5f71\u54cd\u8d8a\u5c0f\uff1b\u67d0\u4e9b\u793e\u4f1a\u4eba\u53e3\u7279\u5f81\u89d2\u8272\u5728\u67d0\u4e9b\u6a21\u578b\u4e2d\u4f1a\u589e\u52a0\u865a\u5047\u62d2\u7edd\uff0c\u4f46\u6a21\u578b\u9009\u62e9\u548c\u4efb\u52a1\u7c7b\u578b\u5bf9\u865a\u5047\u62d2\u7edd\u7684\u5f71\u54cd\u66f4\u663e\u8457", "conclusion": "\u89d2\u8272\u6548\u5e94\u88ab\u9ad8\u4f30\uff0c\u865a\u5047\u62d2\u7edd\u95ee\u9898\u66f4\u591a\u6e90\u4e8e\u6a21\u578b\u9009\u62e9\u548c\u4efb\u52a1\u7c7b\u578b\u7b49\u5176\u4ed6\u56e0\u7d20\uff0c\u800c\u975e\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u672c\u8eab"}}
{"id": "2509.08093", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08093", "abs": "https://arxiv.org/abs/2509.08093", "authors": ["Nathaniel Imel", "Noga Zaslavsky"], "title": "Culturally transmitted color categories in LLMs reflect a learning bias toward efficient compression", "comment": null, "summary": "Converging evidence suggests that systems of semantic categories across human\nlanguages achieve near-optimal compression via the Information Bottleneck (IB)\ncomplexity-accuracy principle. Large language models (LLMs) are not trained for\nthis objective, which raises the question: are LLMs capable of evolving\nefficient human-like semantic systems? To address this question, we focus on\nthe domain of color as a key testbed of cognitive theories of categorization\nand replicate with LLMs (Gemini 2.0-flash and Llama 3.3-70B-Instruct) two\ninfluential human behavioral studies. First, we conduct an English color-naming\nstudy, showing that Gemini aligns well with the naming patterns of native\nEnglish speakers and achieves a significantly high IB-efficiency score, while\nLlama exhibits an efficient but lower complexity system compared to English.\nSecond, to test whether LLMs simply mimic patterns in their training data or\nactually exhibit a human-like inductive bias toward IB-efficiency, we simulate\ncultural evolution of pseudo color-naming systems in LLMs via iterated\nin-context language learning. We find that akin to humans, LLMs iteratively\nrestructure initially random systems towards greater IB-efficiency and\nincreased alignment with patterns observed across the world's languages. These\nfindings demonstrate that LLMs are capable of evolving perceptually grounded,\nhuman-like semantic systems, driven by the same fundamental principle that\ngoverns semantic efficiency across human languages.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u4fe1\u606f\u74f6\u9888\u539f\u5219\u6f14\u5316\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u9ad8\u6548\u8bed\u4e49\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728\u989c\u8272\u547d\u540d\u9886\u57df\uff0c\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u5f52\u7eb3\u504f\u597d\u7684\u6587\u5316\u6f14\u5316\u6a21\u5f0f\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u6f14\u5316\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u9ad8\u6548\u8bed\u4e49\u5206\u7c7b\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u9a8c\u8bc1\u5b83\u4eec\u662f\u5426\u9075\u5faa\u4fe1\u606f\u74f6\u9888\u7684\u590d\u6742\u5ea6-\u51c6\u786e\u6027\u4f18\u5316\u539f\u5219\uff0c\u800c\u975e\u4ec5\u4ec5\u6a21\u4eff\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6a21\u5f0f\u3002", "method": "\u4f7f\u7528Gemini 2.0-flash\u548cLlama 3.3-70B-Instruct\u6a21\u578b\uff0c\u590d\u73b0\u4e24\u4e2a\u91cd\u8981\u7684\u4eba\u7c7b\u884c\u4e3a\u7814\u7a76\uff1a\u82f1\u8bed\u989c\u8272\u547d\u540d\u7814\u7a76\u548c\u901a\u8fc7\u4e0a\u4e0b\u6587\u8bed\u8a00\u5b66\u4e60\u6a21\u62df\u6587\u5316\u6f14\u5316\u8fc7\u7a0b\u3002", "result": "Gemini\u4e0e\u82f1\u8bed\u6bcd\u8bed\u8005\u7684\u547d\u540d\u6a21\u5f0f\u9ad8\u5ea6\u4e00\u81f4\u4e14\u8fbe\u5230\u9ad8IB\u6548\u7387\u5206\u6570\uff0cLlama\u8868\u73b0\u51fa\u9ad8\u6548\u4f46\u590d\u6742\u5ea6\u8f83\u4f4e\u7684\u7cfb\u7edf\u3002\u901a\u8fc7\u8fed\u4ee3\u5b66\u4e60\uff0c\u4e24\u79cd\u6a21\u578b\u90fd\u80fd\u5c06\u521d\u59cb\u968f\u673a\u7cfb\u7edf\u91cd\u6784\u4e3a\u66f4\u9ad8IB\u6548\u7387\u7684\u7cfb\u7edf\uff0c\u5e76\u4e0e\u4e16\u754c\u8bed\u8a00\u6a21\u5f0f\u66f4\u52a0\u4e00\u81f4\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6f14\u5316\u51fa\u611f\u77e5\u57fa\u7840\u7684\u4eba\u7c7b\u5316\u8bed\u4e49\u7cfb\u7edf\uff0c\u5176\u9a71\u52a8\u673a\u5236\u4e0e\u652f\u914d\u4eba\u7c7b\u8bed\u8a00\u8bed\u4e49\u6548\u7387\u7684\u57fa\u672c\u539f\u7406\u76f8\u540c\uff0c\u8868\u660e\u5b83\u4eec\u5177\u5907\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5f52\u7eb3\u504f\u597d\u3002"}}
{"id": "2509.08105", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08105", "abs": "https://arxiv.org/abs/2509.08105", "authors": ["Kosei Uemura", "David Guzm\u00e1n", "Quang Phuoc Nguyen", "Jesujoba Oluwadara Alabi", "En-shiun Annie Lee", "David Ifeoluwa Adelani"], "title": "MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder and LLM Fusion", "comment": "under submission", "summary": "Large language models excel in English but still struggle with complex\nreasoning in many low-resource languages (LRLs). Existing encoder-plus-decoder\nmethods such as LangBridge and MindMerger raise accuracy on mid and\nhigh-resource languages, yet they leave a large gap on LRLs. We present MERLIN,\na two-stage model-stacking framework that applies a curriculum learning\nstrategy -- from general bilingual bitext to task-specific data -- and adapts\nonly a small set of DoRA weights. On the AfriMGSM benchmark MERLIN improves\nexact-match accuracy by +12.9 pp over MindMerger and outperforms GPT-4o-mini.\nIt also yields consistent gains on MGSM and MSVAMP (+0.9 and +2.8 pp),\ndemonstrating effectiveness across both low and high-resource settings.", "AI": {"tldr": "MERLIN\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6a21\u578b\u5806\u53e0\u6846\u67b6\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff08\u4ece\u901a\u7528\u53cc\u8bed\u6587\u672c\u5230\u4efb\u52a1\u7279\u5b9a\u6570\u636e\uff09\u5e76\u4ec5\u9002\u914d\u5c11\u91cfDoRA\u6743\u91cd\uff0c\u663e\u8457\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u82f1\u8bed\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u590d\u6742\u63a8\u7406\u65b9\u9762\u4ecd\u6709\u56f0\u96be\u3002\u73b0\u6709\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u65b9\u6cd5\u5bf9\u4e2d\u9ad8\u8d44\u6e90\u8bed\u8a00\u6709\u6548\uff0c\u4f46\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6a21\u578b\u5806\u53e0\u6846\u67b6\uff0c\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff1a\u4ece\u901a\u7528\u53cc\u8bed\u6587\u672c\u5230\u4efb\u52a1\u7279\u5b9a\u6570\u636e\uff0c\u4ec5\u9002\u914d\u5c11\u91cfDoRA\u6743\u91cd\u3002", "result": "\u5728AfriMGSM\u57fa\u51c6\u4e0a\u6bd4MindMerger\u51c6\u786e\u7387\u63d0\u534712.9\u4e2a\u767e\u5206\u70b9\uff0c\u4f18\u4e8eGPT-4o-mini\uff1b\u5728MGSM\u548cMSVAMP\u4e0a\u4e5f\u5206\u522b\u83b7\u5f970.9\u548c2.8\u4e2a\u767e\u5206\u70b9\u7684\u63d0\u5347\u3002", "conclusion": "MERLIN\u6846\u67b6\u5728\u4f4e\u8d44\u6e90\u548c\u9ad8\u8d44\u6e90\u73af\u5883\u4e0b\u90fd\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u663e\u8457\u6539\u5584\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2509.08146", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08146", "abs": "https://arxiv.org/abs/2509.08146", "authors": ["Nivedha Sivakumar", "Natalie Mackraz", "Samira Khorshidi", "Krishna Patel", "Barry-John Theobald", "Luca Zappella", "Nicholas Apostoloff"], "title": "Bias after Prompting: Persistent Discrimination in Large Language Models", "comment": null, "summary": "A dangerous assumption that can be made from prior work on the bias transfer\nhypothesis (BTH) is that biases do not transfer from pre-trained large language\nmodels (LLMs) to adapted models. We invalidate this assumption by studying the\nBTH in causal models under prompt adaptations, as prompting is an extremely\npopular and accessible adaptation strategy used in real-world applications. In\ncontrast to prior work, we find that biases can transfer through prompting and\nthat popular prompt-based mitigation methods do not consistently prevent biases\nfrom transferring. Specifically, the correlation between intrinsic biases and\nthose after prompt adaptation remain moderate to strong across demographics and\ntasks -- for example, gender (rho >= 0.94) in co-reference resolution, and age\n(rho >= 0.98) and religion (rho >= 0.69) in question answering. Further, we\nfind that biases remain strongly correlated when varying few-shot composition\nparameters, such as sample size, stereotypical content, occupational\ndistribution and representational balance (rho >= 0.90). We evaluate several\nprompt-based debiasing strategies and find that different approaches have\ndistinct strengths, but none consistently reduce bias transfer across models,\ntasks or demographics. These results demonstrate that correcting bias, and\npotentially improving reasoning ability, in intrinsic models may prevent\npropagation of biases to downstream tasks.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u504f\u89c1\u53ef\u4ee5\u901a\u8fc7\u63d0\u793a\u4ece\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u8f6c\u79fb\u5230\u9002\u5e94\u6a21\u578b\uff0c\u73b0\u6709\u63d0\u793a\u53bb\u504f\u65b9\u6cd5\u65e0\u6cd5\u4e00\u81f4\u963b\u6b62\u504f\u89c1\u8f6c\u79fb\uff0c\u9700\u8981\u5728\u5185\u5728\u6a21\u578b\u4e2d\u7ea0\u6b63\u504f\u89c1\u4ee5\u9632\u6b62\u5411\u4e0b\u6e38\u4efb\u52a1\u4f20\u64ad", "motivation": "\u5148\u524d\u5173\u4e8e\u504f\u89c1\u8f6c\u79fb\u5047\u8bbe\u7684\u7814\u7a76\u53ef\u80fd\u9519\u8bef\u5730\u5047\u8bbe\u504f\u89c1\u4e0d\u4f1a\u4ece\u9884\u8bad\u7ec3LLM\u8f6c\u79fb\u5230\u9002\u5e94\u6a21\u578b\uff0c\u9700\u8981\u9a8c\u8bc1\u63d0\u793a\u9002\u5e94\u7b56\u7565\u4e0b\u7684\u504f\u89c1\u8f6c\u79fb\u60c5\u51b5", "method": "\u7814\u7a76\u56e0\u679c\u6a21\u578b\u5728\u63d0\u793a\u9002\u5e94\u4e0b\u7684\u504f\u89c1\u8f6c\u79fb\uff0c\u5206\u6790\u4e0d\u540c\u5c11\u6837\u672c\u7ec4\u5408\u53c2\u6570\uff08\u6837\u672c\u5927\u5c0f\u3001\u523b\u677f\u5185\u5bb9\u3001\u804c\u4e1a\u5206\u5e03\u7b49\uff09\u7684\u5f71\u54cd\uff0c\u8bc4\u4f30\u591a\u79cd\u63d0\u793a\u53bb\u504f\u7b56\u7565", "result": "\u504f\u89c1\u901a\u8fc7\u63d0\u793a\u8f6c\u79fb\u7684\u76f8\u5173\u6027\u5f88\u5f3a\uff08\u6027\u522brho\u22650.94\uff0c\u5e74\u9f84rho\u22650.98\uff0c\u5b97\u6559rho\u22650.69\uff09\uff0c\u4e0d\u540c\u53bb\u504f\u65b9\u6cd5\u5404\u6709\u4f18\u52bf\u4f46\u65e0\u4e00\u80fd\u4e00\u81f4\u51cf\u5c11\u504f\u89c1\u8f6c\u79fb", "conclusion": "\u9700\u8981\u5728\u5185\u5728\u6a21\u578b\u4e2d\u7ea0\u6b63\u504f\u89c1\u548c\u6539\u8fdb\u63a8\u7406\u80fd\u529b\uff0c\u4ee5\u9632\u6b62\u504f\u89c1\u5411\u4e0b\u6e38\u4efb\u52a1\u4f20\u64ad"}}
{"id": "2509.08150", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08150", "abs": "https://arxiv.org/abs/2509.08150", "authors": ["Supriya Lall", "Christian Farrell", "Hari Pathanjaly", "Marko Pavic", "Sarvesh Chezhian", "Masataro Asai"], "title": "Verbalized Algorithms", "comment": "Submitted to NeurIPS 2025 Workshop on Efficient Reasoning", "summary": "Instead of querying LLMs in a one-shot manner and hoping to get the right\nanswer for a reasoning task, we propose a paradigm we call \\emph{verbalized\nalgorithms} (VAs), which leverage classical algorithms with established\ntheoretical understanding. VAs decompose a task into simple elementary\noperations on natural language strings that they should be able to answer\nreliably, and limit the scope of LLMs to only those simple tasks. For example,\nfor sorting a series of natural language strings, \\emph{verbalized sorting}\nuses an LLM as a binary comparison oracle in a known and well-analyzed sorting\nalgorithm (e.g., bitonic sorting network). We demonstrate the effectiveness of\nthis approach on sorting and clustering tasks.", "AI": {"tldr": "\u63d0\u51faverbalized algorithms\uff08VAs\uff09\u8303\u5f0f\uff0c\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3aLLM\u53ef\u53ef\u9760\u6267\u884c\u7684\u7b80\u5355\u81ea\u7136\u8bed\u8a00\u64cd\u4f5c\uff0c\u4f7f\u7528\u7ecf\u5178\u7b97\u6cd5\u6846\u67b6\u6765\u4fdd\u8bc1\u7406\u8bba\u53ef\u9760\u6027", "motivation": "\u4f20\u7edf\u4e00\u6b21\u6027\u67e5\u8be2LLM\u7684\u65b9\u5f0f\u4e0d\u53ef\u9760\uff0c\u5e0c\u671b\u901a\u8fc7\u7ed3\u5408\u7ecf\u5178\u7b97\u6cd5\u7684\u7406\u8bba\u4fdd\u8bc1\u548cLLM\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u80fd\u529b\uff0c\u63d0\u9ad8\u63a8\u7406\u4efb\u52a1\u7684\u53ef\u9760\u6027", "method": "\u4f7f\u7528verbalized sorting\u65b9\u6cd5\uff0c\u5c06\u6392\u5e8f\u4efb\u52a1\u5206\u89e3\u4e3a\u4e8c\u5143\u6bd4\u8f83\u64cd\u4f5c\uff0c\u8ba9LLM\u4f5c\u4e3a\u6bd4\u8f83oracle\uff0c\u5728\u5df2\u77e5\u7684\u6392\u5e8f\u7b97\u6cd5\u6846\u67b6\uff08\u5982\u53cc\u8c03\u6392\u5e8f\u7f51\u7edc\uff09\u4e2d\u6267\u884c", "result": "\u5728\u6392\u5e8f\u548c\u805a\u7c7b\u4efb\u52a1\u4e0a\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "VAs\u8303\u5f0f\u901a\u8fc7\u9650\u5236LLM\u53ea\u6267\u884c\u7b80\u5355\u53ef\u9760\u7684\u64cd\u4f5c\uff0c\u7ed3\u5408\u7ecf\u5178\u7b97\u6cd5\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u80fd\u591f\u63d0\u9ad8\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u53ef\u9760\u6027"}}
{"id": "2509.08217", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08217", "abs": "https://arxiv.org/abs/2509.08217", "authors": ["Eve Fleisig", "Matthias Orlikowski", "Philipp Cimiano", "Dan Klein"], "title": "Balancing Quality and Variation: Spam Filtering Distorts Data Label Distributions", "comment": null, "summary": "For machine learning datasets to accurately represent diverse opinions in a\npopulation, they must preserve variation in data labels while filtering out\nspam or low-quality responses. How can we balance annotator reliability and\nrepresentation? We empirically evaluate how a range of heuristics for annotator\nfiltering affect the preservation of variation on subjective tasks. We find\nthat these methods, designed for contexts in which variation from a single\nground-truth label is considered noise, often remove annotators who disagree\ninstead of spam annotators, introducing suboptimal tradeoffs between accuracy\nand label diversity. We find that conservative settings for annotator removal\n(<5%) are best, after which all tested methods increase the mean absolute error\nfrom the true average label. We analyze performance on synthetic spam to\nobserve that these methods often assume spam annotators are less random than\nreal spammers tend to be: most spammers are distributionally indistinguishable\nfrom real annotators, and the minority that are distinguishable tend to give\nfixed answers, not random ones. Thus, tasks requiring the preservation of\nvariation reverse the intuition of existing spam filtering methods: spammers\ntend to be less random than non-spammers, so metrics that assume variation is\nspam fare worse. These results highlight the need for spam removal methods that\naccount for label diversity.", "AI": {"tldr": "\u73b0\u6709\u6807\u6ce8\u8005\u8fc7\u6ee4\u65b9\u6cd5\u5728\u4e3b\u89c2\u4efb\u52a1\u4e2d\u4f1a\u9519\u8bef\u79fb\u9664\u610f\u89c1\u4e0d\u540c\u7684\u6807\u6ce8\u8005\u800c\u975e\u5783\u573e\u6807\u6ce8\u8005\uff0c\u5bfc\u81f4\u51c6\u786e\u6027\u548c\u6807\u7b7e\u591a\u6837\u6027\u4e4b\u95f4\u7684\u6b21\u4f18\u6743\u8861\u3002\u7814\u7a76\u53d1\u73b0\u4fdd\u5b88\u7684\u6807\u6ce8\u8005\u79fb\u9664\u6bd4\u4f8b\uff08<5%\uff09\u6700\u4f73\uff0c\u4e14\u5783\u573e\u6807\u6ce8\u8005\u901a\u5e38\u6bd4\u975e\u5783\u573e\u6807\u6ce8\u8005\u66f4\u4e0d\u968f\u673a\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u6570\u636e\u96c6\u4e2d\u5982\u4f55\u5728\u8fc7\u6ee4\u5783\u573e\u6216\u4f4e\u8d28\u91cf\u54cd\u5e94\u7684\u540c\u65f6\u4fdd\u6301\u6807\u7b7e\u591a\u6837\u6027\uff0c\u5e73\u8861\u6807\u6ce8\u8005\u53ef\u9760\u6027\u548c\u4ee3\u8868\u6027\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u8bc4\u4f30\u4e00\u7cfb\u5217\u6807\u6ce8\u8005\u8fc7\u6ee4\u542f\u53d1\u5f0f\u65b9\u6cd5\u5bf9\u4e3b\u89c2\u4efb\u52a1\u4e2d\u53d8\u5f02\u4fdd\u7559\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u5408\u6210\u5783\u573e\u6570\u636e\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u73b0\u6709\u65b9\u6cd5\u7ecf\u5e38\u79fb\u9664\u610f\u89c1\u4e0d\u540c\u7684\u6807\u6ce8\u8005\u800c\u975e\u5783\u573e\u6807\u6ce8\u8005\uff0c\u4fdd\u5b88\u8bbe\u7f6e\uff08<5%\u79fb\u9664\u7387\uff09\u6548\u679c\u6700\u597d\u3002\u5783\u573e\u6807\u6ce8\u8005\u901a\u5e38\u7ed9\u51fa\u56fa\u5b9a\u7b54\u6848\u800c\u975e\u968f\u673a\u7b54\u6848\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u7684\u5047\u8bbe\u76f8\u53cd\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8003\u8651\u6807\u7b7e\u591a\u6837\u6027\u7684\u5783\u573e\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u56e0\u4e3a\u73b0\u6709\u65b9\u6cd5\u57fa\u4e8e\u53d8\u5f02\u5373\u5783\u573e\u7684\u76f4\u89c9\u5728\u9700\u8981\u4fdd\u6301\u53d8\u5f02\u6027\u7684\u4efb\u52a1\u4e2d\u6548\u679c\u4e0d\u4f73\u3002"}}
{"id": "2509.08304", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08304", "abs": "https://arxiv.org/abs/2509.08304", "authors": ["Yehudit Aperstein", "Alon Gottlib", "Gal Benita", "Alexander Apartsin"], "title": "Towards Knowledge-Aware Document Systems: Modeling Semantic Coverage Relations via Answerability Detection", "comment": "27 pages, 1 figure", "summary": "Understanding how information is shared across documents, regardless of the\nformat in which it is expressed, is critical for tasks such as information\nretrieval, summarization, and content alignment. In this work, we introduce a\nnovel framework for modelling Semantic Coverage Relations (SCR), which\nclassifies document pairs based on how their informational content aligns. We\ndefine three core relation types: equivalence, where both texts convey the same\ninformation using different textual forms or styles; inclusion, where one\ndocument fully contains the information of another and adds more; and semantic\noverlap, where each document presents partially overlapping content. To capture\nthese relations, we adopt a question answering (QA)-based approach, using the\nanswerability of shared questions across documents as an indicator of semantic\ncoverage. We construct a synthetic dataset derived from the SQuAD corpus by\nparaphrasing source passages and selectively omitting information, enabling\nprecise control over content overlap. This dataset allows us to benchmark\ngenerative language models and train transformer-based classifiers for SCR\nprediction. Our findings demonstrate that discriminative models significantly\noutperform generative approaches, with the RoBERTa-base model achieving the\nhighest accuracy of 61.4% and the Random Forest-based model showing the best\nbalance with a macro-F1 score of 52.9%. The results show that QA provides an\neffective lens for assessing semantic relations across stylistically diverse\ntexts, offering insights into the capacity of current models to reason about\ninformation beyond surface similarity. The dataset and code developed in this\nstudy are publicly available to support reproducibility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u95ee\u7b54\u7684\u8bed\u4e49\u8986\u76d6\u5173\u7cfb(SCR)\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u6587\u6863\u95f4\u5171\u4eab\u95ee\u9898\u7684\u53ef\u56de\u7b54\u6027\u6765\u5224\u65ad\u4fe1\u606f\u5bf9\u9f50\u5173\u7cfb\uff0c\u5305\u62ec\u7b49\u4ef7\u3001\u5305\u542b\u548c\u8bed\u4e49\u91cd\u53e0\u4e09\u79cd\u7c7b\u578b\u3002", "motivation": "\u7406\u89e3\u4e0d\u540c\u683c\u5f0f\u6587\u6863\u95f4\u7684\u4fe1\u606f\u5171\u4eab\u5bf9\u4e8e\u4fe1\u606f\u68c0\u7d22\u3001\u6458\u8981\u548c\u5185\u5bb9\u5bf9\u9f50\u7b49\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u80fd\u591f\u8bc6\u522b\u6587\u6863\u95f4\u8bed\u4e49\u5185\u5bb9\u5bf9\u9f50\u5173\u7cfb\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u95ee\u7b54\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528SQuAD\u8bed\u6599\u5e93\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6539\u5199\u6e90\u6bb5\u843d\u548c\u9009\u62e9\u6027\u7701\u7565\u4fe1\u606f\u6765\u63a7\u5236\u5185\u5bb9\u91cd\u53e0\uff0c\u4f7f\u7528\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u4e8etransformer\u7684\u5206\u7c7b\u5668\u8fdb\u884cSCR\u9884\u6d4b\u3002", "result": "\u5224\u522b\u5f0f\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u751f\u6210\u5f0f\u65b9\u6cd5\uff0cRoBERTa-base\u6a21\u578b\u8fbe\u523061.4%\u7684\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u57fa\u4e8e\u968f\u673a\u68ee\u6797\u7684\u6a21\u578b\u5728macro-F1\u5206\u6570\u4e0a\u8868\u73b0\u6700\u4f73(52.9%)\u3002", "conclusion": "\u95ee\u7b54\u65b9\u6cd5\u4e3a\u8bc4\u4f30\u98ce\u683c\u591a\u6837\u5316\u6587\u672c\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\u63d0\u4f9b\u4e86\u6709\u6548\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u8d85\u8d8a\u8868\u9762\u76f8\u4f3c\u6027\u8fdb\u884c\u4fe1\u606f\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2509.08345", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08345", "abs": "https://arxiv.org/abs/2509.08345", "authors": ["Alejandro Andrade-Lotero", "Lee Becker", "Joshua Southerland", "Scott Hellman"], "title": "Toward Subtrait-Level Model Explainability in Automated Writing Evaluation", "comment": "Accepted to National Council on Measurement in Education (NCME) 2025\n  Annual Meeting", "summary": "Subtrait (latent-trait components) assessment presents a promising path\ntoward enhancing transparency of automated writing scores. We prototype\nexplainability and subtrait scoring with generative language models and show\nmodest correlation between human subtrait and trait scores, and between\nautomated and human subtrait scores. Our approach provides details to demystify\nscores for educators and students.", "AI": {"tldr": "\u4f7f\u7528\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5199\u4f5c\u8bc4\u5206\u4e2d\u7684\u5b50\u7279\u8d28\u8bc4\u4f30\uff0c\u63d0\u4f9b\u66f4\u900f\u660e\u7684\u81ea\u52a8\u5316\u8bc4\u5206\u89e3\u91ca", "motivation": "\u63d0\u9ad8\u81ea\u52a8\u5316\u5199\u4f5c\u8bc4\u5206\u7684\u900f\u660e\u5ea6\uff0c\u4e3a\u6559\u80b2\u8005\u548c\u5b66\u751f\u63d0\u4f9b\u66f4\u8be6\u7ec6\u7684\u8bc4\u5206\u89e3\u91ca", "method": "\u4f7f\u7528\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b50\u7279\u8d28\u8bc4\u5206\u539f\u578b\u5f00\u53d1\uff0c\u5206\u6790\u4eba\u7c7b\u5b50\u7279\u8d28\u8bc4\u5206\u4e0e\u603b\u4f53\u7279\u8d28\u8bc4\u5206\u7684\u76f8\u5173\u6027", "result": "\u4eba\u7c7b\u5b50\u7279\u8d28\u8bc4\u5206\u4e0e\u603b\u4f53\u7279\u8d28\u8bc4\u5206\u4e4b\u95f4\u5b58\u5728\u9002\u5ea6\u76f8\u5173\u6027\uff0c\u81ea\u52a8\u5316\u5b50\u7279\u8d28\u8bc4\u5206\u4e0e\u4eba\u7c7b\u8bc4\u5206\u4e5f\u5448\u73b0\u9002\u5ea6\u76f8\u5173", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6559\u80b2\u8005\u548c\u5b66\u751f\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u8bc4\u5206\u89e3\u91ca\uff0c\u6709\u52a9\u4e8e\u6d88\u9664\u81ea\u52a8\u5316\u8bc4\u5206\u7684\u9ed1\u7bb1\u95ee\u9898"}}
{"id": "2509.08355", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08355", "abs": "https://arxiv.org/abs/2509.08355", "authors": ["Yashad Samant", "Lee Becker", "Scott Hellman", "Bradley Behan", "Sarah Hughes", "Joshua Southerland"], "title": "Automatic Detection of Inauthentic Templated Responses in English Language Assessments", "comment": "Accepted to National Council on Measurement in Education (NCME) 2025\n  Annual Meeting", "summary": "In high-stakes English Language Assessments, low-skill test takers may employ\nmemorized materials called ``templates'' on essay questions to ``game'' or fool\nthe automated scoring system. In this study, we introduce the automated\ndetection of inauthentic, templated responses (AuDITR) task, describe a machine\nlearning-based approach to this task and illustrate the importance of regularly\nupdating these models in production.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u81ea\u52a8\u5316\u68c0\u6d4b\u6a21\u677f\u5316\u5e94\u8bd5\u4f5c\u6587\u7684\u4efb\u52a1(AuDITR)\uff0c\u5f00\u53d1\u4e86\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6765\u8bc6\u522b\u4f4e\u6c34\u5e73\u8003\u751f\u4f7f\u7528\u7684\u8bb0\u5fc6\u6a21\u677f\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u5b9a\u671f\u66f4\u65b0\u6a21\u578b\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5728\u82f1\u8bed\u8bed\u8a00\u8bc4\u4f30\u4e2d\uff0c\u4f4e\u6c34\u5e73\u8003\u751f\u53ef\u80fd\u4f7f\u7528\u8bb0\u5fc6\u7684\u6a21\u677f\u6750\u6599\u6765\u6b3a\u9a97\u81ea\u52a8\u5316\u8bc4\u5206\u7cfb\u7edf\uff0c\u9700\u8981\u5f00\u53d1\u68c0\u6d4b\u65b9\u6cd5\u6765\u786e\u4fdd\u8bc4\u4f30\u7684\u516c\u6b63\u6027\u548c\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6765\u81ea\u52a8\u68c0\u6d4b\u4e0d\u771f\u5b9e\u7684\u6a21\u677f\u5316\u5e94\u8bd5\u4f5c\u6587\u54cd\u5e94\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u68c0\u6d4b\u6a21\u677f\u5316\u54cd\u5e94\u7684\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u8bc6\u522b\u4f5c\u5f0a\u884c\u4e3a\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5b9a\u671f\u66f4\u65b0\u68c0\u6d4b\u6a21\u578b\u5bf9\u4e8e\u7ef4\u6301\u81ea\u52a8\u5316\u8bc4\u5206\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u9632\u6b62\u8003\u751f\u901a\u8fc7\u6a21\u677f\u4f5c\u5f0a\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.08358", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08358", "abs": "https://arxiv.org/abs/2509.08358", "authors": ["Sergey Pletenev", "Daniil Moskovskiy", "Alexander Panchenko"], "title": "<think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs", "comment": null, "summary": "Modern Large Language Models (LLMs) are excellent at generating synthetic\ndata. However, their performance in sensitive domains such as text\ndetoxification has not received proper attention from the scientific community.\nThis paper explores the possibility of using LLM-generated synthetic toxic data\nas an alternative to human-generated data for training models for\ndetoxification. Using Llama 3 and Qwen activation-patched models, we generated\nsynthetic toxic counterparts for neutral texts from ParaDetox and SST-2\ndatasets. Our experiments show that models fine-tuned on synthetic data\nconsistently perform worse than those trained on human data, with a drop in\nperformance of up to 30% in joint metrics. The root cause is identified as a\ncritical lexical diversity gap: LLMs generate toxic content using a small,\nrepetitive vocabulary of insults that fails to capture the nuances and variety\nof human toxicity. These findings highlight the limitations of current LLMs in\nthis domain and emphasize the continued importance of diverse, human-annotated\ndata for building robust detoxification systems.", "AI": {"tldr": "\u4f7f\u7528LLM\u751f\u6210\u7684\u5408\u6210\u6bd2\u6027\u6570\u636e\u8bad\u7ec3\u53bb\u6bd2\u6a21\u578b\u6548\u679c\u8f83\u5dee\uff0c\u6027\u80fd\u4e0b\u964d\u8fbe30%\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8bcd\u6c47\u591a\u6837\u6027\u4e0d\u8db3", "motivation": "\u63a2\u7d22LLM\u751f\u6210\u7684\u5408\u6210\u6bd2\u6027\u6570\u636e\u662f\u5426\u53ef\u4ee5\u4ee3\u66ff\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7528\u4e8e\u6587\u672c\u53bb\u6bd2\u6a21\u578b\u8bad\u7ec3", "method": "\u4f7f\u7528Llama 3\u548cQwen\u6fc0\u6d3b\u8865\u4e01\u6a21\u578b\u4e3aParaDetox\u548cSST-2\u6570\u636e\u96c6\u4e2d\u7684\u4e2d\u6027\u6587\u672c\u751f\u6210\u5408\u6210\u6bd2\u6027\u5bf9\u5e94\u6587\u672c\uff0c\u7136\u540e\u8bad\u7ec3\u53bb\u6bd2\u6a21\u578b", "result": "\u57fa\u4e8e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u4e00\u81f4\u6bd4\u57fa\u4e8e\u4eba\u5de5\u6570\u636e\u7684\u6a21\u578b\u66f4\u5dee\uff0c\u805a\u5408\u6307\u6807\u4e0b\u964d\u8fbe30%", "conclusion": "\u5f53\u524dLLM\u5728\u6bd2\u6027\u5185\u5bb9\u751f\u6210\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u8bcd\u6c47\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u5f3a\u8c03\u4e86\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u5bf9\u5efa\u7acb\u5065\u58ee\u53bb\u6bd2\u7cfb\u7edf\u7684\u91cd\u8981\u6027"}}
{"id": "2509.08381", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08381", "abs": "https://arxiv.org/abs/2509.08381", "authors": ["Yu Cheng Chih", "Yong Hao Hou"], "title": "Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model", "comment": "13 pages, 8 figures, includes experiments on JSON extraction,\n  knowledge graph extraction, and NER", "summary": "Deploying large language models (LLMs) for structured data extraction in\ndomains such as financial compliance reporting, legal document analytics, and\nmultilingual knowledge base construction is often impractical for smaller teams\ndue to the high cost of running large architectures and the difficulty of\npreparing large, high-quality datasets. Most recent instruction-tuning studies\nfocus on seven-billion-parameter or larger models, leaving limited evidence on\nwhether much smaller models can work reliably under low-resource, multi-task\nconditions. This work presents ETLCH, a billion-parameter LLaMA-based model\nfine-tuned with low-rank adaptation on only a few hundred to one thousand\nsamples per task for JSON extraction, knowledge graph extraction, and named\nentity recognition. Despite its small scale, ETLCH outperforms strong baselines\nacross most evaluation metrics, with substantial gains observed even at the\nlowest data scale. These findings demonstrate that well-tuned small models can\ndeliver stable and accurate structured outputs at a fraction of the\ncomputational cost, enabling cost-effective and reliable information extraction\npipelines in resource-constrained environments.", "AI": {"tldr": "ETLCH\u662f\u4e00\u4e2a\u57fa\u4e8eLLaMA\u768410\u4ebf\u53c2\u6570\u5c0f\u6a21\u578b\uff0c\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u5fae\u8c03\u5728\u7ed3\u6784\u5316\u6570\u636e\u63d0\u53d6\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u672c\u6548\u76ca\u9ad8", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u6570\u636e\u63d0\u53d6\u9886\u57df\u90e8\u7f72\u6210\u672c\u9ad8\u4e14\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u5c0f\u56e2\u961f\u96be\u4ee5\u627f\u53d7\u3002\u76ee\u524d\u7f3a\u4e4f\u5c0f\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u591a\u4efb\u52a1\u6761\u4ef6\u4e0b\u53ef\u9760\u6027\u7684\u7814\u7a76", "method": "\u4f7f\u7528\u4f4e\u79e9\u9002\u5e94(LoRA)\u6280\u672f\u5bf9LLaMA\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u6bcf\u4e2a\u4efb\u52a1\u4ec5\u4f7f\u7528\u51e0\u767e\u5230\u4e00\u5343\u4e2a\u6837\u672c\uff0c\u7528\u4e8eJSON\u63d0\u53d6\u3001\u77e5\u8bc6\u56fe\u8c31\u63d0\u53d6\u548c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b", "result": "ETLCH\u5728\u5927\u591a\u6570\u8bc4\u4f30\u6307\u6807\u4e0a\u8d85\u8d8a\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u5373\u4f7f\u5728\u6700\u4f4e\u6570\u636e\u89c4\u6a21\u4e0b\u4e5f\u89c2\u5bdf\u5230\u663e\u8457\u589e\u76ca", "conclusion": "\u7cbe\u5fc3\u8c03\u4f18\u7684\u5c0f\u6a21\u578b\u80fd\u591f\u4ee5\u6781\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u63d0\u4f9b\u7a33\u5b9a\u51c6\u786e\u7684\u7ed3\u6784\u5316\u8f93\u51fa\uff0c\u4f7f\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u4fe1\u606f\u63d0\u53d6\u7ba1\u9053\u65e2\u7ecf\u6d4e\u53c8\u53ef\u9760"}}
{"id": "2509.08463", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08463", "abs": "https://arxiv.org/abs/2509.08463", "authors": ["Fanzhen Liu", "Alsharif Abuadbba", "Kristen Moore", "Surya Nepal", "Cecile Paris", "Jia Wu", "Jian Yang", "Quan Z. Sheng"], "title": "Adversarial Attacks Against Automated Fact-Checking: A Survey", "comment": "Accepted to the Main Conference of EMNLP 2025. Resources are\n  available at\n  https://github.com/FanzhenLiu/Awesome-Automated-Fact-Checking-Attacks", "summary": "In an era where misinformation spreads freely, fact-checking (FC) plays a\ncrucial role in verifying claims and promoting reliable information. While\nautomated fact-checking (AFC) has advanced significantly, existing systems\nremain vulnerable to adversarial attacks that manipulate or generate claims,\nevidence, or claim-evidence pairs. These attacks can distort the truth, mislead\ndecision-makers, and ultimately undermine the reliability of FC models. Despite\ngrowing research interest in adversarial attacks against AFC systems, a\ncomprehensive, holistic overview of key challenges remains lacking. These\nchallenges include understanding attack strategies, assessing the resilience of\ncurrent models, and identifying ways to enhance robustness. This survey\nprovides the first in-depth review of adversarial attacks targeting FC,\ncategorizing existing attack methodologies and evaluating their impact on AFC\nsystems. Additionally, we examine recent advancements in adversary-aware\ndefenses and highlight open research questions that require further\nexploration. Our findings underscore the urgent need for resilient FC\nframeworks capable of withstanding adversarial manipulations in pursuit of\npreserving high verification accuracy.", "AI": {"tldr": "\u5bf9\u6297\u6027\u653b\u51fb\u5bf9\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u672c\u8c03\u67e5\u9996\u6b21\u5168\u9762\u7efc\u8ff0\u4e86\u9488\u5bf9\u4e8b\u5b9e\u6838\u67e5\u7684\u653b\u51fb\u65b9\u6cd5\u3001\u9632\u5fa1\u7b56\u7565\u548c\u5173\u952e\u6311\u6218", "motivation": "\u5728\u9519\u8bef\u4fe1\u606f\u6cdb\u6ee5\u7684\u65f6\u4ee3\uff0c\u73b0\u6709\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u8fd9\u4e9b\u653b\u51fb\u4f1a\u64cd\u7eb5\u6216\u751f\u6210\u865a\u5047\u58f0\u660e\u3001\u8bc1\u636e\u6216\u58f0\u660e-\u8bc1\u636e\u5bf9\uff0c\u4ece\u800c\u7834\u574f\u4e8b\u5b9e\u6838\u67e5\u6a21\u578b\u7684\u53ef\u9760\u6027", "method": "\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u8fdb\u884c\u6df1\u5165\u7efc\u8ff0\uff0c\u5206\u7c7b\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u5b66\uff0c\u8bc4\u4f30\u5176\u5bf9\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u7684\u5f71\u54cd\uff0c\u5e76\u68c0\u67e5\u6700\u65b0\u7684\u5bf9\u6297\u611f\u77e5\u9632\u5fa1\u8fdb\u5c55", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u5728\u9762\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u65f6\u5b58\u5728\u8106\u5f31\u6027\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u9632\u5fa1\u673a\u5236", "conclusion": "\u8feb\u5207\u9700\u8981\u5f00\u53d1\u80fd\u591f\u62b5\u5fa1\u5bf9\u6297\u6027\u64cd\u7eb5\u7684\u5f39\u6027\u4e8b\u5b9e\u6838\u67e5\u6846\u67b6\uff0c\u4ee5\u4fdd\u6301\u9ad8\u9a8c\u8bc1\u51c6\u786e\u6027"}}
{"id": "2509.08480", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08480", "abs": "https://arxiv.org/abs/2509.08480", "authors": ["Daniel Braun"], "title": "Acquiescence Bias in Large Language Models", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Acquiescence bias, i.e. the tendency of humans to agree with statements in\nsurveys, independent of their actual beliefs, is well researched and\ndocumented. Since Large Language Models (LLMs) have been shown to be very\ninfluenceable by relatively small changes in input and are trained on\nhuman-generated data, it is reasonable to assume that they could show a similar\ntendency. We present a study investigating the presence of acquiescence bias in\nLLMs across different models, tasks, and languages (English, German, and\nPolish). Our results indicate that, contrary to humans, LLMs display a bias\ntowards answering no, regardless of whether it indicates agreement or\ndisagreement.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.08484", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08484", "abs": "https://arxiv.org/abs/2509.08484", "authors": ["Pia Sommerauer", "Giulia Rambelli", "Tommaso Caselli"], "title": "Simulating Identity, Propagating Bias: Abstraction and Stereotypes in LLM-Generated Text", "comment": "Accepted to EMNLP Findings 2025", "summary": "Persona-prompting is a growing strategy to steer LLMs toward simulating\nparticular perspectives or linguistic styles through the lens of a specified\nidentity. While this method is often used to personalize outputs, its impact on\nhow LLMs represent social groups remains underexplored. In this paper, we\ninvestigate whether persona-prompting leads to different levels of linguistic\nabstraction - an established marker of stereotyping - when generating short\ntexts linking socio-demographic categories with stereotypical or\nnon-stereotypical attributes. Drawing on the Linguistic Expectancy Bias\nframework, we analyze outputs from six open-weight LLMs under three prompting\nconditions, comparing 11 persona-driven responses to those of a generic AI\nassistant. To support this analysis, we introduce Self-Stereo, a new dataset of\nself-reported stereotypes from Reddit. We measure abstraction through three\nmetrics: concreteness, specificity, and negation. Our results highlight the\nlimits of persona-prompting in modulating abstraction in language, confirming\ncriticisms about the ecology of personas as representative of socio-demographic\ngroups and raising concerns about the risk of propagating stereotypes even when\nseemingly evoking the voice of a marginalized group.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4eba\u7269\u89d2\u8272\u63d0\u793a\u5e76\u4e0d\u80fd\u6709\u6548\u8c03\u8282\u8bed\u8a00\u62bd\u8c61\u7a0b\u5ea6\uff0c\u53cd\u800c\u53ef\u80fd\u4f20\u64ad\u523b\u677f\u5370\u8c61\uff0c\u5373\u4f7f\u662f\u5728\u6a21\u62df\u8fb9\u7f18\u7fa4\u4f53\u58f0\u97f3\u65f6\u4e5f\u5b58\u5728\u98ce\u9669", "motivation": "\u63a2\u7d22\u4eba\u7269\u89d2\u8272\u63d0\u793a\u65b9\u6cd5\u5bf9LLMs\u5728\u8868\u793a\u793e\u4f1a\u7fa4\u4f53\u65f6\u8bed\u8a00\u62bd\u8c61\u7a0b\u5ea6\uff08\u523b\u677f\u5370\u8c61\u6807\u8bb0\uff09\u7684\u5f71\u54cd\uff0c\u8be5\u65b9\u6cd5\u5728\u4e2a\u6027\u5316\u8f93\u51fa\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u957f\u4f46\u5176\u793e\u4f1a\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76", "method": "\u57fa\u4e8e\u8bed\u8a00\u671f\u671b\u504f\u5dee\u6846\u67b6\uff0c\u5206\u67906\u4e2a\u5f00\u6e90LLM\u5728\u4e09\u79cd\u63d0\u793a\u6761\u4ef6\u4e0b\u7684\u8f93\u51fa\uff0c\u6bd4\u8f8311\u79cd\u4eba\u7269\u89d2\u8272\u9a71\u52a8\u54cd\u5e94\u4e0e\u901a\u7528AI\u52a9\u624b\u7684\u54cd\u5e94\uff0c\u4f7f\u7528\u5177\u4f53\u6027\u3001\u7279\u5f02\u6027\u548c\u5426\u5b9a\u6027\u4e09\u4e2a\u6307\u6807\u6d4b\u91cf\u62bd\u8c61\u7a0b\u5ea6\uff0c\u5e76\u5f15\u5165Self-Stereo\u65b0\u6570\u636e\u96c6", "result": "\u4eba\u7269\u89d2\u8272\u63d0\u793a\u5728\u8c03\u8282\u8bed\u8a00\u62bd\u8c61\u7a0b\u5ea6\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8bc1\u5b9e\u4e86\u4eba\u7269\u89d2\u8272\u4f5c\u4e3a\u793e\u4f1a\u4eba\u53e3\u7fa4\u4f53\u4ee3\u8868\u7684\u6709\u6548\u6027\u6279\u8bc4\uff0c\u5e76\u5f15\u53d1\u4e86\u5bf9\u5373\u4f7f\u770b\u4f3c\u5524\u8d77\u8fb9\u7f18\u7fa4\u4f53\u58f0\u97f3\u65f6\u4e5f\u53ef\u80fd\u4f20\u64ad\u523b\u677f\u5370\u8c61\u7684\u62c5\u5fe7", "conclusion": "\u4eba\u7269\u89d2\u8272\u63d0\u793a\u65b9\u6cd5\u5728\u8c03\u8282\u8bed\u8a00\u62bd\u8c61\u548c\u907f\u514d\u523b\u677f\u5370\u8c61\u65b9\u9762\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u8c28\u614e\u5730\u4f7f\u7528\u8fd9\u79cd\u65b9\u6cd5\u4ee5\u907f\u514d\u5f3a\u5316\u793e\u4f1a\u504f\u89c1"}}
{"id": "2509.08486", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08486", "abs": "https://arxiv.org/abs/2509.08486", "authors": ["Gautam Siddharth Kashyap", "Mark Dras", "Usman Naseem"], "title": "Too Helpful, Too Harmless, Too Honest or Just Right?", "comment": "EMNLP'25 Main", "summary": "Large Language Models (LLMs) exhibit strong performance across a wide range\nof NLP tasks, yet aligning their outputs with the principles of Helpfulness,\nHarmlessness, and Honesty (HHH) remains a persistent challenge. Existing\nmethods often optimize for individual alignment dimensions in isolation,\nleading to trade-offs and inconsistent behavior. While Mixture-of-Experts (MoE)\narchitectures offer modularity, they suffer from poorly calibrated routing,\nlimiting their effectiveness in alignment tasks. We propose TrinityX, a modular\nalignment framework that incorporates a Mixture of Calibrated Experts (MoCaE)\nwithin the Transformer architecture. TrinityX leverages separately trained\nexperts for each HHH dimension, integrating their outputs through a calibrated,\ntask-adaptive routing mechanism that combines expert signals into a unified,\nalignment-aware representation. Extensive experiments on three standard\nalignment benchmarks-Alpaca (Helpfulness), BeaverTails (Harmlessness), and\nTruthfulQA (Honesty)-demonstrate that TrinityX outperforms strong baselines,\nachieving relative improvements of 32.5% in win rate, 33.9% in safety score,\nand 28.4% in truthfulness. In addition, TrinityX reduces memory usage and\ninference latency by over 40% compared to prior MoE-based approaches. Ablation\nstudies highlight the importance of calibrated routing, and cross-model\nevaluations confirm TrinityX's generalization across diverse LLM backbones.", "AI": {"tldr": "TrinityX\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u6821\u51c6\u4e13\u5bb6\u6df7\u5408(MoCaE)\u673a\u5236\u5728Transformer\u67b6\u6784\u4e2d\u5206\u522b\u5904\u7406Helpfulness\u3001Harmlessness\u3001Honesty\u4e09\u4e2a\u5bf9\u9f50\u7ef4\u5ea6\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u8868\u73b0\u548c\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5355\u72ec\u4f18\u5316\u5404\u4e2a\u5bf9\u9f50\u7ef4\u5ea6\uff0c\u5bfc\u81f4\u6743\u8861\u548c\u4e0d\u4e00\u81f4\u884c\u4e3a\u3002\u867d\u7136MoE\u67b6\u6784\u63d0\u4f9b\u6a21\u5757\u5316\uff0c\u4f46\u8def\u7531\u6821\u51c6\u4e0d\u4f73\u9650\u5236\u4e86\u5728\u5bf9\u9f50\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51faTrinityX\u6846\u67b6\uff0c\u5305\u542bMixture of Calibrated Experts (MoCaE)\uff0c\u4e3a\u6bcf\u4e2aHHH\u7ef4\u5ea6\u5206\u522b\u8bad\u7ec3\u4e13\u5bb6\uff0c\u901a\u8fc7\u6821\u51c6\u7684\u4efb\u52a1\u81ea\u9002\u5e94\u8def\u7531\u673a\u5236\u6574\u5408\u4e13\u5bb6\u8f93\u51fa\u3002", "result": "\u5728\u4e09\u4e2a\u6807\u51c6\u5bf9\u9f50\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTrinityX\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u80dc\u7387\u4e0a\u63d0\u534732.5%\uff0c\u5b89\u5168\u5206\u6570\u63d0\u534733.9%\uff0c\u771f\u5b9e\u6027\u63d0\u534728.4%\uff0c\u540c\u65f6\u5185\u5b58\u4f7f\u7528\u548c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e40%\u4ee5\u4e0a\u3002", "conclusion": "TrinityX\u901a\u8fc7\u6821\u51c6\u8def\u7531\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86LLM\u5bf9\u9f50\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u63d0\u9ad8\u6548\u7387\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.08541", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08541", "abs": "https://arxiv.org/abs/2509.08541", "authors": ["Xue Zhang", "Yunlong Liang", "Fandong Meng", "Songming Zhang", "Yufeng Chen", "Jinan Xu", "Jie Zhou"], "title": "CM-Align: Consistency-based Multilingual Alignment for Large Language Models", "comment": "EMNLP 2025 Findings", "summary": "Current large language models (LLMs) generally show a significant performance\ngap in alignment between English and other languages. To bridge this gap,\nexisting research typically leverages the model's responses in English as a\nreference to select the best/worst responses in other languages, which are then\nused for Direct Preference Optimization (DPO) training. However, we argue that\nthere are two limitations in the current methods that result in noisy\nmultilingual preference data and further limited alignment performance: 1) Not\nall English responses are of high quality, and using a response with low\nquality may mislead the alignment for other languages. 2) Current methods\nusually use biased or heuristic approaches to construct multilingual preference\npairs. To address these limitations, we design a consistency-based data\nselection method to construct high-quality multilingual preference data for\nimproving multilingual alignment (CM-Align). Specifically, our method includes\ntwo parts: consistency-guided English reference selection and cross-lingual\nconsistency-based multilingual preference data construction. Experimental\nresults on three LLMs and three common tasks demonstrate the effectiveness and\nsuperiority of our method, which further indicates the necessity of\nconstructing high-quality preference data.", "AI": {"tldr": "\u63d0\u51faCM-Align\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u5f15\u5bfc\u7684\u6570\u636e\u9009\u62e9\u6784\u5efa\u9ad8\u8d28\u91cf\u591a\u8bed\u8a00\u504f\u597d\u6570\u636e\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u82f1\u8bed\u53c2\u8003\u8d28\u91cf\u4e0d\u9ad8\u548c\u504f\u597d\u5bf9\u6784\u5efa\u6709\u504f\u5dee\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00\u5bf9\u9f50\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u82f1\u8bed\u548c\u5176\u4ed6\u8bed\u8a00\u4e4b\u95f4\u7684\u5bf9\u9f50\u6027\u80fd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u82f1\u8bed\u56de\u7b54\u4f5c\u4e3a\u53c2\u8003\u6765\u6784\u5efa\u591a\u8bed\u8a00\u504f\u597d\u6570\u636e\uff0c\u4f46\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a1\uff09\u5e76\u975e\u6240\u6709\u82f1\u8bed\u56de\u7b54\u90fd\u662f\u9ad8\u8d28\u91cf\u7684\uff0c\u4f4e\u8d28\u91cf\u56de\u7b54\u4f1a\u8bef\u5bfc\u5176\u4ed6\u8bed\u8a00\u7684\u5bf9\u9f50\uff1b2\uff09\u5f53\u524d\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u6709\u504f\u89c1\u6216\u542f\u53d1\u5f0f\u7684\u65b9\u6cd5\u6784\u5efa\u591a\u8bed\u8a00\u504f\u597d\u5bf9\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u4e00\u81f4\u6027\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff08CM-Align\uff09\uff0c\u5305\u62ec\u4e24\u4e2a\u90e8\u5206\uff1a\u4e00\u81f4\u6027\u5f15\u5bfc\u7684\u82f1\u8bed\u53c2\u8003\u9009\u62e9\u548c\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\u4e3a\u57fa\u7840\u7684\u591a\u8bed\u8a00\u504f\u597d\u6570\u636e\u6784\u5efa\u3002\u901a\u8fc7\u4e00\u81f4\u6027\u6807\u51c6\u6765\u7b5b\u9009\u9ad8\u8d28\u91cf\u7684\u82f1\u8bed\u56de\u7b54\u4f5c\u4e3a\u53c2\u8003\uff0c\u5e76\u57fa\u4e8e\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\u6784\u5efa\u66f4\u53ef\u9760\u7684\u591a\u8bed\u8a00\u504f\u597d\u5bf9\u3002", "result": "\u5728\u4e09\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4e09\u4e2a\u5e38\u89c1\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u5bf9\u9f50\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u6784\u5efa\u9ad8\u8d28\u91cf\u504f\u597d\u6570\u636e\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u591a\u8bed\u8a00\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u6807\u51c6\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u566a\u58f0\u548c\u6570\u636e\u8d28\u91cf\u95ee\u9898\u3002"}}
{"id": "2509.08596", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08596", "abs": "https://arxiv.org/abs/2509.08596", "authors": ["Dima Galat", "Diego Molla-Aliod"], "title": "LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question Answering for BioASQ Challenge", "comment": "CEUR-WS, CLEF2025", "summary": "Biomedical question answering (QA) poses significant challenges due to the\nneed for precise interpretation of specialized knowledge drawn from a vast,\ncomplex, and rapidly evolving corpus. In this work, we explore how large\nlanguage models (LLMs) can be used for information retrieval (IR), and an\nensemble of zero-shot models can accomplish state-of-the-art performance on a\ndomain-specific Yes/No QA task. Evaluating our approach on the BioASQ challenge\ntasks, we show that ensembles can outperform individual LLMs and in some cases\nrival or surpass domain-tuned systems - all while preserving generalizability\nand avoiding the need for costly fine-tuning or labeled data. Our method\naggregates outputs from multiple LLM variants, including models from Anthropic\nand Google, to synthesize more accurate and robust answers. Moreover, our\ninvestigation highlights a relationship between context length and performance:\nwhile expanded contexts are meant to provide valuable evidence, they\nsimultaneously risk information dilution and model disorientation. These\nfindings emphasize IR as a critical foundation in Retrieval-Augmented\nGeneration (RAG) approaches for biomedical QA systems. Precise, focused\nretrieval remains essential for ensuring LLMs operate within relevant\ninformation boundaries when generating answers from retrieved documents. Our\nresults establish that ensemble-based zero-shot approaches, when paired with\neffective RAG pipelines, constitute a practical and scalable alternative to\ndomain-tuned systems for biomedical question answering.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u5728\u751f\u7269\u533b\u5b66\u95ee\u7b54\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u65e0\u9700\u5fae\u8c03\u6216\u6807\u6ce8\u6570\u636e\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u95ee\u7b54\u9762\u4e34\u4e13\u4e1a\u77e5\u8bc6\u590d\u6742\u3001\u8bed\u6599\u5e9e\u5927\u4e14\u5feb\u901f\u6f14\u53d8\u7684\u6311\u6218\uff0c\u9700\u8981\u63a2\u7d22LLM\u5982\u4f55\u7528\u4e8e\u4fe1\u606f\u68c0\u7d22\uff0c\u4ee5\u53ca\u96c6\u6210\u65b9\u6cd5\u80fd\u5426\u5728\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4f20\u7edf\u5fae\u8c03\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u591a\u4e2aLLM\u53d8\u4f53(\u5305\u62ecAnthropic\u548cGoogle\u7684\u6a21\u578b)\u7684\u8f93\u51fa\u96c6\u6210\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7ba1\u9053\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u65b9\u5f0f\u5408\u6210\u66f4\u51c6\u786e\u548c\u9c81\u68d2\u7684\u7b54\u6848\u3002", "result": "\u96c6\u6210\u65b9\u6cd5\u5728BioASQ\u6311\u6218\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u4e2aLLM\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u5ab2\u7f8e\u6216\u8d85\u8d8a\u9886\u57df\u8c03\u4f18\u7cfb\u7edf\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u6027\u3002\u7814\u7a76\u53d1\u73b0\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0e\u6027\u80fd\u5b58\u5728\u5173\u8054\u5173\u7cfb\u3002", "conclusion": "\u57fa\u4e8e\u96c6\u6210\u7684\u96f6\u6837\u672c\u65b9\u6cd5\u7ed3\u5408\u6709\u6548\u7684RAG\u7ba1\u9053\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u95ee\u7b54\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7cbe\u786e\u68c0\u7d22\u5bf9\u4e8e\u786e\u4fddLLM\u5728\u76f8\u5173\u4fe1\u606f\u8fb9\u754c\u5185\u751f\u6210\u7b54\u6848\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.08604", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08604", "abs": "https://arxiv.org/abs/2509.08604", "authors": ["Anran Li", "Lingfei Qian", "Mengmeng Du", "Yu Yin", "Yan Hu", "Zihao Sun", "Yihang Fu", "Erica Stutz", "Xuguang Ai", "Qianqian Xie", "Rui Zhu", "Jimin Huang", "Yifan Yang", "Siru Liu", "Yih-Chung Tham", "Lucila Ohno-Machado", "Hyunghoon Cho", "Zhiyong Lu", "Hua Xu", "Qingyu Chen"], "title": "Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant potential in\nmedicine. To date, LLMs have been widely applied to tasks such as diagnostic\nassistance, medical question answering, and clinical information synthesis.\nHowever, a key open question remains: to what extent do LLMs memorize medical\ntraining data. In this study, we present the first comprehensive evaluation of\nmemorization of LLMs in medicine, assessing its prevalence (how frequently it\noccurs), characteristics (what is memorized), volume (how much content is\nmemorized), and potential downstream impacts (how memorization may affect\nmedical applications). We systematically analyze common adaptation scenarios:\n(1) continued pretraining on medical corpora, (2) fine-tuning on standard\nmedical benchmarks, and (3) fine-tuning on real-world clinical data, including\nover 13,000 unique inpatient records from Yale New Haven Health System. The\nresults demonstrate that memorization is prevalent across all adaptation\nscenarios and significantly higher than reported in the general domain.\nMemorization affects both the development and adoption of LLMs in medicine and\ncan be categorized into three types: beneficial (e.g., accurate recall of\nclinical guidelines and biomedical references), uninformative (e.g., repeated\ndisclaimers or templated medical document language), and harmful (e.g.,\nregeneration of dataset-specific or sensitive clinical content). Based on these\nfindings, we offer practical recommendations to facilitate beneficial\nmemorization that enhances domain-specific reasoning and factual accuracy,\nminimize uninformative memorization to promote deeper learning beyond\nsurface-level patterns, and mitigate harmful memorization to prevent the\nleakage of sensitive or identifiable patient information.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u5b58\u5728\u663e\u8457\u7684\u8bb0\u5fc6\u95ee\u9898\uff0c\u8bb0\u5fc6\u7387\u9ad8\u4e8e\u666e\u901a\u9886\u57df\uff0c\u5305\u542b\u6709\u76ca\u3001\u65e0\u610f\u4e49\u548c\u6709\u5bb3\u4e09\u7c7b\u8bb0\u5fc6\uff0c\u9700\u8981\u9488\u5bf9\u6027\u7b56\u7565\u6765\u4f18\u5316\u533b\u7597LLM\u5e94\u7528\u3002", "motivation": "\u8bc6\u522b\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\u7684\u7a0b\u5ea6\u548c\u7279\u5f81\uff0c\u4ee5\u4f30\u8ba1\u5176\u5bf9\u533b\u7597\u5e94\u7528\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e09\u79cd\u9002\u914d\u573a\u666f\uff1a\u533b\u5b66\u8bed\u6599\u7eed\u9884\u8bad\u7ec3\u3001\u6807\u51c6\u533b\u5b66\u6307\u6807\u5fae\u8c03\u3001\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u5fae\u8c03\uff08\u5305\u542b\u8d8513,000\u4efd\u75c5\u4eba\u8bb0\u5f55\uff09\u3002\u8bc4\u4f30\u8bb0\u5fc6\u7684\u666e\u904d\u6027\u3001\u7279\u5f81\u3001\u91cf\u7ea7\u548c\u4e0b\u6e38\u5f71\u54cd\u3002", "result": "\u8bb0\u5fc6\u73b0\u8c61\u5728\u6240\u6709\u9002\u914d\u573a\u666f\u4e2d\u90fd\u5f88\u666e\u904d\uff0c\u8bb0\u5fc6\u7387\u663e\u8457\u9ad8\u4e8e\u666e\u901a\u9886\u57df\u3002\u8bb0\u5fc6\u53ef\u5206\u4e3a\u4e09\u7c7b\uff1a\u6709\u76ca\u8bb0\u5fc6\uff08\u4e34\u5e8a\u6307\u5357\u3001\u751f\u7269\u533b\u5b66\u53c2\u8003\uff09\u3001\u65e0\u610f\u4e49\u8bb0\u5fc6\uff08\u6a21\u677f\u5316\u8bed\u8a00\uff09\u3001\u6709\u5bb3\u8bb0\u5fc6\uff08\u654f\u611f\u4e34\u5e8a\u4fe1\u606f\u6cc4\u6f0f\uff09\u3002", "conclusion": "\u8bb0\u5fc6\u95ee\u9898\u5f71\u54cd\u533b\u7597LLM\u7684\u53d1\u5c55\u548c\u91c7\u7528\uff0c\u9700\u8981\u901a\u8fc7\u5b9e\u8df5\u5efa\u8bae\u6765\u4fc3\u8fdb\u6709\u76ca\u8bb0\u5fc6\u3001\u51cf\u5c11\u65e0\u610f\u4e49\u8bb0\u5fc6\u3001\u51cf\u8f7b\u6709\u5bb3\u8bb0\u5fc6\uff0c\u4ee5\u4fdd\u62a4\u75c5\u4eba\u9690\u79c1\u548c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2509.08612", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08612", "abs": "https://arxiv.org/abs/2509.08612", "authors": ["Xinfeng Liao", "Xuanqi Chen", "Lianxi Wang", "Jiahuan Yang", "Zhuowei Chen", "Ziying Rong"], "title": "OTESGN:Optimal Transport Enhanced Syntactic-Semantic Graph Networks for Aspect-Based Sentiment Analysis", "comment": null, "summary": "Aspect-based sentiment analysis (ABSA) aims to identify aspect terms and\ndetermine their sentiment polarity. While dependency trees combined with\ncontextual semantics effectively identify aspect sentiment, existing methods\nrelying on syntax trees and aspect-aware attention struggle to model complex\nsemantic relationships. Their dependence on linear dot-product features fails\nto capture nonlinear associations, allowing noisy similarity from irrelevant\nwords to obscure key opinion terms. Motivated by Differentiable Optimal\nMatching, we propose the Optimal Transport Enhanced Syntactic-Semantic Graph\nNetwork (OTESGN), which introduces a Syntactic-Semantic Collaborative\nAttention. It comprises a Syntactic Graph-Aware Attention for mining latent\nsyntactic dependencies and modeling global syntactic topology, as well as a\nSemantic Optimal Transport Attention designed to uncover fine-grained semantic\nalignments amidst textual noise, thereby accurately capturing sentiment signals\nobscured by irrelevant tokens. A Adaptive Attention Fusion module integrates\nthese heterogeneous features, and contrastive regularization further improves\nrobustness. Experiments demonstrate that OTESGN achieves state-of-the-art\nresults, outperforming previous best models by +1.01% F1 on Twitter and +1.30%\nF1 on Laptop14 benchmarks. Ablative studies and visual analyses corroborate its\nefficacy in precise localization of opinion words and noise resistance.", "AI": {"tldr": "\u63d0\u51faOTESGN\u6a21\u578b\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u589e\u5f3a\u7684\u8bed\u6cd5-\u8bed\u4e49\u56fe\u7f51\u7edc\uff0c\u7ed3\u5408\u8bed\u6cd5\u56fe\u611f\u77e5\u6ce8\u610f\u529b\u548c\u8bed\u4e49\u6700\u4f18\u4f20\u8f93\u6ce8\u610f\u529b\uff0c\u5728ABSA\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8bed\u6cd5\u6811\u548c\u65b9\u9762\u611f\u77e5\u6ce8\u610f\u529b\u7684\u65b9\u6cd5\u96be\u4ee5\u5efa\u6a21\u590d\u6742\u8bed\u4e49\u5173\u7cfb\uff0c\u7ebf\u6027\u70b9\u79ef\u7279\u5f81\u65e0\u6cd5\u6355\u83b7\u975e\u7ebf\u6027\u5173\u8054\uff0c\u5bfc\u81f4\u65e0\u5173\u8bcd\u6c47\u7684\u566a\u58f0\u76f8\u4f3c\u6027\u63a9\u76d6\u5173\u952e\u89c2\u70b9\u8bcd", "method": "OTESGN\u6a21\u578b\u5305\u542b\u8bed\u6cd5-\u8bed\u4e49\u534f\u4f5c\u6ce8\u610f\u529b\u673a\u5236\uff1a\u8bed\u6cd5\u56fe\u611f\u77e5\u6ce8\u610f\u529b\u6316\u6398\u6f5c\u5728\u8bed\u6cd5\u4f9d\u8d56\u548c\u5168\u5c40\u8bed\u6cd5\u62d3\u6251\uff1b\u8bed\u4e49\u6700\u4f18\u4f20\u8f93\u6ce8\u610f\u529b\u5728\u6587\u672c\u566a\u58f0\u4e2d\u53d1\u73b0\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5bf9\u9f50\uff1b\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u6574\u5408\u5f02\u6784\u7279\u5f81\uff1b\u5bf9\u6bd4\u6b63\u5219\u5316\u63d0\u5347\u9c81\u68d2\u6027", "result": "\u5728Twitter\u6570\u636e\u96c6\u4e0aF1\u63d0\u53471.01%\uff0c\u5728Laptop14\u6570\u636e\u96c6\u4e0aF1\u63d0\u53471.30%\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002\u6d88\u878d\u7814\u7a76\u548c\u53ef\u89c6\u5316\u5206\u6790\u8bc1\u5b9e\u5176\u5728\u7cbe\u786e\u5b9a\u4f4d\u89c2\u70b9\u8bcd\u548c\u6297\u566a\u58f0\u65b9\u9762\u7684\u6709\u6548\u6027", "conclusion": "OTESGN\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86ABSA\u4efb\u52a1\u4e2d\u8bed\u4e49\u566a\u58f0\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027"}}
{"id": "2509.08729", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08729", "abs": "https://arxiv.org/abs/2509.08729", "authors": ["Hyunjun Kim", "Junwoo Ha", "Sangyoon Yu", "Haon Park"], "title": "X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates", "comment": null, "summary": "Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one\nstructured prompt, but prior work relied on a handful of manually written\ntemplates. We present X-Teaming Evolutionary M2S, an automated framework that\ndiscovers and optimizes M2S templates through language-model-guided evolution.\nThe system pairs smart sampling from 12 sources with an LLM-as-judge inspired\nby StrongREJECT and records fully auditable logs.\n  Maintaining selection pressure by setting the success threshold to $\\theta =\n0.70$, we obtain five evolutionary generations, two new template families, and\n44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of\n2,500 trials (judge fixed) shows that structural gains transfer but vary by\ntarget; two models score zero at the same threshold. We also find a positive\ncoupling between prompt length and score, motivating length-aware judging.\n  Our results demonstrate that structure-level search is a reproducible route\nto stronger single-turn probes and underscore the importance of threshold\ncalibration and cross-model evaluation. Code, configurations, and artifacts are\navailable at https://github.com/hyunjun1121/M2S-x-teaming.", "AI": {"tldr": "X-Teaming Evolutionary M2S\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u7684\u8fdb\u5316\u6765\u53d1\u73b0\u548c\u4f18\u5316M2S\u6a21\u677f\uff0c\u5c06\u591a\u8f6e\u7ea2\u961f\u6d4b\u8bd5\u538b\u7f29\u4e3a\u5355\u8f6e\u7ed3\u6784\u5316\u63d0\u793a", "motivation": "\u73b0\u6709\u7684M2S\u65b9\u6cd5\u4f9d\u8d56\u5c11\u91cf\u624b\u52a8\u7f16\u5199\u7684\u6a21\u677f\uff0c\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u53d1\u73b0\u548c\u4f18\u5316\u66f4\u6709\u6548\u7684\u6a21\u677f\u7ed3\u6784", "method": "\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u7684\u8fdb\u5316\u7b97\u6cd5\uff0c\u7ed3\u540812\u4e2a\u6765\u6e90\u7684\u667a\u80fd\u91c7\u6837\u548c\u57fa\u4e8eStrongREJECT\u7684LLM-as-judge\u8bc4\u4f30\uff0c\u8bbe\u7f6e\u6210\u529f\u9608\u503c\u03b8=0.70\u8fdb\u884c\u9009\u62e9", "result": "\u83b7\u5f97\u4e865\u4e2a\u8fdb\u5316\u4e16\u4ee3\u30012\u4e2a\u65b0\u6a21\u677f\u5bb6\u65cf\uff0c\u5728GPT-4.1\u4e0a\u8fbe\u523044.8%\u7684\u603b\u4f53\u6210\u529f\u7387\uff08103/230\uff09\u3002\u8de8\u6a21\u578b\u6d4b\u8bd5\u663e\u793a\u7ed3\u6784\u589e\u76ca\u53ef\u8fc1\u79fb\u4f46\u968f\u76ee\u6807\u6a21\u578b\u53d8\u5316", "conclusion": "\u7ed3\u6784\u7ea7\u641c\u7d22\u662f\u53ef\u590d\u73b0\u7684\u5f3a\u5316\u5355\u8f6e\u63a2\u6d4b\u7684\u6709\u6548\u9014\u5f84\uff0c\u5f3a\u8c03\u4e86\u9608\u503c\u6821\u51c6\u548c\u8de8\u6a21\u578b\u8bc4\u4f30\u7684\u91cd\u8981\u6027"}}
{"id": "2509.08753", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08753", "abs": "https://arxiv.org/abs/2509.08753", "authors": ["Neil Zeghidour", "Eugene Kharitonov", "Manu Orsini", "V\u00e1clav Volhejn", "Gabriel de Marmiesse", "Edouard Grave", "Patrick P\u00e9rez", "Laurent Mazar\u00e9", "Alexandre D\u00e9fossez"], "title": "Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling", "comment": null, "summary": "We introduce Delayed Streams Modeling (DSM), a flexible formulation for\nstreaming, multimodal sequence-to-sequence learning. Sequence-to-sequence\ngeneration is often cast in an offline manner, where the model consumes the\ncomplete input sequence before generating the first output timestep.\nAlternatively, streaming sequence-to-sequence rely on learning a policy for\nchoosing when to advance on the input stream, or write to the output stream.\nDSM instead models already time-aligned streams with a decoder-only language\nmodel. By moving the alignment to a pre-processing step,and introducing\nappropriate delays between streams, DSM provides streaming inference of\narbitrary output sequences, from any input combination, making it applicable to\nmany sequence-to-sequence problems. In particular, given text and audio\nstreams, automatic speech recognition (ASR) corresponds to the text stream\nbeing delayed, while the opposite gives a text-to-speech (TTS) model. We\nperform extensive experiments for these two major sequence-to-sequence tasks,\nshowing that DSM provides state-of-the-art performance and latency while\nsupporting arbitrary long sequences, being even competitive with offline\nbaselines. Code, samples and demos are available at\nhttps://github.com/kyutai-labs/delayed-streams-modeling", "AI": {"tldr": "DSM\u662f\u4e00\u79cd\u6d41\u5f0f\u591a\u6a21\u6001\u5e8f\u5217\u5230\u5e8f\u5217\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5ef6\u8fdf\u5bf9\u9f50\u6d41\u5b9e\u73b0\u4efb\u610f\u8f93\u51fa\u5e8f\u5217\u7684\u6d41\u5f0f\u63a8\u7406\uff0c\u5728ASR\u548cTTS\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u4f20\u7edf\u5e8f\u5217\u5230\u5e8f\u5217\u751f\u6210\u8981\u4e48\u662f\u79bb\u7ebf\u65b9\u5f0f\uff08\u5b8c\u6574\u8f93\u5165\u540e\u624d\u8f93\u51fa\uff09\uff0c\u8981\u4e48\u9700\u8981\u5b66\u4e60\u590d\u6742\u7684\u6d41\u63a7\u5236\u7b56\u7565\uff0cDSM\u65e8\u5728\u63d0\u4f9b\u66f4\u7075\u6d3b\u9ad8\u6548\u7684\u6d41\u5f0f\u5904\u7406\u65b9\u6848", "method": "\u4f7f\u7528\u4ec5\u89e3\u7801\u5668\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u5904\u7406\u6b65\u9aa4\u5bf9\u9f50\u6d41\u5e76\u5f15\u5165\u9002\u5f53\u7684\u5ef6\u8fdf\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5904\u7406\u4efb\u610f\u7ec4\u5408\u7684\u8f93\u5165\u8f93\u51fa\u6d41", "result": "\u5728\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u548c\u6587\u672c\u8f6c\u8bed\u97f3\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u5ef6\u8fdf\uff0c\u652f\u6301\u4efb\u610f\u957f\u5e8f\u5217\uff0c\u751a\u81f3\u80fd\u4e0e\u79bb\u7ebf\u57fa\u7ebf\u7ade\u4e89", "conclusion": "DSM\u4e3a\u591a\u6a21\u6001\u6d41\u5f0f\u5e8f\u5217\u5230\u5e8f\u5217\u5b66\u4e60\u63d0\u4f9b\u4e86\u7b80\u5355\u800c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b"}}
{"id": "2509.08778", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08778", "abs": "https://arxiv.org/abs/2509.08778", "authors": ["Minyeong Choe", "Haehyun Cho", "Changho Seo", "Hyunil Kim"], "title": "Do All Autoregressive Transformers Remember Facts the Same Way? A Cross-Architecture Analysis of Recall Mechanisms", "comment": "Accepted at EMNLP 2025", "summary": "Understanding how Transformer-based language models store and retrieve\nfactual associations is critical for improving interpretability and enabling\ntargeted model editing. Prior work, primarily on GPT-style models, has\nidentified MLP modules in early layers as key contributors to factual recall.\nHowever, it remains unclear whether these findings generalize across different\nautoregressive architectures. To address this, we conduct a comprehensive\nevaluation of factual recall across several models -- including GPT, LLaMA,\nQwen, and DeepSeek -- analyzing where and how factual information is encoded\nand accessed. Consequently, we find that Qwen-based models behave differently\nfrom previous patterns: attention modules in the earliest layers contribute\nmore to factual recall than MLP modules. Our findings suggest that even within\nthe autoregressive Transformer family, architectural variations can lead to\nfundamentally different mechanisms of factual recall.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u81ea\u56de\u5f52Transformer\u67b6\u6784\u5728\u4e8b\u5b9e\u56de\u5fc6\u673a\u5236\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u7279\u522b\u662fQwen\u6a21\u578b\u5728\u65e9\u671f\u6ce8\u610f\u529b\u5c42\u800c\u975eMLP\u5c42\u8fdb\u884c\u4e8b\u5b9e\u56de\u5fc6\uff0c\u8fd9\u4e0eGPT\u7b49\u6a21\u578b\u7684\u6a21\u5f0f\u4e0d\u540c", "motivation": "\u7406\u89e3Transformer\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5b58\u50a8\u548c\u68c0\u7d22\u4e8b\u5b9e\u5173\u8054\u5bf9\u4e8e\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u5b9e\u73b0\u9488\u5bf9\u6027\u6a21\u578b\u7f16\u8f91\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5148\u524d\u7814\u7a76\u4e3b\u8981\u57fa\u4e8eGPT\u98ce\u683c\u6a21\u578b\uff0c\u4e0d\u6e05\u695a\u8fd9\u4e9b\u53d1\u73b0\u662f\u5426\u9002\u7528\u4e8e\u4e0d\u540c\u81ea\u56de\u5f52\u67b6\u6784", "method": "\u5bf9\u591a\u4e2a\u6a21\u578b\uff08\u5305\u62ecGPT\u3001LLaMA\u3001Qwen\u548cDeepSeek\uff09\u8fdb\u884c\u4e8b\u5b9e\u56de\u5fc6\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u5206\u6790\u4e8b\u5b9e\u4fe1\u606f\u5728\u4f55\u5904\u4ee5\u53ca\u5982\u4f55\u88ab\u7f16\u7801\u548c\u8bbf\u95ee", "result": "\u53d1\u73b0\u57fa\u4e8eQwen\u7684\u6a21\u578b\u8868\u73b0\u51fa\u4e0e\u5148\u524d\u6a21\u5f0f\u4e0d\u540c\u7684\u884c\u4e3a\uff1a\u6700\u65e9\u5c42\u7684\u6ce8\u610f\u529b\u6a21\u5757\u6bd4MLP\u6a21\u5757\u5bf9\u4e8b\u5b9e\u56de\u5fc6\u7684\u8d21\u732e\u66f4\u5927", "conclusion": "\u5373\u4f7f\u5728\u81ea\u56de\u5f52Transformer\u5bb6\u65cf\u5185\u90e8\uff0c\u67b6\u6784\u53d8\u5316\u4e5f\u53ef\u80fd\u5bfc\u81f4\u6839\u672c\u4e0d\u540c\u7684\u4e8b\u5b9e\u56de\u5fc6\u673a\u5236"}}
{"id": "2509.08809", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08809", "abs": "https://arxiv.org/abs/2509.08809", "authors": ["Cheng Chen", "Haiyan Yin", "Ivor Tsang"], "title": "Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation Through Unsupervised Consistency Signals", "comment": "11 pages, 10 figures", "summary": "Large Language Models (LLMs), when paired with prompt-based tasks, have\nsignificantly reduced data annotation costs and reliance on human annotators.\nHowever, evaluating the quality of their annotations remains challenging in\ndynamic, unsupervised environments where oracle feedback is scarce and\nconventional methods fail. To address this challenge, we propose a novel\nagentic annotation paradigm, where a student model collaborates with a noisy\nteacher (the LLM) to assess and refine annotation quality without relying on\noracle feedback. The student model, acting as an unsupervised feedback\nmechanism, employs a user preference-based majority voting strategy to evaluate\nthe consistency of the LLM outputs. To systematically measure the reliability\nof LLM-generated annotations, we introduce the Consistent and Inconsistent\n(CAI) Ratio, a novel unsupervised evaluation metric. The CAI Ratio not only\nquantifies the annotation quality of the noisy teacher under limited user\npreferences but also plays a critical role in model selection, enabling the\nidentification of robust LLMs in dynamic, unsupervised environments. Applied to\nten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates a\nstrong positive correlation with LLM accuracy, establishing it as an essential\ntool for unsupervised evaluation and model selection in real-world settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763\u6ce8\u91ca\u8bc4\u4f30\u65b9\u6cd5CAI\u6bd4\u7387\uff0c\u901a\u8fc7\u5b66\u751f\u6a21\u578b\u4e0e\u566a\u58f0\u6559\u5e08(LLM)\u534f\u4f5c\uff0c\u5728\u65e0\u76d1\u7763\u73af\u5883\u4e2d\u8bc4\u4f30\u548c\u63d0\u5347LLM\u6ce8\u91ca\u8d28\u91cf", "motivation": "\u5728\u52a8\u6001\u65e0\u76d1\u7763\u73af\u5883\u4e2d\uff0c\u7f3a\u4e4f\u771f\u5b9e\u6807\u7b7e\u53cd\u9988\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u8bc4\u4f30LLM\u6ce8\u91ca\u8d28\u91cf\uff0c\u9700\u8981\u65b0\u7684\u65e0\u76d1\u7763\u8bc4\u4f30\u65b9\u6cd5", "method": "\u91c7\u7528\u5b66\u751f\u6a21\u578b\u4e0e\u566a\u58f0\u6559\u5e08(LLM)\u534f\u4f5c\u6a21\u5f0f\uff0c\u5b66\u751f\u6a21\u578b\u4f5c\u4e3a\u65e0\u76d1\u7763\u53cd\u9988\u673a\u5236\uff0c\u4f7f\u7528\u7528\u6237\u504f\u597d\u591a\u6570\u6295\u7968\u7b56\u7565\u8bc4\u4f30LLM\u8f93\u51fa\u4e00\u81f4\u6027\uff0c\u63d0\u51faCAI\u6bd4\u7387\u6307\u6807", "result": "\u572810\u4e2a\u5f00\u653e\u57dfNLP\u6570\u636e\u96c6\u548c4\u4e2aLLM\u4e0a\u5e94\u7528\uff0cCAI\u6bd4\u7387\u4e0eLLM\u51c6\u786e\u7387\u5448\u5f3a\u6b63\u76f8\u5173\u5173\u7cfb\uff0c\u8bc1\u660e\u5176\u5728\u65e0\u76d1\u7763\u8bc4\u4f30\u548c\u6a21\u578b\u9009\u62e9\u4e2d\u7684\u6709\u6548\u6027", "conclusion": "CAI\u6bd4\u7387\u662f\u4e00\u79cd\u53ef\u9760\u7684\u65e0\u76d1\u7763\u8bc4\u4f30\u6307\u6807\uff0c\u80fd\u591f\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u6709\u6548\u8bc4\u4f30LLM\u6ce8\u91ca\u8d28\u91cf\u548c\u9009\u62e9\u7a33\u5065\u7684\u6a21\u578b"}}
{"id": "2509.08812", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6; H.3.3"], "pdf": "https://arxiv.org/pdf/2509.08812", "abs": "https://arxiv.org/abs/2509.08812", "authors": ["Hailay Kidu Teklehaymanot", "Dren Fazlija", "Wolfgang Nejdl"], "title": "MoVoC: Morphology-Aware Subword Construction for Geez Script Languages", "comment": "This submission is approximately 10 pages in length and includes 1\n  figure and 6 tables", "summary": "Subword-based tokenization methods often fail to preserve morphological\nboundaries, a limitation especially pronounced in low-resource, morphologically\ncomplex languages such as those written in the Geez script. To address this, we\npresent MoVoC (Morpheme-aware Subword Vocabulary Construction) and train\nMoVoC-Tok, a tokenizer that integrates supervised morphological analysis into\nthe subword vocabulary. This hybrid segmentation approach combines\nmorpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological\nintegrity while maintaining lexical meaning. To tackle resource scarcity, we\ncurate and release manually annotated morpheme data for four Geez script\nlanguages and a morpheme-aware vocabulary for two of them. While the proposed\ntokenization method does not lead to significant gains in automatic translation\nquality, we observe consistent improvements in intrinsic metrics, MorphoScore,\nand Boundary Precision, highlighting the value of morphology-aware segmentation\nin enhancing linguistic fidelity and token efficiency. Our morpheme-annotated\ndatasets and tokenizer will be publicly available to support further research\nin low-resource, morphologically rich languages. Our code and data are\navailable on GitHub: https://github.com/hailaykidu/MoVoC", "AI": {"tldr": "\u901a\u8fc7\u878d\u5408\u8d28\u57fa\u5206\u6790\u548cBPE\u7684MoVoC-Tok\u5207\u5206\u65b9\u6cd5\uff0c\u4e3a\u7f00\u57fa\u7c73\u5199\u4f5c\u8bed\u8a00\u63d0\u4f9b\u4fdd\u6301\u8bed\u6cd5\u8fb9\u754c\u7684\u5207\u5206\uff0c\u5728\u5185\u90e8\u8bc4\u6d4b\u6307\u6807\u4e0a\u663e\u793a\u4e00\u81f4\u6539\u5584", "motivation": "\u89e3\u51b3\u5b50\u8bcd\u5207\u5206\u65b9\u6cd5\u5728\u8d28\u6e90\u7a00\u7f3a\u3001\u8bed\u6cd5\u590d\u6742\u7684\u7f00\u57fa\u7c73\u5199\u4f5c\u8bed\u8a00\u4e2d\u65e0\u6cd5\u4fdd\u6301\u8bed\u6cd5\u8fb9\u754c\u7684\u95ee\u9898", "method": "\u63d0\u51faMoVoC\u65b9\u6cd5\uff0c\u8bad\u7ec3MoVoC-Tok\u5207\u5206\u5668\uff0c\u7ed3\u5408\u76d1\u7763\u8bed\u6cd5\u5206\u6790\u548cBPE\u5207\u5206\uff0c\u4f7f\u7528\u878d\u5408\u8bed\u7d20\u57fa\u548cBPE\u6807\u8bb0\u6765\u4fdd\u6301\u8bed\u6cd5\u5b8c\u6574\u6027", "result": "\u867d\u7136\u81ea\u52a8\u7ffb\u8bd1\u8d28\u91cf\u6ca1\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5728MorphoScore\u548cBoundary Precision\u7b49\u5185\u90e8\u8bc4\u6d4b\u6307\u6807\u4e0a\u53d6\u5f97\u4e00\u81f4\u6539\u5584\uff0c\u5e76\u53d1\u5e03\u4e86\u56db\u79cd\u7f00\u57fa\u7c73\u8bed\u8a00\u7684\u624b\u52a8\u6ce8\u91ca\u8bed\u7d20\u6570\u636e\u96c6", "conclusion": "\u8bed\u6cd5\u610f\u8bc6\u5207\u5206\u5728\u63d0\u9ad8\u8bed\u8a00\u4fe1\u5b9e\u6027\u548c\u6807\u8bb0\u6548\u7387\u65b9\u9762\u5177\u6709\u4ef7\u503c\uff0c\u5e76\u4e3a\u8d28\u6e90\u7a00\u7f3a\u8bed\u8a00\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90"}}
{"id": "2509.08824", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08824", "abs": "https://arxiv.org/abs/2509.08824", "authors": ["Thales Sales Almeida", "Rodrigo Nogueira", "Helio Pedrini"], "title": "Building High-Quality Datasets for Portuguese LLMs: From Common Crawl Snapshots to Industrial-Grade Corpora", "comment": null, "summary": "The performance of large language models (LLMs) is deeply influenced by the\nquality and composition of their training data. While much of the existing work\nhas centered on English, there remains a gap in understanding how to construct\neffective training corpora for other languages. We explore scalable methods for\nbuilding web-based corpora for LLMs. We apply them to build a new 120B token\ncorpus in Portuguese that achieves competitive results to an industrial-grade\ncorpus. Using a continual pretraining setup, we study how different data\nselection and preprocessing strategies affect LLM performance when\ntransitioning a model originally trained in English to another language. Our\nfindings demonstrate the value of language-specific filtering pipelines,\nincluding classifiers for education, science, technology, engineering, and\nmathematics (STEM), as well as toxic content. We show that adapting a model to\nthe target language leads to performance improvements, reinforcing the\nimportance of high-quality, language-specific data. While our case study\nfocuses on Portuguese, our methods are applicable to other languages, offering\ninsights for multilingual LLM development.", "AI": {"tldr": "\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u7f51\u7edc\u6570\u636e\u6536\u96c6\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86120B\u6807\u8bb0\u7684\u8461\u8404\u7259\u8bed\u8bed\u6599\u5e93\uff0c\u5e76\u7814\u7a76\u4e86\u4ece\u82f1\u8bed\u5230\u5176\u4ed6\u8bed\u8a00\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u7b56\u7565\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u867d\u7136\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u82f1\u8bed\uff0c\u4f46\u5bf9\u5982\u4f55\u6784\u5efa\u5176\u4ed6\u8bed\u8a00\u7684\u9ad8\u6548\u8bad\u7ec3\u8bed\u6599\u5e93\u4ecd\u7136\u5b58\u5728\u77e5\u8bc6\u7a7a\u767d\uff0c\u9700\u8981\u7814\u7a76\u53ef\u6269\u5c55\u7684\u591a\u8bed\u8a00\u8bed\u6599\u6784\u5efa\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6301\u7eed\u9884\u8bad\u7ec3\u8bbe\u7f6e\uff0c\u7814\u7a76\u4e0d\u540c\u6570\u636e\u9009\u62e9\u548c\u524d\u5904\u7406\u7b56\u7565\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5305\u62ec\u8bed\u8a00\u7279\u5b9a\u7684\u8fc7\u6ee4\u6d41\u6c34\u7ebf\u3001STEM\u5206\u7c7b\u5668\u548c\u6709\u5bb3\u5185\u5bb9\u8bc6\u522b\u7b49\u65b9\u6cd5\u3002", "result": "\u6784\u5efa\u7684120B\u6807\u8bb0\u8461\u8404\u7259\u8bed\u8bed\u6599\u5e93\u8fbe\u5230\u4e86\u4e0e\u5546\u4e1a\u7ea7\u8bed\u6599\u5e93\u7ade\u4e89\u7684\u6548\u679c\uff0c\u8bc1\u660e\u4e86\u8bed\u8a00\u7279\u5b9a\u8fc7\u6ee4\u7ba1\u9053\u7684\u4ef7\u503c\uff0c\u6a21\u578b\u9002\u5e94\u76ee\u6807\u8bed\u8a00\u540e\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u7684\u8bed\u8a00\u7279\u5b9a\u6570\u636e\u5bf9\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u867d\u7136\u6848\u4f8b\u7814\u7a76\u4ee5\u8461\u8404\u7259\u8bed\u4e3a\u4e3b\uff0c\u4f46\u65b9\u6cd5\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u8bed\u8a00\uff0c\u4e3a\u591a\u8bed\u8a00LLM\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.08825", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08825", "abs": "https://arxiv.org/abs/2509.08825", "authors": ["Joachim Baumann", "Paul R\u00f6ttger", "Aleksandra Urman", "Albert Wendsj\u00f6", "Flor Miriam Plaza-del-Arco", "Johannes B. Gruber", "Dirk Hovy"], "title": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation", "comment": null, "summary": "Large language models (LLMs) are rapidly transforming social science research\nby enabling the automation of labor-intensive tasks like data annotation and\ntext analysis. However, LLM outputs vary significantly depending on the\nimplementation choices made by researchers (e.g., model selection, prompting\nstrategy, or temperature settings). Such variation can introduce systematic\nbiases and random errors, which propagate to downstream analyses and cause Type\nI, Type II, Type S, or Type M errors. We call this LLM hacking.\n  We quantify the risk of LLM hacking by replicating 37 data annotation tasks\nfrom 21 published social science research studies with 18 different models.\nAnalyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure\nhow plausible researcher choices affect statistical conclusions. We find\nincorrect conclusions based on LLM-annotated data in approximately one in three\nhypotheses for state-of-the-art models, and in half the hypotheses for small\nlanguage models. While our findings show that higher task performance and\nbetter general model capabilities reduce LLM hacking risk, even highly accurate\nmodels do not completely eliminate it. The risk of LLM hacking decreases as\neffect sizes increase, indicating the need for more rigorous verification of\nfindings near significance thresholds. Our extensive analysis of LLM hacking\nmitigation techniques emphasizes the importance of human annotations in\nreducing false positive findings and improving model selection. Surprisingly,\ncommon regression estimator correction techniques are largely ineffective in\nreducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.\n  Beyond accidental errors, we find that intentional LLM hacking is\nunacceptably simple. With few LLMs and just a handful of prompt paraphrases,\nanything can be presented as statistically significant.", "AI": {"tldr": "LLM\u5728\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u5b58\u5728\"LLM hacking\"\u98ce\u9669\uff0c\u7814\u7a76\u4eba\u5458\u7684\u4e0d\u540c\u5b9e\u73b0\u9009\u62e9\u4f1a\u5bfc\u81f4\u7cfb\u7edf\u6027\u504f\u5dee\u548c\u968f\u673a\u9519\u8bef\uff0c\u7ea61/3\u7684\u5047\u8bbe\u4f1a\u5f97\u51fa\u9519\u8bef\u7ed3\u8bba\uff0c\u5373\u4f7f\u9ad8\u6027\u80fd\u6a21\u578b\u4e5f\u65e0\u6cd5\u5b8c\u5168\u6d88\u9664\u98ce\u9669\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6b63\u5728\u6539\u53d8\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\uff0c\u4f46\u7814\u7a76\u4eba\u5458\u5728\u6a21\u578b\u9009\u62e9\u3001\u63d0\u793a\u7b56\u7565\u7b49\u5b9e\u73b0\u9009\u62e9\u4e0a\u7684\u5dee\u5f02\u4f1a\u5f15\u5165\u7cfb\u7edf\u504f\u5dee\u548c\u968f\u673a\u9519\u8bef\uff0c\u5f71\u54cd\u4e0b\u6e38\u5206\u6790\u7684\u7edf\u8ba1\u7ed3\u8bba\u53ef\u9760\u6027\u3002", "method": "\u590d\u523637\u4e2a\u6570\u636e\u6807\u6ce8\u4efb\u52a1\uff0c\u4f7f\u752818\u4e2a\u4e0d\u540c\u6a21\u578b\u751f\u62101300\u4e07\u6807\u7b7e\uff0c\u6d4b\u8bd52361\u4e2a\u73b0\u5b9e\u5047\u8bbe\uff0c\u5206\u6790\u7814\u7a76\u4eba\u5458\u9009\u62e9\u5bf9\u7edf\u8ba1\u7ed3\u8bba\u7684\u5f71\u54cd\u3002", "result": "\u6700\u5148\u8fdb\u6a21\u578b\u7ea61/3\u5047\u8bbe\u5f97\u51fa\u9519\u8bef\u7ed3\u8bba\uff0c\u5c0f\u8bed\u8a00\u6a21\u578b\u4e00\u534a\u5047\u8bbe\u9519\u8bef\uff1b\u9ad8\u98ce\u9669\u51fa\u73b0\u5728\u6548\u5e94\u503c\u8f83\u5c0f\u7684\u60c5\u51b5\uff1b\u4eba\u7c7b\u6807\u6ce8\u80fd\u51cf\u5c11\u5047\u9633\u6027\u4f46\u5e38\u89c1\u6821\u6b63\u6280\u672f\u6548\u679c\u6709\u9650\uff1b\u6545\u610f\u64cd\u7eb5\u6781\u5176\u7b80\u5355\u3002", "conclusion": "LLM hacking\u98ce\u9669\u771f\u5b9e\u5b58\u5728\uff0c\u9700\u8981\u66f4\u4e25\u683c\u7684\u9a8c\u8bc1\u673a\u5236\uff0c\u7279\u522b\u662f\u5bf9\u63a5\u8fd1\u663e\u8457\u6027\u9608\u503c\u7684\u7814\u7a76\u53d1\u73b0\uff1b\u4eba\u7c7b\u6807\u6ce8\u5728\u51cf\u5c11\u9519\u8bef\u53d1\u73b0\u65b9\u9762\u81f3\u5173\u91cd\u8981\uff1b\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u98ce\u9669\u7f13\u89e3\u6280\u672f\u3002"}}
{"id": "2509.08827", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08827", "abs": "https://arxiv.org/abs/2509.08827", "authors": ["Kaiyan Zhang", "Yuxin Zuo", "Bingxiang He", "Youbang Sun", "Runze Liu", "Che Jiang", "Yuchen Fan", "Kai Tian", "Guoli Jia", "Pengfei Li", "Yu Fu", "Xingtai Lv", "Yuchen Zhang", "Sihang Zeng", "Shang Qu", "Haozhan Li", "Shijie Wang", "Yuru Wang", "Xinwei Long", "Fangfu Liu", "Xiang Xu", "Jiaze Ma", "Xuekai Zhu", "Ermo Hua", "Yihao Liu", "Zonglin Li", "Huayu Chen", "Xiaoye Qu", "Yafu Li", "Weize Chen", "Zhenzhao Yuan", "Junqi Gao", "Dong Li", "Zhiyuan Ma", "Ganqu Cui", "Zhiyuan Liu", "Biqing Qi", "Ning Ding", "Bowen Zhou"], "title": "A Survey of Reinforcement Learning for Large Reasoning Models", "comment": null, "summary": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7efc\u8ff0\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u5206\u6790\u4e86RL\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u7b49\u590d\u6742\u903b\u8f91\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63a2\u8ba8\u4e86\u8be5\u9886\u57df\u9762\u4e34\u7684\u8ba1\u7b97\u8d44\u6e90\u3001\u7b97\u6cd5\u8bbe\u8ba1\u7b49\u6311\u6218\u3002", "motivation": "\u968f\u7740RL\u5728\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6210\u529f\uff0c\u8be5\u9886\u57df\u9762\u4e34\u8ba1\u7b97\u8d44\u6e90\u3001\u7b97\u6cd5\u8bbe\u8ba1\u3001\u8bad\u7ec3\u6570\u636e\u548c\u57fa\u7840\u8bbe\u65bd\u7b49\u65b9\u9762\u7684\u57fa\u7840\u6027\u6311\u6218\uff0c\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u53d1\u5c55\u8f68\u8ff9\u5e76\u63a2\u7d22\u589e\u5f3aRL\u53ef\u6269\u5c55\u6027\u7684\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u8c03\u7814\u5c06RL\u5e94\u7528\u4e8eLLM\u548cLRM\u63a8\u7406\u80fd\u529b\u7684\u7814\u7a76\u5de5\u4f5c\uff0c\u5305\u62ec\u57fa\u7840\u7ec4\u4ef6\u3001\u6838\u5fc3\u95ee\u9898\u3001\u8bad\u7ec3\u8d44\u6e90\u548c\u4e0b\u6e38\u5e94\u7528\u7b49\u65b9\u9762\u7684\u5206\u6790\u3002", "result": "\u8bc6\u522b\u4e86RL for LRMs\u9886\u57df\u7684\u5173\u952e\u53d1\u5c55\u8d8b\u52bf\u548c\u6280\u672f\u6311\u6218\uff0c\u4e3a\u8be5\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\u6307\u660e\u4e86\u672a\u6765\u7684\u673a\u9047\u548c\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8fd9\u7bc7\u7efc\u8ff0\u65e8\u5728\u4fc3\u8fdbRL\u5728\u66f4\u5e7f\u6cdb\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u672a\u6765\u7814\u7a76\uff0c\u4e3a\u901a\u5411\u4eba\u5de5\u8d85\u7ea7\u667a\u80fd(ASI)\u7684\u53ef\u6269\u5c55RL\u53d1\u5c55\u63d0\u4f9b\u6307\u5bfc\u3002"}}
