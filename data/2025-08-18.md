<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.SD](#cs.SD) [Total: 6]
- [cs.MM](#cs.MM) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation](https://arxiv.org/abs/2508.10904)
*Jie Lei,Ruofan Jia,J. Andrew Zhang,Hao Zhang*

Main category: cs.CL

TL;DR: A2HCoder是一个基于大语言模型的算法到硬件描述语言（HDL）的自动转换工具，旨在解决算法设计与硬件实现之间的鸿沟，通过分层框架提高代码生成的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 无线通信系统对超低延迟和功耗的严格要求增加了对高效算法到硬件部署的需求，但算法设计与硬件实现之间存在显著差距，传统方法需要大量领域知识和手动开发。

Method: A2HCoder采用分层框架，水平方向将复杂算法分解为模块化功能块，垂直方向通过逐步细粒度翻译，结合外部工具链（如MATLAB和Vitis HLS）进行调试和电路级综合。

Result: 在5G无线通信领域的实际部署案例中验证了A2HCoder的实用性、可靠性和部署效率。

Conclusion: A2HCoder通过分层方法显著减少了LLM生成代码中的幻觉问题，确保了硬件级正确性，为算法到硬件的敏捷转换提供了可行方案。

Abstract: In wireless communication systems, stringent requirements such as ultra-low
latency and power consumption have significantly increased the demand for
efficient algorithm-to-hardware deployment. However, a persistent and
substantial gap remains between algorithm design and hardware implementation.
Bridging this gap traditionally requires extensive domain expertise and
time-consuming manual development, due to fundamental mismatches between
high-level programming languages like MATLAB and hardware description languages
(HDLs) such as Verilog-in terms of memory access patterns, data processing
manners, and datatype representations. To address this challenge, we propose
A2HCoder: a Hierarchical Algorithm-to-HDL Coding Agent, powered by large
language models (LLMs), designed to enable agile and reliable
algorithm-to-hardware translation. A2HCoder introduces a hierarchical framework
that enhances both robustness and interpretability while suppressing common
hallucination issues in LLM-generated code. In the horizontal dimension,
A2HCoder decomposes complex algorithms into modular functional blocks,
simplifying code generation and improving consistency. In the vertical
dimension, instead of relying on end-to-end generation, A2HCoder performs
step-by-step, fine-grained translation, leveraging external toolchains such as
MATLAB and Vitis HLS for debugging and circuit-level synthesis. This structured
process significantly mitigates hallucinations and ensures hardware-level
correctness. We validate A2HCoder through a real-world deployment case in the
5G wireless communication domain, demonstrating its practicality, reliability,
and deployment efficiency.

</details>


### [2] [PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins](https://arxiv.org/abs/2508.10906)
*Sihan Chen,John P. Lalor,Yi Yang,Ahmed Abbasi*

Main category: cs.CL

TL;DR: PersonaTwin是一个多层次的提示条件框架，通过整合人口统计、行为和心理测量数据构建自适应数字孪生，显著提升了用户模拟的逼真度和情感细腻度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在捕捉用户多维细微差异方面存在不足，因此需要一种更精准的用户建模方法。

Method: 提出PersonaTwin框架，结合多维度数据，并通过8,500多个个体的医疗数据集进行系统评估。

Result: 实验表明，PersonaTwin在模拟逼真度上与基准设置相当，且下游模型在预测和公平性指标上接近真实个体数据训练的结果。

Conclusion: PersonaTwin展示了LLM数字孪生方法在个性化用户建模和行为分析中的潜力。

Abstract: While large language models (LLMs) afford new possibilities for user modeling
and approximation of human behaviors, they often fail to capture the
multidimensional nuances of individual users. In this work, we introduce
PersonaTwin, a multi-tier prompt conditioning framework that builds adaptive
digital twins by integrating demographic, behavioral, and psychometric data.
Using a comprehensive data set in the healthcare context of more than 8,500
individuals, we systematically benchmark PersonaTwin against standard LLM
outputs, and our rigorous evaluation unites state-of-the-art text similarity
metrics with dedicated demographic parity assessments, ensuring that generated
responses remain accurate and unbiased. Experimental results show that our
framework produces simulation fidelity on par with oracle settings. Moreover,
downstream models trained on persona-twins approximate models trained on
individuals in terms of prediction and fairness metrics across both
GPT-4o-based and Llama-based models. Together, these findings underscore the
potential for LLM digital twin-based approaches in producing realistic and
emotionally nuanced user simulations, offering a powerful tool for personalized
digital user modeling and behavior analysis.

</details>


### [3] [gpt-oss-120b & gpt-oss-20b Model Card](https://arxiv.org/abs/2508.10925)
*OpenAI,:,Sandhini Agarwal,Lama Ahmad,Jason Ai,Sam Altman,Andy Applebaum,Edwin Arbus,Rahul K. Arora,Yu Bai,Bowen Baker,Haiming Bao,Boaz Barak,Ally Bennett,Tyler Bertao,Nivedita Brett,Eugene Brevdo,Greg Brockman,Sebastien Bubeck,Che Chang,Kai Chen,Mark Chen,Enoch Cheung,Aidan Clark,Dan Cook,Marat Dukhan,Casey Dvorak,Kevin Fives,Vlad Fomenko,Timur Garipov,Kristian Georgiev,Mia Glaese,Tarun Gogineni,Adam Goucher,Lukas Gross,Katia Gil Guzman,John Hallman,Jackie Hehir,Johannes Heidecke,Alec Helyar,Haitang Hu,Romain Huet,Jacob Huh,Saachi Jain,Zach Johnson,Chris Koch,Irina Kofman,Dominik Kundel,Jason Kwon,Volodymyr Kyrylov,Elaine Ya Le,Guillaume Leclerc,James Park Lennon,Scott Lessans,Mario Lezcano-Casado,Yuanzhi Li,Zhuohan Li,Ji Lin,Jordan Liss,Lily,Liu,Jiancheng Liu,Kevin Lu,Chris Lu,Zoran Martinovic,Lindsay McCallum,Josh McGrath,Scott McKinney,Aidan McLaughlin,Song Mei,Steve Mostovoy,Tong Mu,Gideon Myles,Alexander Neitz,Alex Nichol,Jakub Pachocki,Alex Paino,Dana Palmie,Ashley Pantuliano,Giambattista Parascandolo,Jongsoo Park,Leher Pathak,Carolina Paz,Ludovic Peran,Dmitry Pimenov,Michelle Pokrass,Elizabeth Proehl,Huida Qiu,Gaby Raila,Filippo Raso,Hongyu Ren,Kimmy Richardson,David Robinson,Bob Rotsted,Hadi Salman,Suvansh Sanjeev,Max Schwarzer,D. Sculley,Harshit Sikchi,Kendal Simon,Karan Singhal,Yang Song,Dane Stuckey,Zhiqing Sun,Philippe Tillet,Sam Toizer,Foivos Tsimpourlas,Nikhil Vyas,Eric Wallace,Xin Wang,Miles Wang,Olivia Watkins,Kevin Weil,Amy Wendling,Kevin Whinnery,Cedric Whitney,Hannah Wong,Lin Yang,Yu Yang,Michihiro Yasunaga,Kristen Ying,Wojciech Zaremba,Wenting Zhan,Cyril Zhang,Brian Zhang,Eddie Zhang,Shengjia Zhao*

Main category: cs.CL

TL;DR: 介绍了两个开源推理模型gpt-oss-120b和gpt-oss-20b，采用混合专家Transformer架构，通过大规模蒸馏和强化学习训练，具备强大的代理能力。


<details>
  <summary>Details</summary>
Motivation: 推动准确性和推理成本的前沿，同时提供开源模型以促进广泛使用和研究。

Method: 采用混合专家Transformer架构，结合大规模蒸馏和强化学习训练。

Result: 在数学、编程和安全等基准测试中表现优异。

Conclusion: 开源模型权重、推理实现、工具环境和分词器，以Apache 2.0许可证发布，促进广泛应用和研究。

Abstract: We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models
that push the frontier of accuracy and inference cost. The models use an
efficient mixture-of-expert transformer architecture and are trained using
large-scale distillation and reinforcement learning. We optimize the models to
have strong agentic capabilities (deep research browsing, python tool use, and
support for developer-provided functions), all while using a rendered chat
format that enables clear instruction following and role delineation. Both
models achieve strong results on benchmarks ranging from mathematics, coding,
and safety. We release the model weights, inference implementations, tool
environments, and tokenizers under an Apache 2.0 license to enable broad use
and further research.

</details>


### [4] [Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News](https://arxiv.org/abs/2508.10927)
*Jiaxin Pei,Soumya Vadlamannati,Liang-Kang Huang,Daniel Preotiuc-Pietro,Xinyu Hua*

Main category: cs.CL

TL;DR: 研究提出了一种从新闻文章中自动提取公司风险因素的计算框架，并比较了不同机器学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 识别公司风险对投资者和金融市场的健康至关重要。

Method: 构建了一个包含七个方面的风险分类框架，标注了744篇新闻文章，并测试了多种机器学习模型，包括零样本和小样本提示的LLMs以及微调的预训练语言模型。

Result: 微调的预训练语言模型在大多数风险因素上表现优于LLMs，分析27.7万篇新闻文章展示了其实际应用价值。

Conclusion: 从新闻中识别风险因素能为公司和行业运营提供深入见解。

Abstract: Identifying risks associated with a company is important to investors and the
well-being of the overall financial market. In this study, we build a
computational framework to automatically extract company risk factors from news
articles. Our newly proposed schema comprises seven distinct aspects, such as
supply chain, regulations, and competitions. We sample and annotate 744 news
articles and benchmark various machine learning models. While large language
models have achieved huge progress in various types of NLP tasks, our
experiment shows that zero-shot and few-shot prompting state-of-the-art LLMs
(e.g. LLaMA-2) can only achieve moderate to low performances in identifying
risk factors. And fine-tuned pre-trained language models are performing better
on most of the risk factors. Using this model, we analyze over 277K Bloomberg
news articles and demonstrate that identifying risk factors from news could
provide extensive insight into the operations of companies and industries.

</details>


### [5] [Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules](https://arxiv.org/abs/2508.10971)
*Nasim Shirvani-Mahdavi,Chengkai Li*

Main category: cs.CL

TL;DR: Rule2Text框架利用LLM为知识图谱中的逻辑规则生成自然语言解释，提升可解释性和可用性。通过实验验证，结合人类反馈和LLM评估，优化了模型性能。


<details>
  <summary>Details</summary>
Motivation: 知识图谱的逻辑规则复杂且难以解释，Rule2Text旨在通过自然语言解释提升其可访问性和实用性。

Method: 使用多种LLM和提示策略（如零样本、少样本、Chain-of-Thought）生成解释，并通过人类和LLM评估优化模型。

Result: 实验表明，微调后的模型（如Zephyr）在解释质量上显著提升，尤其在特定领域数据集上表现突出。

Conclusion: Rule2Text成功提升了知识图谱规则的可解释性，并通过开源代码和数据促进进一步研究。

Abstract: Knowledge graphs (KGs) can be enhanced through rule mining; however, the
resulting logical rules are often difficult for humans to interpret due to
their inherent complexity and the idiosyncratic labeling conventions of
individual KGs. This work presents Rule2Text, a comprehensive framework that
leverages large language models (LLMs) to generate natural language
explanations for mined logical rules, thereby improving KG accessibility and
usability. We conduct extensive experiments using multiple datasets, including
Freebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the
ogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically
evaluate several LLMs across a comprehensive range of prompting strategies,
including zero-shot, few-shot, variable type incorporation, and
Chain-of-Thought reasoning. To systematically assess models' performance, we
conduct a human evaluation of generated explanations on correctness and
clarity. To address evaluation scalability, we develop and validate an
LLM-as-a-judge framework that demonstrates strong agreement with human
evaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge,
and human-in-the-loop feedback, we construct high-quality ground truth
datasets, which we use to fine-tune the open-source Zephyr model. Our results
demonstrate significant improvements in explanation quality after fine-tuning,
with particularly strong gains in the domain-specific dataset. Additionally, we
integrate a type inference module to support KGs lacking explicit type
information. All code and data are publicly available at
https://github.com/idirlab/KGRule2NL.

</details>


### [6] [Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling](https://arxiv.org/abs/2508.10995)
*Tejomay Kishor Padole,Suyash P Awate,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: MDMs是一种新型的非自回归生成模型，通过验证器辅助的推理时间扩展方法提升生成质量，实验证明其在文本风格转换任务中优于自回归模型。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过验证器辅助的推理时间扩展方法提升MDMs的生成质量，并验证其在文本生成任务中的优越性。

Method: 提出一种基于验证器的推理时间扩展方法，结合预训练嵌入模型，优化MDMs的生成过程。

Result: 实验表明，该方法显著提升了生成质量，并在文本风格转换任务中优于自回归模型。

Conclusion: MDMs结合验证器辅助方法是一种高效且高质量的文本生成框架，具有广泛应用潜力。

Abstract: Masked diffusion language models (MDMs) have recently gained traction as a
viable generative framework for natural language. This can be attributed to its
scalability and ease of training compared to other diffusion model paradigms
for discrete data, establishing itself as the state-of-the-art
non-autoregressive generator for discrete data. Diffusion models, in general,
have shown excellent ability to improve the generation quality by leveraging
inference-time scaling either by increasing the number of denoising steps or by
using external verifiers on top of the outputs of each step to guide the
generation. In this work, we propose a verifier-based inference-time scaling
method that aids in finding a better candidate generation during the denoising
process of the MDM. Our experiments demonstrate the application of MDMs for
standard text-style transfer tasks and establish MDMs as a better alternative
to autoregressive language models. Additionally, we show that a simple
soft-value-based verifier setup for MDMs using off-the-shelf pre-trained
embedding models leads to significant gains in generation quality even when
used on top of typical classifier-free guidance setups in the existing
literature.

</details>


### [7] [Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation](https://arxiv.org/abs/2508.11189)
*Chenyang Le,Yinfeng Xia,Huiyan Li,Manhong Wang,Yutao Sun,Xingyang Ma,Yanmin Qian*

Main category: cs.CL

TL;DR: 提出了一种创新的寄生双尺度方法，结合增强的推测采样与模型压缩和知识蒸馏技术，显著提升了多语言语音翻译模型的推理效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有的统一多语言语音翻译模型参数量大，难以在本地部署中平衡推理效率与性能。

Method: 基于Whisper Medium模型，提出寄生双尺度方法（KVSPN模块），结合推测采样、模型压缩和知识蒸馏。

Result: 在六种流行语言上实现SOTA性能，推理速度提升40%，无BLEU分数损失；结合蒸馏方法，速度提升2.6倍。

Conclusion: KVSPN模块显著提升了多语言语音翻译的效率和性能，适用于本地部署。

Abstract: Recent advancements in speech-to-text translation have led to the development
of multilingual models capable of handling multiple language pairs
simultaneously. However, these unified models often suffer from large parameter
sizes, making it challenging to balance inference efficiency and performance,
particularly in local deployment scenarios. We propose an innovative Parasitic
Dual-Scale Approach, which combines an enhanced speculative sampling method
with model compression and knowledge distillation techniques. Building on the
Whisper Medium model, we enhance it for multilingual speech translation into
whisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art
(SOTA) performance across six popular languages with improved inference
efficiency. KVSPN enables a 40\% speedup with no BLEU score degradation.
Combined with distillation methods, it represents a 2.6$\times$ speedup over
the original Whisper Medium with superior performance.

</details>


### [8] [SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth](https://arxiv.org/abs/2508.11009)
*Wenpeng Xing,Lanyi Wei,Haixiao Hu,Rongchang Li,Mohan Li,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: 论文提出SproutBench，一个针对儿童和青少年的大型语言模型（LLM）安全评估套件，填补了现有安全框架的不足。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全框架主要针对成人，忽视了儿童和青少年的独特发展脆弱性，需重新评估。

Method: 引入SproutBench，包含1,283个基于发展的对抗性提示，评估47种LLM的安全性。

Result: 发现显著的安全漏洞，揭示了安全性与风险预防之间的相关性，以及交互性与年龄适宜性的负相关。

Conclusion: 研究结果为儿童为中心的AI设计和部署提供了实用指南。

Abstract: The rapid proliferation of large language models (LLMs) in applications
targeting children and adolescents necessitates a fundamental reassessment of
prevailing AI safety frameworks, which are largely tailored to adult users and
neglect the distinct developmental vulnerabilities of minors. This paper
highlights key deficiencies in existing LLM safety benchmarks, including their
inadequate coverage of age-specific cognitive, emotional, and social risks
spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence
(13--18). To bridge these gaps, we introduce SproutBench, an innovative
evaluation suite comprising 1,283 developmentally grounded adversarial prompts
designed to probe risks such as emotional dependency, privacy violations, and
imitation of hazardous behaviors. Through rigorous empirical evaluation of 47
diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by
robust inter-dimensional correlations (e.g., between Safety and Risk
Prevention) and a notable inverse relationship between Interactivity and Age
Appropriateness. These insights yield practical guidelines for advancing
child-centric AI design and deployment.

</details>


### [9] [Representing Speech Through Autoregressive Prediction of Cochlear Tokens](https://arxiv.org/abs/2508.11598)
*Greta Tuckute,Klemen Kotar,Evelina Fedorenko,Daniel L. K. Yamins*

Main category: cs.CL

TL;DR: AuriStream是一个受生物启发的两阶段语音编码模型，模拟人类听觉处理层次结构，表现优异。


<details>
  <summary>Details</summary>
Motivation: 开发更接近人类听觉处理的语音表示学习模型，以高效处理多种语音任务。

Method: 第一阶段将原始音频转换为基于耳蜗的时频表示，提取离散的耳蜗标记；第二阶段对标记应用自回归序列模型。

Result: AuriStream在SUPERB语音任务中表现优异，学习到有意义的音素和词汇表示，并能生成可解码的音频延续。

Conclusion: AuriStream为语音表示学习提供了一个高效的两阶段框架，推动了更接近人类听觉的模型发展。

Abstract: We introduce AuriStream, a biologically inspired model for encoding speech
via a two-stage framework inspired by the human auditory processing hierarchy.
The first stage transforms raw audio into a time-frequency representation based
on the human cochlea, from which we extract discrete \textbf{cochlear tokens}.
The second stage applies an autoregressive sequence model over the cochlear
tokens. AuriStream learns meaningful phoneme and word representations, and
state-of-the-art lexical semantics. AuriStream shows competitive performance on
diverse downstream SUPERB speech tasks. Complementing AuriStream's strong
representational capabilities, it generates continuations of audio which can be
visualized in a spectrogram space and decoded back into audio, providing
insights into the model's predictions. In summary, we present a two-stage
framework for speech representation learning to advance the development of more
human-like models that efficiently handle a range of speech-based tasks.

</details>


### [10] [Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics](https://arxiv.org/abs/2508.11017)
*Carter Blum,Katja Filipova,Ann Yuan,Asma Ghandeharioun,Julian Zimmert,Fred Zhang,Jessica Hoffmann,Tal Linzen,Martin Wattenberg,Lucas Dixon,Mor Geva*

Main category: cs.CL

TL;DR: 研究探讨了大语言模型（LLM）在跨语言知识迁移中的问题，通过训练小型Transformer模型，揭示了跨语言事实表示的统一性对迁移的重要性，并提出改进方法。


<details>
  <summary>Details</summary>
Motivation: LLM在跨语言知识迁移中表现不佳，常产生幻觉，研究旨在揭示其成因并提出解决方案。

Method: 使用合成多语言数据集训练小型Transformer模型，分析事实表示的统一性及其影响因素。

Result: 发现事实表示的统一性对跨语言迁移至关重要，且统一程度受语言与事实间互信息及语言提取难度影响。

Conclusion: 研究通过控制实验揭示了预训练动态，为改进LLM的跨语言迁移提供了新方向。

Abstract: Large language models (LLMs) struggle with cross-lingual knowledge transfer:
they hallucinate when asked in one language about facts expressed in a
different language during training. This work introduces a controlled setting
to study the causes and dynamics of this phenomenon by training small
Transformer models from scratch on synthetic multilingual datasets. We identify
a learning phase wherein a model develops either separate or unified
representations of the same facts across languages, and show that unification
is essential for cross-lingual transfer. We also show that the degree of
unification depends on mutual information between facts and training data
language, and on how easy it is to extract that language. Based on these
insights, we develop methods to modulate the level of cross-lingual transfer by
manipulating data distribution and tokenization, and we introduce metrics and
visualizations to formally characterize their effects on unification. Our work
shows how controlled settings can shed light on pre-training dynamics and
suggests new directions for improving cross-lingual transfer in LLMs.

</details>


### [11] [Hell or High Water: Evaluating Agentic Recovery from External Failures](https://arxiv.org/abs/2508.11027)
*Andrew Wang,Sophia Hager,Adi Asija,Daniel Khashabi,Nicholas Andrews*

Main category: cs.CL

TL;DR: 研究语言模型代理在复杂任务中应对计划失败的能力，发现其难以适应环境反馈并制定备用计划。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型代理在计划失败时如何寻找替代方案，以应对现实世界中的复杂问题。

Method: 设计了一个专门的任务基准，要求代理从数千个函数中搜索并组合调用，同时面对外部失败（如函数不可用）。

Result: 发现语言模型代理难以适应环境反馈，无法有效制定备用计划，即使搜索空间受限。

Conclusion: 当前生成模型在应对环境变化时存在挑战，需进一步研究改进方向。

Abstract: As language model agents are applied to real world problems of increasing
complexity, they will be expected to formulate plans across large search
spaces. If those plans fail for reasons beyond their control, how well do
language agents search for alternative ways to achieve their goals? We devise a
specialized agentic planning benchmark to study this question. Each planning
problem is solved via combinations of function calls. The agent searches for
relevant functions from a set of over four thousand possibilities, and observes
environmental feedback in the form of function outputs or error messages. Our
benchmark confronts the agent with external failures in its workflow, such as
functions that suddenly become unavailable. At the same time, even with the
introduction of these failures, we guarantee that the task remains solvable.
Ideally, an agent's performance on the planning task should not be affected by
the presence of external failures. Overall, we find that language agents
struggle to formulate and execute backup plans in response to environment
feedback. While state-of-the-art models are often able to identify the correct
function to use in the right context, they struggle to adapt to feedback from
the environment and often fail to pursue alternate courses of action, even when
the search space is artificially restricted. We provide a systematic analysis
of the failures of both open-source and commercial models, examining the
effects of search space size, as well as the benefits of scaling model size in
our setting. Our analysis identifies key challenges for current generative
models as well as promising directions for future work.

</details>


### [12] [BIPOLAR: Polarization-based granular framework for LLM bias evaluation](https://arxiv.org/abs/2508.11061)
*Martin Pavlíček,Tomáš Filip,Petr Sosík*

Main category: cs.CL

TL;DR: 该研究提出了一种可重用、细粒度且与主题无关的框架，用于评估大语言模型（LLM）中与极化相关的偏见，并通过合成数据集和情感指标进行验证。


<details>
  <summary>Details</summary>
Motivation: 尽管在偏见检测和缓解方面取得了进展，但某些挑战仍未充分探索，尤其是在处理敏感话题时。

Method: 结合极化敏感的情感指标和合成的平衡数据集，使用预定义的语义类别评估LLM的偏见。

Result: 在俄罗斯-乌克兰战争案例中，发现LLM普遍对乌克兰持更积极态度，但不同语义类别间存在显著差异。

Conclusion: 该框架支持自动化数据集生成和细粒度偏见评估，适用于多种极化驱动场景，并与其他偏见评估策略互补。

Abstract: Large language models (LLMs) are known to exhibit biases in downstream tasks,
especially when dealing with sensitive topics such as political discourse,
gender identity, ethnic relations, or national stereotypes. Although
significant progress has been made in bias detection and mitigation techniques,
certain challenges remain underexplored. This study proposes a reusable,
granular, and topic-agnostic framework to evaluate polarisation-related biases
in LLM (both open-source and closed-source). Our approach combines
polarisation-sensitive sentiment metrics with a synthetically generated
balanced dataset of conflict-related statements, using a predefined set of
semantic categories.
  As a case study, we created a synthetic dataset that focusses on the
Russia-Ukraine war, and we evaluated the bias in several LLMs: Llama-3,
Mistral, GPT-4, Claude 3.5, and Gemini 1.0. Beyond aggregate bias scores, with
a general trend for more positive sentiment toward Ukraine, the framework
allowed fine-grained analysis with considerable variation between semantic
categories, uncovering divergent behavioural patterns among models. Adaptation
to prompt modifications showed further bias towards preconceived language and
citizenship modification.
  Overall, the framework supports automated dataset generation and fine-grained
bias assessment, is applicable to a variety of polarisation-driven scenarios
and topics, and is orthogonal to many other bias-evaluation strategies.

</details>


### [13] [Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs](https://arxiv.org/abs/2508.11068)
*Nicolas Goulet,Alexandre Blondin Massé,Moussa Abdendi*

Main category: cs.CL

TL;DR: 将数字词典嵌入AMR有向图中，利用预训练语言模型，并通过保空间变换简化图，分析其与符号落地问题的关系。


<details>
  <summary>Details</summary>
Motivation: 探索如何将数字词典嵌入AMR有向图中，以解决语义表示和符号落地问题。

Method: 使用预训练语言模型嵌入词典到AMR图中，通过保空间变换简化图。

Result: 分析了简化图的特性，并讨论了其与符号落地问题的关联。

Conclusion: 该方法为语义表示和符号落地问题提供了新的研究视角。

Abstract: Abstract meaning representation (AMR) is a semantic formalism used to
represent the meaning of sentences as directed acyclic graphs. In this paper,
we describe how real digital dictionaries can be embedded into AMR directed
graphs (digraphs), using state-of-the-art pre-trained large language models.
Then, we reduce those graphs in a confluent manner, i.e. with transformations
that preserve their circuit space. Finally, the properties of these reduces
digraphs are analyzed and discussed in relation to the symbol grounding
problem.

</details>


### [14] [Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning](https://arxiv.org/abs/2508.11120)
*Lorenzo Jaime Yu Flores,Junyi Shen,Xiaoyuan Gu*

Main category: cs.CL

TL;DR: 论文提出RAMP框架，结合LLM规划与记忆，提升营销任务中受众筛选的准确性和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在真实营销任务中的可靠性，填补相关研究空白。

Method: 引入RAMP框架，结合迭代规划、工具调用、输出验证和记忆存储。

Result: 准确率提升28%，模糊查询的召回率提升约20%，用户满意度提高。

Conclusion: 为动态行业环境中部署可靠LLM系统提供实践指导。

Abstract: Recent advances in large language models (LLMs) enabled the development of AI
agents that can plan and interact with tools to complete complex tasks.
However, literature on their reliability in real-world applications remains
limited. In this paper, we introduce a multi-agent framework for a marketing
task: audience curation. To solve this, we introduce a framework called RAMP
that iteratively plans, calls tools, verifies the output, and generates
suggestions to improve the quality of the audience generated. Additionally, we
equip the model with a long-term memory store, which is a knowledge base of
client-specific facts and past queries. Overall, we demonstrate the use of LLM
planning and memory, which increases accuracy by 28 percentage points on a set
of 88 evaluation queries. Moreover, we show the impact of iterative
verification and reflection on more ambiguous queries, showing progressively
better recall (roughly +20 percentage points) with more verify/reflect
iterations on a smaller challenge set, and higher user satisfaction. Our
results provide practical insights for deploying reliable LLM-based systems in
dynamic, industry-facing environments.

</details>


### [15] [MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents](https://arxiv.org/abs/2508.11133)
*Tomer Wolfson,Harsh Trivedi,Mor Geva,Yoav Goldberg,Dan Roth,Tushar Khot,Ashish Sabharwal,Reut Tsarfaty*

Main category: cs.CL

TL;DR: MoNaCo是一个包含1,315个自然且复杂问题的基准测试，旨在填补当前LLM基准测试中缺乏耗时问题的空白。前沿LLM在MoNaCo上的表现仅为61.2% F1，显示出其在处理复杂信息检索问题时的不足。


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试缺乏自然且耗时的问题，无法充分评估LLM在复杂信息检索任务中的表现。

Method: 通过分解标注流程，手动构建并回答大量自然且耗时的问题，形成MoNaCo基准测试。

Result: 前沿LLM在MoNaCo上的F1得分为61.2%，召回率低且存在幻觉问题。

Conclusion: MoNaCo为评估LLM在复杂信息检索任务中的表现提供了有效资源，并突显了改进推理模型的必要性。

Abstract: Large language models (LLMs) are emerging as a go-to tool for querying
information. However, current LLM benchmarks rarely feature natural questions
that are both information-seeking as well as genuinely time-consuming for
humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural
and complex questions that require dozens, and at times hundreds, of
intermediate steps to solve -- far more than any existing QA benchmark. To
build MoNaCo, we developed a decomposed annotation pipeline to elicit and
manually answer natural time-consuming questions at scale. Frontier LLMs
evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and
hallucinations. Our results underscore the need for reasoning models that
better handle the complexity and sheer breadth of real-world
information-seeking questions -- with MoNaCo providing an effective resource
for tracking such progress. The MONACO benchmark, codebase, prompts and models
predictions are publicly available at: https://tomerwolgithub.github.io/monaco

</details>


### [16] [MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering](https://arxiv.org/abs/2508.11163)
*Hikaru Asano,Hiroki Ouchi,Akira Kasuga,Ryo Yonetani*

Main category: cs.CL

TL;DR: MobQA是一个用于评估大语言模型（LLMs）对人类移动数据语义理解能力的基准数据集，包含5,800个高质量问答对，涵盖三种问题类型。评估显示LLMs在事实检索上表现良好，但在语义推理和解释问答上存在显著局限。


<details>
  <summary>Details</summary>
Motivation: 现有模型擅长预测人类移动模式，但对其背后原因或语义的理解能力尚不明确，MobQA旨在填补这一空白。

Method: MobQA提供多样化的GPS轨迹数据，通过三种问题类型（事实检索、多选推理和自由解释）评估LLMs的空间、时间和语义推理能力。

Result: LLMs在事实检索上表现优异，但在语义推理和解释问答上表现不佳，轨迹长度对模型效果有显著影响。

Conclusion: MobQA展示了当前LLMs在语义移动理解上的成就与局限，为未来研究提供了方向。

Abstract: This paper presents MobQA, a benchmark dataset designed to evaluate the
semantic understanding capabilities of large language models (LLMs) for human
mobility data through natural language question answering.
  While existing models excel at predicting human movement patterns, it remains
unobvious how much they can interpret the underlying reasons or semantic
meaning of those patterns. MobQA provides a comprehensive evaluation framework
for LLMs to answer questions about diverse human GPS trajectories spanning
daily to weekly granularities. It comprises 5,800 high-quality question-answer
pairs across three complementary question types: factual retrieval (precise
data extraction), multiple-choice reasoning (semantic inference), and free-form
explanation (interpretive description), which all require spatial, temporal,
and semantic reasoning. Our evaluation of major LLMs reveals strong performance
on factual retrieval but significant limitations in semantic reasoning and
explanation question answering, with trajectory length substantially impacting
model effectiveness. These findings demonstrate the achievements and
limitations of state-of-the-art LLMs for semantic mobility
understanding.\footnote{MobQA dataset is available at
https://github.com/CyberAgentAILab/mobqa.}

</details>


### [17] [Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification](https://arxiv.org/abs/2508.11166)
*Anusha M D,Deepthi Vikram,Bharathi Raja Chakravarthi,Parameshwar R Hegde*

Main category: cs.CL

TL;DR: 该研究为低资源德拉维达语Tulu构建了首个社交媒体混合代码的冒犯性语言识别数据集，并评估了多种深度学习模型，发现BiGRU模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: Tulu作为一种低资源语言，缺乏计算资源，但其数字存在感逐渐增强，因此需要构建冒犯性语言识别的基准数据集。

Method: 从YouTube评论中收集并标注了3,845条混合代码的Tulu内容，评估了GRU、LSTM、BiGRU、BiLSTM、CNN、注意力机制及Transformer模型。

Result: BiGRU模型表现最佳（准确率82%，宏F1分数0.81），Transformer模型表现较差。

Conclusion: 该研究为Tulu及其他低资源混合代码语言的NLP研究奠定了基础。

Abstract: Tulu, a low-resource Dravidian language predominantly spoken in southern
India, has limited computational resources despite its growing digital
presence. This study presents the first benchmark dataset for Offensive
Language Identification (OLI) in code-mixed Tulu social media content,
collected from YouTube comments across various domains. The dataset, annotated
with high inter-annotator agreement (Krippendorff's alpha = 0.984), includes
3,845 comments categorized into four classes: Not Offensive, Not Tulu,
Offensive Untargeted, and Offensive Targeted. We evaluate a suite of deep
learning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based
variants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU
model with self-attention achieves the best performance with 82% accuracy and a
0.81 macro F1-score. Transformer models underperform, highlighting the
limitations of multilingual pretraining in code-mixed, under-resourced
contexts. This work lays the foundation for further NLP research in Tulu and
similar low-resource, code-mixed languages.

</details>


### [18] [Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction](https://arxiv.org/abs/2508.11184)
*Tao Wu,Jingyuan Chen,Wang Lin,Jian Zhan,Mengze Li,Kun Kuang,Fei Wu*

Main category: cs.CL

TL;DR: 论文提出了一种个性化干扰项生成方法，通过分析学生的答题记录生成定制化干扰项，以更准确地诊断个体学生的错误推理模式。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的干扰项生成方法仅能捕捉群体层面的错误模式，无法满足个体学生的诊断需求。

Method: 提出一种无需训练的两阶段框架：第一阶段通过蒙特卡洛树搜索从学生错误答案中恢复推理轨迹；第二阶段基于此生成个性化干扰项。

Result: 实验表明，该方法在140名学生中生成个性化干扰项的效果最佳，并能推广到群体层面。

Conclusion: 该方法在个性化干扰项生成任务中表现出色，兼具鲁棒性和适应性。

Abstract: Distractors, incorrect but plausible answer choices in multiple-choice
questions (MCQs), play a critical role in educational assessment by diagnosing
student misconceptions. Recent work has leveraged large language models (LLMs)
to generate shared, group-level distractors by learning common error patterns
across large student populations. However, such distractors often fail to
capture the diverse reasoning errors of individual students, limiting their
diagnostic effectiveness. To address this limitation, we introduce the task of
personalized distractor generation, which aims to generate tailored distractors
based on individual misconceptions inferred from each student's past
question-answering (QA) records, ensuring every student receives options that
effectively exposes their specific reasoning errors. While promising, this task
is challenging because each student typically has only a few QA records, which
often lack the student's underlying reasoning processes, making training-based
group-level approaches infeasible. To overcome this, we propose a training-free
two-stage framework. In the first stage, we construct a student-specific
misconception prototype by applying Monte Carlo Tree Search (MCTS) to recover
the student's reasoning trajectories from past incorrect answers. In the second
stage, this prototype guides the simulation of the student's reasoning on new
questions, enabling the generation of personalized distractors that align with
the student's recurring misconceptions. Experiments show that our approach
achieves the best performance in generating plausible, personalized distractors
for 140 students, and also effectively generalizes to group-level settings,
highlighting its robustness and adaptability.

</details>


### [19] [E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection](https://arxiv.org/abs/2508.11197)
*Ahmad Mousavi,Yeganeh Abdollahinejad,Roberto Corizzo,Nathalie Japkowicz,Zois Boukouvalas*

Main category: cs.CL

TL;DR: E-CaTCH是一个可解释且可扩展的框架，用于检测社交媒体上的多模态虚假信息。它通过聚类帖子为伪事件，提取并融合多模态特征，结合时间动态建模，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的多模态虚假信息检测面临模态不一致、时间模式变化和类别不平衡等挑战，现有方法未能捕捉事件级结构。

Method: E-CaTCH通过文本相似性和时间接近性聚类帖子为伪事件，提取BERT和ResNet特征，使用自注意力机制和跨模态注意力对齐特征，并通过LSTM建模时间动态。

Result: 在Fakeddit、IND和COVID-19 MISINFOGRAPH数据集上，E-CaTCH表现优于现有基线方法，并展示了跨数据集的鲁棒性和泛化能力。

Conclusion: E-CaTCH通过事件级建模和多模态特征融合，有效解决了虚假信息检测的挑战，具有实际应用价值。

Abstract: Detecting multimodal misinformation on social media remains challenging due
to inconsistencies between modalities, changes in temporal patterns, and
substantial class imbalance. Many existing methods treat posts independently
and fail to capture the event-level structure that connects them across time
and modality. We propose E-CaTCH, an interpretable and scalable framework for
robustly detecting misinformation. If needed, E-CaTCH clusters posts into
pseudo-events based on textual similarity and temporal proximity, then
processes each event independently. Within each event, textual and visual
features are extracted using pre-trained BERT and ResNet encoders, refined via
intra-modal self-attention, and aligned through bidirectional cross-modal
attention. A soft gating mechanism fuses these representations to form
contextualized, content-aware embeddings of each post. To model temporal
evolution, E-CaTCH segments events into overlapping time windows and uses a
trend-aware LSTM, enhanced with semantic shift and momentum signals, to encode
narrative progression over time. Classification is performed at the event
level, enabling better alignment with real-world misinformation dynamics. To
address class imbalance and promote stable learning, the model integrates
adaptive class weighting, temporal consistency regularization, and hard-example
mining. The total loss is aggregated across all events. Extensive experiments
on Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH
consistently outperforms state-of-the-art baselines. Cross-dataset evaluations
further demonstrate its robustness, generalizability, and practical
applicability across diverse misinformation scenarios.

</details>


### [20] [Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2508.11247)
*Changjian Wang,Weihong Deng,Weili Guan,Quan Lu,Ning Jiang*

Main category: cs.CL

TL;DR: HGRAG是一种新的检索增强生成方法，通过超图实现结构和语义信息的跨粒度整合，显著提升了多跳问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在多跳问答中因忽略知识的结构关联而效果有限，而GraphRAG方法又过度依赖结构信息。HGRAG旨在平衡结构和语义信息。

Method: 构建实体超图，节点为细粒度实体，超边为粗粒度段落；设计超图检索方法结合实体和段落相似性；使用检索增强模块优化结果。

Result: 在基准数据集上，HGRAG的问答性能优于现有方法，检索效率提升6倍。

Conclusion: HGRAG通过超图整合结构和语义信息，有效提升多跳问答的准确性和效率。

Abstract: Multi-hop question answering (MHQA) requires integrating knowledge scattered
across multiple passages to derive the correct answer. Traditional
retrieval-augmented generation (RAG) methods primarily focus on coarse-grained
textual semantic similarity and ignore structural associations among dispersed
knowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods
address this by leveraging knowledge graphs (KGs) to capture structural
associations, but they tend to overly rely on structural information and
fine-grained word- or phrase-level retrieval, resulting in an underutilization
of textual semantics. In this paper, we propose a novel RAG approach called
HGRAG for MHQA that achieves cross-granularity integration of structural and
semantic information via hypergraphs. Structurally, we construct an entity
hypergraph where fine-grained entities serve as nodes and coarse-grained
passages as hyperedges, and establish knowledge association through shared
entities. Semantically, we design a hypergraph retrieval method that integrates
fine-grained entity similarity and coarse-grained passage similarity via
hypergraph diffusion. Finally, we employ a retrieval enhancement module, which
further refines the retrieved results both semantically and structurally, to
obtain the most relevant passages as context for answer generation with the
LLM. Experimental results on benchmark datasets demonstrate that our approach
outperforms state-of-the-art methods in QA performance, and achieves a
6$\times$ speedup in retrieval efficiency.

</details>


### [21] [UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?](https://arxiv.org/abs/2508.11260)
*Mukund Choudhary,KV Aditya Srivatsa,Gaurja Aeron,Antara Raaghavi Bhattacharya,Dang Khoa Dang Dinh,Ikhlasul Akmal Hanif,Daria Kotova,Ekaterina Kochmar,Monojit Choudhury*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在语言推理任务中表现不佳，尤其是在涉及高形态复杂性的语言学谜题上。通过分析41种低资源语言的629个问题，研究发现LLMs在英语中常见的语言特征上表现较好，但需要更智能的分词器。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在低资源语言中的语言学推理能力，揭示其弱点。

Method: 分析629个语言学谜题，标注语言特征，并测试分词预处理的效果。

Result: LLMs在高形态复杂性任务中表现差，但在英语类似特征上表现较好；分词预处理能提升解决能力。

Conclusion: 研究揭示了LLMs在语言推理中的挑战，需改进分词器以支持低资源语言。

Abstract: Large language models (LLMs) have demonstrated potential in reasoning tasks,
but their performance on linguistics puzzles remains consistently poor. These
puzzles, often derived from Linguistics Olympiad (LO) contests, provide a
minimal contamination environment to assess LLMs' linguistic reasoning
abilities across low-resource languages. This work analyses LLMs' performance
on 629 problems across 41 low-resource languages by labelling each with
linguistically informed features to unveil weaknesses. Our analyses show that
LLMs struggle with puzzles involving higher morphological complexity and
perform better on puzzles involving linguistic features that are also found in
English. We also show that splitting words into morphemes as a pre-processing
step improves solvability, indicating a need for more informed and
language-specific tokenisers. These findings thus offer insights into some
challenges in linguistic reasoning and modelling of low-resource languages.

</details>


### [22] [LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought](https://arxiv.org/abs/2508.11280)
*Ruiyan Qi,Congding Wen,Weibo Zhou,Shangsong Liang,Lingbo Li*

Main category: cs.CL

TL;DR: 论文提出LETToT框架，利用专家思维树结构评估旅游领域大语言模型，无需标注数据，结果显示其优于基线方法，并验证了模型规模与推理能力的关系。


<details>
  <summary>Details</summary>
Motivation: 由于标注数据成本高且存在幻觉问题，评估旅游领域大语言模型具有挑战性。

Method: 通过专家反馈迭代优化思维树结构，并应用于不同规模的模型评估。

Result: 优化后的专家思维树相对基线提升4.99-14.15%；小规模推理增强模型能缩小与大规模模型的差距。

Conclusion: LETToT为领域特定模型评估提供了一种可扩展且无需标注的替代方案。

Abstract: Evaluating large language models (LLMs) in specific domain like tourism
remains challenging due to the prohibitive cost of annotated benchmarks and
persistent issues like hallucinations. We propose $\textbf{L}$able-Free
$\textbf{E}$valuation of LLM on $\textbf{T}$ourism using Expert
$\textbf{T}$ree-$\textbf{o}$f-$\textbf{T}$hought (LETToT), a framework that
leverages expert-derived reasoning structures-instead of labeled data-to access
LLMs in tourism. First, we iteratively refine and validate hierarchical ToT
components through alignment with generic quality dimensions and expert
feedback. Results demonstrate the effectiveness of our systematically optimized
expert ToT with 4.99-14.15\% relative quality gains over baselines. Second, we
apply LETToT's optimized expert ToT to evaluate models of varying scales
(32B-671B parameters), revealing: (1) Scaling laws persist in specialized
domains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,
DeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit
reasoning architectures outperform counterparts in accuracy and conciseness
($p<0.05$). Our work established a scalable, label-free paradigm for
domain-specific LLM evaluation, offering a robust alternative to conventional
annotated benchmarks.

</details>


### [23] [ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection](https://arxiv.org/abs/2508.11281)
*Axel Delaval,Shujian Yang,Haicheng Wang,Han Qiu,Jialiang Lu*

Main category: cs.CL

TL;DR: 论文介绍了TOXIFRENCH数据集，用于法语毒性检测，并提出一种新颖的CoT微调策略，显著提升了小语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 法语毒性检测因缺乏大规模数据集而发展不足，需构建新数据集并改进模型性能。

Method: 采用半自动标注流程构建TOXIFRENCH数据集，提出动态加权损失的CoT微调策略。

Result: 4B模型性能最优，F1提升13%，超越GPT-40和Gemini-2.5，并展示多语言能力。

Conclusion: 方法可扩展至其他语言和安全关键任务，小模型在毒性检测中表现优异。

Abstract: Detecting toxic content using language models is crucial yet challenging.
While substantial progress has been made in English, toxicity detection in
French remains underdeveloped, primarily due to the lack of culturally
relevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new
public benchmark of 53,622 French online comments, constructed via a
semi-automated annotation pipeline that reduces manual labeling to only 10%
through high-confidence LLM-based pre-annotation and human verification. Then,
we benchmark a broad range of models and uncover a counterintuitive insight:
Small Language Models (SLMs) outperform many larger models in robustness and
generalization under the toxicity detection task. Motivated by this finding, we
propose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic
weighted loss that progressively emphasizes the model's final decision,
significantly improving faithfulness. Our fine-tuned 4B model achieves
state-of-the-art performance, improving its F1 score by 13% over its baseline
and outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a
cross-lingual toxicity benchmark demonstrates strong multilingual ability,
suggesting that our methodology can be effectively extended to other languages
and safety-critical classification tasks.

</details>


### [24] [AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries](https://arxiv.org/abs/2508.11285)
*Arya VarastehNezhad,Reza Tavasoli,Soroush Elyasi,MohammadHossein LotfiNia,Hamed Farbeh*

Main category: cs.CL

TL;DR: 研究分析了8种大语言模型（LLMs）对抑郁、焦虑和压力问题的情感回复，发现不同模型和问题类型显著影响情感表达，而用户人口统计特征影响较小。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在心理健康问题回复中的情感表达差异，为模型选择提供依据。

Method: 对8种LLMs生成的2,880条回复进行情感和情绪评分，分析模型、问题类型和用户特征的影响。

Result: 不同模型和问题类型显著影响情感表达（如Mixtral负面情绪高，Llama乐观），人口统计特征影响较小。

Conclusion: 模型选择对心理健康应用至关重要，因其情感表达差异可能显著影响用户体验。

Abstract: Depression, anxiety, and stress are widespread mental health concerns that
increasingly drive individuals to seek information from Large Language Models
(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini
Pro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty
pragmatic questions about depression, anxiety, and stress when those questions
are framed for six user profiles (baseline, woman, man, young, old, and
university student). The models generated 2,880 answers, which we scored for
sentiment and emotions using state-of-the-art tools. Our analysis revealed that
optimism, fear, and sadness dominated the emotional landscape across all
outputs, with neutral sentiment maintaining consistently high values.
Gratitude, joy, and trust appeared at moderate levels, while emotions such as
anger, disgust, and love were rarely expressed. The choice of LLM significantly
influenced emotional expression patterns. Mixtral exhibited the highest levels
of negative emotions including disapproval, annoyance, and sadness, while Llama
demonstrated the most optimistic and joyful responses. The type of mental
health condition dramatically shaped emotional responses: anxiety prompts
elicited extraordinarily high fear scores (0.974), depression prompts generated
elevated sadness (0.686) and the highest negative sentiment, while
stress-related queries produced the most optimistic responses (0.755) with
elevated joy and trust. In contrast, demographic framing of queries produced
only marginal variations in emotional tone. Statistical analyses confirmed
significant model-specific and condition-specific differences, while
demographic influences remained minimal. These findings highlight the critical
importance of model selection in mental health applications, as each LLM
exhibits a distinct emotional signature that could significantly impact user
experience and outcomes.

</details>


### [25] [SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory](https://arxiv.org/abs/2508.11290)
*Utsav Maskey,Sumit Yadav,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 论文提出SafeConstellations方法，通过跟踪任务特定的轨迹模式，减少LLMs的过度拒绝行为，提升实用性。


<details>
  <summary>Details</summary>
Motivation: LLMs的安全机制导致模型过度拒绝看似有害但实际无害的指令，降低了生产应用中的实用性。

Method: 通过分析LLMs在嵌入空间中的轨迹模式，提出SafeConstellations方法，选择性引导模型行为。

Result: 该方法将过度拒绝率降低达73%，同时对实用性影响最小。

Conclusion: SafeConstellations提供了一种减少LLMs过度拒绝行为的有效方法。

Abstract: LLMs increasingly exhibit over-refusal behavior, where safety mechanisms
cause models to reject benign instructions that superficially resemble harmful
content. This phenomena diminishes utility in production applications that
repeatedly rely on common prompt templates or applications that frequently rely
on LLMs for specific tasks (e.g. sentiment analysis, language translation).
Through comprehensive evaluation, we demonstrate that LLMs still tend to refuse
responses to harmful instructions when those instructions are reframed to
appear as benign tasks. Our mechanistic analysis reveal that LLMs follow
distinct "constellation" patterns in embedding space as representations
traverse layers, with each task maintaining consistent trajectories that shift
predictably between refusal and non-refusal cases. We introduce
SafeConstellations, an inference-time trajectory-shifting approach that tracks
task-specific trajectory patterns and guides representations toward non-refusal
pathways. By selectively guiding model behavior only on tasks prone to
over-refusal, and by preserving general model behavior, our method reduces
over-refusal rates by up to 73% with minimal impact on utility-offering a
principled approach to mitigating over-refusals.

</details>


### [26] [SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems](https://arxiv.org/abs/2508.11310)
*Beichen Guo,Zhiyuan Wen,Yu Yang,Peng Gao,Ruosong Yang,Jiaxing Shen*

Main category: cs.CL

TL;DR: SGSimEval 是一个用于自动调查生成（ASG）的综合评估基准，结合了相似性增强评估和多方面指标，解决了现有评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在偏见、缺乏人类偏好和过度依赖 LLMs 作为评判的问题，需要更全面的评估框架。

Method: 提出 SGSimEval，整合大纲、内容和参考文献的评估，结合 LLM 评分与定量指标，并引入人类偏好指标。

Result: 实验表明，当前 ASG 系统在大纲生成上表现优异，但在内容和参考文献生成上仍有改进空间，评估指标与人类判断一致。

Conclusion: SGSimEval 提供了一个多方面的评估框架，显著提升了 ASG 系统的评估质量。

Abstract: The growing interest in automatic survey generation (ASG), a task that
traditionally required considerable time and effort, has been spurred by recent
advances in large language models (LLMs). With advancements in
retrieval-augmented generation (RAG) and the rising popularity of multi-agent
systems (MASs), synthesizing academic surveys using LLMs has become a viable
approach, thereby elevating the need for robust evaluation methods in this
domain. However, existing evaluation methods suffer from several limitations,
including biased metrics, a lack of human preference, and an over-reliance on
LLMs-as-judges. To address these challenges, we propose SGSimEval, a
comprehensive benchmark for Survey Generation with Similarity-Enhanced
Evaluation that evaluates automatic survey generation systems by integrating
assessments of the outline, content, and references, and also combines
LLM-based scoring with quantitative metrics to provide a multifaceted
evaluation framework. In SGSimEval, we also introduce human preference metrics
that emphasize both inherent quality and similarity to humans. Extensive
experiments reveal that current ASG systems demonstrate human-comparable
superiority in outline generation, while showing significant room for
improvement in content and reference generation, and our evaluation metrics
maintain strong consistency with human assessments.

</details>


### [27] [LLM Compression: How Far Can We Go in Balancing Size and Performance?](https://arxiv.org/abs/2508.11318)
*Sahil Sk,Debasish Dhal,Sonal Khosla,Sk Shahid,Sambit Shekhar,Akash Dhaka,Shantipriya Parida,Dilip K. Prasad,Ondřej Bojar*

Main category: cs.CL

TL;DR: 该研究探讨了4位组缩放量化（GSQ）和生成预训练变换器量化（GPTQ）对LLaMA 1B、Qwen 0.5B和PHI 1.5B模型的影响，评估了它们在多个NLP任务中的表现和效率。


<details>
  <summary>Details</summary>
Motivation: 量化技术可以减少大型语言模型的内存占用和计算成本，同时保持性能，从而提高其可访问性。

Method: 研究应用GSQ和GPTQ技术对三种模型进行量化，并在MS MARCO、BoolQ和GSM8K数据集上评估其准确性和效率。

Result: 研究量化了模型压缩与任务性能之间的权衡，分析了准确性、推理延迟和吞吐量等关键指标。

Conclusion: 研究结果为低比特量化在实际部署中的适用性提供了见解，并讨论了GSQ和GPTQ技术在不同规模模型上的优缺点。

Abstract: Quantization is an essential and popular technique for improving the
accessibility of large language models (LLMs) by reducing memory usage and
computational costs while maintaining performance. In this study, we apply
4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer
Quantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their
impact across multiple NLP tasks. We benchmark these models on MS MARCO
(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K
(Mathematical Reasoning) datasets, assessing both accuracy and efficiency
across various tasks. The study measures the trade-offs between model
compression and task performance, analyzing key evaluation metrics, namely
accuracy, inference latency, and throughput (total output tokens generated per
second), providing insights into the suitability of low-bit quantization for
real-world deployment. Using the results, users can then make suitable
decisions based on the specifications that need to be met. We discuss the pros
and cons of GSQ and GPTQ techniques on models of different sizes, which also
serve as a benchmark for future experiments.

</details>


### [28] [SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis](https://arxiv.org/abs/2508.11343)
*Haitong Luo,Weiyao Zhang,Suhang Wang,Wenji Zou,Chungang Lin,Xuying Meng,Yujun Zhang*

Main category: cs.CL

TL;DR: 论文提出了一种基于信号处理的新方法（SpecDetect和SpecDetect++），通过分析文本生成过程中的频谱特性来检测LLM生成的文本，效果优于现有方法且更高效。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法依赖表面统计特征，忽略了文本生成过程的信号特性，需要更可靠和高效的检测手段。

Method: 将检测问题转化为信号处理问题，利用全局离散傅里叶变换（DFT）和局部短时傅里叶变换（STFT）分析文本的频谱能量特性。

Result: 实验表明，该方法在检测性能和运行时间上均优于现有技术。

Conclusion: 信号处理技术为LLM生成文本检测提供了高效且可解释的新途径。

Abstract: The proliferation of high-quality text from Large Language Models (LLMs)
demands reliable and efficient detection methods. While existing training-free
approaches show promise, they often rely on surface-level statistics and
overlook fundamental signal properties of the text generation process. In this
work, we reframe detection as a signal processing problem, introducing a novel
paradigm that analyzes the sequence of token log-probabilities in the frequency
domain. By systematically analyzing the signal's spectral properties using the
global Discrete Fourier Transform (DFT) and the local Short-Time Fourier
Transform (STFT), we find that human-written text consistently exhibits
significantly higher spectral energy. This higher energy reflects the
larger-amplitude fluctuations inherent in human writing compared to the
suppressed dynamics of LLM-generated text. Based on this key insight, we
construct SpecDetect, a detector built on a single, robust feature from the
global DFT: DFT total energy. We also propose an enhanced version,
SpecDetect++, which incorporates a sampling discrepancy mechanism to further
boost robustness. Extensive experiments demonstrate that our approach
outperforms the state-of-the-art model while running in nearly half the time.
Our work introduces a new, efficient, and interpretable pathway for
LLM-generated text detection, showing that classical signal processing
techniques offer a surprisingly powerful solution to this modern challenge.

</details>


### [29] [Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning](https://arxiv.org/abs/2508.11364)
*Sylvio Rüdian,Yassin Elsir,Marvin Kretschmer,Sabine Cayrou,Niels Pinkwart*

Main category: cs.CL

TL;DR: 研究探讨了使用大语言模型Llama 3.1从学生提交的语言学习课程中提取反馈指标的方法，并验证了其与人工评分的相关性。结果表明，LLM生成的指标与人工评分具有显著强相关性，为未来自动生成透明反馈奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 自动生成反馈可以提升学生学习效率并帮助教师优化时间，但需要先提取高质量的反馈指标。

Method: 使用Llama 3.1从学生提交中提取反馈指标，并分析其与人工评分的相关性。

Result: LLM生成的指标与人工评分表现出显著强相关性，包括未预期的指标组合。

Conclusion: 该方法为未来利用LLM自动生成透明反馈提供了可行基础。

Abstract: Automated feedback generation has the potential to enhance students' learning
progress by providing timely and targeted feedback. Moreover, it can assist
teachers in optimizing their time, allowing them to focus on more strategic and
personalized aspects of teaching. To generate high-quality, information-rich
formative feedback, it is essential first to extract relevant indicators, as
these serve as the foundation upon which the feedback is constructed. Teachers
often employ feedback criteria grids composed of various indicators that they
evaluate systematically. This study examines the initial phase of extracting
such indicators from students' submissions of a language learning course using
the large language model Llama 3.1. Accordingly, the alignment between
indicators generated by the LLM and human ratings across various feedback
criteria is investigated. The findings demonstrate statistically significant
strong correlations, even in cases involving unanticipated combinations of
indicators and criteria. The methodology employed in this paper offers a
promising foundation for extracting indicators from students' submissions using
LLMs. Such indicators can potentially be utilized to auto-generate explainable
and transparent formative feedback in future research.

</details>


### [30] [When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs](https://arxiv.org/abs/2508.11383)
*Mikhail Seleznyov,Mikhail Chaichuk,Gleb Ershov,Alexander Panchenko,Elena Tutubalina,Oleg Somov*

Main category: cs.CL

TL;DR: 论文系统评估了5种提升提示鲁棒性的方法，在52个任务上测试了8个模型，并扩展到前沿模型GPT-4.1和DeepSeek V3，为实际应用提供了实用建议。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）对提示的细微变化非常敏感，需要系统评估提升鲁棒性的方法。

Method: 在统一实验框架下评估5种方法，测试了8个模型（Llama、Qwen、Gemma系列）和52个任务，涵盖微调和上下文学习范式。

Result: 提供了不同鲁棒性方法的相对有效性，并评估了前沿模型对格式扰动的鲁棒性。

Conclusion: 研究结果为实际应用中实现稳定可靠的LLM性能提供了实用指导。

Abstract: Large Language Models (LLMs) are highly sensitive to subtle, non-semantic
variations in prompt phrasing and formatting. In this work, we present the
first systematic evaluation of 5 methods for improving prompt robustness within
a unified experimental framework. We benchmark these techniques on 8 models
from Llama, Qwen and Gemma families across 52 tasks from Natural Instructions
dataset. Our evaluation covers robustness methods from both fine-tuned and
in-context learning paradigms, and tests their generalization against multiple
types of distribution shifts. Finally, we extend our analysis to GPT-4.1 and
DeepSeek V3 to assess frontier models' current robustness to format
perturbations. Our findings offer actionable insights into the relative
effectiveness of these robustness methods, enabling practitioners to make
informed decisions when aiming for stable and reliable LLM performance in
real-world applications. Code:
https://github.com/AIRI-Institute/when-punctuation-matters.

</details>


### [31] [Retrieval-augmented reasoning with lean language models](https://arxiv.org/abs/2508.11386)
*Ryan Sze-Yin Chan,Federico Nanni,Tomas Lazauskas,Rosie Wood,Penelope Yong,Lionel Tarassenko,Mark Girolami,James Geddes,Andrew Duncan*

Main category: cs.CL

TL;DR: 提出了一种结合推理与检索增强生成（RAG）的轻量级语言模型架构，适用于资源受限或隐私敏感环境。


<details>
  <summary>Details</summary>
Motivation: 解决现有RAG系统依赖大规模模型和外部API的问题，满足高效、隐私保护的本地部署需求。

Method: 采用密集检索器与微调Qwen2.5-Instruct模型，结合合成查询生成和推理轨迹，基于NHS A-to-Z数据集优化。

Result: 在答案准确性和一致性上显著提升，接近前沿模型性能，适合本地部署。

Conclusion: 通过领域特定微调和轻量架构，实现了高效、隐私保护的RAG系统，代码已开源。

Abstract: This technical report details a novel approach to combining reasoning and
retrieval augmented generation (RAG) within a single, lean language model
architecture. While existing RAG systems typically rely on large-scale models
and external APIs, our work addresses the increasing demand for performant and
privacy-preserving solutions deployable in resource-constrained or secure
environments. Building on recent developments in test-time scaling and
small-scale reasoning models, we develop a retrieval augmented conversational
agent capable of interpreting complex, domain-specific queries using a
lightweight backbone model. Our system integrates a dense retriever with
fine-tuned Qwen2.5-Instruct models, using synthetic query generation and
reasoning traces derived from frontier models (e.g., DeepSeek-R1) over a
curated corpus, in this case, the NHS A-to-Z condition pages. We explore the
impact of summarisation-based document compression, synthetic data design, and
reasoning-aware fine-tuning on model performance. Evaluation against both
non-reasoning and general-purpose lean models demonstrates that our
domain-specific fine-tuning approach yields substantial gains in answer
accuracy and consistency, approaching frontier-level performance while
remaining feasible for local deployment. All implementation details and code
are publicly released to support reproducibility and adaptation across domains.

</details>


### [32] [Model Interpretability and Rationale Extraction by Input Mask Optimization](https://arxiv.org/abs/2508.11388)
*Marc Brinner,Sina Zarriess*

Main category: cs.CL

TL;DR: 提出一种基于梯度优化和正则化的新方法，为神经网络预测生成提取性解释，适用于文本和图像输入。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络模型的发展，对其预测进行解释的需求日益增长，尤其是在自然语言处理和计算机视觉领域。

Method: 通过梯度优化和新的正则化方案，掩蔽输入中模型认为与预测无关的部分，确保解释的充分性、全面性和紧凑性。

Result: 方法在文本和图像分类任务中均能生成高质量解释，证明其广泛适用性。

Conclusion: 该方法无需训练专用模型，仅基于已训练分类器即可实现理性提取，填补了模型可解释性与理性提取之间的空白。

Abstract: Concurrent to the rapid progress in the development of neural-network based
models in areas like natural language processing and computer vision, the need
for creating explanations for the predictions of these black-box models has
risen steadily. We propose a new method to generate extractive explanations for
predictions made by neural networks, that is based on masking parts of the
input which the model does not consider to be indicative of the respective
class. The masking is done using gradient-based optimization combined with a
new regularization scheme that enforces sufficiency, comprehensiveness and
compactness of the generated explanation, three properties that are known to be
desirable from the related field of rationale extraction in natural language
processing. In this way, we bridge the gap between model interpretability and
rationale extraction, thereby proving that the latter of which can be performed
without training a specialized model, only on the basis of a trained
classifier. We further apply the same method to image inputs and obtain high
quality explanations for image classifications, which indicates that the
conditions proposed for rationale extraction in natural language processing are
more broadly applicable to different input types.

</details>


### [33] [Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training](https://arxiv.org/abs/2508.11393)
*Marc Brinner,Sina Zarrieß*

Main category: cs.CL

TL;DR: 提出一种端到端可微分的训练范式，用于稳定训练基于Transformer的分类器，同时生成输入标记的相关性分数。


<details>
  <summary>Details</summary>
Motivation: 简化传统的三玩家游戏训练方法，避免训练不稳定性，并提升与人类标注的对齐效果。

Method: 通过单一模型同时扮演三个角色（理由选择器、分类器和互补分类器），并结合参数化和正则化技术生成类别相关理由。

Result: 显著提升与人类标注的对齐效果，达到最先进水平，且无需显式监督。

Conclusion: 该方法高效稳定，能同时完成分类和理由生成任务，性能优于现有方法。

Abstract: We propose an end-to-end differentiable training paradigm for stable training
of a rationalized transformer classifier. Our approach results in a single
model that simultaneously classifies a sample and scores input tokens based on
their relevance to the classification. To this end, we build on the widely-used
three-player-game for training rationalized models, which typically relies on
training a rationale selector, a classifier and a complement classifier. We
simplify this approach by making a single model fulfill all three roles,
leading to a more efficient training paradigm that is not susceptible to the
common training instabilities that plague existing approaches. Further, we
extend this paradigm to produce class-wise rationales while incorporating
recent advances in parameterizing and regularizing the resulting rationales,
thus leading to substantially improved and state-of-the-art alignment with
human annotations without any explicit supervision.

</details>


### [34] [Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions](https://arxiv.org/abs/2508.11414)
*Shangrui Nie,Florian Mai,David Kaczér,Charles Welch,Zhixue Zhao,Lucie Flek*

Main category: cs.CL

TL;DR: 通过微调语言模型回答价值观调查问题，可以显著改变其在下游任务中的行为，实现价值观对齐。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过简单的微调方法调整语言模型的价值观系统，以减少对大量训练数据的依赖。

Method: 构建价值观调查问卷作为基线，微调模型回答问卷，并评估其在领域内和领域外任务中的行为变化。

Result: 微调不仅改变了模型对调查问题的回答，还显著影响了其在情境化道德判断和文本冒险游戏中的行为。

Conclusion: 简单的价值观微调方法能有效调整语言模型的价值观系统，适用于下游任务。

Abstract: Large language models implicitly encode preferences over human values, yet
steering them often requires large training data. In this work, we investigate
a simple approach: Can we reliably modify a model's value system in downstream
behavior by training it to answer value survey questions accordingly? We first
construct value profiles of several open-source LLMs by asking them to rate a
series of value-related descriptions spanning 20 distinct human values, which
we use as a baseline for subsequent experiments. We then investigate whether
the value system of a model can be governed by fine-tuning on the value
surveys. We evaluate the effect of finetuning on the model's behavior in two
ways; first, we assess how answers change on in-domain, held-out survey
questions. Second, we evaluate whether the model's behavior changes in
out-of-domain settings (situational scenarios). To this end, we construct a
contextualized moral judgment dataset based on Reddit posts and evaluate
changes in the model's behavior in text-based adventure games. We demonstrate
that our simple approach can not only change the model's answers to in-domain
survey questions, but also produces substantial shifts (value alignment) in
implicit downstream task behavior.

</details>


### [35] [HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor](https://arxiv.org/abs/2508.11429)
*Shivam Dubey*

Main category: cs.CL

TL;DR: HumorPlanSearch是一个模块化流程，通过多阶段建模上下文提升AI生成幽默的质量，实验显示其HGS评分比基线高15.4%。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型生成的幽默常显得通用、重复或不恰当，因幽默依赖文化背景和即时语境。

Method: 采用Plan-Search、Humor Chain-of-Thought模板、知识图谱、新颖性过滤和迭代修订循环。

Result: 实验表明，完整流程（知识图谱+修订）在九主题测试中显著提升HGS评分15.4%。

Conclusion: HumorPlanSearch通过全程关注上下文，推动AI幽默向更连贯、适应性强和文化敏感的方向发展。

Abstract: Automated humor generation with Large Language Models (LLMs) often yields
jokes that feel generic, repetitive, or tone-deaf because humor is deeply
situated and hinges on the listener's cultural background, mindset, and
immediate context. We introduce HumorPlanSearch, a modular pipeline that
explicitly models context through: (1) Plan-Search for diverse, topic-tailored
strategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and
stylistic reasoning; (3) a Knowledge Graph to retrieve and adapt
high-performing historical strategies; (4) novelty filtering via semantic
embeddings; and (5) an iterative judge-driven revision loop. To evaluate
context sensitivity and comedic quality, we propose the Humor Generation Score
(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,
and topic relevance. In experiments across nine topics with feedback from 13
human judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent
(p < 0.05) over a strong baseline. By foregrounding context at every stage from
strategy planning to multi-signal evaluation, HumorPlanSearch advances
AI-driven humor toward more coherent, adaptive, and culturally attuned comedy.

</details>


### [36] [Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse](https://arxiv.org/abs/2508.11434)
*Aditi Dutta,Susan Banducci*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在区分反性别歧视言论与性别歧视言论时的困难，发现模型常将反性别歧视言论误判为有害内容，建议改进内容审核设计。


<details>
  <summary>Details</summary>
Motivation: 自动化内容审核系统可能误判反性别歧视言论，导致边缘化声音被压制，研究旨在揭示这一问题并提出改进方案。

Method: 分析五种LLMs对英国2022年涉及女性议员的推文分类，重点关注高显著性触发事件。

Result: 模型常将反性别歧视言论误判为有害，尤其在政治敏感事件中。

Conclusion: 建议内容审核设计超越二元分类，纳入人工审核和反歧视言论训练数据，以保护数字政治空间中的抵抗言论。

Abstract: Anti-sexist speech, i.e., public expressions that challenge or resist
gendered abuse and sexism, plays a vital role in shaping democratic debate
online. Yet automated content moderation systems, increasingly powered by large
language models (LLMs), may struggle to distinguish such resistance from the
sexism it opposes. This study examines how five LLMs classify sexist,
anti-sexist, and neutral political tweets from the UK, focusing on
high-salience trigger events involving female Members of Parliament in the year
2022. Our analysis show that models frequently misclassify anti-sexist speech
as harmful, particularly during politically charged events where rhetorical
styles of harm and resistance converge. These errors risk silencing those who
challenge sexism, with disproportionate consequences for marginalised voices.
We argue that moderation design must move beyond binary harmful/not-harmful
schemas, integrate human-in-the-loop review during sensitive events, and
explicitly include counter-speech in training data. By linking feminist
scholarship, event-based analysis, and model evaluation, this work highlights
the sociotechnical challenges of safeguarding resistance speech in digital
political spaces.

</details>


### [37] [CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity](https://arxiv.org/abs/2508.11442)
*Bowen Zhang,Zixin Song,Chunquan Chen,Qian-Wen Zhang,Di Yin,Xing Sun*

Main category: cs.CL

TL;DR: CoDiEmb框架通过任务专用目标、动态采样器和模型融合策略，解决了信息检索（IR）和语义文本相似性（STS）任务联合训练中的负迁移问题。


<details>
  <summary>Details</summary>
Motivation: 解决联合训练IR和STS任务时负迁移导致的性能下降问题。

Method: 提出CoDiEmb框架，包括任务专用目标、动态采样器、delta引导的模型融合策略和高效单阶段训练。

Result: 在15个基准测试中验证了CoDiEmb的有效性，不仅缓解了任务间冲突，还改善了嵌入空间的几何特性。

Conclusion: CoDiEmb成功解决了IR和STS联合训练中的挑战，为统一文本嵌入学习提供了新思路。

Abstract: Learning unified text embeddings that excel across diverse downstream tasks
is a central goal in representation learning, yet negative transfer remains a
persistent obstacle. This challenge is particularly pronounced when jointly
training a single encoder for Information Retrieval (IR) and Semantic Textual
Similarity (STS), two essential but fundamentally disparate tasks for which
naive co-training typically yields steep performance trade-offs. We argue that
resolving this conflict requires systematically decoupling task-specific
learning signals throughout the training pipeline. To this end, we introduce
CoDiEmb, a unified framework that reconciles the divergent requirements of IR
and STS in a collaborative yet distinct manner. CoDiEmb integrates three key
innovations for effective joint optimization: (1) Task-specialized objectives
paired with a dynamic sampler that forms single-task batches and balances
per-task updates, thereby preventing gradient interference. For IR, we employ a
contrastive loss with multiple positives and hard negatives, augmented by
cross-device sampling. For STS, we adopt order-aware objectives that directly
optimize correlation and ranking consistency. (2) A delta-guided model fusion
strategy that computes fine-grained merging weights for checkpoints by
analyzing each parameter's deviation from its pre-trained initialization,
proving more effective than traditional Model Soups. (3) An efficient,
single-stage training pipeline that is simple to implement and converges
stably. Extensive experiments on 15 standard IR and STS benchmarks across three
base encoders validate CoDiEmb. Our results and analysis demonstrate that the
framework not only mitigates cross-task trade-offs but also measurably improves
the geometric properties of the embedding space.

</details>


### [38] [Reference Points in LLM Sentiment Analysis: The Role of Structured Context](https://arxiv.org/abs/2508.11454)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 研究表明，结构化提示（如JSON格式）能提升小型语言模型在情感分析中的表现，无需微调即可超越基线。


<details>
  <summary>Details</summary>
Motivation: 现有NLP研究多基于纯文本情感分析，但营销理论指出客户评价还受其他参考点影响。本文探讨补充信息的内容和格式对情感分析的影响。

Method: 比较自然语言和JSON格式提示，使用轻量级3B参数模型，在Yelp的餐厅和夜生活类别上进行实验。

Result: JSON提示表现最佳，Macro-F1提升1.6%和4%，RMSE降低16%和9.1%，适用于资源受限设备。

Conclusion: 结构化提示使小型模型具备竞争力，为大规模模型部署提供实用替代方案。

Abstract: Large language models (LLMs) are now widely used across many fields,
including marketing research. Sentiment analysis, in particular, helps firms
understand consumer preferences. While most NLP studies classify sentiment from
review text alone, marketing theories, such as prospect theory and
expectation--disconfirmation theory, point out that customer evaluations are
shaped not only by the actual experience but also by additional reference
points. This study therefore investigates how the content and format of such
supplementary information affect sentiment analysis using LLMs. We compare
natural language (NL) and JSON-formatted prompts using a lightweight 3B
parameter model suitable for practical marketing applications. Experiments on
two Yelp categories (Restaurant and Nightlife) show that the JSON prompt with
additional information outperforms all baselines without fine-tuning: Macro-F1
rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it
deployable in resource-constrained edge devices. Furthermore, a follow-up
analysis confirms that performance gains stem from genuine contextual reasoning
rather than label proxying. This work demonstrates that structured prompting
can enable smaller models to achieve competitive performance, offering a
practical alternative to large-scale model deployment.

</details>


### [39] [Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models](https://arxiv.org/abs/2508.11534)
*Monika Jotautaitė,Lucius Caviola,David A. Brewster,Thilo Hagendorff*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）是否表现出物种主义偏见，并评估其对非人类动物的价值观。通过三个范式分析，发现LLMs能识别物种主义言论但很少谴责，且在权衡中更倾向于人类。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，研究其伦理倾向，尤其是物种主义偏见，对AI公平性和道德对齐至关重要。

Method: 通过三个范式分析：(1) SpeciesismBench基准测试，(2) 心理学测量比较模型与人类反应，(3) 文本生成任务探究物种主义合理化。

Result: LLMs能识别物种主义言论但很少谴责，在权衡中更倾向于人类，但对认知能力平等的个体无物种偏好。

Conclusion: 需扩展AI公平性框架以明确包含非人类道德主体，减少物种主义偏见对社会的影响。

Abstract: As large language models (LLMs) become more widely deployed, it is crucial to
examine their ethical tendencies. Building on research on fairness and
discrimination in AI, we investigate whether LLMs exhibit speciesist bias --
discrimination based on species membership -- and how they value non-human
animals. We systematically examine this issue across three paradigms: (1)
SpeciesismBench, a 1,003-item benchmark assessing recognition and moral
evaluation of speciesist statements; (2) established psychological measures
comparing model responses with those of human participants; (3) text-generation
tasks probing elaboration on, or resistance to, speciesist rationalizations. In
our benchmark, LLMs reliably detected speciesist statements but rarely
condemned them, often treating speciesist attitudes as morally acceptable. On
psychological measures, results were mixed: LLMs expressed slightly lower
explicit speciesism than people, yet in direct trade-offs they more often chose
to save one human over multiple animals. A tentative interpretation is that
LLMs may weight cognitive capacity rather than species per se: when capacities
were equal, they showed no species preference, and when an animal was described
as more capable, they tended to prioritize it over a less capable human. In
open-ended text generation tasks, LLMs frequently normalized or rationalized
harm toward farmed animals while refusing to do so for non-farmed animals.
These findings suggest that while LLMs reflect a mixture of progressive and
mainstream human views, they nonetheless reproduce entrenched cultural norms
around animal exploitation. We argue that expanding AI fairness and alignment
frameworks to explicitly include non-human moral patients is essential for
reducing these biases and preventing the entrenchment of speciesist attitudes
in AI systems and the societies they influence.

</details>


### [40] [Language models align with brain regions that represent concepts across modalities](https://arxiv.org/abs/2508.11536)
*Maria Ryskina,Greta Tuckute,Alexander Fung,Ashley Malkin,Evelina Fedorenko*

Main category: cs.CL

TL;DR: 研究探讨了语言模型（LMs）与大脑对齐的关系，发现语言模型可能内部表征跨模态的概念意义。


<details>
  <summary>Details</summary>
Motivation: 认知科学和神经科学长期面临区分语言表征与概念意义表征的挑战，本研究旨在通过语言模型与大脑对齐来解决这一问题。

Method: 通过两个神经指标（句子处理时的大脑激活水平和跨输入模态的意义一致性）分析语言模型与大脑对齐的关系。

Result: 实验表明，语言模型在意义一致性较高的大脑区域预测信号更好，即使这些区域对语言处理不敏感。

Conclusion: 语言模型可能内部表征跨模态的概念意义，而不仅仅是语言处理。

Abstract: Cognitive science and neuroscience have long faced the challenge of
disentangling representations of language from representations of conceptual
meaning. As the same problem arises in today's language models (LMs), we
investigate the relationship between LM--brain alignment and two neural
metrics: (1) the level of brain activation during processing of sentences,
targeting linguistic processing, and (2) a novel measure of meaning consistency
across input modalities, which quantifies how consistently a brain region
responds to the same concept across paradigms (sentence, word cloud, image)
using an fMRI dataset (Pereira et al., 2018). Our experiments show that both
language-only and language-vision models predict the signal better in more
meaning-consistent areas of the brain, even when these areas are not strongly
sensitive to language processing, suggesting that LMs might internally
represent cross-modal conceptual meaning.

</details>


### [41] [AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment](https://arxiv.org/abs/2508.11567)
*Jinpeng Hu,Ao Wang,Qianqian Xie,Hui Ma,Zhuo Li,Dan Guo*

Main category: cs.CL

TL;DR: 提出了一种基于多智能体的心理健康评估框架，模拟医患对话，通过动态交互和自适应提问机制提升评估效果。


<details>
  <summary>Details</summary>
Motivation: 传统心理健康评估依赖专业人员且静态文本分析能力有限，无法捕捉动态交互中的深层信息。

Method: 采用多智能体框架，包括提问、评估、评分和更新模块，结合自适应提问机制和树状记忆结构动态跟踪信息。

Result: 在DAIC-WOZ数据集上验证，性能优于现有方法。

Conclusion: 多智能体框架能有效提升心理健康评估的深度和准确性。

Abstract: Mental health assessment is crucial for early intervention and effective
treatment, yet traditional clinician-based approaches are limited by the
shortage of qualified professionals. Recent advances in artificial intelligence
have sparked growing interest in automated psychological assessment, yet most
existing approaches are constrained by their reliance on static text analysis,
limiting their ability to capture deeper and more informative insights that
emerge through dynamic interaction and iterative questioning. Therefore, in
this paper, we propose a multi-agent framework for mental health evaluation
that simulates clinical doctor-patient dialogues, with specialized agents
assigned to questioning, adequacy evaluation, scoring, and updating. We
introduce an adaptive questioning mechanism in which an evaluation agent
assesses the adequacy of user responses to determine the necessity of
generating targeted follow-up queries to address ambiguity and missing
information. Additionally, we employ a tree-structured memory in which the root
node encodes the user's basic information, while child nodes (e.g., topic and
statement) organize key information according to distinct symptom categories
and interaction turns. This memory is dynamically updated throughout the
interaction to reduce redundant questioning and further enhance the information
extraction and contextual tracking capabilities. Experimental results on the
DAIC-WOZ dataset illustrate the effectiveness of our proposed method, which
achieves better performance than existing approaches.

</details>


### [42] [Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models](https://arxiv.org/abs/2508.11582)
*Qiguang Chen,Dengyun Peng,Jinhao Liu,HuiKang Su,Jiannan Guan,Libo Qin,Wanxiang Che*

Main category: cs.CL

TL;DR: 论文提出动态推理边界自感知框架（DR. SAF），通过动态调整推理深度提升效率，减少冗余，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有长链思维（CoT）方法在复杂推理任务中存在冗余问题，且依赖人工定义的难度先验，效率低下。

Method: DR. SAF包含边界自感知对齐、自适应奖励管理和边界保护机制，动态优化推理过程。

Result: 实验显示，DR. SAF减少49.27%的响应token，效率提升6.59倍，训练时间减少5倍，极端训练下准确率提升16%。

Conclusion: DR. SAF显著提升推理效率与准确性，适用于资源受限场景。

Abstract: Recent advancements in large language models (LLMs) have greatly improved
their capabilities on complex reasoning tasks through Long Chain-of-Thought
(CoT). However, this approach often results in substantial redundancy,
impairing computational efficiency and causing significant delays in real-time
applications. To improve the efficiency, current methods often rely on
human-defined difficulty priors, which do not align with the LLM's self-awared
difficulty, leading to inefficiencies. In this paper, we introduce the Dynamic
Reasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to
dynamically assess and adjust their reasoning depth in response to problem
complexity. DR. SAF integrates three key components: Boundary Self-Awareness
Alignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.
These components allow models to optimize their reasoning processes, balancing
efficiency and accuracy without compromising performance. Our experimental
results demonstrate that DR. SAF achieves a 49.27% reduction in total response
tokens with minimal loss in accuracy. The framework also delivers a 6.59x gain
in token efficiency and a 5x reduction in training time, making it well-suited
to resource-limited settings. During extreme training, DR. SAF can even surpass
traditional instruction-based models in token efficiency with more than 16%
accuracy improvement.

</details>


### [43] [Dataset Creation for Visual Entailment using Generative AI](https://arxiv.org/abs/2508.11605)
*Rob Reijtenbach,Suzan Verberne,Gijs Wijnholds*

Main category: cs.CL

TL;DR: 提出并验证了一种用于训练视觉蕴含模型的新合成数据集，基于SNLI数据集生成图像，实验表明合成数据在数据稀缺时是可行方案。


<details>
  <summary>Details</summary>
Motivation: 现有视觉蕴含数据集规模小且稀疏，手动创建耗时费力，需探索替代方案。

Method: 利用SNLI文本前提通过Stable Diffusion生成图像，构建合成数据集，并通过CLIP特征向量训练分类器进行内外评估。

Result: 合成数据训练的分类器在SNLI-VE和SICK-VTE上F-score略低于真实数据（0.686 vs 0.703；0.384 vs 0.400）。

Conclusion: 合成数据在数据稀缺时是训练视觉蕴含模型的有效替代方案，性能接近真实数据。

Abstract: In this paper we present and validate a new synthetic dataset for training
visual entailment models. Existing datasets for visual entailment are small and
sparse compared to datasets for textual entailment. Manually creating datasets
is labor-intensive. We base our synthetic dataset on the SNLI dataset for
textual entailment. We take the premise text from SNLI as input prompts in a
generative image model, Stable Diffusion, creating an image to replace each
textual premise. We evaluate our dataset both intrinsically and extrinsically.
For extrinsic evaluation, we evaluate the validity of the generated images by
using them as training data for a visual entailment classifier based on CLIP
feature vectors. We find that synthetic training data only leads to a slight
drop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when
trained on real data. We also compare the quality of our generated training
data to original training data on another dataset: SICK-VTE. Again, there is
only a slight drop in F-score: from 0.400 to 0.384. These results indicate that
in settings with data sparsity, synthetic data can be a promising solution for
training visual entailment models.

</details>


### [44] [TinyTim: A Family of Language Models for Divergent Generation](https://arxiv.org/abs/2508.11607)
*Christopher J. Agostino*

Main category: cs.CL

TL;DR: TinyTim是一个基于《芬尼根的守灵夜》微调的大语言模型家族，其生成文本具有高词汇多样性和低语义连贯性。


<details>
  <summary>Details</summary>
Motivation: 探索专门化语言模型在创造性架构中作为发散知识源的潜力，支持自动化发现机制。

Method: 通过定量评估与基线模型对比，分析TinyTim V1的生成特性。

Result: TinyTim V1生成文本具有高词汇多样性和低语义连贯性。

Conclusion: 此类专门化模型可作为创造性架构的发散知识源，推动自动化发现。

Abstract: This work introduces TinyTim, a family of large language models fine-tuned on
James Joyce's `Finnegans Wake'. Through quantitative evaluation against
baseline models, we demonstrate that TinyTim V1 produces a statistically
distinct generative profile characterized by high lexical diversity and low
semantic coherence. These findings are interpreted through theories of
creativity and complex problem-solving, arguing that such specialized models
can function as divergent knowledge sources within more extensive creative
architectures, powering automated discovery mechanisms in diverse settings.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [45] [Perturbed Public Voices (P$^{2}$V): A Dataset for Robust Audio Deepfake Detection](https://arxiv.org/abs/2508.10949)
*Chongyang Gao,Marco Postiglione,Isabel Gortner,Sarit Kraus,V. S. Subrahmanian*

Main category: cs.SD

TL;DR: P$^{2}$V数据集揭示现有音频深度伪造检测器的脆弱性，提出新基准以提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有音频深度伪造检测器在真实世界中表现不佳，缺乏对恶意伪造的全面测试。

Method: 引入P$^{2}$V数据集，包含身份一致文本、环境噪声和先进语音克隆技术，测试22种检测器。

Result: 现有检测器在P$^{2}$V上性能下降43%，对抗扰动和克隆技术进一步降低性能。P$^{2}$V训练的模型表现稳健。

Conclusion: P$^{2}$V为音频深度伪造检测提供了更鲁棒的基准，未来将公开数据集。

Abstract: Current audio deepfake detectors cannot be trusted. While they excel on
controlled benchmarks, they fail when tested in the real world. We introduce
Perturbed Public Voices (P$^{2}$V), an IRB-approved dataset capturing three
critical aspects of malicious deepfakes: (1) identity-consistent transcripts
via LLMs, (2) environmental and adversarial noise, and (3) state-of-the-art
voice cloning (2020-2025). Experiments reveal alarming vulnerabilities of 22
recent audio deepfake detectors: models trained on current datasets lose 43%
performance when tested on P$^{2}$V, with performance measured as the mean of
F1 score on deepfake audio, AUC, and 1-EER. Simple adversarial perturbations
induce up to 16% performance degradation, while advanced cloning techniques
reduce detectability by 20-30%. In contrast, P$^{2}$V-trained models maintain
robustness against these attacks while generalizing to existing datasets,
establishing a new benchmark for robust audio deepfake detection. P$^{2}$V will
be publicly released upon acceptance by a conference/journal.

</details>


### [46] [LD-LAudio-V1: Video-to-Long-Form-Audio Generation Extension with Dual Lightweight Adapters](https://arxiv.org/abs/2508.11074)
*Haomin Zhang,Kristin Qi,Shuxin Yang,Zihao Chen,Chaofan Ding,Xinhan Di*

Main category: cs.SD

TL;DR: 论文提出了一种名为LD-LAudio-V1的方法，用于从视频生成高质量且时间同步的长音频，解决了现有方法在长音频生成中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对短音频生成（10秒以内）或依赖噪声数据集，无法满足长视频音频合成的需求。

Method: 引入LD-LAudio-V1，扩展了现有视频到音频模型，采用双轻量适配器实现长音频生成，并发布了一个干净的人工标注数据集。

Result: 方法显著减少了拼接伪影和时间不一致性，计算效率高，多项指标显著提升（如FD、KL、IS等）。

Conclusion: LD-LAudio-V1在长视频音频生成中表现优异，发布的数据集将推动该领域进一步研究。

Abstract: Generating high-quality and temporally synchronized audio from video content
is essential for video editing and post-production tasks, enabling the creation
of semantically aligned audio for silent videos. However, most existing
approaches focus on short-form audio generation for video segments under 10
seconds or rely on noisy datasets for long-form video-to-audio zsynthesis. To
address these limitations, we introduce LD-LAudio-V1, an extension of
state-of-the-art video-to-audio models and it incorporates dual lightweight
adapters to enable long-form audio generation. In addition, we release a clean
and human-annotated video-to-audio dataset that contains pure sound effects
without noise or artifacts. Our method significantly reduces splicing artifacts
and temporal inconsistencies while maintaining computational efficiency.
Compared to direct fine-tuning with short training videos, LD-LAudio-V1
achieves significant improvements across multiple metrics: $FD_{\text{passt}}$
450.00 $\rightarrow$ 327.29 (+27.27%), $FD_{\text{panns}}$ 34.88 $\rightarrow$
22.68 (+34.98%), $FD_{\text{vgg}}$ 3.75 $\rightarrow$ 1.28 (+65.87%),
$KL_{\text{panns}}$ 2.49 $\rightarrow$ 2.07 (+16.87%), $KL_{\text{passt}}$ 1.78
$\rightarrow$ 1.53 (+14.04%), $IS_{\text{panns}}$ 4.17 $\rightarrow$ 4.30
(+3.12%), $IB_{\text{score}}$ 0.25 $\rightarrow$ 0.28 (+12.00%),
$Energy\Delta10\text{ms}$ 0.3013 $\rightarrow$ 0.1349 (+55.23%),
$Energy\Delta10\text{ms(vs.GT)}$ 0.0531 $\rightarrow$ 0.0288 (+45.76%), and
$Sem.\,Rel.$ 2.73 $\rightarrow$ 3.28 (+20.15%). Our dataset aims to facilitate
further research in long-form video-to-audio generation and is available at
https://github.com/deepreasonings/long-form-video2audio.

</details>


### [47] [Benchmarking Prosody Encoding in Discrete Speech Tokens](https://arxiv.org/abs/2508.11224)
*Kentaro Onda,Satoru Fukayama,Daisuke Saito,Nobuaki Minematsu*

Main category: cs.SD

TL;DR: 研究分析了自监督学习离散令牌在语音语言模型中捕捉韵律信息的能力，并提出了设计离散令牌的实用指南。


<details>
  <summary>Details</summary>
Motivation: 现有离散令牌通常与语言模型或下游任务分离学习，且缺乏对韵律信息捕捉的研究。

Method: 通过分析离散令牌对人工修改韵律的敏感性，进行全面的韵律编码研究。

Result: 研究揭示了离散令牌在捕捉韵律信息方面的能力。

Conclusion: 为设计离散令牌提供了实用指南，填补了现有研究的空白。

Abstract: Recently, discrete tokens derived from self-supervised learning (SSL) models
via k-means clustering have been actively studied as pseudo-text in speech
language models and as efficient intermediate representations for various
tasks. However, these discrete tokens are typically learned in advance,
separately from the training of language models or downstream tasks. As a
result, choices related to discretization, such as the SSL model used or the
number of clusters, must be made heuristically. In particular, speech language
models are expected to understand and generate responses that reflect not only
the semantic content but also prosodic features. Yet, there has been limited
research on the ability of discrete tokens to capture prosodic information. To
address this gap, this study conducts a comprehensive analysis focusing on
prosodic encoding based on their sensitivity to the artificially modified
prosody, aiming to provide practical guidelines for designing discrete tokens.

</details>


### [48] [Mitigating Category Imbalance: Fosafer System for the Multimodal Emotion and Intent Joint Understanding Challenge](https://arxiv.org/abs/2508.11362)
*Honghong Wang,Yankai Wang,Dejun Zhang,Jing Deng,Rong Zheng*

Main category: cs.SD

TL;DR: Fosafer方法通过多模态数据增强和加权焦点对比损失解决类别不平衡问题，在Track 2 Mandarin挑战中取得第二名的成绩。


<details>
  <summary>Details</summary>
Motivation: 解决多模态情绪和意图联合识别中的类别不平衡问题。

Method: 使用数据增强技术、加权焦点对比损失、Hubert模型微调和模态丢弃策略。

Result: 在Track 2 Mandarin挑战中取得第二名。

Conclusion: Fosafer方法在多模态情绪和意图联合识别中表现优异。

Abstract: This paper presents Fosafer approach to the Track 2 Mandarin in the
Multimodal Emotion and Intent Joint Understandingchallenge, which focuses on
achieving joint recognition of emotion and intent in Mandarin, despite the
issue of category imbalance. To alleviate this issue, we use a variety of data
augmentation techniques across text, video, and audio modalities. Additionally,
we introduce the SampleWeighted Focal Contrastive loss, designed to address the
challenges of recognizing minority class samples and those that are
semantically similar but difficult to distinguish. Moreover, we fine-tune the
Hubert model to adapt the emotion and intent joint recognition. To mitigate
modal competition, we introduce a modal dropout strategy. For the final
predictions, a plurality voting approach is used to determine the results. The
experimental results demonstrate the effectiveness of our method, which
achieves the second-best performance in the Track 2 Mandarin challenge.

</details>


### [49] [Speech Emotion Recognition Using Fine-Tuned DWFormer:A Study on Track 1 of the IERPChallenge 2024](https://arxiv.org/abs/2508.11371)
*Honghong Wang,Xupeng Jia,Jing Deng,Rong Zheng*

Main category: cs.SD

TL;DR: 本文介绍了Fosafer团队在IERP Challenge 2024 Track 1中的工作，通过结合数据增强和分数融合策略，优化了预训练的语音情感识别模型DWFormer，取得了比赛第一名。


<details>
  <summary>Details</summary>
Motivation: 现有情感识别模型多关注离散情感标签预测的精度，而忽略了人格特质对情感表达的影响。IERP Challenge 2024将人格特质引入情感识别研究，以解决个体间情感表达的差异问题。

Method: 团队使用音频特征，结合数据增强和分数融合策略，对预训练的语音情感识别模型DWFormer进行微调。

Result: 在IERP Challenge 2024 Track 1中，Fosafer团队取得了第一名。

Conclusion: 结合人格特质和音频特征的情感识别方法具有潜力，未来可进一步探索多模态融合策略。

Abstract: The field of artificial intelligence has a strong interest in the topic of
emotion recognition. The majority of extant emotion recognition models are
oriented towards enhancing the precision of discrete emotion label prediction.
Given the direct relationship between human personality and emotion, as well as
the significant inter-individual differences in subjective emotional
expression, the IERP Challenge 2024 incorporates personality traits into
emotion recognition research. This paper presents the Fosafer submissions to
the Track 1 of the IERP Challenge 2024. This task primarily concerns the
recognition of emotions in audio, while also providing text and audio features.
In Track 1, we utilized exclusively audio-based features and fine-tuned a
pre-trained speech emotion recognition model, DWFormer, through the integration
of data augmentation and score fusion strategies, thereby achieving the first
place among the participating teams.

</details>


### [50] [Pretrained Conformers for Audio Fingerprinting and Retrieval](https://arxiv.org/abs/2508.11609)
*Kemal Altwlkany,Elmedin Selmanovic,Sead Delalic*

Main category: cs.SD

TL;DR: 利用自监督对比学习框架训练Conformer编码器，生成音频片段的独特嵌入，在音频检索任务中取得最佳效果。


<details>
  <summary>Details</summary>
Motivation: Conformer在语音处理中表现出色，但如何进一步优化其在小音频片段上的表现和泛化能力仍需探索。

Method: 采用自监督对比学习框架训练Conformer编码器，生成短音频片段的嵌入。

Result: 在仅使用3秒音频生成嵌入的情况下，音频检索任务达到最佳效果，且对时间错位和音频失真（如噪声、混响）具有强鲁棒性。

Conclusion: 该方法在音频检索任务中表现优异，代码和模型公开，易于复现。

Abstract: Conformers have shown great results in speech processing due to their ability
to capture both local and global interactions. In this work, we utilize a
self-supervised contrastive learning framework to train conformer-based
encoders that are capable of generating unique embeddings for small segments of
audio, generalizing well to previously unseen data. We achieve state-of-the-art
results for audio retrieval tasks while using only 3 seconds of audio to
generate embeddings. Our models are almost completely immune to temporal
misalignments and achieve state-of-the-art results in cases of other audio
distortions such as noise, reverb or extreme temporal stretching. Code and
models are made publicly available and the results are easy to reproduce as we
train and test using popular and freely available datasets of different sizes.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [51] [Failures to Surface Harmful Contents in Video Large Language Models](https://arxiv.org/abs/2508.10974)
*Yuxin Cao,Wei Song,Derui Wang,Jingling Xue,Jin Song Dong*

Main category: cs.MM

TL;DR: VideoLLMs存在安全漏洞，可能忽略视频中的有害内容，原因包括帧采样不足、空间信息丢失和编码-解码脱节。


<details>
  <summary>Details</summary>
Motivation: 揭示VideoLLMs在生成视频摘要时可能忽略有害内容的安全漏洞，并提出改进需求。

Method: 通过根因分析发现三个设计缺陷，并设计三种零查询黑盒攻击验证漏洞。

Result: 实验显示，主流VideoLLMs在90%以上的情况下忽略有害内容，即使内容明显存在。

Conclusion: 当前VideoLLMs设计存在根本性漏洞，需改进采样策略、令牌压缩和解码机制以确保语义覆盖。

Abstract: Video Large Language Models (VideoLLMs) are increasingly deployed on numerous
critical applications, where users rely on auto-generated summaries while
casually skimming the video stream. We show that this interaction hides a
critical safety gap: if harmful content is embedded in a video, either as
full-frame inserts or as small corner patches, state-of-the-art VideoLLMs
rarely mention the harmful content in the output, despite its clear visibility
to human viewers. A root-cause analysis reveals three compounding design flaws:
(1) insufficient temporal coverage resulting from the sparse, uniformly spaced
frame sampling used by most leading VideoLLMs, (2) spatial information loss
introduced by aggressive token downsampling within sampled frames, and (3)
encoder-decoder disconnection, whereby visual cues are only weakly utilized
during text generation. Leveraging these insights, we craft three zero-query
black-box attacks, aligning with these flaws in the processing pipeline. Our
large-scale evaluation across five leading VideoLLMs shows that the harmfulness
omission rate exceeds 90% in most cases. Even when harmful content is clearly
present in all frames, these models consistently fail to identify it. These
results underscore a fundamental vulnerability in current VideoLLMs' designs
and highlight the urgent need for sampling strategies, token compression, and
decoding mechanisms that guarantee semantic coverage rather than speed alone.

</details>
