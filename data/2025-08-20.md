<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.SD](#cs.SD) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Fair Play in the Newsroom: Actor-Based Filtering Gender Discrimination in Text Corpora](https://arxiv.org/abs/2508.13169)
*Stefanie Urchs,Veronika Thurner,Matthias Aßenmacher,Christian Heumann,Stephanie Thiemichen*

Main category: cs.CL

TL;DR: 这篇论文提出了一种扩展的演员级流水线，用于检测和减少大规模文本语料库中的性别偏见，通过新的演员级指标在德国报纸语料库中实现了显著的性别平衡改善。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型输出常反映训练数据中的结构性性别不平衡问题，需要新方法来检测和减少这些偏见。

Method: 基于话语感知公平性分析的前期工作，提出了新的演员级指标，包括情感、句法主体性和引用风格的不对称性。支持诊断性语料分析和排除基于平衡。

Result: 在taz2024full德国报纸语料库(1980-2024)中实现了多个语言维度上的显著性别平衡改善，表面层面的不对称性可通过筛选和重新平衡来减少，但情感和框架方面的更细微偏见仍持续存在。

Conclusion: 该流水线能够有效检测和减少文本语料库中的性别偏见，并释放了工具和报告以支持更进一步的话语基于公平性审计和公平语料库构建研究。

Abstract: Large language models are increasingly shaping digital communication, yet
their outputs often reflect structural gender imbalances that originate from
their training data. This paper presents an extended actor-level pipeline for
detecting and mitigating gender discrimination in large-scale text corpora.
Building on prior work in discourse-aware fairness analysis, we introduce new
actor-level metrics that capture asymmetries in sentiment, syntactic agency,
and quotation styles. The pipeline supports both diagnostic corpus analysis and
exclusion-based balancing, enabling the construction of fairer corpora. We
apply our approach to the taz2024full corpus of German newspaper articles from
1980 to 2024, demonstrating substantial improvements in gender balance across
multiple linguistic dimensions. Our results show that while surface-level
asymmetries can be mitigated through filtering and rebalancing, subtler forms
of bias persist, particularly in sentiment and framing. We release the tools
and reports to support further research in discourse-based fairness auditing
and equitable corpus construction.

</details>


### [2] [MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents](https://arxiv.org/abs/2508.13186)
*Shilong Li,Xingyuan Bu,Wenjie Wang,Jiaheng Liu,Jun Dong,Haoyang He,Hao Lu,Haozhe Zhang,Chenchen Jing,Zhen Li,Chuanhao Li,Jiayi Tian,Chenchen Zhang,Tianhao Peng,Yancheng He,Jihao Gu,Yuanxing Zhang,Jian Yang,Ge Zhang,Wenhao Huang,Wangchunshu Zhou,Zhaoxiang Zhang,Ruizhe Ding,Shilei Wen*

Main category: cs.CL

TL;DR: MM-BrowseComp是一个新的多模态网页浏览基准测试，包含224个精心设计的问题，专门评估AI代理的多模态检索和推理能力，现有最先进模型仅达到29.02%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的网页浏览基准测试主要关注文本信息，忽略了网页中普遍存在的多模态内容（图像、视频等），需要建立一个专门评估多模态检索和推理能力的基准。

Method: 创建了包含224个挑战性问题的MM-BrowseComp基准，这些问题在提示中包含图像，且搜索推理过程中的关键信息可能嵌入在网页的图像或视频中。为每个问题提供验证清单以进行细粒度分析。

Result: 对最先进模型的评估显示，即使是OpenAI o3等顶级模型也仅达到29.02%的准确率，表明当前模型的多模态能力和原生多模态推理存在严重不足。

Conclusion: MM-BrowseComp基准揭示了当前AI代理在多模态网页浏览方面的显著局限性，强调了开发真正多模态推理能力的必要性。

Abstract: AI agents with advanced reasoning and tool use capabilities have demonstrated
impressive performance in web browsing for deep search. While existing
benchmarks such as BrowseComp evaluate these browsing abilities, they primarily
focus on textual information, overlooking the prevalence of multimodal content.
To bridge this gap, we introduce MM-BrowseComp, a novel benchmark comprising
224 challenging, hand-crafted questions specifically designed to assess agents'
multimodal retrieval and reasoning capabilities. These questions often
incorporate images in prompts, and crucial information encountered during the
search and reasoning process may also be embedded within images or videos on
webpages. Consequently, methods relying solely on text prove insufficient for
our benchmark. Additionally, we provide a verified checklist for each question,
enabling fine-grained analysis of multimodal dependencies and reasoning paths.
Our comprehensive evaluation of state-of-the-art models on MM-BrowseComp
reveals that even top models like OpenAI o3 with tools achieve only 29.02\%
accuracy, highlighting the suboptimal multimodal capabilities and lack of
native multimodal reasoning in current models.

</details>


### [3] [Overcoming Latency Bottlenecks in On-Device Speech Translation: A Cascaded Approach with Alignment-Based Streaming MT](https://arxiv.org/abs/2508.13358)
*Zeeshan Ahmed,Frank Seide,Niko Moritz,Ju Lin,Ruiming Xie,Simone Merello,Zhe Liu,Christian Fuegen*

Main category: cs.CL

TL;DR: 这篇论文提出了一种同步翻译方法，解决在设备上实时流媒体语音识别和机器翻译集成的挑战，在保持语言质量的同时控制延迟。


<details>
  <summary>Details</summary>
Motivation: 虽然现有的语音识别系统能够实时转写，但实现实时流媒体翻译仍面临重大挑战，需要找到语言质量和延迟之间的平衡点。

Method: 提出同步翻译方法，利用ASR系统生成的语言线索来管理上下文，采用高效的检索剪枝技术（如超时和强制终止）来维持系统的实时性能。

Result: 在设备上的双语对话语音翻译中证明，该方法在延迟和质量方面都超过了基准方法，缩小了与非流媒体翻译系统的质量差距。

Conclusion: 该技术为更准确和高效的实时语音翻译开陌了新路径，有力地解决了实时流媒体翻译的挑战。

Abstract: This paper tackles several challenges that arise when integrating Automatic
Speech Recognition (ASR) and Machine Translation (MT) for real-time, on-device
streaming speech translation. Although state-of-the-art ASR systems based on
Recurrent Neural Network Transducers (RNN-T) can perform real-time
transcription, achieving streaming translation in real-time remains a
significant challenge. To address this issue, we propose a simultaneous
translation approach that effectively balances translation quality and latency.
We also investigate efficient integration of ASR and MT, leveraging linguistic
cues generated by the ASR system to manage context and utilizing efficient
beam-search pruning techniques such as time-out and forced finalization to
maintain system's real-time factor. We apply our approach to an on-device
bilingual conversational speech translation and demonstrate that our techniques
outperform baselines in terms of latency and quality. Notably, our technique
narrows the quality gap with non-streaming translation systems, paving the way
for more accurate and efficient real-time speech translation.

</details>


### [4] [Stands to Reason: Investigating the Effect of Reasoning on Idiomaticity Detection](https://arxiv.org/abs/2508.13365)
*Dylan Phelps,Rodrigo Wilkens,Edward Gow-Smith,Thomas Pickard,Maggie Mi,Aline Villavicencio*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型的推理能力对习语检测性能的影响，发现推理效果比预期小且多变，大模型能较好理解习语含义而小模型表现较差。


<details>
  <summary>Details</summary>
Motivation: 利用推理模型提升大型语言模型在逻辑推理任务中的表现，特别是习语检测这种需要先理解再推理的语言任务。

Method: 评估DeepSeek-R1蒸馏模型系列（1.5B到70B参数）在四个习语检测数据集上的表现，分析思维链推理和模型大小的影响。

Result: 小模型通过思维链推理有所提升但不及基础模型水平，大模型（14B、32B、70B）表现适度改善；大模型能准确理解习语含义，小模型则经常失败。

Conclusion: 模型大小对习语理解能力有显著影响，为小模型提供定义提示可以在某些情况下改善性能。

Abstract: The recent trend towards utilisation of reasoning models has improved the
performance of Large Language Models (LLMs) across many tasks which involve
logical steps. One linguistic task that could benefit from this framing is
idiomaticity detection, as a potentially idiomatic expression must first be
understood before it can be disambiguated and serves as a basis for reasoning.
In this paper, we explore how reasoning capabilities in LLMs affect
idiomaticity detection performance and examine the effect of model size. We
evaluate, as open source representative models, the suite of DeepSeek-R1
distillation models ranging from 1.5B to 70B parameters across four
idiomaticity detection datasets. We find the effect of reasoning to be smaller
and more varied than expected. For smaller models, producing chain-of-thought
(CoT) reasoning increases performance from Math-tuned intermediate models, but
not to the levels of the base models, whereas larger models (14B, 32B, and 70B)
show modest improvements. Our in-depth analyses reveal that larger models
demonstrate good understanding of idiomaticity, successfully producing accurate
definitions of expressions, while smaller models often fail to output the
actual meaning. For this reason, we also experiment with providing definitions
in the prompts of smaller models, which we show can improve performance in some
cases.

</details>


### [5] [Whispering Context: Distilling Syntax and Semantics for Long Speech Transcripts](https://arxiv.org/abs/2508.13376)
*Duygu Altinok*

Main category: cs.CL

TL;DR: 通过从LLaMA模型精炼上下文知识到Whisper中，提出了两种策略来提升长音频ASR转写的语法和语义准确性


<details>
  <summary>Details</summary>
Motivation: ASR系统在长音频转写中常常无法维持语法和语义的准确性，影响命名实体识别、大写化和标点符号等任务的性能

Method: 1）使用最优运输的标记级利精炼来对齐维度和序列长度；2）最小化Whisper和LLaMA句子嵌入表示之间的表征损失，融合语法和语义

Result: 在Spoken Wikipedia数据集上评估显示，在词错误率、命名实体识别、大写化和标点符号成功率方面都取得了显著改进

Conclusion: 该研究强调了将语言上下文集成到转写中的价值，为长语音中健壮、具有上下文意识的ASR系统奠定了基础

Abstract: ASR systems often struggle with maintaining syntactic and semantic accuracy
in long audio transcripts, impacting tasks like Named Entity Recognition (NER),
capitalization, and punctuation. We propose a novel approach that enhances ASR
by distilling contextual knowledge from LLaMA models into Whisper. Our method
uses two strategies: (1) token level distillation with optimal transport to
align dimensions and sequence lengths, and (2) representation loss minimization
between sentence embeddings of Whisper and LLaMA, blending syntax and
semantics. Evaluations on the Spoken Wikipedia dataset, a benchmark with long
audios and rich entities demonstrate significant improvements in Word Error
Rate (WER), NER, capitalization, and punctuation success. By introducing novel
NER metrics and exploring semantics aware ASR, our work highlights the value of
integrating linguistic context into transcription, setting a foundation for
robust, context-aware ASR in longform speech.

</details>


### [6] [Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data Analysis](https://arxiv.org/abs/2508.13382)
*Ayoub Ben Chaliah,Hela Dellagi*

Main category: cs.CL

TL;DR: Datarus-R1-14B是一个14B参数的开源语言模型，基于Qwen 2.5-14B-Instruct微调，专门用于虚拟数据分析和研究生级别问题求解。通过轨迹式训练、双奖励框架和GRPO优化，在多个基准测试中超越同规模模型，准确率提升30%，同时减少18-49%的token使用。


<details>
  <summary>Details</summary>
Motivation: 解决传统LLM在复杂问题求解中存在的格式崩溃、冗余循环和推理效率低下的问题，通过完整的分析轨迹训练来提升模型的推理能力和执行效率。

Method: 使用轨迹中心合成数据生成器创建14.4万个标记笔记本片段，采用双奖励框架（结构信号+分层奖励模型），结合GRPO优化技术，支持双推理模式（代理模式和反思模式）。

Result: 在AIME 2024/2025和LiveCodeBench等基准测试中准确率提升30%，比同类模型减少18-49%的token使用，达到甚至超越更大规模模型（如QwQ-32B）的性能水平。

Conclusion: Datarus通过轨迹式训练和双推理接口设计，有效解决了LLM在复杂问题求解中的常见问题，展示了在保持高效推理的同时显著提升准确性的能力，为专业级AI助手的发展提供了新方向。

Abstract: We present Datarus-R1-14B, a 14 B-parameter open-weights language model
fine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and
graduate-level problem solver. Datarus is trained not on isolated
question-answer pairs but on full analytical trajectories including reasoning
steps, code execution, error traces, self-corrections, and final conclusions,
all captured in a ReAct-style notebook format spanning finance, medicine,
numerical analysis, and other quantitative domains. Our training pipeline
combines (i) a trajectory-centric synthetic data generator that yielded 144 000
tagged notebook episodes, (ii) a dual-reward framework blending a lightweight
tag-based structural signal with a Hierarchical Reward Model (HRM) that scores
both single-step soundness and end-to-end coherence, and (iii) a
memory-optimized implementation of Group Relative Policy Optimization (GRPO)
featuring KV-cache reuse, sequential generation, and reference-model sharding.
A cosine curriculum smoothly shifts emphasis from structural fidelity to
semantic depth, reducing the format collapse and verbosity that often plague
RL-aligned LLMs. A central design choice in Datarus is it dual reasoning
interface. In agentic mode the model produces ReAct-tagged steps that invoke
Python tools to execute real code; in reflection mode it outputs compact
Chain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On
demanding postgraduate-level problems, Datarus exhibits an "AHA-moment"
pattern: it sketches hypotheses, revises them once or twice, and converges
avoiding the circular, token-inflating loops common to contemporary systems.
Across standard public benchmarks Datarus surpasses similar size models and
even reaches the level of larger reasoning models such as QwQ-32B achieving up
to 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting
18-49% fewer tokens per solution.

</details>


### [7] [ALIGN: Word Association Learning for Cross-Cultural Generalization in Large Language Models](https://arxiv.org/abs/2508.13426)
*Chunhua Liu,Kabir Manandhar Shrestha,Sukai Huang*

Main category: cs.CL

TL;DR: 通过参数效率微调基于本土语者词语联想规范的文化对齐方法，在不需重新训练的情况下显著提升了语言模型的文化对齐能力


<details>
  <summary>Details</summary>
Motivation: 解决LLM在跨文化交流中反映训练语料偏见的问题，对准文化对齐的挑战

Method: 使用本土语者自由词语联想规范进行监督微调(SFT)和PPO偏好优化，基于英语-美国和普通话联想数据

Result: 在词语联想准确率提升16-165%，价值观问卷答案分布向目标文化偏移，7-8B模型效果超迈原始70B模型

Conclusion: 少量文化基础联想数据能有效实现价值对齐，强调基于人类认知的文化对齐研究的重要性

Abstract: As large language models (LLMs) increasingly mediate cross-cultural
communication, their behavior still reflects the distributional bias of the
languages and viewpoints that are over-represented in their pre-training
corpora. Yet, it remains a challenge to model and align culture due to limited
cultural knowledge and a lack of exploration into effective learning
approaches. We introduce a cost-efficient, cognitively grounded remedy:
parameter-efficient fine-tuning on native speakers' free word-association
norms, which encode implicit cultural schemas. Leveraging English-US and
Mandarin associations from the Small-World-of-Words project, we adapt
Llama-3.1-8B and Qwen-2.5-7B via supervised fine-tuning (SFT) and PPO-based
preference optimization. SFT boosts held-out association Precision at 5 by
16-20% in English and 43-165% in Mandarin, lifts median concreteness by +0.20,
and attains human-level valence and arousal. These lexical gains transfer: on
World-Values-Survey questions, fine-tuned models shift answer distributions
toward the target culture, and on a 50-item high-tension subset, Qwen's
Chinese-aligned responses double while Llama's US bias drops by one-third. Our
7-8B models rival or beat vanilla 70B baselines, showing that a few million
culture-grounded associations can instill value alignment without costly
retraining. Our work highlights both the promise and the need for future
research grounded in human cognition in improving cultural alignment in AI
models.

</details>


### [8] [ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs](https://arxiv.org/abs/2508.13514)
*Hongxin Ding,Baixiang Huang,Yue Fang,Weibin Liao,Xinke Jiang,Zheng Li,Junfeng Zhao,Yasha Wang*

Main category: cs.CL

TL;DR: ProMed是一个强化学习框架，通过Shapley信息增益奖励机制，使医疗大语言模型从被动回答转向主动提问的交互范式，显著提升医疗诊断准确性


<details>
  <summary>Details</summary>
Motivation: 现有医疗大语言模型主要采用被动回答模式，缺乏主动信息收集能力，在真实临床交互场景中存在误诊风险

Method: 提出基于Shapley信息增益(SIG)奖励的强化学习框架，包含SIG引导模型初始化和SIG增强策略优化两阶段训练，使用蒙特卡洛树搜索构建高质量交互轨迹

Result: 在两个新构建的部分信息医疗基准测试中，ProMed平均优于最先进方法6.29%，相比被动范式提升54.45%，且在域外案例中表现稳健

Conclusion: ProMed成功实现了医疗LLM从被动到主动交互范式的转变，通过量化临床价值的问题提问显著提升了诊断准确性，为智能医疗咨询系统提供了有效解决方案

Abstract: Interactive medical questioning is essential in real-world clinical
consultations, where physicians must actively gather information from patients.
While medical Large Language Models (LLMs) have shown impressive capabilities
in static medical question answering, they predominantly operate under a
reactive paradigm: generating answers directly without seeking additional
information, which risks incorrect diagnoses in such interactive settings. To
address this limitation, we propose ProMed, a reinforcement learning (RL)
framework that transitions medical LLMs toward a proactive paradigm, equipping
them with the ability to ask clinically valuable questions before
decision-making. At the core of ProMed is the Shapley Information Gain (SIG)
reward, which quantifies the clinical utility of each question by combining the
amount of newly acquired information with its contextual importance, estimated
via Shapley values. We integrate SIG into a two-stage training pipeline: (1)
SIG-Guided Model Initialization uses Monte Carlo Tree Search (MCTS) to
construct high-reward interaction trajectories to supervise the model, and (2)
SIG-Augmented Policy Optimization, which integrates SIG and enhances RL with a
novel SIG-guided Reward Distribution Mechanism that assigns higher rewards to
informative questions for targeted optimization. Extensive experiments on two
newly curated partial-information medical benchmarks demonstrate that ProMed
significantly outperforms state-of-the-art methods by an average of 6.29% and
delivers a 54.45% gain over the reactive paradigm, while also generalizing
robustly to out-of-domain cases.

</details>


### [9] [Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation](https://arxiv.org/abs/2508.13525)
*Hassan Barmandah*

Main category: cs.CL

TL;DR: 通过LoRA微调ALLAM-7B模型，使其能够生成沙特阿拉伯方言（哈吉洛和纳吉德），显著提升了方言控制能力和文本保真度


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语言模型主要支持现代标准阿拉伯语，对沙特方言支持有限，影响了对真实方言变体的抓取能力

Method: 使用私有沙特方言指令数据集（5,466条合成指令-响应对），通过LoRA微调ALLAM-7B模型，测试了两种方案：方言标签训练和无标签训练

Result: 方言标签模型表现最佳，将沙特方言生成率从47.97%提升到84.21%，并降低了标准阿语泄漏，文本保真度也有显著提升，超越了多个基线模型

Conclusion: 通过显式方言标签和LoRA微调技术，可以有效地使大语言模型适配特定方言，为低资源语言的模型开发提供了可行方案

Abstract: Large language models (LLMs) for Arabic are still dominated by Modern
Standard Arabic (MSA), with limited support for Saudi dialects such as Najdi
and Hijazi. This underrepresentation hinders their ability to capture authentic
dialectal variation. Using a privately curated Saudi Dialect Instruction
dataset (Hijazi and Najdi; 5,466 synthetic instruction-response pairs; 50/50
split), we LoRA-tune ALLaM-7B-Instruct-preview, the first foundation model
developed in Saudi Arabia, for Saudi dialect generation. We investigate two
variants: (i) Dialect-Token training, which prepends an explicit dialect tag to
the instruction, and (ii) No-Token training, which omits the tag at formatting
time. Evaluation on a held-out test set combines an external dialect classifier
with text fidelity metrics (chrF++ and BERTScore) and diversity measures. The
Dialect-Token model achieves the best control, raising the Saudi rate from
47.97% to 84.21% and reducing MSA leakage from 32.63% to 6.21%; fidelity also
improves (chrF++ +3.53, BERTScore +0.059). Both LoRA variants outperform strong
generic instruction models (Falcon-7B-Instruct, Llama-3.1-8B-Instruct,
Qwen-2.5-7B-Instruct, AceGPT-v2-8B-Chat, JAIS-13B-Chat) in dialect control and
fidelity, while avoiding metadata-tag echoing that these baselines frequently
exhibit. We do not release the dataset or any model weights/adapters; instead,
we release training/evaluation/inference code and a detailed datasheet (schema
and aggregate statistics) to support independent verification.

</details>


### [10] [MATA (māta): Mindful Assessment of the Telugu Abilities of Large Language Models](https://arxiv.org/abs/2508.13526)
*Chalamalasetti Kranti,Sowmya Vajjala*

Main category: cs.CL

TL;DR: 这篇论文介绍了MATA数据集，用于评估大语言模型在泰卢固语中的表现，包含729道多选题和开放式问题，并对11个模型进行了细粒度分析。


<details>
  <summary>Details</summary>
Motivation: 为了评估大语言模型在泰卢固语这种低资源语言中的表现，理解模型的局限性，并为发展更强的语言模型提供基础。

Method: 构建了包含729道多选题和开放式问题的MATA数据集，涉及多样化的语言维度，对11个开源和闭源LLM进行评测，分析模型依赖的浅层算法和判断模式。

Result: 实验显示LLM在多选题中依赖答案位置和干扰项模式等浅层算法，对比了LLM作为判官与人类评估在开放式问题上的可靠性差异。

Conclusion: 细粒度评估对理解模型局限性至关重要，能够指导更强语言能力模型的开发，同时为泰卢固语NLP研究奠定基础。

Abstract: In this paper, we introduce MATA, a novel evaluation dataset to assess the
ability of Large Language Models (LLMs) in Telugu language, comprising 729
carefully curated multiple-choice and open-ended questions that span diverse
linguistic dimensions. We evaluate 11 open-weight and closed-source LLMs on our
dataset and present a fine-grained analysis of their performance. Further, we
empirically show how LLMs rely on superficial heuristics such as answer
position and distractor patterns for multiple-choice questions. Finally, we
also compare LLM-as-a-judge evaluation with human evaluation for open-ended
questions and draw some conclusions on its reliability in a low-resource
language. We argue that such fine-grained evaluation is essential for
understanding model limitations and can inform the development of more
linguistically capable LLMs, while also serving as a foundation for future
research in Telugu NLP.

</details>


### [11] [Compressed Models are NOT Trust-equivalent to Their Large Counterparts](https://arxiv.org/abs/2508.13533)
*Rohit Raj Rai,Chirag Kothari,Siddhesh Shelke,Amit Awekar*

Main category: cs.CL

TL;DR: 模型压缩后虽然精度相近，但在解释性和检验性方面与原始大模型存在显著差异，不能直接作为可信赖的替代品


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注压缩对模型精度的影响，但性能平等并不保证可信赖性相等，需要从解释性和检验性两个维度评估压缩模型的可信赖等效性

Method: 提出了两维度的可信赖性评估框架：1）解释性对齐（使用LIME和SHAP测试验证模型是否基于相同特征做决策）；2）检验相似性（通过ECE、MCE、Brier分数和可靠性图评估模型预测概率的可靠性）

Result: 实验结果显示，压缩模型与原始BERT-base模型在解释性对齐度上很低，检验相似性也存在显著差异，即使在精度几乎相同的情况下也如此

Conclusion: 压缩模型并不与大模型可信赖等效，在部署时需要进行谨慎评估，不能仅以性能指标作为唯一判断标准

Abstract: Large Deep Learning models are often compressed before being deployed in a
resource-constrained environment. Can we trust the prediction of compressed
models just as we trust the prediction of the original large model? Existing
work has keenly studied the effect of compression on accuracy and related
performance measures. However, performance parity does not guarantee
trust-equivalence. We propose a two-dimensional framework for trust-equivalence
evaluation. First, interpretability alignment measures whether the models base
their predictions on the same input features. We use LIME and SHAP tests to
measure the interpretability alignment. Second, calibration similarity measures
whether the models exhibit comparable reliability in their predicted
probabilities. It is assessed via ECE, MCE, Brier Score, and reliability
diagrams. We conducted experiments using BERT-base as the large model and its
multiple compressed variants. We focused on two text classification tasks:
natural language inference and paraphrase identification. Our results reveal
low interpretability alignment and significant mismatch in calibration
similarity. It happens even when the accuracies are nearly identical between
models. These findings show that compressed models are not trust-equivalent to
their large counterparts. Deploying compressed models as a drop-in replacement
for large models requires careful assessment, going beyond performance parity.

</details>


### [12] [A Comparative Study of Decoding Strategies in Medical Text Generation](https://arxiv.org/abs/2508.13580)
*Oriana Presacan,Alireza Nik,Vajira Thambawita,Bogdan Ionescu,Michael Riegler*

Main category: cs.CL

TL;DR: 医疗领域LLM解码策略研究：确定性方法优于随机方法，beam search表现最佳，医疗专用模型在部分任务中表现更好但对解码策略更敏感


<details>
  <summary>Details</summary>
Motivation: 在医疗领域文本生成中，解码策略对输出质量影响显著但研究不足，需要系统评估不同解码策略在医疗任务中的表现

Method: 在5个开放式医疗任务（翻译、摘要、问答、对话、图像描述）中评估11种解码策略，使用医疗专用和通用LLM不同尺寸模型

Result: 确定性策略普遍优于随机策略，beam search得分最高；大模型整体表现更好但推理时间更长；医疗LLM在两个任务中表现更好但对解码策略更敏感；不同评估指标相关性因任务而异

Conclusion: 医疗应用中需要谨慎选择解码方法，其影响有时甚至超过模型选择本身，MAUVE等指标与BERTScore、ROUGE相关性较弱且对解码策略更敏感

Abstract: Large Language Models (LLMs) rely on various decoding strategies to generate
text, and these choices can significantly affect output quality. In healthcare,
where accuracy is critical, the impact of decoding strategies remains
underexplored. We investigate this effect in five open-ended medical tasks,
including translation, summarization, question answering, dialogue, and image
captioning, evaluating 11 decoding strategies with medically specialized and
general-purpose LLMs of different sizes. Our results show that deterministic
strategies generally outperform stochastic ones: beam search achieves the
highest scores, while {\eta} and top-k sampling perform worst. Slower decoding
methods tend to yield better quality. Larger models achieve higher scores
overall but have longer inference times and are no more robust to decoding.
Surprisingly, while medical LLMs outperform general ones in two of the five
tasks, statistical analysis shows no overall performance advantage and reveals
greater sensitivity to decoding choice. We further compare multiple evaluation
metrics and find that correlations vary by task, with MAUVE showing weak
agreement with BERTScore and ROUGE, as well as greater sensitivity to the
decoding strategy. These results highlight the need for careful selection of
decoding methods in medical applications, as their influence can sometimes
exceed that of model choice.

</details>


### [13] [Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of a Speech-LLM](https://arxiv.org/abs/2508.13603)
*Dariia Puhach,Amir H. Payberah,Éva Székely*

Main category: cs.CL

TL;DR: 这篇论文研究了语音大语言模型(Speech-LLMs)的性别偏见问题，通过分析Bark TTS模型的默认讲话人分配来识别模型中的性别偏向


<details>
  <summary>Details</summary>
Motivation: 虽然语音大语言模型与文本LLMs都具有出现能力和上下文意识，但是否同样存在性别偏见仍是个待解决的问题。语音模型需要生成具有性别特征的声音，这使得讲话人选择成为显性的偏见线索

Method: 构建了两个数据集：(i)职业数据集（包含性别刻板印象的职业）和(ii)性别色彩词汇数据集（具有性别含义的词语），通过分析Bark TTS模型对文本提示的默认讲话人分配来识别偏见模式

Result: 虽然Bark模型没有显示出系统性的偏见，但证明了具有性别意识，并且存在一些性别倾向

Conclusion: 这种通过讲话人分配分析性别偏见的方法论为评估语音大语言模型的偏见提供了有效工具，虽然模型没有显著偏见，但性别意识的存在仍需要关注

Abstract: Similar to text-based Large Language Models (LLMs), Speech-LLMs exhibit
emergent abilities and context awareness. However, whether these similarities
extend to gender bias remains an open question. This study proposes a
methodology leveraging speaker assignment as an analytic tool for bias
investigation. Unlike text-based models, which encode gendered associations
implicitly, Speech-LLMs must produce a gendered voice, making speaker selection
an explicit bias cue. We evaluate Bark, a Text-to-Speech (TTS) model, analyzing
its default speaker assignments for textual prompts. If Bark's speaker
selection systematically aligns with gendered associations, it may reveal
patterns in its training data or model design. To test this, we construct two
datasets: (i) Professions, containing gender-stereotyped occupations, and (ii)
Gender-Colored Words, featuring gendered connotations. While Bark does not
exhibit systematic bias, it demonstrates gender awareness and has some gender
inclinations.

</details>


### [14] [AdaDocVQA: Adaptive Framework for Long Document Visual Question Answering in Low-Resource Settings](https://arxiv.org/abs/2508.13606)
*Haoxuan Li,Wei Song,Aofan Liu,Peiwu Qin*

Main category: cs.CL

TL;DR: AdaDocVQA是一个自适应框架，通过混合文本检索架构、智能数据增强和自适应集成推理，解决了低资源环境下长文档VQA的挑战，在日语文档VQA基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 解决低资源环境下处理长文档时面临的上下文限制和训练数据不足的问题，特别是在日语等资源有限的语言环境中。

Method: 提出三个核心创新：混合文本检索架构进行文档分割、智能数据增强管道自动生成高质量问答对、自适应集成推理机制。

Result: 在JDocQA数据集上，Yes/No问题准确率达到83.04%，事实性问题52.66%，数值问题44.12%；在LAVA数据集上达到59%准确率，创下日语文档VQA的新SOTA。

Conclusion: 该框架为低资源语言和专门领域提供了可扩展的基础，每个组件都有显著贡献，在日语文档VQA任务中表现出色。

Abstract: Document Visual Question Answering (Document VQA) faces significant
challenges when processing long documents in low-resource environments due to
context limitations and insufficient training data. This paper presents
AdaDocVQA, a unified adaptive framework addressing these challenges through
three core innovations: a hybrid text retrieval architecture for effective
document segmentation, an intelligent data augmentation pipeline that
automatically generates high-quality reasoning question-answer pairs with
multi-level verification, and adaptive ensemble inference with dynamic
configuration generation and early stopping mechanisms. Experiments on Japanese
document VQA benchmarks demonstrate substantial improvements with 83.04\%
accuracy on Yes/No questions, 52.66\% on factual questions, and 44.12\% on
numerical questions in JDocQA, and 59\% accuracy on LAVA dataset. Ablation
studies confirm meaningful contributions from each component, and our framework
establishes new state-of-the-art results for Japanese document VQA while
providing a scalable foundation for other low-resource languages and
specialized domains. Our code available at:
https://github.com/Haoxuanli-Thu/AdaDocVQA.

</details>


### [15] [CRISP: Persistent Concept Unlearning via Sparse Autoencoders](https://arxiv.org/abs/2508.13650)
*Tomer Ashuach,Dana Arad,Aaron Mueller,Martin Tutek,Yonatan Belinkov*

Main category: cs.CL

TL;DR: CRISP是一种基于稀疏自编码器的参数高效方法，用于实现大语言模型中的持久性概念遗忘，通过自动识别并抑制多层的显著特征来实现精确的知识移除。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在现实应用中的部署增加，需要选择性移除有害知识同时保持模型效用的方法。现有稀疏自编码器方法主要在推理时干预，无法实现参数层面的持久改变，容易被恶意行为者绕过。

Method: CRISP使用稀疏自编码器自动识别多个层中的显著特征，并抑制这些特征的激活，实现参数高效的持久性概念遗忘。

Result: 在两个大语言模型上的实验表明，CRISP在WMDP基准测试的安全关键遗忘任务上优于先前方法，成功移除有害知识的同时保持通用和领域内能力。特征级分析显示该方法实现了目标和良性概念的语义连贯分离。

Conclusion: CRISP提供了一种有效的参数级持久概念遗忘方法，能够精确抑制目标特征，为大语言模型的安全部署提供了重要技术手段。

Abstract: As large language models (LLMs) are increasingly deployed in real-world
applications, the need to selectively remove unwanted knowledge while
preserving model utility has become paramount. Recent work has explored sparse
autoencoders (SAEs) to perform precise interventions on monosemantic features.
However, most SAE-based methods operate at inference time, which does not
create persistent changes in the model's parameters. Such interventions can be
bypassed or reversed by malicious actors with parameter access. We introduce
CRISP, a parameter-efficient method for persistent concept unlearning using
SAEs. CRISP automatically identifies salient SAE features across multiple
layers and suppresses their activations. We experiment with two LLMs and show
that our method outperforms prior approaches on safety-critical unlearning
tasks from the WMDP benchmark, successfully removing harmful knowledge while
preserving general and in-domain capabilities. Feature-level analysis reveals
that CRISP achieves semantically coherent separation between target and benign
concepts, allowing precise suppression of the target features.

</details>


### [16] [ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?](https://arxiv.org/abs/2508.13680)
*Vy Tuong Dang,An Vo,Quang Tau,Duc Dm,Daeyoung Kim*

Main category: cs.CL

TL;DR: 本文提出了ViExam基准测试，首次全面评估了视觉语言模型在越南语多模态教育考试中的表现，发现当前最先进的VLM仅达到57.74%的准确率，远低于人类平均水平66.54%和最佳表现99.60%。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型在英语多模态任务上表现出色，但它们在低资源语言（如越南语）上的多模态推理能力尚未得到充分探索，特别是在真实教育评估场景中。

Method: 构建ViExam基准测试，包含2,548个多模态越南语教育问题，涵盖7个学术领域。测试了主流VLM模型，并尝试了跨语言提示和人类协作等策略。

Result: 最先进VLM平均准确率57.74%，开源模型27.70%。只有思考型VLM o3（74.07%）超过人类平均水平，但仍远低于人类最佳表现。跨语言提示反而降低性能，人类协作可提升5个百分点。

Conclusion: 当前VLM在越南语多模态教育评估中表现不佳，跨语言迁移能力有限，需要针对低资源语言开发更有效的多模态模型和训练策略。

Abstract: Vision language models (VLMs) demonstrate remarkable capabilities on English
multimodal tasks, but their performance on low-resource languages with
genuinely multimodal educational content remains largely unexplored. In this
work, we test how VLMs perform on Vietnamese educational assessments,
investigating whether VLMs trained predominantly on English data can handle
real-world cross-lingual multimodal reasoning. Our work presents the first
comprehensive evaluation of VLM capabilities on multimodal Vietnamese exams
through proposing ViExam, a benchmark containing 2,548 multimodal questions. We
find that state-of-the-art VLMs achieve only 57.74% while open-source models
achieve 27.70% mean accuracy across 7 academic domains, including Mathematics,
Physics, Chemistry, Biology, Geography, Driving Test, and IQ Test. Most VLMs
underperform average human test-takers (66.54%), with only the thinking VLM o3
(74.07%) exceeding human average performance, yet still falling substantially
short of human best performance (99.60%). Cross-lingual prompting with English
instructions while maintaining Vietnamese content fails to improve performance,
decreasing accuracy by 1 percentage point for SOTA VLMs. Human-in-the-loop
collaboration can partially improve VLM performance by 5 percentage points.
Code and data are available at: https://vi-exam.github.io.

</details>


### [17] [Generics and Default Reasoning in Large Language Models](https://arxiv.org/abs/2508.13718)
*James Ravi Kirkpatrick,Rachel Katharine Sterken*

Main category: cs.CL

TL;DR: 这篇论文评估了28个大型语言模型在包含通用概括的20种可撤销推理模式上的表现，发现前沿模型在默认推理上有一定能力但性能差异较大，链式提示通常会导致性能严重下降。


<details>
  <summary>Details</summary>
Motivation: 通用概括在语言学、哲学、逻辑学和认知科学中具有重要地位，它们允许异常情况的复杂性对默认推理、认知和概念获取至关重要。研究者想了解当前大型语言模型在这些基础推理能力上的表现。

Method: 研究使用20种可撤销推理模式来评测28个LLM的表现，测试了不同提示方式（零次提示、少次提示、链式提示）对模型性能的影响。

Result: 结果显示：前沿模型在默认推理问题上表现良好，但不同模型间性能差异显著；少次提示对某些模型有轻微改善，但链式提示导致性能严重下降（在零次提示准确率超过75%的模型中，平均准确率下降11.14%）；大多数模型无法区分可撤销推理和式推理，或将通用概括误解为普遍声明。

Conclusion: 这些发现呈现了当前LLM在默认推理方面的潜力与限制，展示了它们在处理复杂逻辑问题时的优势和短板。

Abstract: This paper evaluates the capabilities of 28 large language models (LLMs) to
reason with 20 defeasible reasoning patterns involving generic generalizations
(e.g., 'Birds fly', 'Ravens are black') central to non-monotonic logic.
Generics are of special interest to linguists, philosophers, logicians, and
cognitive scientists because of their complex exception-permitting behaviour
and their centrality to default reasoning, cognition, and concept acquisition.
We find that while several frontier models handle many default reasoning
problems well, performance varies widely across models and prompting styles.
Few-shot prompting modestly improves performance for some models, but
chain-of-thought (CoT) prompting often leads to serious performance degradation
(mean accuracy drop -11.14%, SD 15.74% in models performing above 75% accuracy
in zero-shot condition, temperature 0). Most models either struggle to
distinguish between defeasible and deductive inference or misinterpret generics
as universal statements. These findings underscore both the promise and limits
of current LLMs for default reasoning.

</details>


### [18] [Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings](https://arxiv.org/abs/2508.13729)
*Hanna Herasimchyk,Alhassan Abdelhalim,Sören Laue,Michaela Regneri*

Main category: cs.CL

TL;DR: 本文挑战了通过预测准确率来评估词嵌入中语义知识编码的常见方法，证明即使随机信息也能被成功预测，表明这种方法主要反映向量空间几何相似性而非真正的语义表征。


<details>
  <summary>Details</summary>
Motivation: 理解深度学习模型中隐含编码的知识对于提高AI系统可解释性至关重要。现有方法通过将词嵌入映射到人类可解释的语义特征来评估知识编码，但作者质疑仅凭预测准确率是否能可靠反映真正的语义知识。

Method: 通过分析常见词嵌入解释方法，证明这些方法即使对随机信息也能获得高预测准确率，表明预测性能主要受算法上限限制而非语义内容决定。

Result: 研究发现预测准确率不能可靠指示词嵌入中真正的基于特征的可解释性，比较不同数据集时仅基于预测性能无法确定哪个数据集被词嵌入更好捕获。

Conclusion: 词嵌入到语义特征的映射主要反映向量空间内的几何相似性，而非语义属性的真正涌现，需要更可靠的方法来评估词嵌入中的知识编码。

Abstract: Understanding what knowledge is implicitly encoded in deep learning models is
essential for improving the interpretability of AI systems. This paper examines
common methods to explain the knowledge encoded in word embeddings, which are
core elements of large language models (LLMs). These methods typically involve
mapping embeddings onto collections of human-interpretable semantic features,
known as feature norms. Prior work assumes that accurately predicting these
semantic features from the word embeddings implies that the embeddings contain
the corresponding knowledge. We challenge this assumption by demonstrating that
prediction accuracy alone does not reliably indicate genuine feature-based
interpretability.
  We show that these methods can successfully predict even random information,
concluding that the results are predominantly determined by an algorithmic
upper bound rather than meaningful semantic representation in the word
embeddings. Consequently, comparisons between datasets based solely on
prediction performance do not reliably indicate which dataset is better
captured by the word embeddings. Our analysis illustrates that such mappings
primarily reflect geometric similarity within vector spaces rather than
indicating the genuine emergence of semantic properties.

</details>


### [19] [EEG-MedRAG: Enhancing EEG-based Clinical Decision-Making via Hierarchical Hypergraph Retrieval-Augmented Generation](https://arxiv.org/abs/2508.13735)
*Yi Wang,Haoran Luo,Lu Meng*

Main category: cs.CL

TL;DR: EEG-MedRAG是一个基于超图的三层检索增强生成框架，用于统一EEG领域知识、患者病例和大规模存储库，实现联合语义-时间检索和因果链诊断生成。


<details>
  <summary>Details</summary>
Motivation: 随着脑电图(EEG)在神经科学和临床实践中的广泛应用，高效检索和语义解释大规模、多源、异构的EEG数据成为迫切挑战。

Method: 提出三层超图框架，将EEG领域知识、个体患者病例和大规模存储库统一为可遍历的n元关系超图，支持联合语义-时间检索和因果链诊断生成。

Result: 实验显示EEG-MedRAG在答案准确性和检索性能上显著优于TimeRAG和HyperGraphRAG，展现了在真实临床决策支持中的强大潜力。

Conclusion: 该框架为解决大规模EEG数据检索和解释问题提供了有效解决方案，具有重要的临床应用价值。

Abstract: With the widespread application of electroencephalography (EEG) in
neuroscience and clinical practice, efficiently retrieving and semantically
interpreting large-scale, multi-source, heterogeneous EEG data has become a
pressing challenge. We propose EEG-MedRAG, a three-layer hypergraph-based
retrieval-augmented generation framework that unifies EEG domain knowledge,
individual patient cases, and a large-scale repository into a traversable n-ary
relational hypergraph, enabling joint semantic-temporal retrieval and
causal-chain diagnostic generation. Concurrently, we introduce the first
cross-disease, cross-role EEG clinical QA benchmark, spanning seven disorders
and five authentic clinical perspectives. This benchmark allows systematic
evaluation of disease-agnostic generalization and role-aware contextual
understanding. Experiments show that EEG-MedRAG significantly outperforms
TimeRAG and HyperGraphRAG in answer accuracy and retrieval, highlighting its
strong potential for real-world clinical decision support. Our data and code
are publicly available at https://github.com/yi9206413-boop/EEG-MedRAG.

</details>


### [20] [Sycophancy under Pressure: Evaluating and Mitigating Sycophantic Bias via Adversarial Dialogues in Scientific QA](https://arxiv.org/abs/2508.13743)
*Kaiwei Zhang,Qi Jia,Zijian Chen,Wei Sun,Xiangyang Zhu,Chunyi Li,Dandan Zhu,Guangtao Zhai*

Main category: cs.CL

TL;DR: 这篇论文研究大语言模型的奇恶珍御现象（sycophancy），即模型不顾事实而绝对尊重用户信念的倾向，并提出了评估框架和缓解方法Pressure-Tune。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在需要事实严谨的领域中常常显示奇恶珍御行为，这种倾向在科学问答等高风险场景中带来严重风险，但目前这个现象在事实问答上得到的关注不够。

Method: 提出统一的评估框架来量化奇恶珍御上下文对模型行为的影响，包括对抗性提问设置和目标指标。还提出了Pressure-Tune方法，通过在合成对抗性对话上进行轻量微调，配合思维链理由来拒绝用户错误信息并强化事实接纳。

Result: 对开源和专有模型的系统评估显示了普遍的奇恶珍御倾向，这更多由对齐策略驱动而非模型大小。Pressure-Tune在具有挑战性的科学问答测试中显著提高了奇恶珍御抵抗力，而不影响准确性或对有效反馈的响应能力。

Conclusion: 奇恶珍御是大语言模型的一个普遍问题，特别是在高风险科学场景中。Pressure-Tune提供了一种实用的解决方案，能够在不搁杀模型其他性能的前提下提升模型的真实性和原则性。

Abstract: Large language models (LLMs), while increasingly used in domains requiring
factual rigor, often display a troubling behavior: sycophancy, the tendency to
align with user beliefs regardless of correctness. This tendency is reinforced
by preference-based alignment techniques that optimize for user satisfaction
but can undermine truthfulness. While relatively benign in casual dialogue,
sycophancy poses serious risks in high-stakes settings such as scientific
question answering (QA), where model outputs may shape collaborative reasoning,
decision-making, and knowledge formation. Despite its importance, this
phenomenon remains underexamined in factual QA contexts. We address this gap by
introducing a unified evaluation framework to quantify the impact of
sycophantic context on model behavior in scientific QA, measuring how much
user-imposed social pressure distorts model outputs. The framework incorporates
adversarial prompting setups and targeted metrics, such as misleading
resistance and sycophancy resistance, that capture a model's ability to
maintain factual consistency under misleading cues. Systematic evaluations
across open-source and proprietary models reveal pervasive sycophantic
tendencies, driven more by alignment strategy than by model size. To mitigate
this issue, we propose Pressure-Tune, a lightweight post-training method that
fine-tunes models on synthetic adversarial dialogues paired with
chain-of-thought rationales. These rationales reject user misinformation while
reinforcing factual commitments. Experiments on challenging scientific QA
benchmarks show that Pressure-Tune significantly enhances sycophancy resistance
without compromising accuracy or responsiveness to valid feedback, offering a
practical pathway toward more truthful and principled model behavior.

</details>


### [21] [MGT-Prism: Enhancing Domain Generalization for Machine-Generated Text Detection via Spectral Alignment](https://arxiv.org/abs/2508.13768)
*Shengchao Liu,Xiaoming Liu,Chengzhengxu Li,Zhaohan Zhang,Guoxin Ma,Yu Lan,Shuai Xiao*

Main category: cs.CL

TL;DR: MGT-Prism是一种基于频域分析的机器生成文本检测方法，通过低频滤波和动态频谱对齐策略实现更好的跨领域泛化性能


<details>
  <summary>Details</summary>
Motivation: 当前机器生成文本检测器在相同领域训练测试时表现良好，但在面对领域偏移时泛化能力差，需要开发能够跨领域泛化的检测方法

Method: 从频域角度分析文本表示，发现机器生成文本和人类撰写文本在频谱模式上存在显著差异。设计低频域滤波模块过滤对领域偏移敏感的文档级特征，采用动态频谱对齐策略提取任务特定和领域不变的特征

Result: 在11个测试数据集和三种领域泛化场景中，MGT-Prism平均准确率比最先进基线高0.90%，F1分数高0.92%

Conclusion: 频域分析为机器生成文本检测提供了有效的跨领域泛化解决方案，MGT-Prism方法在多个场景下均表现出优越性能

Abstract: Large Language Models have shown growing ability to generate fluent and
coherent texts that are highly similar to the writing style of humans. Current
detectors for Machine-Generated Text (MGT) perform well when they are trained
and tested in the same domain but generalize poorly to unseen domains, due to
domain shift between data from different sources. In this work, we propose
MGT-Prism, an MGT detection method from the perspective of the frequency domain
for better domain generalization. Our key insight stems from analyzing text
representations in the frequency domain, where we observe consistent spectral
patterns across diverse domains, while significant discrepancies in magnitude
emerge between MGT and human-written texts (HWTs). The observation initiates
the design of a low frequency domain filtering module for filtering out the
document-level features that are sensitive to domain shift, and a dynamic
spectrum alignment strategy to extract the task-specific and domain-invariant
features for improving the detector's performance in domain generalization.
Extensive experiments demonstrate that MGT-Prism outperforms state-of-the-art
baselines by an average of 0.90% in accuracy and 0.92% in F1 score on 11 test
datasets across three domain-generalization scenarios.

</details>


### [22] [Can Large Language Models (LLMs) Describe Pictures Like Children? A Comparative Corpus Study](https://arxiv.org/abs/2508.13769)
*Hanna Woloszyn,Benjamin Gagl*

Main category: cs.CL

TL;DR: 研究评估大语言模型生成文本与实际儿童语言的相似度，发现LLM生成文本在词汇丰富性、词频分布、语义等方面与儿童语言存在显著差异


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在教育领域的应用增多，需要了解LLM生成文本是否能够模仿儿童语言特点，以确保在儿童教育工具中的适宜性

Method: 使用图片故事作为提示，通过零次提示和少次提示两种方式生成LLM文本语料库，与实际德语儿童描述语料进行对比分析，包括词频、词汇丰富度、句长、词性标注和语义相似性等心理语言学指标

Result: LLM生成文本更长但词汇丰富度更低，依赖更多高频词，名词使用不足，语义空间分析显示两者语料库语义相似性很低，少次提示仅轻微提高相似性但仍无法复现儿童语言的词汇和语义模式

Conclusion: LLM目前无法有效模仿儿童语言特点，这对心理语言学研究和教育应用具有重要意义，同时引发对LLM生成语言在儿童教育工具中适宜性的重新考虑

Abstract: The role of large language models (LLMs) in education is increasing, yet
little attention has been paid to whether LLM-generated text resembles child
language. This study evaluates how LLMs replicate child-like language by
comparing LLM-generated texts to a collection of German children's descriptions
of picture stories. We generated two LLM-based corpora using the same picture
stories and two prompt types: zero-shot and few-shot prompts specifying a
general age from the children corpus. We conducted a comparative analysis
across psycholinguistic text properties, including word frequency, lexical
richness, sentence and word length, part-of-speech tags, and semantic
similarity with word embeddings. The results show that LLM-generated texts are
longer but less lexically rich, rely more on high-frequency words, and
under-represent nouns. Semantic vector space analysis revealed low similarity,
highlighting differences between the two corpora on the level of corpus
semantics. Few-shot prompt increased similarities between children and LLM text
to a minor extent, but still failed to replicate lexical and semantic patterns.
The findings contribute to our understanding of how LLMs approximate child
language through multimodal prompting (text + image) and give insights into
their use in psycholinguistic research and education while raising important
questions about the appropriateness of LLM-generated language in child-directed
educational tools.

</details>


### [23] [TracSum: A New Benchmark for Aspect-Based Summarization with Sentence-Level Traceability in Medical Domain](https://arxiv.org/abs/2508.13798)
*Bohao Chu,Meijie Li,Sameh Frihat,Chengyu Gu,Georg Lodde,Elisabeth Livingstone,Norbert Fuhr*

Main category: cs.CL

TL;DR: TracSum是一个用于可追溯、基于方面的医学摘要生成的新基准，包含500个医学摘要的3.5K摘要-引用对，提出了评估框架和Track-Then-Sum基线方法


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成医学摘要时事实准确性不足的问题，通过提供句子级引用让用户能够追溯原始证据来源

Method: 构建TracSum基准数据集，提出四指标评估框架，开发Track-Then-Sum摘要管道（先追踪句子再生成摘要）

Result: TracSum能有效评估可追溯摘要任务，显式执行句子级追踪能提高生成准确性，包含完整上下文能进一步提升完整性

Conclusion: TracSum为可追溯、基于方面的摘要任务提供了有效基准，追踪机制和完整上下文对提升医学摘要质量至关重要

Abstract: While document summarization with LLMs has enhanced access to textual
information, concerns about the factual accuracy of these summaries persist,
especially in the medical domain. Tracing evidence from which summaries are
derived enables users to assess their accuracy, thereby alleviating this
concern. In this paper, we introduce TracSum, a novel benchmark for traceable,
aspect-based summarization, in which generated summaries are paired with
sentence-level citations, enabling users to trace back to the original context.
First, we annotate 500 medical abstracts for seven key medical aspects,
yielding 3.5K summary-citation pairs. We then propose a fine-grained evaluation
framework for this new task, designed to assess the completeness and
consistency of generated content using four metrics. Finally, we introduce a
summarization pipeline, Track-Then-Sum, which serves as a baseline method for
comparison. In experiments, we evaluate both this baseline and a set of LLMs on
TracSum, and conduct a human evaluation to assess the evaluation results. The
findings demonstrate that TracSum can serve as an effective benchmark for
traceable, aspect-based summarization tasks. We also observe that explicitly
performing sentence-level tracking prior to summarization enhances generation
accuracy, while incorporating the full context further improves completeness.

</details>


### [24] [Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding](https://arxiv.org/abs/2508.13804)
*Maciej Skorski,Alina Landowska*

Main category: cs.CL

TL;DR: 这项研究通过贝叶斯方法评估主流大语言模型的道德理解能力，发现AI模型在道德判断上能达到前25%人类注释者的水平，且偏差错率更低


<details>
  <summary>Details</summary>
Motivation: 之前的研究多使用确定性真实标签（多数决或包含规则），无法抓取人类注释者之间的争议，因此需要建立更精细的评估框架来分析模型的道德理解能力

Method: 采用GPU优化的贝叶斯框架，模型化注释者争议以抓取随机不确定性（人类内在争议）和认知不确定性（模型领域敏感性），对Claude Sonnet 4、DeepSeek-V3、Llama 4 Maverick等顶级模型进行评测

Result: 模型通常能进入前25%人类注释者排名，平衡准确率远超平均水平，且AI产生的偏差错率远低于人类，显示其更敏感的道德检测能力

Conclusion: 大语言模型在道德理解方面表现突出，不仅能达到人类上水平，而且在某些方面甚至更加敏感和准确

Abstract: How do large language models understand moral dimensions compared to humans?
  This first large-scale Bayesian evaluation of market-leading language models
provides the answer. In contrast to prior work using deterministic ground truth
(majority or inclusion rules), we model annotator disagreements to capture both
aleatoric uncertainty (inherent human disagreement) and epistemic uncertainty
(model domain sensitivity). We evaluate top language models (Claude Sonnet 4,
DeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from ~700 annotators on
100K+ texts spanning social media, news, and forums.
  Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing
that AI models typically rank among the top 25\% of human annotators, achieving
much better-than-average balanced accuracy. Importantly, we find that AI
produces far fewer false negatives than humans, highlighting their more
sensitive moral detection capabilities.

</details>


### [25] [Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs](https://arxiv.org/abs/2508.13805)
*Juncheng Xie,Hung-yi Lee*

Main category: cs.CL

TL;DR: 通过提示工程在提示中添加倒计时标记和计数规则，使大语言模型能够准确控制生成文本长度，将GPT-4.1的严格长度遵循率从30%以下提升到95%以上


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成文本时经常无法可靠控制文本长度，要么超过要求要么不足，因为模型无法可靠进行内部词段计数

Method: 提出一种基于提示的一次性策略，在提示中添加倒计时标记和明确的计数规则，使模型在写作过程中同时进行计数

Result: 在MT-Bench-LI任务中，GPT-4.1的严格长度遵循率从素谱提示的30%以下提升到95%以下，超越了流行的草稿-然后修订基线方法，同时保持了评价质量

Conclusion: 精确的长度控制可以仅通过提示工程实现，为训练或解码基于方法提供了轻量级的替代方案

Abstract: Controlling the length of text produced by large language models (LLMs)
remains challenging: models frequently overshoot or undershoot explicit length
instructions because they cannot reliably keep an internal token count. We
present a prompt-based, one-shot strategy that compels an off-the-shelf LLM to
generate exactly a desired number of tokens - words (English) or characters
(Chinese) - without any fine-tuning or iterative sampling. The prompt appends
countdown markers and explicit counting rules so that the model "writes while
counting." We evaluate on four settings: open-ended generation (1-1000 tokens),
XSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH
equal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps
from below 30% under naive prompts to above 95% with our countdown prompt,
surpassing the popular draft-then-revise baseline, while judged answer quality
is preserved. These results show that precise length control can be achieved
through prompt engineering alone, offering a lightweight alternative to
training- or decoding-based methods.

</details>


### [26] [The illusion of a perfect metric: Why evaluating AI's words is harder than it looks](https://arxiv.org/abs/2508.13816)
*Maria Paz Oliva,Adriana Correia,Ivan Vankov,Viktor Botev*

Main category: cs.CL

TL;DR: 本文对自然语言生成自动评估指标进行了系统性分析，发现现有指标都存在局限性，没有单一完美指标，建议根据任务需求选择指标并采用互补评估方法。


<details>
  <summary>Details</summary>
Motivation: 自然语言生成评估是AI应用的关键挑战，虽然人工评估是金标准但成本高且难以扩展。自动评估指标不断发展但缺乏统一标准，研究旨在系统分析现有指标的有效性和局限性。

Method: 对现有自动评估指标进行彻底检查，包括方法学、文档化优势限制、验证方法以及与人工评估的相关性分析，特别关注最新的LLM-as-a-Judge方法和RAG任务评估。

Result: 发现指标通常只捕捉文本质量的特定方面，有效性因任务和数据集而异，验证实践不规范，与人工评估的相关性不一致。这些挑战在最新的LLM评估器和RAG评估中同样存在。

Conclusion: 挑战了寻找'完美指标'的追求，建议基于任务特定需求选择指标，采用互补评估方法，新指标应专注于改进验证方法学。

Abstract: Evaluating Natural Language Generation (NLG) is crucial for the practical
adoption of AI, but has been a longstanding research challenge. While human
evaluation is considered the de-facto standard, it is expensive and lacks
scalability. Practical applications have driven the development of various
automatic evaluation metrics (AEM), designed to compare the model output with
human-written references, generating a score which approximates human judgment.
Over time, AEMs have evolved from simple lexical comparisons, to semantic
similarity models and, more recently, to LLM-based evaluators. However, it
seems that no single metric has emerged as a definitive solution, resulting in
studies using different ones without fully considering the implications. This
paper aims to show this by conducting a thorough examination of the
methodologies of existing metrics, their documented strengths and limitations,
validation methods, and correlations with human judgment. We identify several
key challenges: metrics often capture only specific aspects of text quality,
their effectiveness varies by task and dataset, validation practices remain
unstructured, and correlations with human judgment are inconsistent.
Importantly, we find that these challenges persist in the most recent type of
metric, LLM-as-a-Judge, as well as in the evaluation of Retrieval Augmented
Generation (RAG), an increasingly relevant task in academia and industry. Our
findings challenge the quest for the 'perfect metric'. We propose selecting
metrics based on task-specific needs and leveraging complementary evaluations
and advocate that new metrics should focus on enhanced validation
methodologies.

</details>


### [27] [Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling](https://arxiv.org/abs/2508.13833)
*Insaf Nahri,Romain Pinquié,Philippe Véron,Nicolas Bus,Mathieu Thorel*

Main category: cs.CL

TL;DR: 使用CamemBERT和Fr_core_news_lg模型在法语BTS文档中自动提取建筑要求，NER模型F1分过90%，RE模型F1分过80%


<details>
  <summary>Details</summary>
Motivation: 解决建筑行业中非结构化法语建筑技术规范文档的要求提取问题，通过BIM与NLP技术结合实现自动化处理

Method: 采用命名实体识别(NER)和关系提取(RE)技术，使用CamemBERT和Fr_core_news_lg转移学习模型，并开发了从规则基础到深度学习的多种方法进行对比

Result: NER方面CamemBERT和Fr_core_news_lg表现最优，F1分数超过90%；RE方面Random Forest模型效果最好，F1分数超过80%

Conclusion: 研究成功开发了高效的法语BTS文档要求提取方法，为建筑行业自动化验证系统奠定了基础，未来将用知识图表示结果

Abstract: This study explores the integration of Building Information Modeling (BIM)
with Natural Language Processing (NLP) to automate the extraction of
requirements from unstructured French Building Technical Specification (BTS)
documents within the construction industry. Employing Named Entity Recognition
(NER) and Relation Extraction (RE) techniques, the study leverages the
transformer-based model CamemBERT and applies transfer learning with the French
language model Fr\_core\_news\_lg, both pre-trained on a large French corpus in
the general domain. To benchmark these models, additional approaches ranging
from rule-based to deep learning-based methods are developed. For RE, four
different supervised models, including Random Forest, are implemented using a
custom feature vector. A hand-crafted annotated dataset is used to compare the
effectiveness of NER approaches and RE models. Results indicate that CamemBERT
and Fr\_core\_news\_lg exhibited superior performance in NER, achieving
F1-scores over 90\%, while Random Forest proved most effective in RE, with an
F1 score above 80\%. The outcomes are intended to be represented as a knowledge
graph in future work to further enhance automatic verification systems.

</details>


### [28] [MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2508.13938)
*Jiacheng Ruan,Dan Jiang,Xian Gao,Ting Liu,Yuzhuo Fu,Yangyang Kang*

Main category: cs.CL

TL;DR: MME-SCI是一个新的多模态大语言模型科学领域评测基准，解决了现有基准在跨语言推理、多模态覆盖和细粒度知识标注方面的不足，包含1019个高质量问题，覆盖4个学科和5种语言，实验显示对现有模型具有很大挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有科学领域评测基准存在三个关键问题：多语言场景下推理能力评估不足、多模态覆盖不充分、科学知识点缺乏细粒度标注，需要构建更全面的评测体系。

Method: 精心收集1019个高质量问答对，涵盖数学、物理、化学、生物4个学科，支持中英法西日5种语言，包含3种评测模式，对16个开源模型和4个闭源模型进行广泛实验。

Result: 实验结果表明MME-SCI对现有MLLMs极具挑战性，如图像模式下o4-mini在数理化生准确率仅为52.11%、24.73%、36.57%、29.80%，显著高于现有基准难度。

Conclusion: MME-SCI是一个全面且具有挑战性的基准，通过多语言和细粒度知识属性深入分析模型性能，识别了特定领域的弱点，为MLLMs在科学领域的进一步发展提供了重要评测工具。

Abstract: Recently, multimodal large language models (MLLMs) have achieved significant
advancements across various domains, and corresponding evaluation benchmarks
have been continuously refined and improved. In this process, benchmarks in the
scientific domain have played an important role in assessing the reasoning
capabilities of MLLMs. However, existing benchmarks still face three key
challenges: 1) Insufficient evaluation of models' reasoning abilities in
multilingual scenarios; 2) Inadequate assessment of MLLMs' comprehensive
modality coverage; 3) Lack of fine-grained annotation of scientific knowledge
points. To address these gaps, we propose MME-SCI, a comprehensive and
challenging benchmark. We carefully collected 1,019 high-quality
question-answer pairs, which involve 3 distinct evaluation modes. These pairs
cover four subjects, namely mathematics, physics, chemistry, and biology, and
support five languages: Chinese, English, French, Spanish, and Japanese. We
conducted extensive experiments on 16 open-source models and 4 closed-source
models, and the results demonstrate that MME-SCI is widely challenging for
existing MLLMs. For instance, under the Image-only evaluation mode, o4-mini
achieved accuracy of only 52.11%, 24.73%, 36.57%, and 29.80% in mathematics,
physics, chemistry, and biology, respectively, indicating a significantly
higher difficulty level compared to existing benchmarks. More importantly,
using MME-SCI's multilingual and fine-grained knowledge attributes, we analyzed
existing models' performance in depth and identified their weaknesses in
specific domains. The Data and Evaluation Code are available at
https://github.com/JCruan519/MME-SCI.

</details>


### [29] [ReviewGraph: A Knowledge Graph Embedding Based Framework for Review Rating Prediction with Sentiment Features](https://arxiv.org/abs/2508.13953)
*A. J. W. de Vink,Natalia Amat-Lefort,Lifeng Han*

Main category: cs.CL

TL;DR: 提出了ReviewGraph框架，将客户评论转换为知识图谱进行评分预测，性能媲美LLM但计算成本更低，具有更好的可解释性和可视化优势


<details>
  <summary>Details</summary>
Motivation: 酒店业需要理解影响客户评分的因素来提升客户满意度和业务表现，传统NLP方法在可解释性和计算成本方面存在局限

Method: 将文本评论提取为(主语,谓语,宾语)三元组并关联情感分数构建知识图谱，使用Node2Vec图嵌入和情感特征，通过机器学习分类器预测评分

Result: 在HotelRec数据集上表现与最佳模型相当但计算成本更低，在Cohen's Kappa等一致性指标上优于基线，性能媲美LLM

Conclusion: 基于图的表示方法在评论分析中具有潜力，为未来集成图神经网络和微调LLM提取方法奠定了基础，提供了更好的可解释性和可视化能力

Abstract: In the hospitality industry, understanding the factors that drive customer
review ratings is critical for improving guest satisfaction and business
performance. This work proposes ReviewGraph for Review Rating Prediction (RRP),
a novel framework that transforms textual customer reviews into knowledge
graphs by extracting (subject, predicate, object) triples and associating
sentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the
framework predicts review rating scores through machine learning classifiers.
We compare ReviewGraph performance with traditional NLP baselines (such as Bag
of Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating
them in the HotelRec dataset. In comparison to the state of the art literature,
our proposed model performs similar to their best performing model but with
lower computational cost (without ensemble).
  While ReviewGraph achieves comparable predictive performance to LLMs and
outperforms baselines on agreement-based metrics such as Cohen's Kappa, it
offers additional advantages in interpretability, visual exploration, and
potential integration into Retrieval-Augmented Generation (RAG) systems. This
work highlights the potential of graph-based representations for enhancing
review analytics and lays the groundwork for future research integrating
advanced graph neural networks and fine-tuned LLM-based extraction methods. We
will share ReviewGraph output and platform open-sourced on our GitHub page
https://github.com/aaronlifenghan/ReviewGraph

</details>


### [30] [Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization](https://arxiv.org/abs/2508.13993)
*Shaohua Duan,Xinze Li,Zhenghao Liu,Xiaoyuan Yi,Yukun Yan,Shuo Wang,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CL

TL;DR: LongMab-PO是一个新颖的框架，使用多臂老虎机策略从长上下文中选择最有信息量的片段，生成高质量多样化的响应，并通过DPO训练优化LLM的长上下文能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于合成数据微调LLM的方法存在数据多样性低和事实不一致的问题，限制了长上下文建模的效果。

Method: 将上下文块视为老虎机的臂，基于预期奖励分数选择块输入LLM生成响应，迭代更新分数，收集高质量响应构建偏好数据对进行DPO训练。

Result: 实验结果表明LongMab-PO显著提高了偏好数据对的多样性和质量，在长上下文推理基准上达到了最先进的性能。

Conclusion: 提出的MAB rollout策略能有效识别关键上下文片段，生成高质量多样化数据，通过DPO进一步优化LLM的长上下文能力。

Abstract: Long-context modeling is critical for a wide range of real-world tasks,
including long-context question answering, summarization, and complex reasoning
tasks. Recent studies have explored fine-tuning Large Language Models (LLMs)
with synthetic data to enhance their long-context capabilities. However, the
effectiveness of such approaches is often limited by the low diversity and
factual inconsistencies in the generated data. To address these challenges, we
propose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB)
rollout strategy to identify the most informative chunks from the given long
context for sampling high-quality and diverse responses and constructing
preference data pairs for Direct Preference Optimization (DPO) training.
Specifically, we treat context chunks as arms of MAB, select chunks based on
their expected reward scores to input into LLMs to generate responses, and
iteratively update these scores based on reward feedback. This exploration and
exploitation process enables the model to focus on the most relevant context
segments, thereby generating and collecting high-quality and diverse responses.
Finally, we collect these generated responses from the rollout process and
apply the DPO method to further optimize the LLM. Experimental results show
that LongMab-PO significantly improves the diversity and quality of preference
data pairs, achieving state-of-the-art performance on long-context reasoning
benchmarks. All code and data will be released on
https://github.com/NEUIR/LongMab-PO.

</details>


### [31] [Ask Good Questions for Large Language Models](https://arxiv.org/abs/2508.14025)
*Qi Wu,Zhongqi Lu*

Main category: cs.CL

TL;DR: 提出了Ask-Good-Question (AGQ)框架，使用改进的Concept-Enhanced Item Response Theory (CEIRT)模型结合大语言模型，能够更好地识别用户知识水平并生成引导性问题，显著提升问答过程中的信息检索效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对话系统在提供准确主题引导方面存在不足，主要原因是无法有效识别用户在相关概念上的困惑，这限制了对话系统的指导效果。

Method: 引入AGQ框架，采用改进的Concept-Enhanced Item Response Theory (CEIRT)模型来更好地识别用户知识水平，并结合大语言模型直接从启发文本生成引导性问题。

Result: 通过与基线方法的比较，该方法显著提升了用户的信息检索体验，在性能表现上优于其他方法。

Conclusion: AGQ框架通过结合CEIRT模型和大语言模型，有效解决了对话系统中概念困惑识别的问题，为提升问答系统的指导能力提供了有效解决方案。

Abstract: Recent advances in large language models (LLMs) have significantly improved
the performance of dialog systems, yet current approaches often fail to provide
accurate guidance of topic due to their inability to discern user confusion in
related concepts. To address this, we introduce the Ask-Good-Question (AGQ)
framework, which features an improved Concept-Enhanced Item Response Theory
(CEIRT) model to better identify users' knowledge levels. Our contributions
include applying the CEIRT model along with LLMs to directly generate guiding
questions based on the inspiring text, greatly improving information retrieval
efficiency during the question & answer process. Through comparisons with other
baseline methods, our approach outperforms by significantly enhencing the
users' information retrieval experiences.

</details>


### [32] [Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR](https://arxiv.org/abs/2508.14029)
*Xiao Liang,Zhongzhi Li,Yeyun Gong,Yelong Shen,Ying Nian Wu,Zhijiang Guo,Weizhu Chen*

Main category: cs.CL

TL;DR: 提出SvS策略，通过自博弈和变分问题合成来维持RLVR训练中的策略熵，显著提升Pass@k性能


<details>
  <summary>Details</summary>
Motivation: 标准RLVR训练会降低策略熵和生成多样性，限制Pass@k性能，需要解决熵崩溃问题

Method: 在线自博弈与变分问题合成策略，利用策略的正确解合成变分问题同时保持参考答案不变

Result: 在AIME24和AIME25基准上Pass@32性能分别提升18.3%和22.8%，在3B到32B模型的12个推理基准上表现一致

Conclusion: SvS策略能有效维持训练过程中的策略熵，显著提升RLVR训练的Pass@k性能，具有很好的泛化性和鲁棒性

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a key paradigm for post-training Large Language Models (LLMs), particularly for
complex reasoning tasks. However, vanilla RLVR training has been shown to
improve Pass@1 performance at the expense of policy entropy, leading to reduced
generation diversity and limiting the Pass@k performance, which typically
represents the upper bound of LLM reasoning capability. In this paper, we
systematically analyze the policy's generation diversity from the perspective
of training problems and find that augmenting and updating training problems
helps mitigate entropy collapse during training. Based on these observations,
we propose an online Self-play with Variational problem Synthesis (SvS)
strategy for RLVR training, which uses the policy's correct solutions to
synthesize variational problems while ensuring their reference answers remain
identical to the originals. This self-improving strategy effectively maintains
policy entropy during training and substantially improves Pass@k compared with
standard RLVR, sustaining prolonged improvements and achieving absolute gains
of 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and
AIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model
sizes from 3B to 32B consistently demonstrate the generalizability and
robustness of SvS.

</details>


### [33] [Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation](https://arxiv.org/abs/2508.14031)
*Dongyoon Hahm,Taywon Min,Woogyeol Jin,Kimin Lee*

Main category: cs.CL

TL;DR: LLM智能体微调过程中存在安全隐患，可能导致模型更容易执行有害任务。本文提出PING方法，通过在响应前添加自动生成的自然语言前缀来引导模型拒绝有害请求，同时保持良性任务性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在向智能体系统演进时，通过特定任务微调提升能力，但安全考虑常被忽视。研究发现对齐的LLM在微调后可能无意中失准，更易执行有害任务且拒绝倾向降低。

Method: 提出Prefix INjection Guard (PING)方法，采用迭代方式：(1)生成候选前缀，(2)选择优化任务性能和拒绝行为的前缀。通过线性探针分析隐藏状态，发现前缀标记对行为修改至关重要。

Result: 实验表明PING显著提升了微调LLM智能体的安全性，且不牺牲有效性。在网页导航和代码生成任务中，PING持续优于现有提示方法。

Conclusion: PING是一种简单有效的安全增强方法，通过自然语言前缀注入成功解决了LLM智能体微调过程中的安全隐患，为构建更安全的AI系统提供了可行方案。

Abstract: Beyond simple text generation, Large Language Models (LLMs) have evolved into
agentic systems capable of planning and interacting with external tools to
solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific
tasks to enhance their proficiency. However, safety concerns are frequently
overlooked during this fine-tuning process. In this work, we show that aligned
LLMs can become unintentionally misaligned, leading to a higher likelihood of
executing harmful tasks and a reduced tendency to refuse them when fine-tuned
to execute agentic tasks. To address these safety challenges, we propose Prefix
INjection Guard (PING), a simple yet effective method that prepends
automatically generated natural language prefixes to agent responses, guiding
them to refuse harmful requests while preserving performance on benign tasks.
Specifically, we introduce an iterative approach that alternates between (1)
generating candidate prefixes and (2) selecting those that optimize both task
performance and refusal behavior. Experimental results demonstrate that PING
significantly enhances the safety of fine-tuned LLM agents without sacrificing
their effectiveness. PING consistently outperforms existing prompting
approaches across diverse benchmarks in both web navigation and code generation
tasks. Our analysis of internal hidden states via linear probes reveals that
prefix tokens are crucial for behavior modification, explaining the performance
gains. WARNING: This paper contains contents that are unethical or offensive in
nature.

</details>


### [34] [The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities](https://arxiv.org/abs/2508.14032)
*Xiancheng Li,Georgios D. Karampatakis,Helen E. Wood,Chris J. Griffiths,Borislava Mihaylova,Neil S. Coulson,Alessio Pasinato,Pietro Panzarasa,Marco Viviani,Anna De Simoni*

Main category: cs.CL

TL;DR: 这研究探索使用大语言模型通过上下文学习集成专家知识进行健康社区情感分析，解决了医疗领域专业知识缺乏和数据隐私限制的挑战，达到了专家级判断水平。


<details>
  <summary>Details</summary>
Motivation: 数字健康分析面临专业知识缺乏、数据短缺和隐私限制的挑战，特别是在在线健康社区中的复杂情感和医疗语境分析。

Method: 研究发展了结构化指南码本系统编码专家解释指南，通过上下文学习让LLM应用领域知识，比较了六款GPT模型、DeepSeek、LLaMA 3.1与BioBERT变种和词典方法的性能。

Result: LLM在400个专家注释帖子上表现出艰出性能，达到了专家级判断一致性，与专家间一致性没有统计学显著差异。

Conclusion: 该方法通过上下文学习集成专家知识，为数字健康分析提供了可扩展的解决方案，能够支持实时患者监测、干预评估和基于证据的健康策略。

Abstract: Digital health analytics face critical challenges nowadays. The sophisticated
analysis of patient-generated health content, which contains complex emotional
and medical contexts, requires scarce domain expertise, while traditional ML
approaches are constrained by data shortage and privacy limitations in
healthcare settings. Online Health Communities (OHCs) exemplify these
challenges with mixed-sentiment posts, clinical terminology, and implicit
emotional expressions that demand specialised knowledge for accurate Sentiment
Analysis (SA). To address these challenges, this study explores how Large
Language Models (LLMs) can integrate expert knowledge through in-context
learning for SA, providing a scalable solution for sophisticated health data
analysis. Specifically, we develop a structured codebook that systematically
encodes expert interpretation guidelines, enabling LLMs to apply
domain-specific knowledge through targeted prompting rather than extensive
training. Six GPT models validated alongside DeepSeek and LLaMA 3.1 are
compared with pre-trained language models (BioBERT variants) and lexicon-based
methods, using 400 expert-annotated posts from two OHCs. LLMs achieve superior
performance while demonstrating expert-level agreement. This high agreement,
with no statistically significant difference from inter-expert agreement
levels, suggests knowledge integration beyond surface-level pattern
recognition. The consistent performance across diverse LLM models, supported by
in-context learning, offers a promising solution for digital health analytics.
This approach addresses the critical challenge of expert knowledge shortage in
digital health research, enabling real-time, expert-quality analysis for
patient monitoring, intervention assessment, and evidence-based health
strategies.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [35] [Robust Live Streaming over LEO Satellite Constellations: Measurement, Analysis, and Handover-Aware Adaptation](https://arxiv.org/abs/2508.13402)
*Hao Fang,Haoyuan Zhao,Jianxin Shi,Miao Zhang,Guanzhen Wu,Yi Ching Chou,Feng Wang,Jiangchuan Liu*

Main category: cs.MM

TL;DR: 提出SARA中间件，通过智能调节视频播放速度和提供卫星网络特性洞察，显著减少低轨卫星网络中直播流媒体因卫星切换导致的卡顿问题


<details>
  <summary>Details</summary>
Motivation: 低轨卫星网络(LSNs)如Starlink为全球互联网接入提供新方案，但现有直播平台在卫星切换时频繁出现视频卡顿，当前学习型自适应码率算法无法有效处理这种突发网络变化

Method: 设计卫星感知码率适配(SARA)中间件，可无缝集成各种ABR算法，通过智能调节播放速度和提供LSNs网络特性信息来辅助码率选择决策

Result: SARA平均减少39.41%的卡顿时间，略微改善0.65%的延迟，仅引入0.13%的码率损失

Conclusion: SARA是首个专门针对LSNs的解决方案，能有效提升卫星网络上的直播流媒体体验，显著减少卫星切换导致的卡顿问题

Abstract: Live streaming has experienced significant growth recently. Yet this rise in
popularity contrasts with the reality that a substantial segment of the global
population still lacks Internet access. The emergence of Low Earth orbit
Satellite Networks (LSNs), such as SpaceX's Starlink and Amazon's Project
Kuiper, presents a promising solution to fill this gap. Nevertheless, our
measurement study reveals that existing live streaming platforms may not be
able to deliver a smooth viewing experience on LSNs due to frequent satellite
handovers, which lead to frequent video rebuffering events. Current
state-of-the-art learning-based Adaptive Bitrate (ABR) algorithms, even when
trained on LSNs' network traces, fail to manage the abrupt network variations
associated with satellite handovers effectively. To address these challenges,
for the first time, we introduce Satellite-Aware Rate Adaptation (SARA), a
versatile and lightweight middleware that can seamlessly integrate with various
ABR algorithms to enhance the performance of live streaming over LSNs. SARA
intelligently modulates video playback speed and furnishes ABR algorithms with
insights derived from the distinctive network characteristics of LSNs, thereby
aiding ABR algorithms in making informed bitrate selections and effectively
minimizing rebuffering events that occur during satellite handovers. Our
extensive evaluation shows that SARA can effectively reduce the rebuffering
time by an average of $39.41\%$ and slightly improve latency by $0.65\%$ while
only introducing an overall loss in bitrate by $0.13\%$.

</details>


### [36] [INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video](https://arxiv.org/abs/2508.13756)
*Ruonan Chai,Yixiang Zhu,Xinjiao Li,Jiawei Li,Zili Meng,Dirk Kutscher*

Main category: cs.MM

TL;DR: INDS是一个基于信息中心网络的自适应流媒体框架，专门针对点云视频流媒体，通过利用Octree结构和细粒度命名方案，实现了渐进式部分检索、高效缓存和多用户数据重用，相比现有系统显著降低了延迟并提高了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决实时点云视频流媒体在大数据量和动态网络条件下的挑战，现有TCP和QUIC等协议虽然缓解了传输层效率问题，但仍受限于粗粒度的分段传输模型和集中式控制循环，无法实现细粒度自适应和有效缓存。

Method: 基于信息中心网络(ICN)设计INDS框架，利用点云视频的Octree结构和表达性内容命名，支持根据消费者带宽和解码能力进行增强层的渐进式部分检索。结合时间窗口和帧组(GoF)的命名方案，实现细粒度网络内缓存和高效多用户数据重用。

Result: 原型实现显示相比最先进的DASH风格系统，延迟降低高达80%，吞吐量提高15-50%，缓存命中率提升20-30%。

Conclusion: INDS是一个可扩展、缓存友好的实时点云流媒体解决方案，适用于可变和有损网络条件，同时与MoQ覆盖网络的兼容性使其成为新兴沉浸式媒体系统的实用且向前兼容的架构。

Abstract: Real-time streaming of point cloud video, characterized by massive data
volumes and high sensitivity to packet loss, remains a key challenge for
immersive applications under dynamic network conditions. While
connection-oriented protocols such as TCP and more modern alternatives like
QUIC alleviate some transport-layer inefficiencies, including head-of-line
blocking, they still retain a coarse-grained, segment-based delivery model and
a centralized control loop that limit fine-grained adaptation and effective
caching. We introduce INDS (Incremental Named Data Streaming), an adaptive
streaming framework based on Information-Centric Networking (ICN) that rethinks
delivery for hierarchical, layered media. INDS leverages the Octree structure
of point cloud video and expressive content naming to support progressive,
partial retrieval of enhancement layers based on consumer bandwidth and
decoding capability. By combining time-windows with Group-of-Frames (GoF),
INDS's naming scheme supports fine-grained in-network caching and facilitates
efficient multi-user data reuse. INDS can be deployed as an overlay, remaining
compatible with QUIC-based transport infrastructure as well as future
Media-over-QUIC (MoQ) architectures, without requiring changes to underlying IP
networks. Our prototype implementation shows up to 80% lower delay, 15-50%
higher throughput, and 20-30% increased cache hit rates compared to
state-of-the-art DASH-style systems. Together, these results establish INDS as
a scalable, cache-friendly solution for real-time point cloud streaming under
variable and lossy conditions, while its compatibility with MoQ overlays
further positions it as a practical, forward-compatible architecture for
emerging immersive media systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [37] [Is Transfer Learning Necessary for Violin Transcription?](https://arxiv.org/abs/2508.13516)
*Yueh-Po Peng,Ting-Kang Wang,Li Su,Vincent K. M. Cheung*

Main category: cs.SD

TL;DR: 通过在中等规模小提琴数据集上从头训练，得到了与基于钢琴预训练模型微调相比较的或更优的自动小提琴试翻性能，说明不必依赖钢琴表征也能实现强大的小提琴识别。


<details>
  <summary>Details</summary>
Motivation: 小提琴自动试翻(AMT)因注释数据有限而研究较少，常见方法是微调钢琴预训练模型，但在音色和拳法差异下的效果不明确。本文想知道在中等规模小提琴数据集上从头训练能否跟微调钢琴模型的性能相比。

Method: 采用钢琴试翻结构不作任何修改，在MOSA数据集(约30小时对齐小提琴录音)上从头训练。在URMP和Bach10数据集上进行实验测试。

Result: 从头训练的模型在性能上与微调的钢琴预训练模型相比较或更优。

Conclusion: 强大的小提琴AMT可以不依赖钢琴预训练表征，强调了专门乐器特定数据收集和扩充策略的重要性。

Abstract: Automatic music transcription (AMT) has achieved remarkable progress for
instruments such as the piano, largely due to the availability of large-scale,
high-quality datasets. In contrast, violin AMT remains underexplored due to
limited annotated data. A common approach is to fine-tune pretrained models for
other downstream tasks, but the effectiveness of such transfer remains unclear
in the presence of timbral and articulatory differences. In this work, we
investigate whether training from scratch on a medium-scale violin dataset can
match the performance of fine-tuned piano-pretrained models. We adopt a piano
transcription architecture without modification and train it on the MOSA
dataset, which contains about 30 hours of aligned violin recordings. Our
experiments on URMP and Bach10 show that models trained from scratch achieved
competitive or even superior performance compared to fine-tuned counterparts.
These findings suggest that strong violin AMT is possible without relying on
pretrained piano representations, highlighting the importance of
instrument-specific data collection and augmentation strategies.

</details>


### [38] [Leveraging Mamba with Full-Face Vision for Audio-Visual Speech Enhancement](https://arxiv.org/abs/2508.13624)
*Rong Chao,Wenze Ren,You-Jin Li,Kuo-Hsuan Hung,Sung-Feng Huang,Szu-Wei Fu,Wen-Huang Cheng,Yu Tsao*

Main category: cs.SD

TL;DR: AVSEMamba是一个基于Mamba的音频-视觉语音增强模型，通过整合全脸视觉线索和Mamba时序骨干网络，在复杂多说话人环境中显著提升目标语音提取性能，在AVSEC-4挑战赛中取得单声道排行榜第一名。


<details>
  <summary>Details</summary>
Motivation: 现有的Mamba语音增强模型如SEMamba仅限于单说话人场景，在鸡尾酒会等多说话人复杂环境中表现不佳，需要引入视觉信息来提升目标语音提取能力。

Method: 提出AVSEMamba模型，整合全脸视觉线索与Mamba时序骨干网络，利用时空视觉信息在挑战性条件下更准确地提取目标语音。

Result: 在AVSEC-4挑战赛开发和盲测集上评估，AVSEMamba在语音可懂度(STOI)、感知质量(PESQ)和非侵入式质量(UTMOS)方面优于其他单声道基线方法，获得单声道排行榜第一名。

Conclusion: AVSEMamba通过音频-视觉融合有效解决了多说话人环境下的语音增强问题，证明了视觉信息对提升复杂场景中目标语音提取性能的重要价值。

Abstract: Recent Mamba-based models have shown promise in speech enhancement by
efficiently modeling long-range temporal dependencies. However, models like
Speech Enhancement Mamba (SEMamba) remain limited to single-speaker scenarios
and struggle in complex multi-speaker environments such as the cocktail party
problem. To overcome this, we introduce AVSEMamba, an audio-visual speech
enhancement model that integrates full-face visual cues with a Mamba-based
temporal backbone. By leveraging spatiotemporal visual information, AVSEMamba
enables more accurate extraction of target speech in challenging conditions.
Evaluated on the AVSEC-4 Challenge development and blind test sets, AVSEMamba
outperforms other monaural baselines in speech intelligibility (STOI),
perceptual quality (PESQ), and non-intrusive quality (UTMOS), and achieves
\textbf{1st place} on the monaural leaderboard.

</details>


### [39] [DegDiT: Controllable Audio Generation with Dynamic Event Graph Guided Diffusion Transformer](https://arxiv.org/abs/2508.13786)
*Yisu Liu,Chenxing Li,Wanqian Zhang,Wenfu Wang,Meng Yu,Ruibo Fu,Zheng Lin,Weiping Wang,Dong Yu*

Main category: cs.SD

TL;DR: DegDiT是一个基于动态事件图引导的扩散变换器框架，用于开放词汇的文本到音频生成控制，通过结构化图表示和共识偏好优化实现精确的时序控制。


<details>
  <summary>Details</summary>
Motivation: 现有文本到音频生成方法在准确时序定位、开放词汇扩展性和实际效率之间存在固有权衡，需要解决这些挑战以实现更精确的音频内容控制。

Method: 使用动态事件图编码描述中的事件，节点包含语义特征、时序属性和事件间连接；采用图变换器生成上下文事件嵌入作为扩散模型指导；引入质量平衡数据选择流程和共识偏好优化方法。

Result: 在AudioCondition、DESED和AudioTime数据集上的广泛实验表明，DegDiT在各种主客观评估指标上达到了最先进的性能。

Conclusion: DegDiT框架通过结构化事件表示和共识优化，成功解决了可控文本到音频生成中的时序精度、词汇扩展性和效率平衡问题，为精确音频控制提供了有效解决方案。

Abstract: Controllable text-to-audio generation aims to synthesize audio from textual
descriptions while satisfying user-specified constraints, including event
types, temporal sequences, and onset and offset timestamps. This enables
precise control over both the content and temporal structure of the generated
audio. Despite recent progress, existing methods still face inherent trade-offs
among accurate temporal localization, open-vocabulary scalability, and
practical efficiency. To address these challenges, we propose DegDiT, a novel
dynamic event graph-guided diffusion transformer framework for open-vocabulary
controllable audio generation. DegDiT encodes the events in the description as
structured dynamic graphs. The nodes in each graph are designed to represent
three aspects: semantic features, temporal attributes, and inter-event
connections. A graph transformer is employed to integrate these nodes and
produce contextualized event embeddings that serve as guidance for the
diffusion model. To ensure high-quality and diverse training data, we introduce
a quality-balanced data selection pipeline that combines hierarchical event
annotation with multi-criteria quality scoring, resulting in a curated dataset
with semantic diversity. Furthermore, we present consensus preference
optimization, facilitating audio generation through consensus among multiple
reward signals. Extensive experiments on AudioCondition, DESED, and AudioTime
datasets demonstrate that DegDiT achieves state-of-the-art performances across
a variety of objective and subjective evaluation metrics.

</details>


### [40] [Evaluating Identity Leakage in Speaker De-Identification Systems](https://arxiv.org/abs/2508.14012)
*Seungmin Seo,Oleg Aulov,Afzal Godil,Kevin Mangold*

Main category: cs.SD

TL;DR: 这篇论文引入了一个新的评测标准，发现所有当前最优的讲话者去识别系统都存在身份信息泄漏问题，高于随机猜测的性能。


<details>
  <summary>Details</summary>
Motivation: 评估讲话者去识别系统的身份信息泄漏情况，揭示当前技术在隐私保护方面的残缺和风险。

Method: 使用三种互补的误差率指标：等误差率(EER)、累积匹配特征命中率(CMC)、以及通过正交相关分析和Procrustes分析测量的嵌入空间相似度。

Result: 所有状态边的讲话者去识别系统都存在身份信息泄漏。性能最高的系统仅略好于随机猜测，而性能最差的系统在CMC前50名候选人中达到了45%的命中率。

Conclusion: 当前讲话者去识别技术仍然存在持久的隐私风险，需要更有效的方法来保护讲话者身份信息。

Abstract: Speaker de-identification aims to conceal a speaker's identity while
preserving intelligibility of the underlying speech. We introduce a benchmark
that quantifies residual identity leakage with three complementary error rates:
equal error rate, cumulative match characteristic hit rate, and embedding-space
similarity measured via canonical correlation analysis and Procrustes analysis.
Evaluation results reveal that all state-of-the-art speaker de-identification
systems leak identity information. The highest performing system in our
evaluation performs only slightly better than random guessing, while the lowest
performing system achieves a 45% hit rate within the top 50 candidates based on
CMC. These findings highlight persistent privacy risks in current speaker
de-identification technologies.

</details>
