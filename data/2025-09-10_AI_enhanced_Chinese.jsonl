{"id": "2509.07135", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07135", "abs": "https://arxiv.org/abs/2509.07135", "authors": ["Ruggero Marino Lazzaroni", "Alessandro Angioi", "Michelangelo Puliga", "Davide Sanna", "Roberto Marras"], "title": "MedBench-IT: A Comprehensive Benchmark for Evaluating Large Language Models on Italian Medical Entrance Examinations", "comment": "Accepted as an oral presentation at CLiC-it 2025", "summary": "Large language models (LLMs) show increasing potential in education, yet\nbenchmarks for non-English languages in specialized domains remain scarce. We\nintroduce MedBench-IT, the first comprehensive benchmark for evaluating LLMs on\nItalian medical university entrance examinations. Sourced from Edizioni Simone,\na leading preparatory materials publisher, MedBench-IT comprises 17,410\nexpert-written multiple-choice questions across six subjects (Biology,\nChemistry, Logic, General Culture, Mathematics, Physics) and three difficulty\nlevels. We evaluated diverse models including proprietary LLMs (GPT-4o, Claude\nseries) and resource-efficient open-source alternatives (<30B parameters)\nfocusing on practical deployability.\n  Beyond accuracy, we conducted rigorous reproducibility tests (88.86% response\nconsistency, varying by subject), ordering bias analysis (minimal impact), and\nreasoning prompt evaluation. We also examined correlations between question\nreadability and model performance, finding a statistically significant but\nsmall inverse relationship. MedBench-IT provides a crucial resource for Italian\nNLP community, EdTech developers, and practitioners, offering insights into\ncurrent capabilities and standardized evaluation methodology for this critical\ndomain.", "AI": {"tldr": "MedBench-IT\u662f\u9996\u4e2a\u9488\u5bf9\u610f\u5927\u5229\u533b\u5b66\u5927\u5b66\u5165\u5b66\u8003\u8bd5\u7684LLM\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b17,410\u4e2a\u4e13\u5bb6\u7f16\u5199\u7684\u9009\u62e9\u9898\uff0c\u6db5\u76d66\u4e2a\u5b66\u79d1\u548c3\u4e2a\u96be\u5ea6\u7ea7\u522b\uff0c\u4e3a\u610f\u5927\u5229NLP\u793e\u533a\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "LLM\u5728\u6559\u80b2\u9886\u57df\u6f5c\u529b\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u975e\u82f1\u8bed\u4e13\u4e1a\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\u7a00\u7f3a\uff0c\u7279\u522b\u662f\u610f\u5927\u5229\u8bed\u533b\u5b66\u5165\u5b66\u8003\u8bd5\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u4eceEdizioni Simone\u83b7\u53d617,410\u4e2a\u9009\u62e9\u9898\uff0c\u8bc4\u4f30\u5305\u62ecGPT-4o\u3001Claude\u7cfb\u5217\u7b49\u4e13\u6709\u6a21\u578b\u548c<30B\u53c2\u6570\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u8fdb\u884c\u51c6\u786e\u6027\u3001\u53ef\u91cd\u590d\u6027\u3001\u6392\u5e8f\u504f\u5dee\u548c\u63a8\u7406\u63d0\u793a\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u54cd\u5e94\u4e00\u81f4\u6027\u8fbe88.86%\uff08\u56e0\u5b66\u79d1\u800c\u5f02\uff09\uff0c\u6392\u5e8f\u504f\u5dee\u5f71\u54cd\u6781\u5c0f\uff0c\u95ee\u9898\u53ef\u8bfb\u6027\u4e0e\u6a21\u578b\u6027\u80fd\u5b58\u5728\u7edf\u8ba1\u663e\u8457\u4f46\u8f83\u5c0f\u7684\u8d1f\u76f8\u5173\u5173\u7cfb\u3002", "conclusion": "MedBench-IT\u4e3a\u610f\u5927\u5229NLP\u793e\u533a\u3001\u6559\u80b2\u6280\u672f\u5f00\u53d1\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5173\u952e\u8d44\u6e90\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u80fd\u529b\u5e76\u4e3a\u8fd9\u4e00\u91cd\u8981\u9886\u57df\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2509.07139", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.07139", "abs": "https://arxiv.org/abs/2509.07139", "authors": ["William Chen", "Chutong Meng", "Jiatong Shi", "Martijn Bartelds", "Shih-Heng Wang", "Hsiu-Hsuan Wang", "Rafael Mosquera", "Sara Hincapie", "Dan Jurafsky", "Antonis Anastasopoulos", "Hung-yi Lee", "Karen Livescu", "Shinji Watanabe"], "title": "The ML-SUPERB 2.0 Challenge: Towards Inclusive ASR Benchmarking for All Language Varieties", "comment": "Interspeech 2025", "summary": "Recent improvements in multilingual ASR have not been equally distributed\nacross languages and language varieties. To advance state-of-the-art (SOTA) ASR\nmodels, we present the Interspeech 2025 ML-SUPERB 2.0 Challenge. We construct a\nnew test suite that consists of data from 200+ languages, accents, and dialects\nto evaluate SOTA multilingual speech models. The challenge also introduces an\nonline evaluation server based on DynaBench, allowing for flexibility in model\ndesign and architecture for participants. The challenge received 5 submissions\nfrom 3 teams, all of which outperformed our baselines. The best-performing\nsubmission achieved an absolute improvement in LID accuracy of 23% and a\nreduction in CER of 18% when compared to the best baseline on a general\nmultilingual test set. On accented and dialectal data, the best submission\nobtained 30.2% lower CER and 15.7% higher LID accuracy, showing the importance\nof community challenges in making speech technologies more inclusive.", "AI": {"tldr": "Interspeech 2025 ML-SUPERB 2.0\u6311\u6218\u8d5b\u6784\u5efa\u4e86\u5305\u542b200+\u8bed\u8a00\u3001\u53e3\u97f3\u548c\u65b9\u8a00\u7684\u65b0\u6d4b\u8bd5\u5957\u4ef6\uff0c\u6700\u4f73\u63d0\u4ea4\u5728LID\u51c6\u786e\u7387\u4e0a\u63d0\u534723%\uff0cCER\u964d\u4f4e18%\uff0c\u5728\u53e3\u97f3\u548c\u65b9\u8a00\u6570\u636e\u4e0aCER\u964d\u4f4e30.2%\uff0cLID\u51c6\u786e\u7387\u63d0\u534715.7%\u3002", "motivation": "\u5f53\u524d\u591a\u8bed\u8a00ASR\u6280\u672f\u7684\u6539\u8fdb\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u8bed\u8a00\u53d8\u4f53\u95f4\u5206\u5e03\u4e0d\u5747\uff0c\u9700\u8981\u63a8\u52a8\u66f4\u5305\u5bb9\u7684\u8bed\u97f3\u6280\u672f\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u5305\u542b200+\u8bed\u8a00\u3001\u53e3\u97f3\u548c\u65b9\u8a00\u7684\u65b0\u6d4b\u8bd5\u5957\u4ef6\uff0c\u57fa\u4e8eDynaBench\u5efa\u7acb\u5728\u7ebf\u8bc4\u4f30\u670d\u52a1\u5668\uff0c\u5141\u8bb8\u53c2\u4e0e\u8005\u7075\u6d3b\u8bbe\u8ba1\u6a21\u578b\u67b6\u6784\u3002", "result": "\u6536\u52303\u4e2a\u56e2\u961f\u76845\u4efd\u63d0\u4ea4\uff0c\u6240\u6709\u63d0\u4ea4\u5747\u4f18\u4e8e\u57fa\u7ebf\u3002\u6700\u4f73\u63d0\u4ea4\u5728\u901a\u7528\u591a\u8bed\u8a00\u6d4b\u8bd5\u96c6\u4e0aLID\u51c6\u786e\u7387\u63d0\u534723%\uff0cCER\u964d\u4f4e18%\uff1b\u5728\u53e3\u97f3\u548c\u65b9\u8a00\u6570\u636e\u4e0aCER\u964d\u4f4e30.2%\uff0cLID\u51c6\u786e\u7387\u63d0\u534715.7%\u3002", "conclusion": "\u793e\u533a\u6311\u6218\u8d5b\u5bf9\u4e8e\u4f7f\u8bed\u97f3\u6280\u672f\u66f4\u52a0\u5305\u5bb9\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u5c55\u793a\u4e86\u5728\u591a\u8bed\u8a00ASR\u65b9\u9762\u7684\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2509.07142", "categories": ["cs.CL", "cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2509.07142", "abs": "https://arxiv.org/abs/2509.07142", "authors": ["Zhiyin Tan", "Jennifer D'Souza"], "title": "Toward Purpose-oriented Topic Model Evaluation enabled by Large Language Models", "comment": "Accepted for publication in International Journal on Digital\n  Libraries (IJDL)", "summary": "This study presents a framework for automated evaluation of dynamically\nevolving topic models using Large Language Models (LLMs). Topic modeling is\nessential for organizing and retrieving scholarly content in digital library\nsystems, helping users navigate complex and evolving knowledge domains.\nHowever, widely used automated metrics, such as coherence and diversity, often\ncapture only narrow statistical patterns and fail to explain semantic failures\nin practice. We introduce a purpose-oriented evaluation framework that employs\nnine LLM-based metrics spanning four key dimensions of topic quality: lexical\nvalidity, intra-topic semantic soundness, inter-topic structural soundness, and\ndocument-topic alignment soundness. The framework is validated through\nadversarial and sampling-based protocols, and is applied across datasets\nspanning news articles, scholarly publications, and social media posts, as well\nas multiple topic modeling methods and open-source LLMs. Our analysis shows\nthat LLM-based metrics provide interpretable, robust, and task-relevant\nassessments, uncovering critical weaknesses in topic models such as redundancy\nand semantic drift, which are often missed by traditional metrics. These\nresults support the development of scalable, fine-grained evaluation tools for\nmaintaining topic relevance in dynamic datasets. All code and data supporting\nthis work are accessible at\nhttps://github.com/zhiyintan/topic-model-LLMjudgment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u52a8\u6001\u4e3b\u9898\u6a21\u578b\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b9\u4e2aLLM\u6307\u6807\uff0c\u8986\u76d6\u56db\u4e2a\u5173\u952e\u8d28\u91cf\u7ef4\u5ea6\uff0c\u76f8\u6bd4\u4f20\u7edf\u6307\u6807\u80fd\u66f4\u597d\u5730\u53d1\u73b0\u8bed\u4e49\u7f3a\u9677\u3002", "motivation": "\u4f20\u7edf\u4e3b\u9898\u6a21\u578b\u8bc4\u4f30\u6307\u6807\uff08\u5982\u8fde\u8d2f\u6027\u548c\u591a\u6837\u6027\uff09\u53ea\u80fd\u6355\u6349\u72ed\u7a84\u7684\u7edf\u8ba1\u6a21\u5f0f\uff0c\u65e0\u6cd5\u89e3\u91ca\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8bed\u4e49\u5931\u8d25\uff0c\u9700\u8981\u66f4\u5168\u9762\u3001\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u9762\u5411\u76ee\u7684\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u75289\u4e2a\u57fa\u4e8eLLM\u7684\u6307\u6807\uff0c\u6db5\u76d6\u8bcd\u6c47\u6709\u6548\u6027\u3001\u4e3b\u9898\u5185\u8bed\u4e49\u5408\u7406\u6027\u3001\u4e3b\u9898\u95f4\u7ed3\u6784\u5408\u7406\u6027\u548c\u6587\u6863-\u4e3b\u9898\u5bf9\u9f50\u5408\u7406\u6027\u56db\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u901a\u8fc7\u5bf9\u6297\u6027\u548c\u62bd\u6837\u534f\u8bae\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "LLM\u6307\u6807\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u7a33\u5065\u4e14\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u8bc4\u4f30\uff0c\u80fd\u591f\u53d1\u73b0\u4f20\u7edf\u6307\u6807\u7ecf\u5e38\u5ffd\u7565\u7684\u5173\u952e\u5f31\u70b9\uff0c\u5982\u5197\u4f59\u548c\u8bed\u4e49\u6f02\u79fb\u3002", "conclusion": "\u8be5\u6846\u67b6\u652f\u6301\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u5de5\u5177\uff0c\u7528\u4e8e\u7ef4\u62a4\u52a8\u6001\u6570\u636e\u96c6\u4e2d\u7684\u4e3b\u9898\u76f8\u5173\u6027\uff0c\u4e3a\u6570\u5b57\u56fe\u4e66\u9986\u7cfb\u7edf\u7684\u4e3b\u9898\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.07177", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07177", "abs": "https://arxiv.org/abs/2509.07177", "authors": ["Amal Chebbi", "Babajide Kolade"], "title": "Towards EnergyGPT: A Large Language Model Specialized for the Energy Sector", "comment": null, "summary": "Large Language Models have demonstrated impressive capabilities across\nvarious domains. However, their general-purpose nature often limits their\neffectiveness in specialized fields such as energy, where deep technical\nexpertise and precise domain knowledge are essential. In this paper, we\nintroduce EnergyGPT, a domain-specialized language model tailored for the\nenergy sector, developed by fine-tuning LLaMA 3.1-8B model using Supervised\nFine-Tuning on a high-quality, curated corpus of energy-related texts. We\npresent a complete development pipeline, including data collection and\ncuration, model fine-tuning, benchmark design and LLM-judge choice, evaluation\nand deployment. Through this work, we demonstrate that our training strategy\nenables improvements in domain relevance and performance without the need for\nlarge-scale infrastructure. By evaluating the performance of the model using\ndomain-specific question-answering benchmarks, our results demonstrate that\nEnergyGPT outperforms the base model in most of the energy-related language\nunderstanding and generation tasks.", "AI": {"tldr": "EnergyGPT\u662f\u57fa\u4e8eLLaMA 3.1-8B\u5fae\u8c03\u7684\u4e13\u4e1a\u80fd\u6e90\u9886\u57df\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u80fd\u6e90\u76f8\u5173\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7840\u6a21\u578b", "motivation": "\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u80fd\u6e90\u7b49\u4e13\u4e1a\u9886\u57df\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u6df1\u5ea6\u6280\u672f\u4e13\u4e1a\u77e5\u8bc6\u548c\u7cbe\u786e\u9886\u57df\u77e5\u8bc6", "method": "\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\uff0c\u5728\u9ad8\u8d28\u91cf\u80fd\u6e90\u6587\u672c\u8bed\u6599\u4e0a\u5fae\u8c03LLaMA 3.1-8B\u6a21\u578b\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u6574\u7406\u3001\u6a21\u578b\u5fae\u8c03\u3001\u57fa\u51c6\u8bbe\u8ba1\u548c\u8bc4\u4f30\u90e8\u7f72\u7684\u5b8c\u6574\u6d41\u7a0b", "result": "EnergyGPT\u5728\u5927\u591a\u6570\u80fd\u6e90\u76f8\u5173\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u57fa\u7840\u6a21\u578b\u7684\u8868\u73b0", "conclusion": "\u901a\u8fc7\u4e13\u95e8\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u4e0d\u9700\u8981\u5927\u89c4\u6a21\u57fa\u7840\u8bbe\u65bd\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u7684\u76f8\u5173\u6027\u548c\u6027\u80fd"}}
{"id": "2509.06964", "categories": ["cs.SD", "cs.AR", "cs.HC", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.06964", "abs": "https://arxiv.org/abs/2509.06964", "authors": ["Huihong Liang", "Dongxuan Jia", "Youquan Wang", "Longtao Huang", "Shida Zhong", "Luping Xiang", "Lei Huang", "Tao Yuan"], "title": "Prototype: A Keyword Spotting-Based Intelligent Audio SoC for IoT", "comment": null, "summary": "In this demo, we present a compact intelligent audio system-on-chip (SoC)\nintegrated with a keyword spotting accelerator, enabling ultra-low latency,\nlow-power, and low-cost voice interaction in Internet of Things (IoT) devices.\nThrough algorithm-hardware co-design, the system's energy efficiency is\nmaximized. We demonstrate the system's capabilities through a live FPGA-based\nprototype, showcasing stable performance and real-time voice interaction for\nedge intelligence applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u5173\u952e\u8bcd\u8bc6\u522b\u52a0\u901f\u5668\u7684\u7d27\u51d1\u667a\u80fd\u97f3\u9891SoC\uff0c\u901a\u8fc7\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u5b9e\u73b0\u8d85\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u529f\u8017\u548c\u4f4e\u6210\u672c\u7684\u7269\u8054\u7f51\u8bbe\u5907\u8bed\u97f3\u4ea4\u4e92", "motivation": "\u4e3a\u7269\u8054\u7f51\u8bbe\u5907\u63d0\u4f9b\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u8bed\u97f3\u4ea4\u4e92\u89e3\u51b3\u65b9\u6848\uff0c\u6ee1\u8db3\u8fb9\u7f18\u667a\u80fd\u5e94\u7528\u5bf9\u5b9e\u65f6\u6027\u548c\u80fd\u6548\u7684\u9700\u6c42", "method": "\u91c7\u7528\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u96c6\u6210\u4e13\u7528\u5173\u952e\u8bcd\u8bc6\u522b\u52a0\u901f\u5668\uff0c\u5e76\u5728FPGA\u5e73\u53f0\u4e0a\u6784\u5efa\u539f\u578b\u7cfb\u7edf", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e86\u8d85\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u529f\u8017\u548c\u7a33\u5b9a\u6027\u80fd\uff0c\u80fd\u591f\u652f\u6301\u5b9e\u65f6\u8bed\u97f3\u4ea4\u4e92", "conclusion": "\u8be5\u667a\u80fd\u97f3\u9891SoC\u7cfb\u7edf\u4e3a\u7269\u8054\u7f51\u8bbe\u5907\u7684\u8bed\u97f3\u4ea4\u4e92\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u8fb9\u7f18\u667a\u80fd\u5e94\u7528\u7684\u6f5c\u529b"}}
{"id": "2509.07817", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.07817", "abs": "https://arxiv.org/abs/2509.07817", "authors": ["Xiaolin Chen", "Xuemeng Song", "Haokun Wen", "Weili Guan", "Xiangyu Zhao", "Liqiang Nie"], "title": "Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems", "comment": null, "summary": "Textual response generation is pivotal for multimodal \\mbox{task-oriented}\ndialog systems, which aims to generate proper textual responses based on the\nmultimodal context. While existing efforts have demonstrated remarkable\nprogress, there still exist the following limitations: 1) \\textit{neglect of\nunstructured review knowledge} and 2) \\textit{underutilization of large\nlanguage models (LLMs)}. Inspired by this, we aim to fully utilize dual\nknowledge (\\textit{i.e., } structured attribute and unstructured review\nknowledge) with LLMs to promote textual response generation in multimodal\ntask-oriented dialog systems. However, this task is non-trivial due to two key\nchallenges: 1) \\textit{dynamic knowledge type selection} and 2)\n\\textit{intention-response decoupling}. To address these challenges, we propose\na novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for\nmultimodal dialog systems (named DK2R). To be specific, DK2R first extracts\nboth structured attribute and unstructured review knowledge from external\nknowledge base given the dialog context. Thereafter, DK2R uses an LLM to\nevaluate each knowledge type's utility by analyzing LLM-generated provisional\nprobe responses. Moreover, DK2R separately summarizes the intention-oriented\nkey clues via dedicated reasoning, which are further used as auxiliary signals\nto enhance LLM-based textual response generation. Extensive experiments\nconducted on a public dataset verify the superiority of DK2R. We have released\nthe codes and parameters.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86DK2R\u6a21\u578b\uff0c\u901a\u8fc7\u5229\u7528\u53cc\u91cd\u77e5\u8bc6\uff08\u7ed3\u6784\u5316\u5c5e\u6027\u548c\u975e\u7ed3\u6784\u5316\u8bc4\u8bba\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u591a\u6a21\u6001\u4efb\u52a1\u5bf9\u8bdd\u7cfb\u7edf\u7684\u6587\u672c\u54cd\u5e94\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u975e\u7ed3\u6784\u5316\u8bc4\u8bba\u77e5\u8bc6\u5e76\u6ca1\u6709\u5145\u5206\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u9700\u8981\u66f4\u597d\u5730\u7ed3\u5408\u53cc\u91cd\u77e5\u8bc6\u6765\u63d0\u5347\u54cd\u5e94\u751f\u6210\u8d28\u91cf\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u9636\u6bb5\u7684DK2R\u6a21\u578b\uff1a\u5148\u4ece\u5916\u90e8\u77e5\u8bc6\u5e93\u63d0\u53d6\u7ed3\u6784\u5316\u5c5e\u6027\u548c\u975e\u7ed3\u6784\u5316\u8bc4\u8bba\u77e5\u8bc6\uff0c\u7136\u540e\u5229\u7528LLM\u8bc4\u4f30\u5404\u79cd\u77e5\u8bc6\u7c7b\u578b\u7684\u6709\u7528\u6027\uff0c\u5206\u79bb\u603b\u7ed3\u610f\u56fe\u5173\u952e\u7ebf\u7d22\u5e76\u4f5c\u4e3a\u8f85\u52a9\u4fe1\u53f7\u6765\u589e\u5f3a\u54cd\u5e94\u751f\u6210\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DK2R\u7684\u4f18\u52bf\u6027\uff0c\u8868\u660e\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u4efb\u52a1\u5bf9\u8bdd\u7cfb\u7edf\u7684\u6587\u672c\u54cd\u5e94\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "DK2R\u901a\u8fc7\u521b\u65b0\u5730\u7ed3\u5408\u53cc\u91cd\u77e5\u8bc6\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001\u77e5\u8bc6\u7c7b\u578b\u9009\u62e9\u548c\u610f\u56fe-\u54cd\u5e94\u89e3\u8026\u7684\u6311\u6218\uff0c\u4e3a\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u7684\u6587\u672c\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.07188", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07188", "abs": "https://arxiv.org/abs/2509.07188", "authors": ["Zonghai Yao", "Michael Sun", "Won Seok Jang", "Sunjae Kwon", "Soie Kwon", "Hong Yu"], "title": "DischargeSim: A Simulation Benchmark for Educational Doctor-Patient Communication at Discharge", "comment": "Equal contribution for the first two authors. To appear in the\n  proceedings of the Main Conference on Empirical Methods in Natural Language\n  Processing (EMNLP) 2025", "summary": "Discharge communication is a critical yet underexplored component of patient\ncare, where the goal shifts from diagnosis to education. While recent large\nlanguage model (LLM) benchmarks emphasize in-visit diagnostic reasoning, they\nfail to evaluate models' ability to support patients after the visit. We\nintroduce DischargeSim, a novel benchmark that evaluates LLMs on their ability\nto act as personalized discharge educators. DischargeSim simulates post-visit,\nmulti-turn conversations between LLM-driven DoctorAgents and PatientAgents with\ndiverse psychosocial profiles (e.g., health literacy, education, emotion).\nInteractions are structured across six clinically grounded discharge topics and\nassessed along three axes: (1) dialogue quality via automatic and LLM-as-judge\nevaluation, (2) personalized document generation including free-text summaries\nand structured AHRQ checklists, and (3) patient comprehension through a\ndownstream multiple-choice exam. Experiments across 18 LLMs reveal significant\ngaps in discharge education capability, with performance varying widely across\npatient profiles. Notably, model size does not always yield better education\noutcomes, highlighting trade-offs in strategy use and content prioritization.\nDischargeSim offers a first step toward benchmarking LLMs in post-visit\nclinical education and promoting equitable, personalized patient support.", "AI": {"tldr": "DischargeSim\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u51fa\u9662\u6c9f\u901a\u4e2d\u4f5c\u4e3a\u4e2a\u6027\u5316\u6559\u80b2\u8005\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u6a21\u62df\u533b\u60a3\u5bf9\u8bdd\u6765\u6d4b\u8bd5\u6a21\u578b\u5728\u5bf9\u8bdd\u8d28\u91cf\u3001\u4e2a\u6027\u5316\u6587\u6863\u751f\u6210\u548c\u60a3\u8005\u7406\u89e3\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684LLM\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u8bca\u65ad\u63a8\u7406\uff0c\u4f46\u5ffd\u89c6\u4e86\u51fa\u9662\u540e\u60a3\u8005\u6559\u80b2\u8fd9\u4e00\u5173\u952e\u73af\u8282\u3002\u51fa\u9662\u6c9f\u901a\u5bf9\u60a3\u8005\u62a4\u7406\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u5728\u8fd9\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u521b\u5efaDischargeSim\u57fa\u51c6\uff0c\u6a21\u62df\u51fa\u9662\u540e\u591a\u8f6e\u533b\u60a3\u5bf9\u8bdd\uff0c\u4f7f\u7528LLM\u9a71\u52a8\u7684\u533b\u751f\u4ee3\u7406\u548c\u5177\u6709\u4e0d\u540c\u5fc3\u7406\u793e\u4f1a\u7279\u5f81\u7684\u60a3\u8005\u4ee3\u7406\u8fdb\u884c\u4ea4\u4e92\uff0c\u6db5\u76d6\u516d\u4e2a\u4e34\u5e8a\u4e3b\u9898\uff0c\u5e76\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6d4b\u8bd518\u4e2aLLM\u53d1\u73b0\u51fa\u9662\u6559\u80b2\u80fd\u529b\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u6027\u80fd\u56e0\u60a3\u8005\u7279\u5f81\u800c\u5f02\uff0c\u6a21\u578b\u5927\u5c0f\u5e76\u4e0d\u603b\u662f\u5e26\u6765\u66f4\u597d\u7684\u6559\u80b2\u6548\u679c\u3002", "conclusion": "DischargeSim\u4e3a\u8bc4\u4f30LLM\u5728\u4e34\u5e8a\u540e\u8bbf\u6559\u80b2\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u9996\u4e2a\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u4fc3\u8fdb\u516c\u5e73\u3001\u4e2a\u6027\u5316\u7684\u60a3\u8005\u652f\u6301\u3002"}}
{"id": "2509.07038", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.07038", "abs": "https://arxiv.org/abs/2509.07038", "authors": ["Yerin Ryu", "Inseop Shin", "Chanwoo Kim"], "title": "Controllable Singing Voice Synthesis using Phoneme-Level Energy Sequence", "comment": "Accepted to ASRU 2025", "summary": "Controllable Singing Voice Synthesis (SVS) aims to generate expressive\nsinging voices reflecting user intent. While recent SVS systems achieve high\naudio quality, most rely on probabilistic modeling, limiting precise control\nover attributes such as dynamics. We address this by focusing on dynamic\ncontrol--temporal loudness variation essential for musical expressiveness--and\nexplicitly condition the SVS model on energy sequences extracted from\nground-truth spectrograms, reducing annotation costs and improving\ncontrollability. We also propose a phoneme-level energy sequence for\nuser-friendly control. To the best of our knowledge, this is the first attempt\nenabling user-driven dynamics control in SVS. Experiments show our method\nachieves over 50% reduction in mean absolute error of energy sequences for\nphoneme-level inputs compared to baseline and energy-predictor models, without\ncompromising synthesis quality.", "AI": {"tldr": "\u901a\u8fc7\u663e\u5f0f\u80fd\u91cf\u5e8f\u5217\u6761\u4ef6\u5316\u63a7\u5236\u6b4c\u5531\u8bed\u97f3\u5408\u6210\u7684\u52a8\u6001\u7279\u6027\uff0c\u5b9e\u73b0\u7528\u6237\u53ef\u9a71\u52a8\u7684\u97f3\u91cf\u53d8\u5316\u63a7\u5236", "motivation": "\u5f53\u524d\u6b4c\u5531\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\u591a\u4f9d\u8d56\u6982\u7387\u6a21\u578b\uff0c\u5bf9\u52a8\u6001\u7279\u6027\u7b49\u5c5e\u6027\u7684\u7cbe\u786e\u63a7\u5236\u6709\u9650", "method": "\u4f7f\u7528\u4ece\u771f\u5b9e\u8bed\u97f3\u8c31\u4e2d\u63d0\u53d6\u7684\u80fd\u91cf\u5e8f\u5217\u663e\u5f0f\u6761\u4ef6\u5316SVS\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u97f3\u7d20\u7ea7\u80fd\u91cf\u5e8f\u5217\u7528\u4e8e\u7528\u6237\u53cb\u597d\u63a7\u5236", "result": "\u5b9e\u9a8c\u663e\u793a\u65b9\u6cd5\u5728\u97f3\u7d20\u7ea7\u8f93\u5165\u4e0b\u80fd\u91cf\u5e8f\u5217\u7684\u5747\u65b9\u5dee\u9510\u51cf\u8d85\u8fc750%\uff0c\u4e14\u4e0d\u5f71\u54cd\u5408\u6210\u8d28\u91cf", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0\u4e86SVS\u4e2d\u7528\u6237\u53ef\u9a71\u52a8\u7684\u52a8\u6001\u63a7\u5236\uff0c\u51cf\u5c11\u4e86\u6807\u6ce8\u6210\u672c\u5e76\u63d0\u9ad8\u4e86\u63a7\u5236\u6027"}}
{"id": "2509.07190", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.07190", "abs": "https://arxiv.org/abs/2509.07190", "authors": ["Zahra Atf", "Peter R Lewis"], "title": "Rule-Based Moral Principles for Explaining Uncertainty in Natural Language Generation", "comment": "This paper was accepted for presentation at the 35th IEEE\n  International Conference on Collaborative Advances in Software and Computing.\n  Conference website:https://conf.researchr.org/home/cascon-2025", "summary": "Large language models (LLMs) are increasingly used in high-stakes settings,\nwhere explaining uncertainty is both technical and ethical. Probabilistic\nmethods are often opaque and misaligned with expectations of transparency. We\npropose a framework based on rule-based moral principles for handling\nuncertainty in LLM-generated text. Using insights from moral psychology and\nvirtue ethics, we define rules such as precaution, deference, and\nresponsibility to guide responses under epistemic or aleatoric uncertainty.\nThese rules are encoded in a lightweight Prolog engine, where uncertainty\nlevels (low, medium, high) trigger aligned system actions with plain-language\nrationales. Scenario-based simulations benchmark rule coverage, fairness, and\ntrust calibration. Use cases in clinical and legal domains illustrate how moral\nreasoning can improve trust and interpretability. Our approach offers a\ntransparent, lightweight alternative to probabilistic models for socially\nresponsible natural language generation.", "AI": {"tldr": "\u57fa\u4e8e\u9053\u5fb7\u539f\u5219\u7684\u89c4\u5219\u7cfb\u7edf\u7528\u4e8e\u5904\u7406LLM\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u9884\u9632\u3001\u5c0a\u91cd\u3001\u8d23\u4efb\u7b49\u9053\u5fb7\u89c4\u5219\u6765\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027", "motivation": "\u6982\u7387\u65b9\u6cd5\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u4e0d\u900f\u660e\u4e14\u4e0e\u900f\u660e\u6027\u671f\u671b\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u65b9\u6848", "method": "\u57fa\u4e8e\u9053\u5fb7\u5fc3\u7406\u5b66\u548c\u7f8e\u5fb7\u4f26\u7406\u5b66\uff0c\u5b9a\u4e49\u9884\u9632\u3001\u5c0a\u91cd\u3001\u8d23\u4efb\u7b49\u9053\u5fb7\u89c4\u5219\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7Prolog\u5f15\u64ce\u7f16\u7801\uff0c\u6839\u636e\u4e0d\u786e\u5b9a\u6027\u7ea7\u522b\u89e6\u53d1\u76f8\u5e94\u7cfb\u7edf\u52a8\u4f5c\u5e76\u63d0\u4f9b\u7406\u7531", "result": "\u901a\u8fc7\u573a\u666f\u6a21\u62df\u6d4b\u8bd5\u89c4\u5219\u8986\u76d6\u7387\u3001\u516c\u5e73\u6027\u548c\u4fe1\u4efb\u68c0\u9a8c\uff0c\u5728\u4e34\u5e8a\u548c\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\u6848\u4f8b\u8bc1\u660e\u9053\u5fb7\u63a8\u7406\u80fd\u591f\u63d0\u9ad8\u4fe1\u4efb\u548c\u53ef\u89e3\u91ca\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u900f\u660e\u3001\u8f7b\u91cf\u7ea7\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u53ef\u4ee5\u5728\u793e\u4f1a\u8d23\u4efb\u804c\u8d23\u7684\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4e2d\u66f4\u597d\u5730\u5904\u7406\u4e0d\u786e\u5b9a\u6027"}}
{"id": "2509.07051", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07051", "abs": "https://arxiv.org/abs/2509.07051", "authors": ["Pietro Bartoli", "Tommaso Bondini", "Christian Veronesi", "Andrea Giudici", "Niccol\u00f2 Antonello", "Franco Zappa"], "title": "End-to-End Efficiency in Keyword Spotting: A System-Level Approach for Embedded Microcontrollers", "comment": "4 pages, 2 figures, 1 table. Accepted for publication in IEEE Sensors\n  2025. \\c{opyright} 2025 IEEE. Personal use permitted. Permission from IEEE\n  required for all other uses", "summary": "Keyword spotting (KWS) is a key enabling technology for hands-free\ninteraction in embedded and IoT devices, where stringent memory and energy\nconstraints challenge the deployment of AI-enabeld devices. In this work, we\nsystematically evaluate and compare several state-of-the-art lightweight neural\nnetwork architectures, including DS-CNN, LiCoNet, and TENet, alongside our\nproposed Typman-KWS (TKWS) architecture built upon MobileNet, specifically\ndesigned for efficient KWS on microcontroller units (MCUs). Unlike prior\nstudies focused solely on model inference, our analysis encompasses the entire\nprocessing pipeline, from Mel-Frequency Cepstral Coefficient (MFCC) feature\nextraction to neural inference, and is benchmarked across three STM32 platforms\n(N6, H7, and U5). Our results show that TKWS with three residual blocks\nachieves up to 92.4% F1-score with only 14.4k parameters, reducing memory\nfootprint without compromising the accuracy. Moreover, the N6 MCU with\nintegrated neural acceleration achieves the best energy-delay product (EDP),\nenabling efficient, low-latency operation even with high-resolution features.\nOur findings highlight the model accuracy alone does not determine real-world\neffectiveness; rather, optimal keyword spotting deployments require careful\nconsideration of feature extraction parameters and hardware-specific\noptimization.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6027\u8bc4\u4f30\u591a\u79cd\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u5728\u5fae\u63a7\u5236\u5668\u4e0a\u7684\u5173\u952e\u8bcd\u68c0\u6d4b\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8eMobileNet\u7684TKWS\u6a21\u578b\uff0c\u5728\u4ec5\u536014.4k\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u8fbe\u523092.4%\u7684F1\u5206\u6570\uff0c\u5e76\u5206\u6790\u4e86\u4ece\u7279\u5f81\u63d0\u53d6\u5230\u63a8\u7406\u7684\u5168\u5904\u7406\u6d41\u7a0b\u3002", "motivation": "\u5173\u952e\u8bcd\u68c0\u6d4b(KWS)\u662f\u5d4c\u5165\u5f0f\u548cIoT\u8bbe\u5907\u4e2d\u65e0\u624b\u4ea4\u4e92\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46\u5185\u5b58\u548c\u80fd\u6e90\u7ea6\u675f\u4e3aAI\u8bbe\u5907\u90e8\u7f72\u5e26\u6765\u6311\u6218\u3002\u9700\u8981\u627e\u5230\u65e2\u7cbe\u786e\u53c8\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u6027\u8bc4\u4f30DS-CNN\u3001LiCoNet\u3001TENet\u7b49\u8f7b\u91cf\u7ea7\u7f51\u7edc\u67b6\u6784\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8eMobileNet\u7684TKWS\u67b6\u6784\u3002\u5206\u6790\u4eceMFCC\u7279\u5f81\u63d0\u53d6\u5230\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u7684\u5168\u5904\u7406\u6d41\u7a0b\uff0c\u5728\u4e09\u79cdSTM32\u5e73\u53f0(N6\u3001H7\u3001U5)\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "TKWS\u4f7f\u75283\u4e2a\u6b8a\u5fcd\u5757\u8fbe\u523092.4%\u7684F1\u5206\u6570\uff0c\u53c2\u6570\u4ec5\u536014.4k\u3002\u96c6\u6210\u795e\u7ecf\u52a0\u901f\u7684N6 MCU\u83b7\u5f97\u6700\u4f73\u80fd\u6e90-\u5ef6\u8fdf\u4e58\u79ef(EDP)\uff0c\u80fd\u591f\u5728\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u4e0b\u5b9e\u73b0\u9ad8\u6548\u4f4e\u5ef6\u8fdf\u8fd0\u884c\u3002", "conclusion": "\u6a21\u578b\u51c6\u786e\u6027\u4e0d\u662f\u552f\u4e00\u51b3\u5b9a\u56e0\u7d20\uff0c\u6700\u4f73\u7684\u5173\u952e\u8bcd\u68c0\u6d4b\u90e8\u7f72\u9700\u8981\u7ef4\u5ea6\u8003\u8651\u7279\u5f81\u63d0\u53d6\u53c2\u6570\u548c\u786c\u4ef6\u7279\u5b9a\u4f18\u5316\u3002"}}
{"id": "2509.07274", "categories": ["cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07274", "abs": "https://arxiv.org/abs/2509.07274", "authors": ["Aida Kostikova", "Ole P\u00fctz", "Steffen Eger", "Olga Sabelfeld", "Benjamin Paassen"], "title": "LLM Analysis of 150+ years of German Parliamentary Debates on Migration Reveals Shift from Post-War Solidarity to Anti-Solidarity in the Last Decade", "comment": null, "summary": "Migration has been a core topic in German political debate, from millions of\nexpellees post World War II over labor migration to refugee movements in the\nrecent past. Studying political speech regarding such wide-ranging phenomena in\ndepth traditionally required extensive manual annotations, limiting the scope\nof analysis to small subsets of the data. Large language models (LLMs) have the\npotential to partially automate even complex annotation tasks. We provide an\nextensive evaluation of a multiple LLMs in annotating (anti-)solidarity\nsubtypes in German parliamentary debates compared to a large set of thousands\nof human reference annotations (gathered over a year). We evaluate the\ninfluence of model size, prompting differences, fine-tuning, historical versus\ncontemporary data; and we investigate systematic errors. Beyond methodological\nevaluation, we also interpret the resulting annotations from a social science\nlense, gaining deeper insight into (anti-)solidarity trends towards migrants in\nthe German post-World War II period and recent past. Our data reveals a high\ndegree of migrant-directed solidarity in the postwar period, as well as a\nstrong trend towards anti-solidarity in the German parliament since 2015,\nmotivating further research. These findings highlight the promise of LLMs for\npolitical text analysis and the importance of migration debates in Germany,\nwhere demographic decline and labor shortages coexist with rising polarization.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u6807\u6ce8\u5fb7\u56fd\u8bae\u4f1a\u8fa9\u8bba\u4e2d\u7684(\u53cd)\u56e2\u7ed3\u7c7b\u578b\uff0c\u4e0e\u4f20\u7edf\u4eba\u5de5\u6807\u6ce8\u5bf9\u6bd4\uff0c\u53d1\u73b0LLM\u5728\u653f\u6cbb\u6587\u672c\u5206\u6790\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u63ed\u793a\u4e86\u5fb7\u56fd\u6218\u540e\u79fb\u6c11\u56e2\u7ed3\u5ea6\u8f83\u9ad8\u800c2015\u5e74\u540e\u53cd\u56e2\u7ed3\u8d8b\u52bf\u589e\u5f3a\u7684\u73b0\u8c61", "motivation": "\u4f20\u7edf\u4eba\u5de5\u6807\u6ce8\u653f\u6cbb\u6f14\u8bb2\u4e2d\u7684(\u53cd)\u56e2\u7ed3\u7c7b\u578b\u8017\u65f6\u8017\u529b\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u6570\u636e\u5206\u6790\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLM\u5728\u81ea\u52a8\u5316\u590d\u6742\u6807\u6ce8\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u6df1\u5165\u5206\u6790\u5fb7\u56fd\u6218\u540e\u79fb\u6c11\u653f\u7b56\u8fa9\u8bba\u4e2d\u7684\u56e2\u7ed3\u8d8b\u52bf", "method": "\u4f7f\u7528\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u5fb7\u56fd\u8bae\u4f1a\u8fa9\u8bba\u8fdb\u884c(\u53cd)\u56e2\u7ed3\u5b50\u7c7b\u578b\u6807\u6ce8\uff0c\u4e0e\u6570\u5343\u4e2a\u4eba\u5de5\u53c2\u8003\u6807\u6ce8\u5bf9\u6bd4\uff0c\u8bc4\u4f30\u6a21\u578b\u5927\u5c0f\u3001\u63d0\u793a\u5dee\u5f02\u3001\u5fae\u8c03\u3001\u5386\u53f2\u4e0e\u5f53\u4ee3\u6570\u636e\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u7cfb\u7edf\u9519\u8bef", "result": "LLM\u5728\u653f\u6cbb\u6587\u672c\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u6f5c\u529b\uff0c\u6570\u636e\u63ed\u793a\u6218\u540e\u65f6\u671f\u5bf9\u79fb\u6c11\u7684\u9ad8\u5ea6\u56e2\u7ed3\uff0c\u4ee5\u53ca2015\u5e74\u4ee5\u6765\u5fb7\u56fd\u8bae\u4f1a\u4e2d\u5f3a\u70c8\u7684\u53cd\u56e2\u7ed3\u8d8b\u52bf", "conclusion": "LLM\u4e3a\u653f\u6cbb\u6587\u672c\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u5de5\u5177\uff0c\u5fb7\u56fd\u79fb\u6c11\u8fa9\u8bba\u5728\u4eba\u53e3\u8870\u9000\u548c\u52b3\u52a8\u529b\u77ed\u7f3a\u4e0e\u65e5\u76ca\u6781\u5316\u7684\u80cc\u666f\u4e0b\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u8fd9\u4e9b\u53d1\u73b0\u6fc0\u52b1\u8fdb\u4e00\u6b65\u7814\u7a76"}}
{"id": "2509.07132", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07132", "abs": "https://arxiv.org/abs/2509.07132", "authors": ["Kutub Uddin", "Muhammad Umar Farooq", "Awais Khan", "Khalid Mahmood Malik"], "title": "Adversarial Attacks on Audio Deepfake Detection: A Benchmark and Comparative Study", "comment": null, "summary": "The widespread use of generative AI has shown remarkable success in producing\nhighly realistic deepfakes, posing a serious threat to various voice biometric\napplications, including speaker verification, voice biometrics, audio\nconferencing, and criminal investigations. To counteract this, several\nstate-of-the-art (SoTA) audio deepfake detection (ADD) methods have been\nproposed to identify generative AI signatures to distinguish between real and\ndeepfake audio. However, the effectiveness of these methods is severely\nundermined by anti-forensic (AF) attacks that conceal generative signatures.\nThese AF attacks span a wide range of techniques, including statistical\nmodifications (e.g., pitch shifting, filtering, noise addition, and\nquantization) and optimization-based attacks (e.g., FGSM, PGD, C \\& W, and\nDeepFool). In this paper, we investigate the SoTA ADD methods and provide a\ncomparative analysis to highlight their effectiveness in exposing deepfake\nsignatures, as well as their vulnerabilities under adversarial conditions. We\nconducted an extensive evaluation of ADD methods on five deepfake benchmark\ndatasets using two categories: raw and spectrogram-based approaches. This\ncomparative analysis enables a deeper understanding of the strengths and\nlimitations of SoTA ADD methods against diverse AF attacks. It does not only\nhighlight vulnerabilities of ADD methods, but also informs the design of more\nrobust and generalized detectors for real-world voice biometrics. It will\nfurther guide future research in developing adaptive defense strategies that\ncan effectively counter evolving AF techniques.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u73b0\u6709\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u7684\u6548\u679c\uff0c\u5e76\u8bc4\u4f30\u5b83\u4eec\u5728\u53cd\u4f8b\u5b66\u653b\u51fb\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u4ee5\u6307\u5bfc\u66f4\u7a33\u5065\u7684\u68c0\u6d4b\u5668\u8bbe\u8ba1\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u4ea7\u751f\u9ad8\u4f2a\u771f\u6df1\u5ea6\u4f2a\u9020\u97f3\u9891\u7684\u666e\u53ca\uff0c\u5bf9\u8bed\u97f3\u751f\u7269\u8bc6\u522b\u5e94\u7528\u6784\u6210\u4e86\u4e25\u91cd\u5a01\u80c1\u3002\u867d\u7136\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u80fd\u8bc6\u522b\u751f\u6210\u5f0f\u7b7e\u540d\uff0c\u4f46\u53cd\u4f8b\u5b66\u653b\u51fb\u53ef\u4ee5\u9690\u85cf\u8fd9\u4e9b\u7b7e\u540d\uff0c\u5f88\u5927\u7a0b\u5ea6\u4e0a\u524a\u5f31\u4e86\u68c0\u6d4b\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u8bc4\u4f30\u73b0\u6709\u6700\u5148\u8fdb\u7684\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5305\u62ec\u539f\u59cb\u97f3\u9891\u548c\u8c31\u56fe\u57fa\u4e8e\u7684\u65b9\u6cd5\u3002\u4f7f\u75285\u4e2a\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u5bf9\u4e24\u7c7b\u53cd\u4f8b\u5b66\u653b\u51fb\u7684\u8010\u53d7\u6027\uff1a\u7edf\u8ba1\u4fee\u6539\uff08\u5982\u97f3\u9ad8\u53d8\u6362\u3001\u7b5b\u6ce2\u3001\u566a\u58f0\u6dfb\u52a0\u3001\u91cf\u5316\uff09\u548c\u4f18\u5316\u57fa\u4e8e\u653b\u51fb\uff08\u5982FGSM\u3001PGD\u3001C&W\u3001DeepFool\uff09\u3002", "result": "\u5bf9\u6bd4\u5206\u6790\u63ed\u793a\u4e86\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5728\u8bc6\u522b\u6df1\u5ea6\u4f2a\u9020\u7b7e\u540d\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u4ee5\u53ca\u5728\u5404\u79cd\u53cd\u4f8b\u5b66\u653b\u51fb\u4e0b\u7684\u660e\u663e\u8106\u5f31\u6027\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5bf9\u6297\u53d8\u5316\u591a\u7aef\u7684\u53cd\u4f8b\u5b66\u6280\u672f\u65f6\u663e\u793a\u51fa\u4e86\u4e0d\u540c\u7a0b\u5ea6\u7684\u6f5c\u5728\u6f0f\u6d1e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u63ed\u793a\u4e86\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u7a33\u5065\u3001\u901a\u7528\u6027\u66f4\u5f3a\u7684\u68c0\u6d4b\u5668\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u5e76\u5c06\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u53d1\u5c55\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u65e5\u76ca\u6f5c\u5165\u7684\u53cd\u4f8b\u5b66\u6280\u672f\u7684\u9002\u5e94\u6027\u9632\u5fa1\u7b56\u7565\u3002"}}
{"id": "2509.07301", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07301", "abs": "https://arxiv.org/abs/2509.07301", "authors": ["Zhuoqing Song", "Peng Sun", "Huizhuo Yuan", "Quanquan Gu"], "title": "Causal Attention with Lookahead Keys", "comment": null, "summary": "In standard causal attention, each token's query, key, and value (QKV) are\nstatic and encode only preceding context. We introduce CAuSal aTtention with\nLookahead kEys (CASTLE), an attention mechanism that continually updates each\ntoken's keys as the context unfolds. We term these updated keys lookahead keys\nbecause they belong to earlier positions yet integrate information from tokens\nthat appear later relative to those positions, while strictly preserving the\nautoregressive property. Although the mechanism appears sequential, we derive a\nmathematical equivalence that avoids explicitly materializing lookahead keys at\neach position and enables efficient parallel training. On language modeling\nbenchmarks, CASTLE consistently outperforms standard causal attention across\nmodel scales, reducing validation perplexity and improving performance on a\nrange of downstream tasks.", "AI": {"tldr": "CASTLE\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0token\u7684keys\u6765\u6574\u5408\u540e\u7eed\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u6301\u81ea\u56de\u5f52\u7279\u6027\uff0c\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6\u56e0\u679c\u6ce8\u610f\u529b\u3002", "motivation": "\u6807\u51c6\u56e0\u679c\u6ce8\u610f\u529b\u4e2d\u6bcf\u4e2atoken\u7684QKV\u662f\u9759\u6001\u7684\uff0c\u53ea\u80fd\u7f16\u7801\u524d\u6587\u4fe1\u606f\u3002\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u6574\u5408\u540e\u7eed\u4e0a\u4e0b\u6587\u4fe1\u606f\u4f46\u4ecd\u4fdd\u6301\u81ea\u56de\u5f52\u7279\u6027\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002", "method": "\u63d0\u51faCASTLE\u673a\u5236\uff0c\u6301\u7eed\u66f4\u65b0\u6bcf\u4e2atoken\u7684keys\uff08\u79f0\u4e3alookahead keys\uff09\uff0c\u8fd9\u4e9bkeys\u5c5e\u4e8e\u8f83\u65e9\u4f4d\u7f6e\u4f46\u6574\u5408\u4e86\u76f8\u5bf9\u8fd9\u4e9b\u4f4d\u7f6e\u4e4b\u540e\u51fa\u73b0\u7684token\u4fe1\u606f\u3002\u901a\u8fc7\u6570\u5b66\u7b49\u4ef7\u6027\u907f\u514d\u663e\u5f0f\u5b58\u50a8lookahead keys\uff0c\u5b9e\u73b0\u9ad8\u6548\u5e76\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCASTLE\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u6807\u51c6\u56e0\u679c\u6ce8\u610f\u529b\uff0c\u964d\u4f4e\u4e86\u9a8c\u8bc1\u56f0\u60d1\u5ea6\uff0c\u5e76\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "CASTLE\u901a\u8fc7\u52a8\u6001\u66f4\u65b0keys\u6574\u5408\u540e\u7eed\u4fe1\u606f\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2509.07323", "categories": ["cs.SD", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.07323", "abs": "https://arxiv.org/abs/2509.07323", "authors": ["Bin Hu", "Kunyang Huang", "Daehan Kwak", "Meng Xu", "Kuan Huang"], "title": "When Fine-Tuning is Not Enough: Lessons from HSAD on Hybrid and Adversarial Audio Spoof Detection", "comment": "13 pages, 11 figures.This work has been submitted to the IEEE for\n  possible publication", "summary": "The rapid advancement of AI has enabled highly realistic speech synthesis and\nvoice cloning, posing serious risks to voice authentication, smart assistants,\nand telecom security. While most prior work frames spoof detection as a binary\ntask, real-world attacks often involve hybrid utterances that mix genuine and\nsynthetic speech, making detection substantially more challenging. To address\nthis gap, we introduce the Hybrid Spoofed Audio Dataset (HSAD), a benchmark\ncontaining 1,248 clean and 41,044 degraded utterances across four classes:\nhuman, cloned, zero-shot AI-generated, and hybrid audio. Each sample is\nannotated with spoofing method, speaker identity, and degradation metadata to\nenable fine-grained analysis. We evaluate six transformer-based models,\nincluding spectrogram encoders (MIT-AST, MattyB95-AST) and self-supervised\nwaveform models (Wav2Vec2, HuBERT). Results reveal critical lessons: pretrained\nmodels overgeneralize and collapse under hybrid conditions; spoof-specific\nfine-tuning improves separability but struggles with unseen compositions; and\ndataset-specific adaptation on HSAD yields large performance gains (AST greater\nthan 97 percent and F1 score is approximately 99 percent), though residual\nerrors persist for complex hybrids. These findings demonstrate that fine-tuning\nalone is not sufficient-robust hybrid-aware benchmarks like HSAD are essential\nto expose calibration failures, model biases, and factors affecting spoof\ndetection in adversarial environments. HSAD thus provides both a dataset and an\nanalytic framework for building resilient and trustworthy voice authentication\nsystems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u6df7\u5408\u8bc1\u8bc1\u97f3\u9891\u6570\u636e\u96c6HSAD\uff0c\u7528\u4e8e\u5904\u7406\u771f\u5b9e\u548c\u5408\u6210\u8bed\u97f3\u6df7\u5408\u7684\u6b3a\u9a97\u653b\u51fb\uff0c\u8bc1\u660e\u73b0\u6709\u6a21\u578b\u5728\u6df7\u5408\u6761\u4ef6\u4e0b\u8868\u73b0\u5dee\u5f3a\uff0c\u9700\u8981\u4e13\u95e8\u7684\u6570\u636e\u96c6\u6765\u5efa\u7acb\u7a33\u5065\u7684\u8bed\u97f3\u8ba4\u8bc1\u7cfb\u7edf\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u8bed\u97f3\u5408\u6210\u548c\u58f0\u97f3\u514b\u9686\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u5bf9\u58f0\u97f3\u8ba4\u8bc1\u3001\u667a\u80fd\u52a9\u624b\u548c\u901a\u4fe1\u5b89\u5168\u6784\u6210\u4e86\u4e25\u91cd\u5a01\u80c1\u3002\u73b0\u6709\u7684\u6b3a\u9a97\u68c0\u6d4b\u591a\u4e3a\u4e8c\u5143\u5206\u7c7b\uff0c\u800c\u771f\u5b9e\u653b\u51fb\u5e38\u5305\u542b\u771f\u5b9e\u548c\u5408\u6210\u8bed\u97f3\u7684\u6df7\u5408\u5185\u5bb9\uff0c\u4f7f\u68c0\u6d4b\u66f4\u52a0\u56f0\u96be\u3002", "method": "\u7814\u7a76\u4eba\u5458\u6784\u5efa\u4e86\u6df7\u5408\u8bc1\u8bc1\u97f3\u9891\u6570\u636e\u96c6HSAD\uff0c\u5305\u542b41,044\u4e2a\u9000\u5316\u8bed\u97f3\u548c1,248\u4e2a\u6e05\u6d01\u8bed\u97f3\uff0c\u5206\u4e3a\u56db\u4e2a\u7c7b\u522b\uff1a\u4eba\u7c7b\u3001\u514b\u9686\u97f3\u9891\u3001\u96f6\u6837\u672cAI\u751f\u6210\u97f3\u9891\u548c\u6df7\u5408\u97f3\u9891\u3002\u6bcf\u4e2a\u6837\u672c\u90fd\u6709\u6b3a\u9a97\u65b9\u6cd5\u3001\u8bf4\u8bdd\u4eba\u8eab\u4efd\u548c\u9000\u5316\u5143\u6570\u636e\u6807\u7b7e\u3002\u8bc4\u4f30\u4e86\u516d\u79cd\u57fa\u4e8etransformer\u7684\u6a21\u578b\uff0c\u5305\u62ec\u8c31\u56fe\u7f16\u7801\u5668\u548c\u81ea\u76d1\u7763\u6ce2\u5f62\u6a21\u578b\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u6df7\u5408\u6761\u4ef6\u4e0b\u5b58\u5728\u8fc7\u5ea6\u666e\u9002\u548c\u6027\u80fd\u5448\u73b0\u7a81\u7136\u4e0b\u964d\u7684\u95ee\u9898\uff1b\u9488\u5bf9\u6b3a\u9a97\u68c0\u6d4b\u7684\u7ec6\u8c03\u80fd\u63d0\u9ad8\u53ef\u5206\u79bb\u6027\uff0c\u4f46\u5bf9\u672a\u89c1\u7ec4\u5408\u6548\u679c\u4e0d\u4f73\uff1b\u5728HSAD\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6570\u636e\u96c6\u7279\u5b9a\u9002\u5e94\u80fd\u5927\u5e45\u63d0\u5347\u6027\u80fd\uff08AST\u6a21\u578b\u51c6\u786e\u7387\u8d85\u8fc797%\uff0cF1\u5f97\u5206\u7ea699%\uff09\uff0c\u4f46\u590d\u6742\u6df7\u5408\u97f3\u9891\u4ecd\u6709\u6b8b\u4f59\u9519\u8bef\u3002", "conclusion": "\u7ec6\u8c03\u5355\u72ec\u4e0d\u8db3\u4ee5\u5efa\u7acb\u7a33\u5065\u7684\u6df7\u5408\u8bc1\u8bc1\u68c0\u6d4b\u7cfb\u7edf\u3002HSAD\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u6d4b\u8bd5\u57fa\u51c6\u548c\u5206\u6790\u6846\u67b6\uff0c\u80fd\u591f\u66dd\u9732\u6a21\u578b\u7684\u6821\u51c6\u5931\u8d25\u3001\u504f\u89c1\u548c\u5f71\u54cd\u6b3a\u9a97\u68c0\u6d4b\u7684\u56e0\u7d20\uff0c\u4e3a\u5efa\u7acb\u53ef\u9760\u548c\u53ef\u4fe1\u8d56\u7684\u58f0\u97f3\u8ba4\u8bc1\u7cfb\u7edf\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2509.07308", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07308", "abs": "https://arxiv.org/abs/2509.07308", "authors": ["David Oprea", "Sam Powers"], "title": "Basis Vector Metric: A Method for Robust Open-Ended State Change Detection", "comment": "24 pages", "summary": "We test a new method, which we will abbreviate using the acronym BVM (Basis\nVectors Method), in its ability to judge the state changes in images through\nusing language embeddings. We used the MIT-States dataset, containing about\n53,000 images, to gather all of our data, which has 225 nouns and 115\nadjectives, with each noun having about 9 different adjectives, forming\napproximately 1000 noun-adjective pairs. For our first experiment, we test our\nmethod's ability to determine the state of each noun class separately against\nother metrics for comparison. These metrics are cosine similarity, dot product,\nproduct quantization, binary index, Naive Bayes, and a custom neural network.\nAmong these metrics, we found that our proposed BVM performs the best in\nclassifying the states for each noun. We then perform a second experiment where\nwe try using BVM to determine if it can differentiate adjectives from one\nanother for each adjective separately. We compared the abilities of BVM to\ndifferentiate adjectives against the proposed method the MIT-States paper\nsuggests: using a logistic regression model. In the end, we did not find\nconclusive evidence that our BVM metric could perform better than the logistic\nregression model at discerning adjectives. Yet, we were able to find evidence\nfor possible improvements to our method; this leads to the chance of increasing\nour method's accuracy through certain changes in our methodologies.", "AI": {"tldr": "BVM\u65b9\u6cd5\u5728MIT-States\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u56fe\u50cf\u72b6\u6001\u5224\u65ad\u80fd\u529b\uff0c\u5728\u540d\u8bcd\u5206\u7c7b\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5728\u5f62\u5bb9\u8bcd\u533a\u5206\u65b9\u9762\u4e0d\u5982\u903b\u8f91\u56de\u5f52\u6a21\u578b", "motivation": "\u6d4b\u8bd5BVM\u65b9\u6cd5\u5728\u8bed\u8a00\u5d4c\u5165\u4e2d\u5224\u65ad\u56fe\u50cf\u72b6\u6001\u53d8\u5316\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5bf9\u540d\u8bcd\u72b6\u6001\u5206\u7c7b\u548c\u5f62\u5bb9\u8bcd\u533a\u5206\u7684\u6027\u80fd", "method": "\u4f7f\u7528BVM\uff08\u57fa\u5411\u91cf\u65b9\u6cd5\uff09\u5728MIT-States\u6570\u636e\u96c6\uff0853,000\u5f20\u56fe\u50cf\uff0c225\u4e2a\u540d\u8bcd\u548c115\u4e2a\u5f62\u5bb9\u8bcd\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4e0e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3001\u70b9\u79ef\u3001\u4e58\u79ef\u91cf\u5316\u3001\u4e8c\u8fdb\u5236\u7d22\u5f15\u3001\u6734\u7d20\u8d1d\u53f6\u65af\u548c\u81ea\u5b9a\u4e49\u795e\u7ecf\u7f51\u7edc\u7b49\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83", "result": "BVM\u5728\u540d\u8bcd\u72b6\u6001\u5206\u7c7b\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5728\u5f62\u5bb9\u8bcd\u533a\u5206\u65b9\u9762\u672a\u80fd\u8d85\u8d8a\u903b\u8f91\u56de\u5f52\u6a21\u578b", "conclusion": "BVM\u5728\u540d\u8bcd\u5206\u7c7b\u65b9\u9762\u6709\u6548\uff0c\u4f46\u5728\u5f62\u5bb9\u8bcd\u533a\u5206\u65b9\u9762\u9700\u8981\u65b9\u6cd5\u6539\u8fdb\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027"}}
{"id": "2509.07376", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.07376", "abs": "https://arxiv.org/abs/2509.07376", "authors": ["Yejin Jeon", "Youngjae Kim", "Jihyun Lee", "Hyounghun Kim", "Gary Geunbae Lee"], "title": "Progressive Facial Granularity Aggregation with Bilateral Attribute-based Enhancement for Face-to-Speech Synthesis", "comment": "EMNLP Findings", "summary": "For individuals who have experienced traumatic events such as strokes, speech\nmay no longer be a viable means of communication. While text-to-speech (TTS)\ncan be used as a communication aid since it generates synthetic speech, it\nfails to preserve the user's own voice. As such, face-to-voice (FTV) synthesis,\nwhich derives corresponding voices from facial images, provides a promising\nalternative. However, existing methods rely on pre-trained visual encoders, and\nfinetune them to align with speech embeddings, which strips fine-grained\ninformation from facial inputs such as gender or ethnicity, despite their known\ncorrelation with vocal traits. Moreover, these pipelines are multi-stage, which\nrequires separate training of multiple components, thus leading to training\ninefficiency. To address these limitations, we utilize fine-grained facial\nattribute modeling by decomposing facial images into non-overlapping segments\nand progressively integrating them into a multi-granular representation. This\nrepresentation is further refined through multi-task learning of speaker\nattributes such as gender and ethnicity at both the visual and acoustic\ndomains. Moreover, to improve alignment robustness, we adopt a multi-view\ntraining strategy by pairing various visual perspectives of a speaker in terms\nof different angles and lighting conditions, with identical speech recordings.\nExtensive subjective and objective evaluations confirm that our approach\nsubstantially enhances face-voice congruence and synthesis stability.", "AI": {"tldr": "\u901a\u8fc7\u7ec6\u7c92\u5ea6\u9762\u90e8\u5c5e\u6027\u5efa\u6a21\u548c\u591a\u89c6\u89d2\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u7684\u9762\u90e8\u5230\u58f0\u97f3\u5408\u6210\u65b9\u6cd5\uff0c\u4fdd\u7559\u4e86\u6027\u522b\u548c\u79cd\u65cf\u7b49\u7ec6\u7c92\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9762\u90e8-\u58f0\u97f3\u4e00\u81f4\u6027\u548c\u5408\u6210\u7a33\u5b9a\u6027\u3002", "motivation": "\u4e3a\u89e3\u51b3\u4e2d\u98ce\u7b49\u4f24\u5bb3\u540e\u5931\u8bed\u60c5\u51b5\u4e0b\u7684\u6c9f\u901a\u95ee\u9898\uff0c\u5f53\u524d\u7684\u6587\u672c\u5230\u8bed\u97f3(TTS)\u6280\u672f\u65e0\u6cd5\u4fdd\u7559\u7528\u6237\u539f\u6709\u58f0\u97f3\u7279\u5f81\u3002\u9762\u90e8\u5230\u58f0\u97f3(FTV)\u5408\u6210\u6280\u672f\u5e94\u8fd0\u800c\u751f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7ec6\u7c92\u5ea6\u4fe1\u606f\u635f\u5931\u548c\u8bad\u7ec3\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u5c06\u9762\u90e8\u56fe\u50cf\u5206\u89e3\u4e3a\u975e\u91cd\u53e0\u5206\u5757\uff0c\u9010\u6b65\u96c6\u6210\u591a\u7c92\u5ea6\u8868\u5f81\u3002\u901a\u8fc7\u5728\u89c6\u89c9\u548c\u97f3\u54cd\u9886\u57df\u8fdb\u884c\u6027\u522b\u3001\u79cd\u65cf\u7b49\u8bb2\u8005\u5c5e\u6027\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6765\u7cbe\u70bc\u8868\u5f81\u3002\u91c7\u7528\u591a\u89c6\u89d2\u8bad\u7ec3\u7b56\u7565\uff0c\u5c06\u4e0d\u540c\u89d2\u5ea6\u548c\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u9762\u90e8\u56fe\u50cf\u4e0e\u540c\u4e00\u8bed\u97f3\u8bb0\u5f55\u8fdb\u884c\u5339\u914d\u3002", "result": "\u7ecf\u8fc7\u5e7f\u6cdb\u4e3b\u89c2\u548c\u5ba2\u89c2\u8bc4\u4f30\uff0c\u8bc1\u5b9e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9762\u90e8-\u58f0\u97f3\u4e00\u81f4\u6027\u548c\u5408\u6210\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u7ec6\u7c92\u5ea6\u9762\u90e8\u5c5e\u6027\u5efa\u6a21\u548c\u591a\u89c6\u89d2\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709FTV\u5408\u6210\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5931\u8bed\u75c5\u4eba\u63d0\u4f9b\u4e86\u66f4\u4fdd\u771f\u548c\u9ad8\u6548\u7684\u6c9f\u901a\u5e2e\u52a9\u65b9\u6848\u3002"}}
{"id": "2509.07309", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07309", "abs": "https://arxiv.org/abs/2509.07309", "authors": ["Chi-Yang Hsu", "Alexander Braylan", "Yiheng Su", "Omar Alonso", "Matthew Lease"], "title": "Instance-level Performance Prediction for Long-form Generation Tasks", "comment": null, "summary": "We motivate and share a new benchmark for instance-level performance\nprediction of long-form generation tasks having multi-faceted, fine-grained\nquality metrics. Our task-, model- and metric-agnostic formulation predicts\ncontinuous evaluation metric scores given only black-box model inputs and\noutputs. Beyond predicting point estimates of metric scores, the benchmark also\nrequires inferring prediction intervals to quantify uncertainty around point\nestimates. Evaluation spans 11 long-form datasets/tasks with multiple LLMs,\nbaselines, and metrics per task. We show that scores can be effectively\npredicted across long-form generation tasks using as few as 16 training\nexamples. Overall, we introduce a novel and useful task, a valuable benchmark\nto drive progress, and baselines ready for practical adoption today.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u957f\u6587\u672c\u751f\u6210\u4efb\u52a1\u7684\u5b9e\u4f8b\u7ea7\u6027\u80fd\u9884\u6d4b\u6d4b\u8bd5\u6807\u51c6\uff0c\u652f\u6301\u591a\u7ef4\u5ea6\u8bc4\u4ef7\u6307\u6807\u548c\u4e0d\u786e\u5b9a\u6027\u9884\u6d4b\uff0c\u4ec5\u970016\u4e2a\u8bad\u7ec3\u793a\u4f8b\u5373\u53ef\u5b9e\u73b0\u6709\u6548\u9884\u6d4b\u3002", "motivation": "\u4e3a\u957f\u6587\u672c\u751f\u6210\u4efb\u52a1\u6784\u5efa\u4e00\u4e2a\u4efb\u52a1\u3001\u6a21\u578b\u548c\u8bc4\u4ef7\u6307\u6807\u65e0\u5173\u7684\u6027\u80fd\u9884\u6d4b\u6807\u51c6\uff0c\u652f\u6301\u9ed1\u76d2\u6a21\u578b\u7684\u8f93\u5165\u8f93\u51fa\u5206\u6790\uff0c\u5e76\u80fd\u591f\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u901a\u8fc7\u9ed1\u76d2\u6a21\u578b\u7684\u8f93\u5165\u548c\u8f93\u51fa\u6765\u9884\u6d4b\u8fde\u7eed\u7684\u8bc4\u4ef7\u6307\u6807\u5206\u6570\uff0c\u8fd8\u5305\u62ec\u9884\u6d4b\u533a\u95f4\u4ee5\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002\u6d4b\u8bd5\u8986\u76d611\u4e2a\u957f\u6587\u672c\u6570\u636e\u96c6/\u4efb\u52a1\uff0c\u591a\u4e2aLLM\u6a21\u578b\u548c\u591a\u79cd\u8bc4\u4ef7\u6307\u6807\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u4ec5\u4f7f\u752816\u4e2a\u8bad\u7ec3\u793a\u4f8b\u5373\u53ef\u5728\u5404\u79cd\u957f\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u6709\u6548\u9884\u6d4b\u6027\u80fd\u6307\u6807\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9898\u4e14\u5b9e\u7528\u7684\u4efb\u52a1\uff0c\u4e3a\u6027\u80fd\u9884\u6d4b\u9886\u57df\u63d0\u4f9b\u4e86\u4ef7\u503c\u8f83\u9ad8\u7684\u6d4b\u8bd5\u6807\u51c6\u548c\u53ef\u76f4\u63a5\u5e94\u7528\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2509.07521", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.07521", "abs": "https://arxiv.org/abs/2509.07521", "authors": ["Taihui Wang", "Rilin Chen", "Tong Lei", "Andong Li", "Jinzheng Zhao", "Meng Yu", "Dong Yu"], "title": "Target matching based generative model for speech enhancement", "comment": "12 pages, 5 figures", "summary": "The design of mean and variance schedules for the perturbed signal is a\nfundamental challenge in generative models. While score-based and Schr\\\"odinger\nbridge-based models require careful selection of the stochastic differential\nequation to derive the corresponding schedules, flow-based models address this\nissue via vector field matching. However, this strategy often leads to\nhallucination artifacts and inefficient training and inference processes due to\nthe potential inclusion of stochastic components in the vector field.\nAdditionally, the widely adopted diffusion backbone, NCSN++, suffers from high\ncomputational complexity. To overcome these limitations, we propose a novel\ntarget-based generative framework that enhances both the flexibility of\nmean/variance schedule design and the efficiency of training and inference\nprocesses. Specifically, we eliminate the stochastic components in the training\nloss by reformulating the generative speech enhancement task as a target signal\nestimation problem, which therefore leads to more stable and efficient training\nand inference processes. In addition, we employ a logistic mean schedule and a\nbridge variance schedule, which yield a more favorable signal-to-noise ratio\ntrajectory compared to several widely used schedules and thus leads to a more\nefficient perturbation strategy. Furthermore, we propose a new diffusion\nbackbone for audio, which significantly improves the efficiency over NCSN++ by\nexplicitly modeling long-term frame correlations and cross-band dependencies.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u76ee\u6807\u57fa\u4e8e\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6d88\u9664\u8bad\u7ec3\u635f\u5931\u4e2d\u7684\u968f\u673a\u6210\u5206\u3001\u91c7\u7528\u903b\u8f91\u5e73\u5747\u8c03\u5ea6\u548c\u6865\u65b9\u5dee\u8c03\u5ea6\uff0c\u4ee5\u53ca\u8bbe\u8ba1\u65b0\u7684\u6269\u6563\u80cc\u699c\uff0c\u6765\u63d0\u9ad8\u751f\u6210\u6a21\u578b\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u751f\u6210\u6a21\u578b\u4e2d\u5e73\u5747/\u65b9\u5dee\u8c03\u5ea6\u8bbe\u8ba1\u7684\u6311\u6218\uff0c\u5305\u62ec\u6d41\u57fa\u6a21\u578b\u5bfc\u81f4\u7684\u5e7b\u89c9\u6548\u5e94\u3001\u8bad\u7ec3/\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\uff0c\u4ee5\u53caNCSN++\u6269\u6563\u80cc\u699c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u91cd\u6784\u751f\u6210\u8bed\u97f3\u589e\u5f3a\u4efb\u52a1\u4e3a\u76ee\u6807\u4fe1\u53f7\u4f30\u8ba1\u95ee\u9898\uff0c\u6d88\u9664\u8bad\u7ec3\u635f\u5931\u4e2d\u7684\u968f\u673a\u6210\u5206\uff1b\u91c7\u7528\u903b\u8f91\u5e73\u5747\u8c03\u5ea6\u548c\u6865\u65b9\u5dee\u8c03\u5ea6\uff1b\u8bbe\u8ba1\u65b0\u7684\u97f3\u9891\u6269\u6563\u80cc\u699c\uff0c\u663e\u5f0f\u5efa\u6a21\u957f\u671f\u5e27\u76f8\u5173\u6027\u548c\u8de8\u5e26\u4f9d\u8d56\u6027\u3002", "result": "\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u548c\u9ad8\u6548\u7684\u8bad\u7ec3/\u63a8\u7406\u8fc7\u7a0b\uff0c\u83b7\u5f97\u4e86\u66f4\u4f18\u79c0\u7684\u4fe1\u566a\u6bd4\u8f68\u8ff9\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u751f\u6210\u6a21\u578b\u7684\u9650\u5236\uff0c\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u8c03\u5ea6\u8bbe\u8ba1\u548c\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3/\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2509.07311", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07311", "abs": "https://arxiv.org/abs/2509.07311", "authors": ["Sihyun Park"], "title": "Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations", "comment": null, "summary": "Recent advances in large language models (LLMs) have been driven by\npretraining, supervised fine tuning (SFT), and alignment tuning. Among these,\nSFT plays a crucial role in transforming a model 's general knowledge into\nstructured responses tailored to specific tasks. However, there is no clearly\nestablished methodology for effective training data selection. Simply\nincreasing the volume of data does not guarantee performance improvements,\nwhile preprocessing, sampling, and validation require substantial time and\ncost.\n  To address this issue, a variety of data selection methods have been\nproposed. Among them, knowledge based selection approaches identify suitable\ntraining data by analyzing the model 's responses. Nevertheless, these methods\ntypically rely on prompt engineering, making them sensitive to variations and\nincurring additional costs for prompt design.\n  In this study, we propose Knowledge Analysis via Model Internal\nRepresentations (KAMIR), a novel approach that overcomes these limitations by\nanalyzing data based on the model 's internal representations. KAMIR computes\nsimilarities between the hidden states of each layer (block) and the final\nhidden states for a given input to assess the data. Unlike prior methods that\nwere largely limited to multiple choice tasks, KAMIR can be applied to a wide\nrange of tasks such as machine reading comprehension and summarization.\nMoreover, it selects data useful for training based on the model 's familiarity\nwith the input, even with a small dataset and a simple classifier architecture.\nExperiments across diverse task datasets demonstrate that training with less\nfamiliar data leads to better generalization performance.", "AI": {"tldr": "KAMIR\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u5185\u90e8\u8868\u793a\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u9690\u85cf\u72b6\u6001\u76f8\u4f3c\u5ea6\u6765\u8bc4\u4f30\u6570\u636e\uff0c\u65e0\u9700\u63d0\u793a\u5de5\u7a0b\u5373\u53ef\u6709\u6548\u9009\u62e9\u8bad\u7ec3\u6570\u636e", "motivation": "\u89e3\u51b3SFT\u8bad\u7ec3\u6570\u636e\u9009\u62e9\u7f3a\u4e4f\u660e\u786e\u65b9\u6cd5\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b\u4e14\u6210\u672c\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6570\u636e\u9009\u62e9\u6280\u672f", "method": "KAMIR\u901a\u8fc7\u8ba1\u7b97\u6bcf\u5c42\u9690\u85cf\u72b6\u6001\u4e0e\u6700\u7ec8\u9690\u85cf\u72b6\u6001\u7684\u76f8\u4f3c\u5ea6\u6765\u8bc4\u4f30\u6570\u636e\uff0c\u57fa\u4e8e\u6a21\u578b\u5bf9\u8f93\u5165\u7684\u719f\u6089\u7a0b\u5ea6\u9009\u62e9\u8bad\u7ec3\u6570\u636e", "result": "\u5b9e\u9a8c\u8868\u660e\u4f7f\u7528\u4e0d\u592a\u719f\u6089\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u80fd\u83b7\u5f97\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u7c7b\u578b", "conclusion": "KAMIR\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u65e0\u9700\u590d\u6742\u63d0\u793a\u5de5\u7a0b\uff0c\u5728\u5c0f\u6570\u636e\u96c6\u548c\u7b80\u5355\u5206\u7c7b\u5668\u67b6\u6784\u4e0b\u4e5f\u80fd\u6709\u6548\u5de5\u4f5c"}}
{"id": "2509.07526", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07526", "abs": "https://arxiv.org/abs/2509.07526", "authors": ["Gokul Karthik Kumar", "Rishabh Saraf", "Ludovick Lepauloux", "Abdul Muneer", "Billel Mokeddem", "Hakim Hacid"], "title": "Competitive Audio-Language Models with Data-Efficient Single-Stage Training on Public Data", "comment": "Accepted at ASRU 2025", "summary": "Large language models (LLMs) have transformed NLP, yet their integration with\naudio remains underexplored -- despite audio's centrality to human\ncommunication. We introduce Falcon3-Audio, a family of Audio-Language Models\n(ALMs) built on instruction-tuned LLMs and Whisper encoders. Using a remarkably\nsmall amount of public audio data -- less than 30K hours (5K unique) --\nFalcon3-Audio-7B matches the best reported performance among open-weight models\non the MMAU benchmark, with a score of 64.14, matching R1-AQA, while\ndistinguishing itself through superior data and parameter efficiency,\nsingle-stage training, and transparency. Notably, our smallest 1B model remains\ncompetitive with larger open models ranging from 2B to 13B parameters. Through\nextensive ablations, we find that common complexities -- such as curriculum\nlearning, multiple audio encoders, and intricate cross-attention connectors --\nare not required for strong performance, even compared to models trained on\nover 500K hours of data.", "AI": {"tldr": "Falcon3-Audio\u662f\u4e00\u4e2a\u57fa\u4e8e\u6307\u4ee4\u8c03\u4f18LLM\u548cWhisper\u7f16\u7801\u5668\u7684\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\uff0c\u4ec5\u7528\u4e0d\u52303\u4e07\u5c0f\u65f6\u516c\u5f00\u97f3\u9891\u6570\u636e\u5c31\u5728MMAU\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u4f73\u5f00\u6e90\u6a21\u578b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u6570\u636e\u6548\u7387\u548c\u7b80\u5316\u67b6\u6784\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u97f3\u9891\u5728\u4eba\u7c7b\u4ea4\u6d41\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u97f3\u9891\u7684\u6574\u5408\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5f00\u53d1\u9ad8\u6548\u7684\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\uff0c\u51cf\u5c11\u5bf9\u5927\u91cf\u6570\u636e\u548c\u590d\u6742\u67b6\u6784\u7684\u4f9d\u8d56\u3002", "method": "\u57fa\u4e8e\u6307\u4ee4\u8c03\u4f18\u7684LLM\u548cWhisper\u7f16\u7801\u5668\u6784\u5efaFalcon3-Audio\u6a21\u578b\u5bb6\u65cf\uff0c\u4f7f\u7528\u5c11\u91cf\u516c\u5f00\u97f3\u9891\u6570\u636e\uff08<30K\u5c0f\u65f6\uff09\u8fdb\u884c\u5355\u9636\u6bb5\u8bad\u7ec3\uff0c\u907f\u514d\u4e86\u8bfe\u7a0b\u5b66\u4e60\u3001\u591a\u7f16\u7801\u5668\u548c\u590d\u6742\u4ea4\u53c9\u6ce8\u610f\u529b\u8fde\u63a5\u5668\u7b49\u5e38\u89c1\u590d\u6742\u6027\u3002", "result": "Falcon3-Audio-7B\u5728MMAU\u57fa\u51c6\u4e0a\u83b7\u5f9764.14\u5206\uff0c\u5339\u914dR1-AQA\u7684\u6700\u4f73\u62a5\u544a\u6027\u80fd\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6570\u636e\u548c\u53c2\u6570\u6548\u7387\u3002\u6700\u5c0f\u76841B\u6a21\u578b\u4e5f\u80fd\u4e0e2B-13B\u53c2\u6570\u7684\u5927\u578b\u5f00\u6e90\u6a21\u578b\u7ade\u4e89\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5e38\u89c1\u7684\u590d\u6742\u67b6\u6784\u8bbe\u8ba1\u5bf9\u4e8e\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u6027\u80fd\u5e76\u975e\u5fc5\u9700\uff0c\u5373\u4f7f\u4e0e\u4f7f\u7528\u8d85\u8fc750\u4e07\u5c0f\u65f6\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u7b80\u5316\u67b6\u6784\u548c\u5c11\u91cf\u6570\u636e\u4e5f\u80fd\u53d6\u5f97\u4f18\u5f02\u7ed3\u679c\u3002"}}
{"id": "2509.07324", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07324", "abs": "https://arxiv.org/abs/2509.07324", "authors": ["Nakyung Lee", "Yeongoon Kim", "Minhae Oh", "Suhwan Kim", "Jin Woo Koo", "Hyewon Jo", "Jungwoo Lee"], "title": "Mitigating Attention Localization in Small Scale: Self-Attention Refinement via One-step Belief Propagation", "comment": "Accepted at EMNLP 2025", "summary": "Transformer-based self-attention mechanism serves as the core of modern\nlanguage models, yet it often suffers from localization, where attentions\ncollapse onto a limited subset of tokens and fail to capture long-range\ndependencies. To address this issue, we propose Self-Attention One-step Belief\nPropagation (SAOBP), a refinement framework that injects multi-hop\nrelationships through a belief propagation process. To interpret and quantify\nthese interactions, we introduce Global Token Dependency (GTD) that captures\nthe relative contribution of multihop connections within the attention graph.\nEmpirical results indicate that SAOBP helps prevent entropy collapse in deeper\nlayers and adaptively maintains GTD at task-appropriate levels, thereby\nsupporting improvements in model performance. Importantly, we observe\ncompetitive gains in small-scale models, highlighting its potential for\nimproving inference quality in resource-constrained scenarios.", "AI": {"tldr": "\u63d0\u51faSAOBP\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u5ff5\u4f20\u64ad\u6ce8\u5165\u591a\u8df3\u5173\u7cfb\u6765\u89e3\u51b3\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u5b9a\u4f4d\u95ee\u9898\uff0c\u9632\u6b62\u71b5\u5d29\u6e83\u5e76\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u5728\u5c0f\u578b\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "Transformer\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b58\u5728\u5b9a\u4f4d\u95ee\u9898\uff0c\u6ce8\u610f\u529b\u96c6\u4e2d\u5728\u6709\u9650token\u5b50\u96c6\u4e0a\uff0c\u96be\u4ee5\u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb", "method": "\u63d0\u51faSelf-Attention One-step Belief Propagation (SAOBP)\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u5ff5\u4f20\u64ad\u8fc7\u7a0b\u6ce8\u5165\u591a\u8df3\u5173\u7cfb\uff0c\u5e76\u5f15\u5165Global Token Dependency (GTD)\u6765\u91cf\u5316\u548c\u89e3\u91ca\u8fd9\u4e9b\u4ea4\u4e92", "result": "SAOBP\u80fd\u9632\u6b62\u6df1\u5c42\u7f51\u7edc\u4e2d\u7684\u71b5\u5d29\u6e83\uff0c\u81ea\u9002\u5e94\u5730\u7ef4\u6301\u4efb\u52a1\u9002\u5f53\u7684GTD\u6c34\u5e73\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5728\u5c0f\u578b\u6a21\u578b\u4e2d\u8868\u73b0\u5c24\u5176\u7a81\u51fa", "conclusion": "SAOBP\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u5b9a\u4f4d\u95ee\u9898\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u63a8\u7406\u8d28\u91cf\u63d0\u5347"}}
{"id": "2509.07635", "categories": ["cs.SD", "cs.LG", "eess.AS", "68T07", "H.5.5; J.5; I.5.4"], "pdf": "https://arxiv.org/pdf/2509.07635", "abs": "https://arxiv.org/abs/2509.07635", "authors": ["Paolo Combes", "Stefan Weinzierl", "Klaus Obermayer"], "title": "Neural Proxies for Sound Synthesizers: Learning Perceptually Informed Preset Representations", "comment": "17 pages, 4 figures, published in the Journal of the Audio\n  Engineering Society", "summary": "Deep learning appears as an appealing solution for Automatic Synthesizer\nProgramming (ASP), which aims to assist musicians and sound designers in\nprogramming sound synthesizers. However, integrating software synthesizers into\ntraining pipelines is challenging due to their potential non-differentiability.\nThis work tackles this challenge by introducing a method to approximate\narbitrary synthesizers. Specifically, we train a neural network to map\nsynthesizer presets onto an audio embedding space derived from a pretrained\nmodel. This facilitates the definition of a neural proxy that produces compact\nyet effective representations, thereby enabling the integration of audio\nembedding loss into neural-based ASP systems for black-box synthesizers. We\nevaluate the representations derived by various pretrained audio models in the\ncontext of neural-based nASP and assess the effectiveness of several neural\nnetwork architectures, including feedforward, recurrent, and transformer-based\nmodels, in defining neural proxies. We evaluate the proposed method using both\nsynthetic and hand-crafted presets from three popular software synthesizers and\nassess its performance in a synthesizer sound matching downstream task. While\nthe benefits of the learned representation are nuanced by resource\nrequirements, encouraging results were obtained for all synthesizers, paving\nthe way for future research into the application of synthesizer proxies for\nneural-based ASP systems.", "AI": {"tldr": "\u901a\u8fc7\u8bad\u7ec3\u795e\u7ecf\u7f51\u7ed8\u5236\u5408\u6210\u5668\u9884\u8bbe\u5230\u97f3\u9891\u5d4c\u5165\u7a7a\u95f4\u7684\u6620\u5c04\uff0c\u6784\u5efa\u795e\u7ecf\u4ee3\u7406\u4ee3\u66ff\u975e\u53ef\u5fae\u7684\u9ed1\u76d2\u5408\u6210\u5668\uff0c\u4e3a\u795e\u7ecf\u57fa\u81ea\u52a8\u5408\u6210\u5668\u7f16\u7a0b\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u81ea\u52a8\u5408\u6210\u5668\u7f16\u7a0b(ASP)\u4e2d\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u8f6f\u4ef6\u5408\u6210\u5668\u7684\u975e\u53ef\u5fae\u6027\u4e3a\u8bad\u7ec3\u6d41\u7a0b\u5e26\u6765\u6311\u6218\u3002\u9700\u8981\u627e\u5230\u65b9\u6cd5\u8fd1\u4f3c\u4efb\u610f\u5408\u6210\u5668\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bad\u7ec3\u795e\u7ecf\u7f51\u7ed8\u5236\u5408\u6210\u5668\u9884\u8bbe\u5230\u9884\u8bad\u7ec3\u97f3\u9891\u6a21\u578b\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u5efa\u7acb\u795e\u7ecf\u4ee3\u7406\u3002\u6d4b\u8bd5\u4e86\u524d\u5411\u4f20\u64ad\u3001\u5faa\u73af\u548cTransformer\u7b49\u591a\u79cd\u7f51\u7edc\u7ed3\u6784\u7684\u6548\u679c\u3002", "result": "\u5728\u4e09\u6b3e\u6d41\u884c\u8f6f\u4ef6\u5408\u6210\u5668\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5305\u62ec\u5408\u6210\u548c\u624b\u5de5\u5236\u4f5c\u7684\u9884\u8bbe\u3002\u5728\u5408\u6210\u5668\u97f3\u6548\u5339\u914d\u4efb\u52a1\u4e2d\u83b7\u5f97\u4e86\u9f13\u821e\u4eba\u5fc3\u7684\u7ed3\u679c\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6253\u5f00\u4e86\u9014\u5f84\u3002", "conclusion": "\u867d\u7136\u5b66\u4e60\u8868\u5f81\u7684\u4f18\u52bf\u53d7\u5230\u8d44\u6e90\u9700\u6c42\u7684\u5f71\u54cd\uff0c\u4f46\u65b9\u6cd5\u5728\u6240\u6709\u6d4b\u8bd5\u5408\u6210\u5668\u4e0a\u90fd\u53d6\u5f97\u4e86\u9f13\u821e\u4eba\u5fc3\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u795e\u7ecf\u4ee3\u7406\u5728\u9ed1\u76d2\u5408\u6210\u5668\u81ea\u52a8\u7f16\u7a0b\u4e2d\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2509.07370", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07370", "abs": "https://arxiv.org/abs/2509.07370", "authors": ["Yixuan Tang", "Yi Yang", "Ahmed Abbasi"], "title": "PersonaFuse: A Personality Activation-Driven Framework for Enhancing Human-LLM Interactions", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) demonstrate remarkable\ncapabilities across various fields. These developments have led to more direct\ncommunication between humans and LLMs in various situations, such as social\ncompanionship and psychological support. However, LLMs often exhibit\nlimitations in emotional perception and social competence during real-world\nconversations. These limitations partly originate from their inability to adapt\ntheir communication style and emotional expression to different social and task\ncontexts. In this work, we introduce PersonaFuse, a novel LLM post-training\nframework that enables LLMs to adapt and express different personalities for\nvarying situations. Inspired by Trait Activation Theory and the Big Five\npersonality model, PersonaFuse employs a Mixture-of-Expert architecture that\ncombines persona adapters with a dynamic routing network, enabling contextual\ntrait expression. Experimental results show that PersonaFuse substantially\noutperforms baseline models across multiple dimensions of social-emotional\nintelligence. Importantly, these gains are achieved without sacrificing general\nreasoning ability or model safety, which remain common limitations of direct\nprompting and supervised fine-tuning approaches. PersonaFuse also delivers\nconsistent improvements in downstream human-centered applications, such as\nmental health counseling and review-based customer service. Finally, human\npreference evaluations against leading LLMs, including GPT-4o and DeepSeek,\ndemonstrate that PersonaFuse achieves competitive response quality despite its\ncomparatively smaller model size. These findings demonstrate that\nPersonaFuse~offers a theoretically grounded and practical approach for\ndeveloping social-emotional enhanced LLMs, marking a significant advancement\ntoward more human-centric AI systems.", "AI": {"tldr": "PersonaFuse\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u4e94\u4eba\u683c\u6a21\u578b\u548c\u7279\u8d28\u6fc0\u6d3b\u7406\u8bba\u7684LLM\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u4f7f\u6a21\u578b\u80fd\u591f\u6839\u636e\u4e0d\u540c\u60c5\u5883\u81ea\u9002\u5e94\u8868\u8fbe\u4e0d\u540c\u4eba\u683c\u7279\u8d28\uff0c\u663e\u8457\u63d0\u5347\u793e\u4ea4\u60c5\u611f\u667a\u80fd\u800c\u4e0d\u635f\u5bb3\u901a\u7528\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u5bf9\u8bdd\u4e2d\u8868\u73b0\u51fa\u60c5\u611f\u611f\u77e5\u548c\u793e\u4ea4\u80fd\u529b\u7684\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6839\u636e\u4e0d\u540c\u7684\u793e\u4ea4\u548c\u4efb\u52a1\u60c5\u5883\u8c03\u6574\u6c9f\u901a\u98ce\u683c\u548c\u60c5\u611f\u8868\u8fbe\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u793e\u4ea4\u966a\u4f34\u548c\u5fc3\u7406\u652f\u6301\u7b49\u4eba\u7c7b\u4e2d\u5fc3\u5e94\u7528\u4e2d\u7684\u6548\u679c\u3002", "method": "\u57fa\u4e8e\u7279\u8d28\u6fc0\u6d3b\u7406\u8bba\u548c\u5927\u4e94\u4eba\u683c\u6a21\u578b\uff0c\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u7ed3\u5408\u4eba\u683c\u9002\u914d\u5668\u548c\u52a8\u6001\u8def\u7531\u7f51\u7edc\uff0c\u5b9e\u73b0\u60c5\u5883\u5316\u7684\u7279\u8d28\u8868\u8fbe\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aPersonaFuse\u5728\u793e\u4ea4\u60c5\u611f\u667a\u80fd\u7684\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u4e0b\u6e38\u5e94\u7528\u5982\u5fc3\u7406\u5065\u5eb7\u54a8\u8be2\u548c\u5ba2\u6237\u670d\u52a1\u4e2d\u8868\u73b0\u4e00\u81f4\u63d0\u5347\uff0c\u4eba\u7c7b\u504f\u597d\u8bc4\u4f30\u663e\u793a\u5176\u54cd\u5e94\u8d28\u91cf\u4e0eGPT-4o\u548cDeepSeek\u7b49\u9886\u5148\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "PersonaFuse\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u57fa\u7840\u624e\u5b9e\u4e14\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5f00\u53d1\u793e\u4ea4\u60c5\u611f\u589e\u5f3a\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6807\u5fd7\u7740\u5411\u66f4\u4eba\u6027\u5316AI\u7cfb\u7edf\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u4e14\u4e0d\u727a\u7272\u6a21\u578b\u5b89\u5168\u6027\u6216\u901a\u7528\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2509.07677", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07677", "abs": "https://arxiv.org/abs/2509.07677", "authors": ["Kamel Kamel", "Hridoy Sankar Dutta", "Keshav Sood", "Sunil Aryal"], "title": "Spectral Masking and Interpolation Attack (SMIA): A Black-box Adversarial Attack against Voice Authentication and Anti-Spoofing Systems", "comment": null, "summary": "Voice Authentication Systems (VAS) use unique vocal characteristics for\nverification. They are increasingly integrated into high-security sectors such\nas banking and healthcare. Despite their improvements using deep learning, they\nface severe vulnerabilities from sophisticated threats like deepfakes and\nadversarial attacks. The emergence of realistic voice cloning complicates\ndetection, as systems struggle to distinguish authentic from synthetic audio.\nWhile anti-spoofing countermeasures (CMs) exist to mitigate these risks, many\nrely on static detection models that can be bypassed by novel adversarial\nmethods, leaving a critical security gap. To demonstrate this vulnerability, we\npropose the Spectral Masking and Interpolation Attack (SMIA), a novel method\nthat strategically manipulates inaudible frequency regions of AI-generated\naudio. By altering the voice in imperceptible zones to the human ear, SMIA\ncreates adversarial samples that sound authentic while deceiving CMs. We\nconducted a comprehensive evaluation of our attack against state-of-the-art\n(SOTA) models across multiple tasks, under simulated real-world conditions.\nSMIA achieved a strong attack success rate (ASR) of at least 82% against\ncombined VAS/CM systems, at least 97.5% against standalone speaker verification\nsystems, and 100% against countermeasures. These findings conclusively\ndemonstrate that current security postures are insufficient against adaptive\nadversarial attacks. This work highlights the urgent need for a paradigm shift\ntoward next-generation defenses that employ dynamic, context-aware frameworks\ncapable of evolving with the threat landscape.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSMIA\u7684\u65b0\u578b\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728AI\u751f\u6210\u97f3\u9891\u7684\u4e0d\u53ef\u542c\u9891\u7387\u533a\u57df\u8fdb\u884c\u7b56\u7565\u6027\u64cd\u4f5c\uff0c\u6210\u529f\u6b3a\u9a97\u4e86\u8bed\u97f3\u8ba4\u8bc1\u7cfb\u7edf\u548c\u53cd\u6b3a\u9a97\u63aa\u65bd\uff0c\u8bc1\u660e\u4e86\u5f53\u524d\u5b89\u5168\u9632\u62a4\u63aa\u65bd\u7684\u4e0d\u8db3\u3002", "motivation": "\u8bed\u97f3\u8ba4\u8bc1\u7cfb\u7edf\u5728\u9ad8\u5b89\u5168\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u9762\u4e34\u6df1\u5ea6\u4f2a\u9020\u548c\u5bf9\u6297\u653b\u51fb\u7684\u4e25\u91cd\u5a01\u80c1\u3002\u73b0\u6709\u7684\u53cd\u6b3a\u9a97\u63aa\u65bd\u591a\u4e3a\u9759\u6001\u68c0\u6d4b\u6a21\u578b\uff0c\u5bb9\u6613\u88ab\u65b0\u578b\u5bf9\u6297\u65b9\u6cd5\u7ed5\u8fc7\uff0c\u5b58\u5728\u5173\u952e\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u63d0\u51fa\u4e86SMIA\uff08\u9891\u8c31\u63a9\u853d\u548c\u63d2\u503c\u653b\u51fb\uff09\u65b9\u6cd5\uff0c\u7b56\u7565\u6027\u5730\u64cd\u7eb5AI\u751f\u6210\u97f3\u9891\u4e2d\u4eba\u8033\u4e0d\u53ef\u611f\u77e5\u7684\u9891\u7387\u533a\u57df\uff0c\u521b\u5efa\u542c\u8d77\u6765\u771f\u5b9e\u4f46\u80fd\u6b3a\u9a97\u68c0\u6d4b\u7cfb\u7edf\u7684\u5bf9\u6297\u6837\u672c\u3002", "result": "SMIA\u5728\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u7684\u7efc\u5408\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff1a\u5bf9\u7ec4\u5408VAS/CM\u7cfb\u7edf\u7684\u653b\u51fb\u6210\u529f\u7387\u8fbe\u5230\u81f3\u5c1182%\uff0c\u5bf9\u72ec\u7acb\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u7cfb\u7edf\u8fbe\u5230\u81f3\u5c1197.5%\uff0c\u5bf9\u53cd\u6b3a\u9a97\u63aa\u65bd\u8fbe\u5230100%\u3002", "conclusion": "\u5f53\u524d\u7684\u5b89\u5168\u9632\u62a4\u63aa\u65bd\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u81ea\u9002\u5e94\u5bf9\u6297\u653b\u51fb\uff0c\u8feb\u5207\u9700\u8981\u5411\u4e0b\u4e00\u4ee3\u9632\u5fa1\u8303\u5f0f\u8f6c\u53d8\uff0c\u91c7\u7528\u80fd\u591f\u968f\u5a01\u80c1\u73af\u5883\u6f14\u53d8\u7684\u52a8\u6001\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u6846\u67b6\u3002"}}
{"id": "2509.07389", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07389", "abs": "https://arxiv.org/abs/2509.07389", "authors": ["Sankalp Tattwadarshi Swain", "Anshika Krishnatray", "Dhruv Kumar", "Jagat Sesh Challa"], "title": "Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents", "comment": "Under review", "summary": "Existing evaluation studies on linguistic competence of large language models\n(LLM agents) have focused primarily on vocabulary learning, morphological rule\ninduction, syntactic generalization, pragmatic inference, and cross-linguistic\ntransfer. However, none assess whether LLM agents can acquire a language\nthrough pattern recognition and interactive feedback, a central feature of\nhuman language acquisition. We propose a novel experimental framework in which\nan LLM agent is evaluated on its ability to acquire and use a newly constructed\nlanguage (Tinkatongue) in conversation with a bot that understands only\nTinkatongue. Our findings show that LLM agents fail to establish a conversation\nwithin 100 responses, yet they adopt distinct strategies that mirror human\napproaches to language learning. The results suggest a new direction for\nevaluation benchmarks and open pathways to model designs that learn more\neffectively from interactive feedback.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u901a\u8fc7\u6a21\u5f0f\u8bc6\u522b\u548c\u4ea4\u4e92\u53cd\u9988\u5b66\u4e60\u65b0\u8bed\u8a00\u65b9\u9762\u7684\u8bed\u8a00\u83b7\u5f97\u80fd\u529b\u8bc4\u4f30\uff0c\u53d1\u73b0\u6a21\u578b\u65e0\u6cd5\u5728100\u6b21\u56de\u5e94\u5185\u5efa\u7acb\u5bf9\u8bdd\uff0c\u4f46\u91c7\u7528\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8bed\u8a00\u5b66\u4e60\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u6d4b\u8bd5\u8bed\u8a00\u6a21\u578b\u7684\u8bcd\u6c47\u5b66\u4e60\u3001\u8bed\u6cd5\u89c4\u5219\u5f52\u7eb3\u7b49\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u901a\u8fc7\u6a21\u5f0f\u8bc6\u522b\u548c\u4ea4\u4e92\u53cd\u9988\u5b66\u4e60\u8bed\u8a00\u7684\u8bc4\u4f30\uff0c\u8fd9\u662f\u4eba\u7c7b\u8bed\u8a00\u83b7\u5f97\u7684\u6838\u5fc3\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b9e\u9a8c\u6846\u67b6\uff0c\u8ba9\u8bed\u8a00\u6a21\u578b\u4e0e\u4e00\u4e2a\u4ec5\u7406\u89e3\u65b0\u6784\u5efa\u8bed\u8a00(Tinkatongue)\u7684\u673a\u5668\u4eba\u8fdb\u884c\u5bf9\u8bdd\uff0c\u8bc4\u4f30\u5176\u5b66\u4e60\u548c\u4f7f\u7528\u65b0\u8bed\u8a00\u7684\u80fd\u529b\u3002", "result": "\u8bed\u8a00\u6a21\u578b\u5728100\u6b21\u56de\u5e94\u5185\u65e0\u6cd5\u6210\u529f\u5efa\u7acb\u5bf9\u8bdd\uff0c\u4f46\u5b83\u4eec\u91c7\u7528\u4e86\u4e0e\u4eba\u7c7b\u7c7b\u4f3c\u7684\u8bed\u8a00\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u793a\u4e86\u4e00\u5b9a\u7684\u5b66\u4e60\u884c\u4e3a\u3002", "conclusion": "\u8fd9\u4e2a\u7814\u7a76\u4e3a\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u6807\u51c6\u5f00\u542f\u4e86\u65b0\u65b9\u5411\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u80fd\u591f\u66f4\u6709\u6548\u4ece\u4ea4\u4e92\u53cd\u9988\u4e2d\u5b66\u4e60\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u9014\u5f84\u3002"}}
{"id": "2509.07756", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.07756", "abs": "https://arxiv.org/abs/2509.07756", "authors": ["Friedrich Wolf-Monheim"], "title": "Spectral and Rhythm Feature Performance Evaluation for Category and Class Level Audio Classification with Deep Convolutional Neural Networks", "comment": null, "summary": "Next to decision tree and k-nearest neighbours algorithms deep convolutional\nneural networks (CNNs) are widely used to classify audio data in many domains\nlike music, speech or environmental sounds. To train a specific CNN various\nspectral and rhythm features like mel-scaled spectrograms, mel-frequency\ncepstral coefficients (MFCC), cyclic tempograms, short-time Fourier transform\n(STFT) chromagrams, constant-Q transform (CQT) chromagrams and chroma energy\nnormalized statistics (CENS) chromagrams can be used as digital image input\ndata for the neural network. The performance of these spectral and rhythm\nfeatures for audio category level as well as audio class level classification\nis investigated in detail with a deep CNN and the ESC-50 dataset with 2,000\nlabeled environmental audio recordings using an end-to-end deep learning\npipeline. The evaluated metrics accuracy, precision, recall and F1 score for\nmulticlass classification clearly show that the mel-scaled spectrograms and the\nmel-frequency cepstral coefficients (MFCC) perform significantly better then\nthe other spectral and rhythm features investigated in this research for audio\nclassification tasks using deep CNNs.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u591a\u79cd\u8c31\u548c\u8282\u594f\u7279\u5f81\u5728\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u97f3\u9891\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\u5bf9\u6bd4\uff0c\u53d1\u73b0Mel\u6807\u5ea6\u8c31\u56fe\u548cMFCC\u5728\u51c6\u786e\u6027\u3001\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u7279\u5f81\u3002", "motivation": "\u7814\u7a76\u4e0d\u540c\u8c31\u548c\u8282\u594f\u7279\u5f81\u5728\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u97f3\u9891\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4ee5\u786e\u5b9a\u6700\u4f18\u7684\u7279\u5f81\u8868\u793a\u65b9\u5f0f\u3002", "method": "\u4f7f\u7528ESC-50\u6570\u636e\u96c6\uff082,000\u4e2a\u6807\u7b7e\u73af\u5883\u97f3\u9891\u5f55\u97f3\uff09\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6d41\u6c34\u7ebf\u8bc4\u4f30\u591a\u79cd\u7279\u5f81\uff1aMel\u6807\u5ea6\u8c31\u56fe\u3001MFCC\u3001\u5faa\u73af\u8282\u594f\u56fe\u3001STFT\u97f3\u9636\u56fe\u3001CQT\u97f3\u9636\u56fe\u548cCENS\u97f3\u9636\u56fe\u3002", "result": "Mel\u6807\u5ea6\u8c31\u56fe\u548cMFCC\u5728\u591a\u7c7b\u5206\u7c7b\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u663e\u8457\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3001\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\uff0c\u6027\u80fd\u660e\u663e\u4f18\u4e8e\u5176\u4ed6\u8c31\u548c\u8282\u594f\u7279\u5f81\u3002", "conclusion": "\u5bf9\u4e8e\u4f7f\u7528\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u97f3\u9891\u5206\u7c7b\u4efb\u52a1\uff0cMel\u6807\u5ea6\u8c31\u56fe\u548cMFCC\u662f\u6700\u4f18\u7684\u7279\u5f81\u8868\u793a\u65b9\u5f0f\uff0c\u5efa\u8bae\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4f18\u5148\u91c7\u7528\u3002"}}
{"id": "2509.07399", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07399", "abs": "https://arxiv.org/abs/2509.07399", "authors": ["Yi-Jie Cheng", "Oscar Chew", "Yun-Nung Chen"], "title": "The Role of Exploration Modules in Small Language Models for Knowledge Graph Question Answering", "comment": "Extended from ACL 2025 SRW", "summary": "Integrating knowledge graphs (KGs) into the reasoning processes of large\nlanguage models (LLMs) has emerged as a promising approach to mitigate\nhallucination. However, existing work in this area often relies on proprietary\nor extremely large models, limiting accessibility and scalability. In this\nstudy, we investigate the capabilities of existing integration methods for\nsmall language models (SLMs) in KG-based question answering and observe that\ntheir performance is often constrained by their limited ability to traverse and\nreason over knowledge graphs. To address this limitation, we propose leveraging\nsimple and efficient exploration modules to handle knowledge graph traversal in\nplace of the language model itself. Experiment results demonstrate that these\nlightweight modules effectively improve the performance of small language\nmodels on knowledge graph question answering tasks. Source code:\nhttps://github.com/yijie-cheng/SLM-ToG/.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u8f7b\u91cf\u7ea7\u63a2\u7d22\u6a21\u5757\u66ff\u4ee3\u5c0f\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u56fe\u8c31\u904d\u5386\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5c0f\u6a21\u578b\u5728\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5927\u578b\u6216\u4e13\u6709\u6a21\u578b\uff0c\u9650\u5236\u4e86\u77e5\u8bc6\u56fe\u8c31\u4e0e\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e14\u5c0f\u6a21\u578b\u5728\u56fe\u904d\u5386\u548c\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650", "method": "\u5229\u7528\u7b80\u5355\u9ad8\u6548\u7684\u63a2\u7d22\u6a21\u5757\u6765\u5904\u7406\u77e5\u8bc6\u56fe\u8c31\u904d\u5386\uff0c\u66ff\u4ee3\u8bed\u8a00\u6a21\u578b\u81ea\u8eab\u7684\u56fe\u904d\u5386\u80fd\u529b", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8fd9\u4e9b\u8f7b\u91cf\u7ea7\u6a21\u5757\u6709\u6548\u63d0\u5347\u4e86\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u6027\u80fd", "conclusion": "\u901a\u8fc7\u5916\u90e8\u63a2\u7d22\u6a21\u5757\u8f85\u52a9\u5c0f\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u56fe\u8c31\u904d\u5386\u662f\u63d0\u5347\u5176\u95ee\u7b54\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u597d\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u6269\u5c55\u6027"}}
{"id": "2509.07403", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07403", "abs": "https://arxiv.org/abs/2509.07403", "authors": ["Weichu Liu", "Jing Xiong", "Yuxuan Hu", "Zixuan Li", "Minghuan Tan", "Ningning Mao", "Chenyang Zhao", "Zhongwei Wan", "Chaofan Tao", "Wendong Xu", "Hui Shen", "Chengming Li", "Lingpeng Kong", "Ngai Wong"], "title": "LongEmotion: Measuring Emotional Intelligence of Large Language Models in Long-Context Interaction", "comment": "Technical Report", "summary": "Large language models (LLMs) make significant progress in Emotional\nIntelligence (EI) and long-context understanding. However, existing benchmarks\ntend to overlook certain aspects of EI in long-context scenarios, especially\nunder realistic, practical settings where interactions are lengthy, diverse,\nand often noisy. To move towards such realistic settings, we present\nLongEmotion, a benchmark specifically designed for long-context EI tasks. It\ncovers a diverse set of tasks, including Emotion Classification, Emotion\nDetection, Emotion QA, Emotion Conversation, Emotion Summary, and Emotion\nExpression. On average, the input length for these tasks reaches 8,777 tokens,\nwith long-form generation required for Emotion Expression. To enhance\nperformance under realistic constraints, we incorporate Retrieval-Augmented\nGeneration (RAG) and Collaborative Emotional Modeling (CoEM), and compare them\nwith standard prompt-based methods. Unlike conventional approaches, our RAG\nmethod leverages both the conversation context and the large language model\nitself as retrieval sources, avoiding reliance on external knowledge bases. The\nCoEM method further improves performance by decomposing the task into five\nstages, integrating both retrieval augmentation and limited knowledge\ninjection. Experimental results show that both RAG and CoEM consistently\nenhance EI-related performance across most long-context tasks, advancing LLMs\ntoward more practical and real-world EI applications. Furthermore, we conducted\na comparative case study experiment on the GPT series to demonstrate the\ndifferences among various models in terms of EI. Code is available on GitHub at\nhttps://github.com/LongEmotion/LongEmotion, and the project page can be found\nat https://longemotion.github.io/.", "AI": {"tldr": "LongEmotion\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u957f\u4e0a\u4e0b\u6587\u60c5\u611f\u667a\u80fd\u4efb\u52a1\u8bbe\u8ba1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d66\u7c7b\u60c5\u611f\u4efb\u52a1\uff0c\u5e73\u5747\u8f93\u5165\u957f\u5ea68777\u4e2atoken\uff0c\u63d0\u51fa\u4e86RAG\u548cCoEM\u65b9\u6cd5\u6765\u63d0\u5347LLMs\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u60c5\u611f\u667a\u80fd\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5ffd\u89c6\u4e86\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u7684\u60c5\u611f\u667a\u80fd\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u5b58\u5728\u957f\u6587\u672c\u3001\u591a\u6837\u6027\u548c\u566a\u58f0\u7684\u590d\u6742\u4ea4\u4e92\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e86LongEmotion\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u60c5\u611f\u5206\u7c7b\u3001\u68c0\u6d4b\u3001\u95ee\u7b54\u3001\u5bf9\u8bdd\u3001\u603b\u7ed3\u548c\u8868\u8fbe6\u7c7b\u4efb\u52a1\uff1b\u5f00\u53d1\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1aRAG\uff08\u5229\u7528\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u548cLLM\u81ea\u8eab\u4f5c\u4e3a\u68c0\u7d22\u6e90\uff09\u548cCoEM\uff08\u4e94\u9636\u6bb5\u4efb\u52a1\u5206\u89e3\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u548c\u6709\u9650\u77e5\u8bc6\u6ce8\u5165\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eRAG\u548cCoEM\u65b9\u6cd5\u5728\u5927\u591a\u6570\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u90fd\u80fd\u6301\u7eed\u63d0\u5347\u60c5\u611f\u667a\u80fd\u76f8\u5173\u6027\u80fd\uff0c\u63a8\u52a8LLMs\u5411\u66f4\u5b9e\u7528\u7684\u73b0\u5b9e\u4e16\u754c\u60c5\u611f\u667a\u80fd\u5e94\u7528\u53d1\u5c55\u3002", "conclusion": "LongEmotion\u57fa\u51c6\u6d4b\u8bd5\u586b\u8865\u4e86\u957f\u4e0a\u4e0b\u6587\u60c5\u611f\u667a\u80fd\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684RAG\u548cCoEM\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86LLMs\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u60c5\u611f\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2509.07459", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07459", "abs": "https://arxiv.org/abs/2509.07459", "authors": ["Christian Rene Thelen", "Patrick Gustav Blaneck", "Tobias Bornheim", "Niklas Grieger", "Stephan Bialonski"], "title": "AIxcellent Vibes at GermEval 2025 Shared Task on Candy Speech Detection: Improving Model Performance by Span-Level Training", "comment": "6 pages, 1 figure, 2 tables", "summary": "Positive, supportive online communication in social media (candy speech) has\nthe potential to foster civility, yet automated detection of such language\nremains underexplored, limiting systematic analysis of its impact. We\ninvestigate how candy speech can be reliably detected in a 46k-comment German\nYouTube corpus by monolingual and multilingual language models, including\nGBERT, Qwen3 Embedding, and XLM-RoBERTa. We find that a multilingual\nXLM-RoBERTa-Large model trained to detect candy speech at the span level\noutperforms other approaches, ranking first in both binary positive F1: 0.8906)\nand categorized span-based detection (strict F1: 0.6307) subtasks at the\nGermEval 2025 Shared Task on Candy Speech Detection. We speculate that\nspan-based training, multilingual capabilities, and emoji-aware tokenizers\nimproved detection performance. Our results demonstrate the effectiveness of\nmultilingual models in identifying positive, supportive language.", "AI": {"tldr": "\u591a\u8bed\u8a00XLM-RoBERTa-Large\u6a21\u578b\u5728\u5fb7\u8bedYouTube\u8bc4\u8bba\u7684\u7cd6\u679c\u8bed\u97f3\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u901a\u8fc7span\u7ea7\u522b\u8bad\u7ec3\u5b9e\u73b0\u4e86\u4f18\u79c0\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e2d\u79ef\u6781\u652f\u6301\u6027\u7684\u5728\u7ebf\u4ea4\u6d41\uff08\u7cd6\u679c\u8bed\u97f3\uff09\u6709\u52a9\u4e8e\u57f9\u517b\u6587\u660e\u793c\u8c8c\uff0c\u4f46\u6b64\u7c7b\u8bed\u8a00\u7684\u81ea\u52a8\u68c0\u6d4b\u7814\u7a76\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5bf9\u5176\u5f71\u54cd\u7684\u7cfb\u7edf\u6027\u5206\u6790\u3002", "method": "\u4f7f\u7528\u5355\u8bed\u548c\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ecGBERT\u3001Qwen3 Embedding\u548cXLM-RoBERTa\uff09\u572846k\u6761\u5fb7\u8bedYouTube\u8bc4\u8bba\u8bed\u6599\u5e93\u4e2d\u68c0\u6d4b\u7cd6\u679c\u8bed\u97f3\uff0c\u91c7\u7528span\u7ea7\u522b\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u591a\u8bed\u8a00XLM-RoBERTa-Large\u6a21\u578b\u5728GermEval 2025\u7cd6\u679c\u8bed\u97f3\u68c0\u6d4b\u5171\u4eab\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4e8c\u5143\u9633\u6027F1\u5f97\u52060.8906\uff0c\u5206\u7c7bspan\u68c0\u6d4b\u4e25\u683cF1\u5f97\u52060.6307\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u591a\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u79ef\u6781\u652f\u6301\u6027\u8bed\u8a00\u65b9\u9762\u7684\u6709\u6548\u6027\uff0cspan\u7ea7\u522b\u8bad\u7ec3\u3001\u591a\u8bed\u8a00\u80fd\u529b\u548c\u8868\u60c5\u7b26\u53f7\u611f\u77e5\u5206\u8bcd\u5668\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.07462", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07462", "abs": "https://arxiv.org/abs/2509.07462", "authors": ["Yiliang Zhou", "Di Hu", "Tianchu Lyu", "Jasmine Dhillon", "Alexandra L. Beck", "Gelareh Sadigh", "Kai Zheng"], "title": "Understanding Stigmatizing Language Lexicons: A Comparative Analysis in Clinical Contexts", "comment": null, "summary": "Stigmatizing language results in healthcare inequities, yet there is no\nuniversally accepted or standardized lexicon defining which words, terms, or\nphrases constitute stigmatizing language in healthcare. We conducted a\nsystematic search of the literature to identify existing stigmatizing language\nlexicons and then analyzed them comparatively to examine: 1) similarities and\ndiscrepancies between these lexicons, and 2) the distribution of positive,\nnegative, or neutral terms based on an established sentiment dataset. Our\nsearch identified four lexicons. The analysis results revealed moderate\nsemantic similarity among them, and that most stigmatizing terms are related to\njudgmental expressions by clinicians to describe perceived negative behaviors.\nSentiment analysis showed a predominant proportion of negatively classified\nterms, though variations exist across lexicons. Our findings underscore the\nneed for a standardized lexicon and highlight challenges in defining\nstigmatizing language in clinical texts.", "AI": {"tldr": "\u7cfb\u7edf\u6587\u732e\u56de\u987e\u53d1\u73b0\u533b\u7597\u9886\u57df\u6c61\u540d\u5316\u8bed\u8a00\u8bcd\u5178\u5b58\u5728\u4e2d\u7b49\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u4e3b\u8981\u5305\u542b\u4e34\u5e8a\u533b\u751f\u63cf\u8ff0\u8d1f\u9762\u884c\u4e3a\u7684\u8bc4\u5224\u6027\u8868\u8fbe\uff0c\u591a\u4e3a\u8d1f\u9762\u60c5\u611f\u8bcd\u6c47\uff0c\u51f8\u663e\u6807\u51c6\u5316\u8bcd\u5178\u7684\u9700\u6c42\u548c\u6311\u6218\u3002", "motivation": "\u533b\u7597\u6c61\u540d\u5316\u8bed\u8a00\u5bfc\u81f4\u5065\u5eb7\u4e0d\u5e73\u7b49\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7edf\u4e00\u6807\u51c6\u5316\u7684\u6c61\u540d\u5316\u8bed\u8a00\u8bcd\u5178\u6765\u660e\u786e\u5b9a\u4e49\u533b\u7597\u73af\u5883\u4e2d\u7684\u6c61\u540d\u5316\u8bcd\u6c47\u548c\u77ed\u8bed\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u68c0\u7d22\u8bc6\u522b\u73b0\u6709\u6c61\u540d\u5316\u8bed\u8a00\u8bcd\u5178\uff0c\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\uff1a1) \u6bd4\u8f83\u8bcd\u5178\u95f4\u7684\u76f8\u4f3c\u6027\u548c\u5dee\u5f02\uff1b2) \u57fa\u4e8e\u65e2\u5b9a\u60c5\u611f\u6570\u636e\u96c6\u5206\u6790\u6b63\u9762\u3001\u8d1f\u9762\u6216\u4e2d\u6027\u8bcd\u6c47\u7684\u5206\u5e03\u3002", "result": "\u8bc6\u522b\u51fa4\u4e2a\u8bcd\u5178\uff0c\u5206\u6790\u663e\u793a\u5b83\u4eec\u5177\u6709\u4e2d\u7b49\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u5927\u591a\u6570\u6c61\u540d\u5316\u672f\u8bed\u6d89\u53ca\u4e34\u5e8a\u533b\u751f\u63cf\u8ff0\u611f\u77e5\u8d1f\u9762\u884c\u4e3a\u7684\u8bc4\u5224\u6027\u8868\u8fbe\u3002\u60c5\u611f\u5206\u6790\u663e\u793a\u8d1f\u9762\u5206\u7c7b\u8bcd\u6c47\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u5404\u8bcd\u5178\u95f4\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u6807\u51c6\u5316\u8bcd\u5178\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u7a81\u663e\u4e86\u5728\u4e34\u5e8a\u6587\u672c\u4e2d\u5b9a\u4e49\u6c61\u540d\u5316\u8bed\u8a00\u6240\u9762\u4e34\u7684\u6311\u6218\u3002"}}
{"id": "2509.07471", "categories": ["cs.CL", "68T50", "I.7"], "pdf": "https://arxiv.org/pdf/2509.07471", "abs": "https://arxiv.org/abs/2509.07471", "authors": ["Mardiyyah Oduwole", "Oluwatosin Olajide", "Jamiu Suleiman", "Faith Hunja", "Busayo Awobade", "Fatimo Adebanjo", "Comfort Akanni", "Chinonyelum Igwe", "Peace Ododo", "Promise Omoigui", "Steven Kolawole", "Abraham Owodunni"], "title": "From Scarcity to Efficiency: Investigating the Effects of Data Augmentation on African Machine Translation", "comment": "8 pages, 3 tables. Exploratory work on Data Augmentation for African\n  Machine Translation", "summary": "The linguistic diversity across the African continent presents different\nchallenges and opportunities for machine translation. This study explores the\neffects of data augmentation techniques in improving translation systems in\nlow-resource African languages. We focus on two data augmentation techniques:\nsentence concatenation with back translation and switch-out, applying them\nacross six African languages. Our experiments show significant improvements in\nmachine translation performance, with a minimum increase of 25\\% in BLEU score\nacross all six languages.We provide a comprehensive analysis and highlight the\npotential of these techniques to improve machine translation systems for\nlow-resource languages, contributing to the development of more robust\ntranslation systems for under-resourced languages.", "AI": {"tldr": "\u6570\u636e\u589e\u5f3a\u6280\u672f\u663e\u8457\u63d0\u5347\u975e\u6d32\u4f4e\u8d44\u6e90\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1\u6027\u80fd\uff0cBLEU\u5206\u6570\u6700\u4f4e\u63d0\u534725%", "motivation": "\u975e\u6d32\u5927\u9646\u7684\u8bed\u8a00\u591a\u6837\u6027\u4e3a\u673a\u5668\u7ffb\u8bd1\u5e26\u6765\u6311\u6218\u548c\u673a\u9047\uff0c\u9700\u8981\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u7cfb\u7edf\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898", "method": "\u4f7f\u7528\u4e24\u79cd\u6570\u636e\u589e\u5f3a\u6280\u672f\uff1a\u53e5\u5b50\u62fc\u63a5\u4e0e\u56de\u8bd1\u3001switch-out\u6280\u672f\uff0c\u5728\u516d\u79cd\u975e\u6d32\u8bed\u8a00\u4e0a\u8fdb\u884c\u5b9e\u9a8c", "result": "\u6240\u6709\u516d\u79cd\u8bed\u8a00\u7684\u673a\u5668\u7ffb\u8bd1\u6027\u80fd\u90fd\u6709\u663e\u8457\u63d0\u5347\uff0cBLEU\u5206\u6570\u6700\u4f4e\u589e\u52a025%", "conclusion": "\u8fd9\u4e9b\u6280\u672f\u6709\u6f5c\u529b\u6539\u5584\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u4e3a\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u7ffb\u8bd1\u7cfb\u7edf\u505a\u51fa\u8d21\u732e"}}
{"id": "2509.07475", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07475", "abs": "https://arxiv.org/abs/2509.07475", "authors": ["Saumya Goswami", "Siddharth Kurra"], "title": "HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with Calibrated NLI Ensembles and Abstention", "comment": null, "summary": "Detecting content that contradicts or is unsupported by a given source text\nis a critical challenge for the safe deployment of generative language models.\nWe introduce HALT-RAG, a post-hoc verification system designed to identify\nhallucinations in the outputs of Retrieval-Augmented Generation (RAG)\npipelines. Our flexible and task-adaptable framework uses a universal feature\nset derived from an ensemble of two frozen, off-the-shelf Natural Language\nInference (NLI) models and lightweight lexical signals. These features are used\nto train a simple, calibrated, and task-adapted meta-classifier. Using a\nrigorous 5-fold out-of-fold (OOF) training protocol to prevent data leakage and\nproduce unbiased estimates, we evaluate our system on the HaluEval benchmark.\nBy pairing our universal feature set with a lightweight, task-adapted\nclassifier and a precision-constrained decision policy, HALT-RAG achieves\nstrong OOF F1-scores of 0.7756, 0.9786, and 0.7391 on the summarization, QA,\nand dialogue tasks, respectively. The system's well-calibrated probabilities\nenable a practical abstention mechanism, providing a reliable tool for\nbalancing model performance with safety requirements.", "AI": {"tldr": "HALT-RAG\u662f\u4e00\u4e2a\u540e\u9a8c\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u4f7f\u7528NLI\u6a21\u578b\u548c\u8bcd\u6c47\u7279\u5f81\u6765\u68c0\u6d4bRAG\u8f93\u51fa\u4e2d\u7684\u5e7b\u89c9\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u5e76\u652f\u6301\u53ef\u9760\u7684\u5f03\u6743\u673a\u5236\u3002", "motivation": "\u68c0\u6d4b\u751f\u6210\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u4e2d\u4e0e\u6e90\u6587\u672c\u77db\u76fe\u6216\u4e0d\u53d7\u652f\u6301\u7684\u5185\u5bb9\u5bf9\u4e8e\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u6709\u6548\u7684\u5e7b\u89c9\u8bc6\u522b\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u51bb\u7ed3\u7684\u73b0\u6210NLI\u6a21\u578b\u548c\u8f7b\u91cf\u7ea7\u8bcd\u6c47\u4fe1\u53f7\u6784\u5efa\u901a\u7528\u7279\u5f81\u96c6\uff0c\u8bad\u7ec3\u7b80\u5355\u6821\u51c6\u7684\u4efb\u52a1\u9002\u5e94\u5143\u5206\u7c7b\u5668\uff0c\u91c7\u7528\u4e25\u683c\u76845\u6298\u4ea4\u53c9\u9a8c\u8bc1\u534f\u8bae\u9632\u6b62\u6570\u636e\u6cc4\u9732\u3002", "result": "\u5728HaluEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHALT-RAG\u5728\u6458\u8981\u3001QA\u548c\u5bf9\u8bdd\u4efb\u52a1\u4e0a\u5206\u522b\u83b7\u5f970.7756\u30010.9786\u548c0.7391\u7684F1\u5206\u6570\uff0c\u5177\u6709\u826f\u597d\u6821\u51c6\u7684\u6982\u7387\u3002", "conclusion": "HALT-RAG\u901a\u8fc7\u901a\u7528\u7279\u5f81\u96c6\u548c\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u63d0\u4f9b\u5f3a\u5927\u7684\u5e7b\u89c9\u68c0\u6d4b\u80fd\u529b\uff0c\u5176\u6821\u51c6\u6982\u7387\u652f\u6301\u5b9e\u7528\u7684\u5f03\u6743\u673a\u5236\uff0c\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u5b89\u5168\u9700\u6c42\u3002"}}
{"id": "2509.07512", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.07512", "abs": "https://arxiv.org/abs/2509.07512", "authors": ["Zihan Chen", "Lei Shi", "Weize Wu", "Qiji Zhou", "Yue Zhang"], "title": "ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using Demonstration Retrieval", "comment": null, "summary": "Many contemporary data-driven research efforts in the natural sciences, such\nas chemistry and materials science, require large-scale, high-performance\nentity recognition from scientific datasets. Large language models (LLMs) have\nincreasingly been adopted to solve the entity recognition task, with the same\ntrend being observed on all-spectrum NLP tasks. The prevailing entity\nrecognition LLMs rely on fine-tuned technology, yet the fine-tuning process\noften incurs significant cost. To achieve a best performance-cost trade-off, we\npropose ALLabel, a three-stage framework designed to select the most\ninformative and representative samples in preparing the demonstrations for LLM\nmodeling. The annotated examples are used to construct a ground-truth retrieval\ncorpus for LLM in-context learning. By sequentially employing three distinct\nactive learning strategies, ALLabel consistently outperforms all baselines\nunder the same annotation budget across three specialized domain datasets.\nExperimental results also demonstrate that selectively annotating only 5\\%-10\\%\nof the dataset with ALLabel can achieve performance comparable to the method\nannotating the entire dataset. Further analyses and ablation studies verify the\neffectiveness and generalizability of our proposal.", "AI": {"tldr": "ALLabel\u662f\u4e00\u4e2a\u4e09\u9636\u6bb5\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u548c\u4ee3\u8868\u6027\u7684\u6837\u672c\u6765\u6784\u5efaLLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u6f14\u793a\uff0c\u5728\u76f8\u540c\u6807\u6ce8\u9884\u7b97\u4e0b\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4ec5\u9700\u6807\u6ce85%-10%\u6570\u636e\u5373\u53ef\u8fbe\u5230\u5168\u6570\u636e\u96c6\u6807\u6ce8\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u79d1\u5b66\u9886\u57df\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\u4e2dLLM\u5fae\u8c03\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u5bfb\u6c42\u6027\u80fd\u4e0e\u6210\u672c\u4e4b\u95f4\u7684\u6700\u4f73\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u4f9d\u6b21\u91c7\u7528\u4e09\u79cd\u4e0d\u540c\u7684\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u548c\u4ee3\u8868\u6027\u7684\u6837\u672c\uff0c\u6784\u5efa\u771f\u5b9e\u68c0\u7d22\u8bed\u6599\u5e93\u7528\u4e8eLLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002", "result": "\u5728\u4e09\u4e2a\u4e13\u4e1a\u9886\u57df\u6570\u636e\u96c6\u4e0a\uff0cALLabel\u5728\u76f8\u540c\u6807\u6ce8\u9884\u7b97\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4ec5\u6807\u6ce85%-10%\u6570\u636e\u5373\u53ef\u8fbe\u5230\u5168\u6570\u636e\u96c6\u6807\u6ce8\u7684\u76f8\u5f53\u6027\u80fd\u3002", "conclusion": "ALLabel\u6846\u67b6\u6709\u6548\u4e14\u5177\u6709\u901a\u7528\u6027\uff0c\u80fd\u591f\u663e\u8457\u964d\u4f4eLLM\u5b9e\u4f53\u8bc6\u522b\u7684\u6807\u6ce8\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002"}}
{"id": "2509.07553", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07553", "abs": "https://arxiv.org/abs/2509.07553", "authors": ["Zheng Wu", "Heyuan Huang", "Xingyu Lou", "Xiangmou Qu", "Pengzhou Cheng", "Zongru Wu", "Weiwen Liu", "Weinan Zhang", "Jun Wang", "Zhaoxiang Wang", "Zhuosheng Zhang"], "title": "VeriOS: Query-Driven Proactive Human-Agent-GUI Interaction for Trustworthy OS Agents", "comment": null, "summary": "With the rapid progress of multimodal large language models, operating system\n(OS) agents become increasingly capable of automating tasks through on-device\ngraphical user interfaces (GUIs). However, most existing OS agents are designed\nfor idealized settings, whereas real-world environments often present\nuntrustworthy conditions. To mitigate risks of over-execution in such\nscenarios, we propose a query-driven human-agent-GUI interaction framework that\nenables OS agents to decide when to query humans for more reliable task\ncompletion. Built upon this framework, we introduce VeriOS-Agent, a trustworthy\nOS agent trained with a two-stage learning paradigm that falicitate the\ndecoupling and utilization of meta-knowledge. Concretely, VeriOS-Agent\nautonomously executes actions in normal conditions while proactively querying\nhumans in untrustworthy scenarios. Experiments show that VeriOS-Agent improves\nthe average step-wise success rate by 20.64\\% in untrustworthy scenarios over\nthe state-of-the-art, without compromising normal performance. Analysis\nhighlights VeriOS-Agent's rationality, generalizability, and scalability. The\ncodes, datasets and models are available at\nhttps://github.com/Wuzheng02/VeriOS.", "AI": {"tldr": "\u63d0\u51faVeriOS-Agent\uff0c\u4e00\u4e2a\u53ef\u4fe1\u7684\u64cd\u4f5c\u7cfb\u7edf\u4ee3\u7406\uff0c\u901a\u8fc7\u67e5\u8be2\u9a71\u52a8\u7684\u4eba\u673a\u4ea4\u4e92\u6846\u67b6\u5728\u4e0d\u53ef\u4fe1\u73af\u5883\u4e0b\u4e3b\u52a8\u8be2\u95ee\u4eba\u7c7b\uff0c\u63d0\u9ad8\u4efb\u52a1\u6267\u884c\u5b89\u5168\u6027", "motivation": "\u73b0\u6709\u64cd\u4f5c\u7cfb\u7edf\u4ee3\u7406\u5728\u7406\u60f3\u73af\u5883\u4e0b\u8bbe\u8ba1\uff0c\u4f46\u771f\u5b9e\u73af\u5883\u5f80\u5f80\u5b58\u5728\u4e0d\u53ef\u4fe1\u6761\u4ef6\uff0c\u9700\u8981\u9632\u6b62\u8fc7\u5ea6\u6267\u884c\u7684\u98ce\u9669", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5b66\u4e60\u8303\u5f0f\u7684\u67e5\u8be2\u9a71\u52a8\u4eba\u673a-GUI\u4ea4\u4e92\u6846\u67b6\uff0c\u5728\u6b63\u5e38\u6761\u4ef6\u4e0b\u81ea\u4e3b\u6267\u884c\uff0c\u5728\u4e0d\u53ef\u4fe1\u573a\u666f\u4e3b\u52a8\u67e5\u8be2\u4eba\u7c7b", "result": "\u5728\u4e0d\u53ef\u4fe1\u573a\u666f\u4e2d\u5e73\u5747\u6b65\u9aa4\u6210\u529f\u7387\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u9ad820.64%\uff0c\u4e14\u4e0d\u5f71\u54cd\u6b63\u5e38\u6027\u80fd", "conclusion": "VeriOS-Agent\u5c55\u73b0\u51fa\u5408\u7406\u6027\u3001\u6cdb\u5316\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u53ef\u4fe1\u64cd\u4f5c\u7cfb\u7edf\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.07555", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07555", "abs": "https://arxiv.org/abs/2509.07555", "authors": ["Yi Liu", "Xiangrong Zhu", "Xiangyu Liu", "Wei Wei", "Wei Hu"], "title": "Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition", "comment": "Accepted in EMNLP Findings 2025", "summary": "In a rapidly evolving world where information updates swiftly, knowledge in\nlarge language models (LLMs) becomes outdated quickly. Retraining LLMs is not a\ncost-effective option, making knowledge editing (KE) without modifying\nparameters particularly necessary. We find that although existing\nretrieval-augmented generation (RAG)-based KE methods excel at editing simple\nknowledge, they struggle with KE in multi-hop question answering due to the\nissue of \"edit skipping\", which refers to skipping the relevant edited fact in\ninference. In addition to the diversity of natural language expressions of\nknowledge, edit skipping also arises from the mismatch between the granularity\nof LLMs in problem-solving and the facts in the edited memory. To address this\nissue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing\nmethod with guided decomposition (IRAKE) through the guidance from single\nedited facts and entire edited cases. Experimental results demonstrate that\nIRAKE mitigates the failure of editing caused by edit skipping and outperforms\nstate-of-the-art methods for KE in multi-hop question answering.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faIRAKE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5bfc\u5206\u89e3\u548c\u8fed\u4ee3\u68c0\u7d22\u6765\u89e3\u51b3\u591a\u8df3\u95ee\u7b54\u4e2d\u77e5\u8bc6\u7f16\u8f91\u7684\"\u7f16\u8f91\u8df3\u8fc7\"\u95ee\u9898\uff0c\u5728\u77e5\u8bc6\u7f16\u8f91\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u5927\u8bed\u8a00\u6a21\u578b\u5feb\u901f\u53d1\u5c55\u7684\u80cc\u666f\u4e0b\uff0c\u77e5\u8bc6\u66f4\u65b0\u9891\u7e41\u4f46\u91cd\u65b0\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u65e0\u9700\u4fee\u6539\u53c2\u6570\u7684\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u3002\u73b0\u6709\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u65b9\u6cd5\u5728\u7b80\u5355\u77e5\u8bc6\u7f16\u8f91\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u8df3\u95ee\u7b54\u4e2d\u5b58\u5728\u7f16\u8f91\u8df3\u8fc7\u95ee\u9898\u3002", "method": "\u63d0\u51faIRAKE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u7f16\u8f91\u4e8b\u5b9e\u548c\u5b8c\u6574\u7f16\u8f91\u6848\u4f8b\u7684\u5f15\u5bfc\u8fdb\u884c\u8fed\u4ee3\u68c0\u7d22\u589e\u5f3a\u77e5\u8bc6\u7f16\u8f91\uff0c\u91c7\u7528\u5f15\u5bfc\u5206\u89e3\u7b56\u7565\u6765\u89e3\u51b3\u7c92\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eIRAKE\u6709\u6548\u7f13\u89e3\u4e86\u7f16\u8f91\u8df3\u8fc7\u5bfc\u81f4\u7684\u7f16\u8f91\u5931\u8d25\u95ee\u9898\uff0c\u5728\u591a\u8df3\u95ee\u7b54\u7684\u77e5\u8bc6\u7f16\u8f91\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "IRAKE\u901a\u8fc7\u8fed\u4ee3\u68c0\u7d22\u548c\u5f15\u5bfc\u5206\u89e3\u6210\u529f\u89e3\u51b3\u4e86\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u77e5\u8bc6\u7f16\u8f91\u6311\u6218\uff0c\u4e3a\u65e0\u9700\u53c2\u6570\u4fee\u6539\u7684\u77e5\u8bc6\u66f4\u65b0\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.07588", "categories": ["cs.CL", "cs.AI", "I.2.7; H.3.3; J.3"], "pdf": "https://arxiv.org/pdf/2509.07588", "abs": "https://arxiv.org/abs/2509.07588", "authors": ["Andrey Sakhovskiy", "Elena Tutubalina"], "title": "BALI: Enhancing Biomedical Language Representations through Knowledge Graph and Language Model Alignment", "comment": "9 pages, 1 figure, published in \"The 48th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval (SIGIR 2025)\"", "summary": "In recent years, there has been substantial progress in using pretrained\nLanguage Models (LMs) on a range of tasks aimed at improving the understanding\nof biomedical texts. Nonetheless, existing biomedical LLMs show limited\ncomprehension of complex, domain-specific concept structures and the factual\ninformation encoded in biomedical Knowledge Graphs (KGs). In this work, we\npropose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel\njoint LM and KG pre-training method that augments an LM with external knowledge\nby the simultaneous learning of a dedicated KG encoder and aligning the\nrepresentations of both the LM and the graph. For a given textual sequence, we\nlink biomedical concept mentions to the Unified Medical Language System (UMLS)\nKG and utilize local KG subgraphs as cross-modal positive samples for these\nmentions. Our empirical findings indicate that implementing our method on\nseveral leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves\ntheir performance on a range of language understanding tasks and the quality of\nentity representations, even with minimal pre-training on a small alignment\ndataset sourced from PubMed scientific abstracts.", "AI": {"tldr": "BALI\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8054\u5408\u8bed\u8a00\u6a21\u578b\u548c\u77e5\u8bc6\u56fe\u8c31\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u65f6\u5b66\u4e60\u4e13\u7528KG\u7f16\u7801\u5668\u548c\u5bf9\u9f50LM\u4e0e\u56fe\u8c31\u8868\u793a\uff0c\u5c06\u5916\u90e8\u77e5\u8bc6\u589e\u5f3a\u5230\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u63d0\u5347\u751f\u7269\u533b\u5b66\u6587\u672c\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u751f\u7269\u533b\u5b66LLM\u5bf9\u590d\u6742\u9886\u57df\u7279\u5b9a\u6982\u5ff5\u7ed3\u6784\u548c\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7f16\u7801\u7684\u4e8b\u5b9e\u4fe1\u606f\u7406\u89e3\u6709\u9650\uff0c\u9700\u8981\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u3002", "method": "\u63d0\u51faBALI\u65b9\u6cd5\uff1a\u5c06\u751f\u7269\u533b\u5b66\u6982\u5ff5\u63d0\u53ca\u94fe\u63a5\u5230UMLS\u77e5\u8bc6\u56fe\u8c31\uff0c\u5229\u7528\u5c40\u90e8KG\u5b50\u56fe\u4f5c\u4e3a\u8de8\u6a21\u6001\u6b63\u6837\u672c\uff0c\u540c\u65f6\u8bad\u7ec3KG\u7f16\u7801\u5668\u5e76\u5bf9\u9f50LM\u548c\u56fe\u8c31\u8868\u793a\u3002", "result": "\u5728PubMedBERT\u548cBioLinkBERT\u7b49\u9886\u5148\u751f\u7269\u533b\u5b66LM\u4e0a\u5b9e\u65bd\u8be5\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u5c0f\u89c4\u6a21PubMed\u6458\u8981\u5bf9\u9f50\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6700\u5c0f\u9884\u8bad\u7ec3\uff0c\u4e5f\u80fd\u63d0\u5347\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u6027\u80fd\u548c\u5b9e\u4f53\u8868\u793a\u8d28\u91cf\u3002", "conclusion": "BALI\u65b9\u6cd5\u6709\u6548\u589e\u5f3a\u4e86\u751f\u7269\u533b\u5b66\u8bed\u8a00\u6a21\u578b\u5bf9\u9886\u57df\u77e5\u8bc6\u7684\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u5bf9\u9f50\u663e\u8457\u6539\u5584\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2509.07622", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07622", "abs": "https://arxiv.org/abs/2509.07622", "authors": ["Libo Ren", "Yee Man Ng", "Lifeng Han"], "title": "MaLei at MultiClinSUM: Summarisation of Clinical Documents using Perspective-Aware Iterative Self-Prompting with LLMs", "comment": "system paper at CLEF 2025", "summary": "Efficient communication between patients and clinicians plays an important\nrole in shared decision-making. However, clinical reports are often lengthy and\nfilled with clinical jargon, making it difficult for domain experts to identify\nimportant aspects in the document efficiently. This paper presents the\nmethodology we applied in the MultiClinSUM shared task for summarising clinical\ncase documents. We used an Iterative Self-Prompting technique on large language\nmodels (LLMs) by asking LLMs to generate task-specific prompts and refine them\nvia example-based few-shot learning. Furthermore, we used lexical and embedding\nspace metrics, ROUGE and BERT-score, to guide the model fine-tuning with\nepochs. Our submission using perspective-aware ISP on GPT-4 and GPT-4o achieved\nROUGE scores (46.53, 24.68, 30.77) and BERTscores (87.84, 83.25, 85.46) for (P,\nR, F1) from the official evaluation on 3,396 clinical case reports from various\nspecialties extracted from open journals. The high BERTscore indicates that the\nmodel produced semantically equivalent output summaries compared to the\nreferences, even though the overlap at the exact lexicon level is lower, as\nreflected in the lower ROUGE scores. This work sheds some light on how\nperspective-aware ISP (PA-ISP) can be deployed for clinical report\nsummarisation and support better communication between patients and clinicians.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u81ea\u63d0\u793a\u6280\u672f(ISP)\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u5e76\u7cbe\u70bc\uff0c\u7528\u4e8e\u4e34\u5e8a\u62a5\u544a\u6458\u8981\uff0c\u5728MultiClinSUM\u4efb\u52a1\u4e2d\u83b7\u5f97\u4e86\u8f83\u9ad8\u7684BERTscore\u8bc4\u5206\u3002", "motivation": "\u4e34\u5e8a\u62a5\u544a\u901a\u5e38\u957f\u7f20\u4e14\u5145\u6ee1\u4e13\u4e1a\u672f\u8bed\uff0c\u5f71\u54cd\u533b\u751f\u4e0e\u60a3\u8005\u4e4b\u95f4\u7684\u6709\u6548\u6c9f\u901a\u548c\u5171\u540c\u51b3\u7b56\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u8bc6\u522b\u91cd\u8981\u4fe1\u606f\u7684\u6458\u8981\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u81ea\u63d0\u793a\u6280\u672f(ISP)\uff0c\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u5c11\u6837\u672c\u5b66\u4e60\u751f\u6210\u548c\u7cbe\u70bc\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u3002\u4f7f\u7528ROUGE\u548cBERT-score\u6307\u6807\u6307\u5bfc\u6a21\u578b\u5fae\u8c03\u3002", "result": "\u57283,396\u4efd\u591a\u5b66\u79d1\u4e34\u5e8a\u6848\u4f8b\u62a5\u544a\u4e0a\uff0c\u4f7f\u7528GPT-4\u548cGPT-4o\u7684\u89c6\u89d2\u611f\u77e5ISP\u65b9\u6cd5\u83b7\u5f97\u4e86ROUGE(46.53, 24.68, 30.77)\u548cBERTscore(87.84, 83.25, 85.46)\u7684\u9ad8\u5206\u3002\u9ad8BERTscore\u8868\u660e\u6a21\u578b\u751f\u6210\u4e86\u8bed\u4e49\u7b49\u4ef7\u7684\u6458\u8981\u3002", "conclusion": "\u89c6\u89d2\u611f\u77e5\u8fed\u4ee3\u81ea\u63d0\u793a\u6280\u672f(PA-ISP)\u53ef\u4ee5\u6709\u6548\u5e94\u7528\u4e8e\u4e34\u5e8a\u62a5\u544a\u6458\u8981\uff0c\u652f\u6301\u66f4\u597d\u7684\u533b\u60a3\u6c9f\u901a\uff0c\u867d\u7136\u8bcd\u6c47\u5c42\u9762\u7684\u91cd\u5408\u5ea6\u8f83\u4f4e\uff0c\u4f46\u8bed\u4e49\u7b49\u4ef7\u6027\u8f83\u9ad8\u3002"}}
{"id": "2509.07666", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.07666", "abs": "https://arxiv.org/abs/2509.07666", "authors": ["Xixi Wu", "Yanchao Tan", "Nan Hou", "Ruiyang Zhang", "Hong Cheng"], "title": "MoLoRAG: Bootstrapping Document Understanding via Multi-modal Logic-aware Retrieval", "comment": "EMNLP Main 2025", "summary": "Document Understanding is a foundational AI capability with broad\napplications, and Document Question Answering (DocQA) is a key evaluation task.\nTraditional methods convert the document into text for processing by Large\nLanguage Models (LLMs), but this process strips away critical multi-modal\ninformation like figures. While Large Vision-Language Models (LVLMs) address\nthis limitation, their constrained input size makes multi-page document\ncomprehension infeasible. Retrieval-augmented generation (RAG) methods mitigate\nthis by selecting relevant pages, but they rely solely on semantic relevance,\nignoring logical connections between pages and the query, which is essential\nfor reasoning.\n  To this end, we propose MoLoRAG, a logic-aware retrieval framework for\nmulti-modal, multi-page document understanding. By constructing a page graph\nthat captures contextual relationships between pages, a lightweight VLM\nperforms graph traversal to retrieve relevant pages, including those with\nlogical connections often overlooked. This approach combines semantic and\nlogical relevance to deliver more accurate retrieval. After retrieval, the\ntop-$K$ pages are fed into arbitrary LVLMs for question answering. To enhance\nflexibility, MoLoRAG offers two variants: a training-free solution for easy\ndeployment and a fine-tuned version to improve logical relevance checking.\nExperiments on four DocQA datasets demonstrate average improvements of 9.68% in\naccuracy over LVLM direct inference and 7.44% in retrieval precision over\nbaselines. Codes and datasets are released at\nhttps://github.com/WxxShirley/MoLoRAG.", "AI": {"tldr": "MoLoRAG\u662f\u4e00\u4e2a\u903b\u8f91\u611f\u77e5\u7684\u68c0\u7d22\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u591a\u9875\u6587\u6863\u7406\u89e3\uff0c\u901a\u8fc7\u6784\u5efa\u9875\u9762\u56fe\u7ed3\u5408\u8bed\u4e49\u548c\u903b\u8f91\u76f8\u5173\u6027\u6765\u63d0\u5347\u6587\u6863\u95ee\u7b54\u6027\u80fd", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u6587\u6863\u8f6c\u4e3a\u6587\u672c\u5904\u7406\u4f1a\u4e22\u5931\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u800c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53d7\u9650\u4e8e\u8f93\u5165\u957f\u5ea6\u65e0\u6cd5\u5904\u7406\u591a\u9875\u6587\u6863\uff0c\u73b0\u6709\u68c0\u7d22\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u8bed\u4e49\u76f8\u5173\u6027\u800c\u5ffd\u7565\u9875\u9762\u95f4\u7684\u903b\u8f91\u8fde\u63a5", "method": "\u6784\u5efa\u9875\u9762\u56fe\u6355\u6349\u9875\u9762\u95f4\u4e0a\u4e0b\u6587\u5173\u7cfb\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7VLM\u8fdb\u884c\u56fe\u904d\u5386\u68c0\u7d22\u76f8\u5173\u9875\u9762\uff0c\u7ed3\u5408\u8bed\u4e49\u548c\u903b\u8f91\u76f8\u5173\u6027\uff0c\u63d0\u4f9b\u8bad\u7ec3\u514d\u8d39\u548c\u5fae\u8c03\u4e24\u79cd\u53d8\u4f53", "result": "\u5728\u56db\u4e2aDocQA\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u6bd4LVLM\u76f4\u63a5\u63a8\u7406\u63d0\u53479.68%\uff0c\u68c0\u7d22\u7cbe\u5ea6\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u53477.44%", "conclusion": "MoLoRAG\u901a\u8fc7\u903b\u8f91\u611f\u77e5\u7684\u68c0\u7d22\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u9875\u6587\u6863\u7406\u89e3\u4e2d\u7684\u903b\u8f91\u8fde\u63a5\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u6863\u95ee\u7b54\u6027\u80fd"}}
{"id": "2509.07730", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07730", "abs": "https://arxiv.org/abs/2509.07730", "authors": ["Zexuan Li", "Hongliang Dai", "Piji Li"], "title": "M-BRe: Discovering Training Samples for Relation Extraction from Unlabeled Texts with Large Language Models", "comment": "Accepted by EMNLP2025 Main Conference", "summary": "For Relation Extraction (RE), the manual annotation of training data may be\nprohibitively expensive, since the sentences that contain the target relations\nin texts can be very scarce and difficult to find. It is therefore beneficial\nto develop an efficient method that can automatically extract training\ninstances from unlabeled texts for training RE models. Recently, large language\nmodels (LLMs) have been adopted in various natural language processing tasks,\nwith RE also benefiting from their advances. However, when leveraging LLMs for\nRE with predefined relation categories, two key challenges arise. First, in a\nmulti-class classification setting, LLMs often struggle to comprehensively\ncapture the semantics of every relation, leading to suboptimal results. Second,\nalthough employing binary classification for each relation individually can\nmitigate this issue, it introduces significant computational overhead,\nresulting in impractical time complexity for real-world applications.\nTherefore, this paper proposes a framework called M-BRe to extract training\ninstances from unlabeled texts for RE. It utilizes three modules to combine the\nadvantages of both of the above classification approaches: Relation Grouping,\nRelation Extraction, and Label Decision. Extensive experiments confirm its\nsuperior capability in discovering high-quality training samples from unlabeled\ntexts for RE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86M-BRe\u6846\u67b6\uff0c\u901a\u8fc7\u5173\u7cfb\u5206\u7ec4\u3001\u5173\u7cfb\u63d0\u53d6\u548c\u6807\u7b7e\u51b3\u7b56\u4e09\u4e2a\u6a21\u5757\uff0c\u7ed3\u5408\u591a\u5206\u7c7b\u548c\u4e8c\u5143\u5206\u7c7b\u7684\u4f18\u52bf\uff0c\u4ece\u672a\u6807\u6ce8\u6587\u672c\u4e2d\u9ad8\u6548\u63d0\u53d6\u5173\u7cfb\u62bd\u53d6\u8bad\u7ec3\u6837\u672c\u3002", "motivation": "\u5173\u7cfb\u62bd\u53d6\u4e2d\u624b\u52a8\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u5305\u542b\u76ee\u6807\u5173\u7cfb\u7684\u53e5\u5b50\u7a00\u5c11\u96be\u627e\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u7528\u4e8e\u5173\u7cfb\u62bd\u53d6\uff0c\u4f46\u591a\u5206\u7c7b\u96be\u4ee5\u5168\u9762\u6355\u6349\u6bcf\u4e2a\u5173\u7cfb\u8bed\u4e49\uff0c\u800c\u4e8c\u5143\u5206\u7c7b\u53c8\u8ba1\u7b97\u5f00\u9500\u8fc7\u5927\u3002", "method": "\u63d0\u51faM-BRe\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u5173\u7cfb\u5206\u7ec4\uff08\u5c06\u76f8\u4f3c\u5173\u7cfb\u805a\u7c7b\uff09\u3001\u5173\u7cfb\u63d0\u53d6\uff08\u5bf9\u6bcf\u4e2a\u5173\u7cfb\u7ec4\u8fdb\u884c\u591a\u5206\u7c7b\uff09\u3001\u6807\u7b7e\u51b3\u7b56\uff08\u786e\u5b9a\u6700\u7ec8\u5173\u7cfb\u6807\u7b7e\uff09\uff0c\u7ed3\u5408\u4e86\u591a\u5206\u7c7b\u548c\u4e8c\u5143\u5206\u7c7b\u7684\u4f18\u70b9\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u5b9e\u8be5\u6846\u67b6\u80fd\u591f\u4ece\u672a\u6807\u6ce8\u6587\u672c\u4e2d\u53d1\u73b0\u9ad8\u8d28\u91cf\u7684\u5173\u7cfb\u62bd\u53d6\u8bad\u7ec3\u6837\u672c\uff0c\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "M-BRe\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5173\u7cfb\u62bd\u53d6\u4e2d\u8bad\u7ec3\u6570\u636e\u83b7\u53d6\u7684\u96be\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6a21\u5757\u8bbe\u8ba1\u5e73\u8861\u4e86\u5206\u7c7b\u6548\u679c\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u5173\u7cfb\u62bd\u53d6\u6a21\u578b\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u81ea\u52a8\u6807\u6ce8\u6570\u636e\u3002"}}
{"id": "2509.07755", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.07755", "abs": "https://arxiv.org/abs/2509.07755", "authors": ["Rochana Prih Hastuti", "Rian Adam Rajagede", "Mansour Al Ghanim", "Mengxin Zheng", "Qian Lou"], "title": "Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for Medical Texts", "comment": "Accepted at EMNLP 2025 Findings", "summary": "As large language models (LLMs) adapted to sensitive domains such as\nmedicine, their fluency raises safety risks, particularly regarding provenance\nand accountability. Watermarking embeds detectable patterns to mitigate these\nrisks, yet its reliability in medical contexts remains untested. Existing\nbenchmarks focus on detection-quality tradeoffs, overlooking factual risks\nunder low-entropy settings often exploited by watermarking's reweighting\nstrategy. We propose a medical-focused evaluation workflow that jointly\nassesses factual accuracy and coherence. Using GPT-Judger and further human\nvalidation, we introduce the Factuality-Weighted Score (FWS), a composite\nmetric prioritizing factual accuracy beyond coherence to guide watermarking\ndeployment in medical domains. Our evaluation shows current watermarking\nmethods substantially compromise medical factuality, with entropy shifts\ndegrading medical entity representation. These findings underscore the need for\ndomain-aware watermarking approaches that preserve the integrity of medical\ncontent.", "AI": {"tldr": "\u533b\u5b66\u9886\u57df\u6c34\u5370\u6280\u672f\u5b58\u5728\u4e8b\u5b9e\u6027\u98ce\u9669\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u71b5\u5ea6\u8bbe\u7f6e\u4e0b\u4f1a\u5f31\u5316\u533b\u5b66\u5b9e\u4f53\u8868\u8fbe\u80fd\u529b\uff0c\u9700\u8981\u9886\u57df\u8c28\u614e\u7684\u6c34\u5370\u65b9\u6848\u6765\u4fdd\u62a4\u533b\u5b66\u5185\u5bb9\u6574\u4f53\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u7b49\u654f\u611f\u9886\u57df\u7684\u5e94\u7528\uff0c\u6c34\u5370\u6280\u672f\u7684\u53ef\u9760\u6027\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002\u73b0\u6709\u8bc4\u6d4b\u6807\u51c6\u8fc7\u4e8e\u5173\u6ce8\u68c0\u6d4b\u8d28\u91cf\u7684\u6743\u8861\uff0c\u800c\u5ffd\u89c6\u4e86\u4f4e\u71b5\u5ea6\u8bbe\u7f6e\u4e0b\u7684\u4e8b\u5b9e\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u533b\u5b66\u4e13\u95e8\u7684\u8bc4\u4f30\u6d41\u7a0b\uff0c\u7ed3\u5408\u8bc4\u4f30\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u8fde\u8d2f\u6027\u3002\u4f7f\u7528GPT-Judger\u548c\u4eba\u5de5\u9a8c\u8bc1\uff0c\u5f15\u5165\u4e86\u4e8b\u5b9e\u6027\u6743\u91cd\u5206\u6570\uff08FWS\uff09\u8fd9\u4e2a\u7ec4\u5408\u6307\u6807\uff0c\u4f18\u5148\u8003\u8651\u4e8b\u5b9e\u51c6\u786e\u6027\u800c\u975e\u4ec5\u4ec5\u662f\u8fde\u8d2f\u6027\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u5f53\u524d\u7684\u6c34\u5370\u65b9\u6cd5\u5728\u533b\u5b66\u4e8b\u5b9e\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u95ee\u9898\uff0c\u71b5\u5ea6\u504f\u79fb\u4f1a\u964d\u4f4e\u533b\u5b66\u5b9e\u4f53\u7684\u8868\u8fbe\u80fd\u529b\u3002", "conclusion": "\u533b\u5b66\u9886\u57df\u9700\u8981\u66f4\u52a0\u5173\u6ce8\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u6c34\u5370\u6280\u672f\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u62a4\u5185\u5bb9\u6574\u4f53\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u9886\u57df\u8c28\u614e\u7684\u6c34\u5370\u65b9\u6848\u3002"}}
{"id": "2509.07768", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07768", "abs": "https://arxiv.org/abs/2509.07768", "authors": ["Michele Joshua Maggini", "Dhia Merzougui", "Rabiraj Bandyopadhyay", "Ga\u00ebl Dias", "Fabrice Maurel", "Pablo Gamallo"], "title": "Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection? Evaluating In-Context Learning vs. Fine-Tuning", "comment": null, "summary": "The spread of fake news, polarizing, politically biased, and harmful content\non online platforms has been a serious concern. With large language models\nbecoming a promising approach, however, no study has properly benchmarked their\nperformance across different models, usage methods, and languages. This study\npresents a comprehensive overview of different Large Language Models adaptation\nparadigms for the detection of hyperpartisan and fake news, harmful tweets, and\npolitical bias. Our experiments spanned 10 datasets and 5 different languages\n(English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and\nmulticlass classification scenarios. We tested different strategies ranging\nfrom parameter efficient Fine-Tuning of language models to a variety of\ndifferent In-Context Learning strategies and prompts. These included zero-shot\nprompts, codebooks, few-shot (with both randomly-selected and\ndiversely-selected examples using Determinantal Point Processes), and\nChain-of-Thought. We discovered that In-Context Learning often underperforms\nwhen compared to Fine-Tuning a model. This main finding highlights the\nimportance of Fine-Tuning even smaller models on task-specific settings even\nwhen compared to the largest models evaluated in an In-Context Learning setup -\nin our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and\nQwen2.5-7B-Instruct.", "AI": {"tldr": "\u672c\u7814\u7a76\u5168\u9762\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u6570\u636e\u96c6\u4e0a\u68c0\u6d4b\u865a\u5047\u65b0\u95fb\u3001\u6709\u5bb3\u5185\u5bb9\u548c\u653f\u6cbb\u504f\u89c1\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5fae\u8c03\u65b9\u6cd5\u901a\u5e38\u4f18\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b56\u7565\u3002", "motivation": "\u5728\u7ebf\u5e73\u53f0\u4e0a\u7684\u865a\u5047\u65b0\u95fb\u3001\u6709\u5bb3\u5185\u5bb9\u548c\u653f\u6cbb\u504f\u89c1\u5185\u5bb9\u4f20\u64ad\u4e25\u91cd\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\u3001\u4f7f\u7528\u65b9\u6cd5\u548c\u8bed\u8a00\u7684\u7cfb\u7edf\u6027\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u572810\u4e2a\u6570\u636e\u96c6\u548c5\u79cd\u8bed\u8a00\u4e0a\u6d4b\u8bd5\u4e86\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u548c\u591a\u79cd\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b56\u7565\uff08\u96f6\u6837\u672c\u63d0\u793a\u3001\u4ee3\u7801\u672c\u3001\u5c11\u6837\u672c\u5b66\u4e60\u3001\u601d\u7ef4\u94fe\u7b49\uff09\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u901a\u5e38\u8868\u73b0\u4e0d\u5982\u6a21\u578b\u5fae\u8c03\uff0c\u5373\u4f7f\u662f\u8f83\u5c0f\u6a21\u578b\u7ecf\u8fc7\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u540e\u4e5f\u80fd\u8d85\u8d8a\u5927\u578b\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u8868\u73b0\u3002", "conclusion": "\u5fae\u8c03\u5bf9\u4e8e\u865a\u5047\u65b0\u95fb\u68c0\u6d4b\u7b49\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u5373\u4f7f\u5bf9\u4e8e\u8f83\u5c0f\u6a21\u578b\uff0c\u4efb\u52a1\u7279\u5b9a\u7684\u5fae\u8c03\u4e5f\u80fd\u4ea7\u751f\u6bd4\u5927\u578b\u6a21\u578b\u4e0a\u4e0b\u6587\u5b66\u4e60\u66f4\u597d\u7684\u6548\u679c\u3002"}}
{"id": "2509.07801", "categories": ["cs.CL", "cs.DL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.07801", "abs": "https://arxiv.org/abs/2509.07801", "authors": ["Decheng Duan", "Yingyi Zhang", "Jitong Peng", "Chengzhi Zhang"], "title": "SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP", "comment": "EMNLP 2025 Main", "summary": "Structured information extraction from scientific literature is crucial for\ncapturing core concepts and emerging trends in specialized fields. While\nexisting datasets aid model development, most focus on specific publication\nsections due to domain complexity and the high cost of annotating scientific\ntexts. To address this limitation, we introduce SciNLP - a specialized\nbenchmark for full-text entity and relation extraction in the Natural Language\nProcessing (NLP) domain. The dataset comprises 60 manually annotated full-text\nNLP publications, covering 7,072 entities and 1,826 relations. Compared to\nexisting research, SciNLP is the first dataset providing full-text annotations\nof entities and their relationships in the NLP domain. To validate the\neffectiveness of SciNLP, we conducted comparative experiments with similar\ndatasets and evaluated the performance of state-of-the-art supervised models on\nthis dataset. Results reveal varying extraction capabilities of existing models\nacross academic texts of different lengths. Cross-comparisons with existing\ndatasets show that SciNLP achieves significant performance improvements on\ncertain baseline models. Using models trained on SciNLP, we implemented\nautomatic construction of a fine-grained knowledge graph for the NLP domain.\nOur KG has an average node degree of 3.2 per entity, indicating rich semantic\ntopological information that enhances downstream applications. The dataset is\npublicly available at https://github.com/AKADDC/SciNLP.", "AI": {"tldr": "SciNLP\u662f\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9NLP\u9886\u57df\u7684\u5168\u6587\u672c\u5b9e\u4f53\u548c\u5173\u7cfb\u62bd\u53d6\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b60\u7bc7\u4eba\u5de5\u6807\u6ce8\u7684\u5b8c\u6574\u8bba\u6587\uff0c\u6db5\u76d67,072\u4e2a\u5b9e\u4f53\u548c1,826\u4e2a\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5b66\u672f\u6587\u672c\u62bd\u53d6\u65b9\u9762\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u79d1\u5b66\u6587\u732e\u4fe1\u606f\u62bd\u53d6\u6570\u636e\u96c6\u5927\u591a\u53ea\u5173\u6ce8\u7279\u5b9a\u7ae0\u8282\uff0c\u53d7\u9650\u4e8e\u9886\u57df\u590d\u6742\u6027\u548c\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u6784\u5efa\u4e13\u95e8\u9488\u5bf9NLP\u9886\u57df\u7684\u5168\u6587\u672c\u6807\u6ce8\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efa\u5305\u542b60\u7bc7NLP\u9886\u57df\u5b8c\u6574\u8bba\u6587\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5b9e\u4f53\u548c\u5173\u7cfb\u6807\u6ce8\uff1b\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u76d1\u7763\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff1b\u57fa\u4e8eSciNLP\u8bad\u7ec3\u6a21\u578b\u81ea\u52a8\u6784\u5efa\u7ec6\u7c92\u5ea6NLP\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u4e0d\u540c\u957f\u5ea6\u5b66\u672f\u6587\u672c\u4e0a\u7684\u62bd\u53d6\u80fd\u529b\u5b58\u5728\u5dee\u5f02\uff1b\u4e0e\u73b0\u6709\u6570\u636e\u96c6\u76f8\u6bd4\uff0cSciNLP\u5728\u67d0\u4e9b\u57fa\u7ebf\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff1b\u6784\u5efa\u7684\u77e5\u8bc6\u56fe\u8c31\u5e73\u5747\u8282\u70b9\u5ea6\u4e3a3.2\uff0c\u5177\u6709\u4e30\u5bcc\u7684\u8bed\u4e49\u62d3\u6251\u4fe1\u606f\u3002", "conclusion": "SciNLP\u586b\u8865\u4e86NLP\u9886\u57df\u5168\u6587\u672c\u5b9e\u4f53\u5173\u7cfb\u62bd\u53d6\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u6784\u5efa\u7684\u77e5\u8bc6\u56fe\u8c31\u5bf9\u4e0b\u6e38\u5e94\u7528\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2509.07829", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07829", "abs": "https://arxiv.org/abs/2509.07829", "authors": ["Mihai Nadas", "Laura Diosan", "Andreea Tomescu", "Andrei Piscoran"], "title": "Small Open Models Achieve Near Parity with Large Models in Low Resource Literary Translation at a Fraction of the Cost", "comment": "25 pages, 8 figures, includes datasets and models released on Hugging\n  Face", "summary": "Literary translation has recently gained attention as a distinct and complex\ntask in machine translation research. However, the translation by small open\nmodels remains an open problem. We contribute to this ongoing research by\nintroducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for\ndataset creation, fine tuning, and evaluation in English-Romanian literary\ntranslations, centred on the creation and open release of both a compact, fine\ntuned language model (TF2-12B) and large scale synthetic parallel datasets\n(DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the\nlargest collection of synthetic English fables to date, we address the need for\nrich, high quality literary datasets in low resource languages such as\nRomanian. Our pipeline first generates 15k high quality Romanian references\nfrom the TF1 pool using a high performing LLM. We then apply a two stage fine\ntuning process to a 12B parameter open weight model: (i) instruction tuning to\ncapture genre specific narrative style, and (ii) adapter compression for\nefficient deployment. Evaluation combines corpus level BLEU and a five\ndimension LLM based rubric (accuracy, fluency, coherence, style, cultural\nadaptation) to provide a nuanced assessment of translation quality. Results\nshow that our fine tuned model achieves fluency and adequacy competitive with\ntop performing large proprietary models, while being open, accessible, and\nsignificantly more cost effective. Alongside the fine tuned model and both\ndatasets, we publicly release all scripts and evaluation prompts. TF2 thus\nprovides an end-to-end, reproducible pipeline for research on cost efficient\ntranslation, cross lingual narrative generation, and the broad adoption of open\nmodels for culturally significant literary content in low resource settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86TINYFABULIST TRANSLATION FRAMEWORK (TF2)\uff0c\u4e00\u4e2a\u7528\u4e8e\u82f1\u7f57\u6587\u5b66\u7ffb\u8bd1\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u96c6\u521b\u5efa\u3001\u5fae\u8c03\u548c\u8bc4\u4f30\uff0c\u5e76\u53d1\u5e03\u4e8612B\u53c2\u6570\u7684\u5fae\u8c03\u6a21\u578b\u548c\u5927\u578b\u5408\u6210\u5e73\u884c\u6570\u636e\u96c6\u3002", "motivation": "\u89e3\u51b3\u5c0f\u89c4\u6a21\u5f00\u653e\u6a21\u578b\u5728\u6587\u5b66\u7ffb\u8bd1\u8fd9\u4e00\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u7279\u522b\u662f\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u5982\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u63d0\u4f9b\u9ad8\u8d28\u91cf\u6587\u5b66\u6570\u636e\u96c6\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5fae\u8c03\u8fc7\u7a0b\uff1a\u9996\u5148\u4f7f\u7528\u9ad8\u6027\u80fdLLM\u751f\u621015k\u9ad8\u8d28\u91cf\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u53c2\u8003\u8bd1\u6587\uff0c\u7136\u540e\u5bf912B\u53c2\u6570\u6a21\u578b\u8fdb\u884c(i)\u6307\u4ee4\u8c03\u4f18\u4ee5\u6355\u6349\u7279\u5b9a\u4f53\u88c1\u53d9\u4e8b\u98ce\u683c\uff0c(ii)\u9002\u914d\u5668\u538b\u7f29\u4ee5\u5b9e\u73b0\u9ad8\u6548\u90e8\u7f72\u3002", "result": "\u5fae\u8c03\u6a21\u578b\u5728\u6d41\u7545\u6027\u548c\u5145\u5206\u6027\u65b9\u9762\u4e0e\u9876\u7ea7\u4e13\u6709\u5927\u6a21\u578b\u7ade\u4e89\uff0c\u540c\u65f6\u5177\u6709\u5f00\u653e\u6027\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u663e\u8457\u7684\u6210\u672c\u6548\u76ca\u4f18\u52bf\u3002", "conclusion": "TF2\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u3001\u53ef\u590d\u73b0\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u7814\u7a76\u6210\u672c\u6548\u76ca\u9ad8\u7684\u7ffb\u8bd1\u3001\u8de8\u8bed\u8a00\u53d9\u4e8b\u751f\u6210\uff0c\u4ee5\u53ca\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u5e7f\u6cdb\u91c7\u7528\u5f00\u653e\u6a21\u578b\u5904\u7406\u5177\u6709\u6587\u5316\u610f\u4e49\u7684\u6587\u5b66\u5185\u5bb9\u3002"}}
{"id": "2509.07869", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.07869", "abs": "https://arxiv.org/abs/2509.07869", "authors": ["Jiahui Li", "Sean Papay", "Roman Klinger"], "title": "Are Humans as Brittle as Large Language Models?", "comment": null, "summary": "The output of large language models (LLM) is unstable, due to both\nnon-determinism of the decoding process as well as to prompt brittleness. While\nthe intrinsic non-determinism of LLM generation may mimic existing uncertainty\nin human annotations through distributional shifts in outputs, it is largely\nassumed, yet unexplored, that the prompt brittleness effect is unique to LLMs.\nThis raises the question: do human annotators show similar sensitivity to\ninstruction changes? If so, should prompt brittleness in LLMs be considered\nproblematic? One may alternatively hypothesize that prompt brittleness\ncorrectly reflects human annotation variances. To fill this research gap, we\nsystematically compare the effects of prompt modifications on LLMs and\nidentical instruction modifications for human annotators, focusing on the\nquestion of whether humans are similarly sensitive to prompt perturbations. To\nstudy this, we prompt both humans and LLMs for a set of text classification\ntasks conditioned on prompt variations. Our findings indicate that both humans\nand LLMs exhibit increased brittleness in response to specific types of prompt\nmodifications, particularly those involving the substitution of alternative\nlabel sets or label formats. However, the distribution of human judgments is\nless affected by typographical errors and reversed label order than that of\nLLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4eba\u7c7b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63d0\u793a\u4fee\u6539\u4e0b\u90fd\u8868\u73b0\u51fa\u8106\u5f31\u6027\uff0c\u7279\u522b\u662f\u5728\u6807\u7b7e\u96c6\u548c\u6807\u7b7e\u683c\u5f0f\u53d8\u5316\u65f6\uff0c\u4f46\u4eba\u7c7b\u5bf9\u62fc\u5199\u9519\u8bef\u548c\u6807\u7b7e\u987a\u5e8f\u53cd\u8f6c\u7684\u654f\u611f\u6027\u4f4e\u4e8eLLM", "motivation": "\u63a2\u7d22\u4eba\u7c7b\u6807\u6ce8\u8005\u662f\u5426\u5bf9\u6307\u4ee4\u4fee\u6539\u8868\u73b0\u51fa\u4e0eLLM\u7c7b\u4f3c\u7684\u654f\u611f\u6027\uff0c\u4ee5\u5224\u65adLLM\u7684\u63d0\u793a\u8106\u5f31\u6027\u662f\u5426\u53cd\u6620\u4e86\u4eba\u7c7b\u6807\u6ce8\u7684\u56fa\u6709\u65b9\u5dee", "method": "\u901a\u8fc7\u6587\u672c\u5206\u7c7b\u4efb\u52a1\uff0c\u5bf9LLM\u548c\u4eba\u7c7b\u6807\u6ce8\u8005\u4f7f\u7528\u76f8\u540c\u7684\u63d0\u793a\u53d8\u4f53\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e24\u8005\u5bf9\u63d0\u793a\u4fee\u6539\u7684\u654f\u611f\u6027", "result": "\u4eba\u7c7b\u548cLLM\u90fd\u5bf9\u7279\u5b9a\u7c7b\u578b\u7684\u63d0\u793a\u4fee\u6539\uff08\u5982\u66ff\u4ee3\u6807\u7b7e\u96c6\u548c\u6807\u7b7e\u683c\u5f0f\u53d8\u5316\uff09\u8868\u73b0\u51fa\u589e\u52a0\u7684\u8106\u5f31\u6027\uff0c\u4f46\u4eba\u7c7b\u5bf9\u62fc\u5199\u9519\u8bef\u548c\u6807\u7b7e\u987a\u5e8f\u53cd\u8f6c\u7684\u654f\u611f\u6027\u8f83\u4f4e", "conclusion": "LLM\u7684\u63d0\u793a\u8106\u5f31\u6027\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u53cd\u6620\u4e86\u4eba\u7c7b\u6807\u6ce8\u7684\u56fa\u6709\u65b9\u5dee\uff0c\u4f46LLM\u5bf9\u67d0\u4e9b\u63d0\u793a\u6270\u52a8\u66f4\u52a0\u654f\u611f"}}
{"id": "2509.07889", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07889", "abs": "https://arxiv.org/abs/2509.07889", "authors": ["Chengyan Wu", "Yiqiang Cai", "Yufei Cheng", "Yun Xue"], "title": "From Detection to Mitigation: Addressing Gender Bias in Chinese Texts via Efficient Tuning and Voting-Based Rebalancing", "comment": "NLPCC 2025", "summary": "This paper presents our team's solution to Shared Task 7 of NLPCC-2025, which\nfocuses on sentence-level gender bias detection and mitigation in Chinese. The\ntask aims to promote fairness and controllability in natural language\ngeneration by automatically detecting, classifying, and mitigating gender bias.\nTo address this challenge, we adopt a fine-tuning approach based on large\nlanguage models (LLMs), efficiently adapt to the bias detection task via\nLow-Rank Adaptation (LoRA). In terms of data processing, we construct a more\nbalanced training set to alleviate class imbalance and introduce heterogeneous\nsamples from multiple sources to enhance model generalization. For the\ndetection and classification sub-tasks, we employ a majority voting strategy\nthat integrates outputs from multiple expert models to boost performance.\nAdditionally, to improve bias generation detection and mitigation, we design a\nmulti-temperature sampling mechanism to capture potential variations in bias\nexpression styles. Experimental results demonstrate the effectiveness of our\napproach in bias detection, classification, and mitigation. Our method\nultimately achieves an average score of 47.90%, ranking fourth in the shared\ntask.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u548cLoRA\u7684\u53e5\u5b50\u7ea7\u6027\u522b\u504f\u89c1\u68c0\u6d4b\u4e0e\u7f13\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u5e73\u8861\u3001\u591a\u6a21\u578b\u96c6\u6210\u548c\u591a\u6e29\u5ea6\u91c7\u6837\u673a\u5236\uff0c\u5728NLPCC-2025\u5171\u4eab\u4efb\u52a1\u4e2d\u83b7\u5f97\u7b2c\u56db\u540d", "motivation": "\u4fc3\u8fdb\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4e2d\u7684\u516c\u5e73\u6027\u548c\u53ef\u63a7\u6027\uff0c\u81ea\u52a8\u68c0\u6d4b\u3001\u5206\u7c7b\u548c\u7f13\u89e3\u4e2d\u6587\u53e5\u5b50\u4e2d\u7684\u6027\u522b\u504f\u89c1", "method": "\u91c7\u7528\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u4f7f\u7528LoRA\u8fdb\u884c\u9ad8\u6548\u9002\u914d\uff1b\u6784\u5efa\u5e73\u8861\u8bad\u7ec3\u96c6\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\uff1b\u5f15\u5165\u591a\u6e90\u5f02\u6784\u6837\u672c\u589e\u5f3a\u6cdb\u5316\uff1b\u4f7f\u7528\u591a\u6570\u6295\u7968\u7b56\u7565\u96c6\u6210\u591a\u4e2a\u4e13\u5bb6\u6a21\u578b\uff1b\u8bbe\u8ba1\u591a\u6e29\u5ea6\u91c7\u6837\u673a\u5236\u6355\u6349\u504f\u89c1\u8868\u8fbe\u98ce\u683c\u53d8\u5316", "result": "\u5728\u504f\u89c1\u68c0\u6d4b\u3001\u5206\u7c7b\u548c\u7f13\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u6700\u7ec8\u83b7\u5f9747.90%\u7684\u5e73\u5747\u5f97\u5206\uff0c\u5728\u5171\u4eab\u4efb\u52a1\u4e2d\u6392\u540d\u7b2c\u56db", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e2d\u6587\u6027\u522b\u504f\u89c1\u5904\u7406\u4efb\u52a1\u4e2d\u5177\u6709\u8f83\u597d\u7684\u6548\u679c\uff0c\u591a\u6a21\u578b\u96c6\u6210\u548c\u591a\u6e29\u5ea6\u91c7\u6837\u673a\u5236\u5bf9\u6027\u80fd\u63d0\u5347\u6709\u663e\u8457\u8d21\u732e"}}
{"id": "2509.07908", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07908", "abs": "https://arxiv.org/abs/2509.07908", "authors": ["Donya Rooein", "Vil\u00e9m Zouhar", "Debora Nozza", "Dirk Hovy"], "title": "Biased Tales: Cultural and Topic Bias in Generating Children's Stories", "comment": null, "summary": "Stories play a pivotal role in human communication, shaping beliefs and\nmorals, particularly in children. As parents increasingly rely on large\nlanguage models (LLMs) to craft bedtime stories, the presence of cultural and\ngender stereotypes in these narratives raises significant concerns. To address\nthis issue, we present Biased Tales, a comprehensive dataset designed to\nanalyze how biases influence protagonists' attributes and story elements in\nLLM-generated stories. Our analysis uncovers striking disparities. When the\nprotagonist is described as a girl (as compared to a boy), appearance-related\nattributes increase by 55.26%. Stories featuring non-Western children\ndisproportionately emphasize cultural heritage, tradition, and family themes\nfar more than those for Western children. Our findings highlight the role of\nsociocultural bias in making creative AI use more equitable and diverse.", "AI": {"tldr": "Biased Tales\u6570\u636e\u96c6\u5206\u6790\u53d1\u73b0LLM\u751f\u6210\u7684\u6545\u4e8b\u4e2d\u5b58\u5728\u663e\u8457\u7684\u6587\u5316\u548c\u6027\u522b\u523b\u677f\u5370\u8c61\uff0c\u5973\u5b69\u4e3b\u89d2\u7684\u5916\u89c2\u76f8\u5173\u5c5e\u6027\u6bd4\u7537\u5b69\u591a55.26%\uff0c\u975e\u897f\u65b9\u513f\u7ae5\u6545\u4e8b\u8fc7\u5ea6\u5f3a\u8c03\u6587\u5316\u4f20\u7edf\u4e3b\u9898", "motivation": "\u968f\u7740\u5bb6\u957f\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u521b\u4f5c\u7761\u524d\u6545\u4e8b\uff0c\u8fd9\u4e9b\u53d9\u4e8b\u4e2d\u5b58\u5728\u7684\u6587\u5316\u548c\u6027\u522b\u523b\u677f\u5370\u8c61\u95ee\u9898\u503c\u5f97\u5173\u6ce8\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u504f\u89c1\u5982\u4f55\u5f71\u54cd\u4e3b\u89d2\u5c5e\u6027\u548c\u6545\u4e8b\u5143\u7d20", "method": "\u6784\u5efaBiased Tales\u7efc\u5408\u6570\u636e\u96c6\uff0c\u5206\u6790LLM\u751f\u6210\u6545\u4e8b\u4e2d\u4e3b\u89d2\u6027\u522b\u548c\u6587\u5316\u80cc\u666f\u5bf9\u6545\u4e8b\u5185\u5bb9\u7684\u5f71\u54cd\uff0c\u91cf\u5316\u6bd4\u8f83\u4e0d\u540c\u7fa4\u4f53\u4e3b\u89d2\u7684\u5c5e\u6027\u5dee\u5f02", "result": "\u53d1\u73b0\u663e\u8457\u5dee\u5f02\uff1a\u5973\u5b69\u4e3b\u89d2\u7684\u5916\u89c2\u76f8\u5173\u5c5e\u6027\u6bd4\u7537\u5b69\u591a55.26%\uff1b\u975e\u897f\u65b9\u513f\u7ae5\u6545\u4e8b\u66f4\u5f3a\u8c03\u6587\u5316\u9057\u4ea7\u3001\u4f20\u7edf\u548c\u5bb6\u5ead\u4e3b\u9898\uff0c\u8fdc\u8d85\u897f\u65b9\u513f\u7ae5\u6545\u4e8b", "conclusion": "\u7814\u7a76\u7ed3\u679c\u51f8\u663e\u4e86\u793e\u4f1a\u6587\u5316\u504f\u89c1\u5728\u521b\u610fAI\u5e94\u7528\u4e2d\u7684\u5f71\u54cd\uff0c\u5f3a\u8c03\u9700\u8981\u4f7fAI\u521b\u4f5c\u66f4\u52a0\u516c\u5e73\u548c\u591a\u6837\u5316"}}
{"id": "2509.07925", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07925", "abs": "https://arxiv.org/abs/2509.07925", "authors": ["Tuo Wang", "Adithya Kulkarni", "Tyler Cody", "Peter A. Beling", "Yujun Yan", "Dawei Zhou"], "title": "GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models", "comment": "Accepted by EMNLP 2025", "summary": "Uncertainty estimation is essential for enhancing the reliability of Large\nLanguage Models (LLMs), particularly in high-stakes applications. Existing\nmethods often overlook semantic dependencies, relying on token-level\nprobability measures that fail to capture structural relationships within the\ngenerated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty\nEstimation for Large Language Models, a structure-aware framework that\nleverages dependency parse trees and hierarchical graph pooling to refine\nuncertainty quantification. By incorporating supervised learning, GENUINE\neffectively models semantic and structural relationships, improving confidence\nassessments. Extensive experiments across NLP tasks show that GENUINE achieves\nup to 29% higher AUROC than semantic entropy-based approaches and reduces\ncalibration errors by over 15%, demonstrating the effectiveness of graph-based\nuncertainty modeling. The code is available at\nhttps://github.com/ODYSSEYWT/GUQ.", "AI": {"tldr": "GENUINE\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684LLM\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u4f9d\u8d56\u89e3\u6790\u6811\u548c\u5c42\u6b21\u56fe\u6c60\u5316\u6765\u6539\u8fdb\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u6bd4\u8bed\u4e49\u71b5\u65b9\u6cd5AUROC\u63d0\u534729%\uff0c\u6821\u51c6\u8bef\u5dee\u964d\u4f4e15%", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u8bed\u4e49\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ec5\u4f9d\u8d56\u8bcd\u7ea7\u6982\u7387\u5ea6\u91cf\uff0c\u65e0\u6cd5\u6355\u6349\u751f\u6210\u6587\u672c\u4e2d\u7684\u7ed3\u6784\u5173\u7cfb\uff0c\u5f71\u54cdLLM\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027", "method": "\u63d0\u51faGENUINE\u6846\u67b6\uff0c\u5229\u7528\u4f9d\u8d56\u89e3\u6790\u6811\u548c\u5c42\u6b21\u56fe\u6c60\u5316\u6280\u672f\uff0c\u7ed3\u5408\u76d1\u7763\u5b66\u4e60\u6765\u5efa\u6a21\u8bed\u4e49\u548c\u7ed3\u6784\u5173\u7cfb\uff0c\u6539\u8fdb\u7f6e\u4fe1\u5ea6\u8bc4\u4f30", "result": "\u5728\u591a\u4e2aNLP\u4efb\u52a1\u4e0a\uff0cGENUINE\u6bd4\u57fa\u4e8e\u8bed\u4e49\u71b5\u7684\u65b9\u6cd5AUROC\u63d0\u5347\u9ad8\u8fbe29%\uff0c\u6821\u51c6\u8bef\u5dee\u964d\u4f4e\u8d85\u8fc715%", "conclusion": "\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86LLM\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u7ed3\u6784\u611f\u77e5\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e2d\u7684\u91cd\u8981\u6027"}}
{"id": "2509.07968", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07968", "abs": "https://arxiv.org/abs/2509.07968", "authors": ["Lukas Haas", "Gal Yona", "Giovanni D'Antonio", "Sasha Goldshtein", "Dipanjan Das"], "title": "SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge", "comment": null, "summary": "We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large\nLanguage Model (LLM) short-form factuality based on OpenAI's SimpleQA. It\naddresses critical limitations in OpenAI's benchmark, including noisy and\nincorrect labels, topical biases, and question redundancy. SimpleQA Verified\nwas created through a rigorous multi-stage filtering process involving\nde-duplication, topic balancing, and source reconciliation to produce a more\nreliable and challenging evaluation set, alongside improvements in the\nautorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a\nstate-of-the-art F1-score of 55.6, outperforming other frontier models,\nincluding GPT-5. This work provides the research community with a\nhigher-fidelity tool to track genuine progress in parametric model factuality\nand to mitigate hallucinations. The benchmark dataset, evaluation code, and\nleaderboard are available at:\nhttps://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.", "AI": {"tldr": "\u7b80\u4ecbSimpleQA Verified\u8bc4\u6d4b\u6807\u51c6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u7b5b\u9009\u89e3\u51b3\u4e86OpenAI SimpleQA\u7684\u6807\u7b7e\u566a\u58f0\u3001\u4e3b\u9898\u504f\u5dee\u548c\u95ee\u9898\u91cd\u590d\u95ee\u9898\uff0cGemini 2.5 Pro\u5728\u8be5\u6807\u51c6\u4e0a\u8fbe\u523055.6\u7684F1\u5206\u6570", "motivation": "\u89e3\u51b3OpenAI SimpleQA\u6807\u51c6\u4e2d\u5b58\u5728\u7684\u6807\u7b7e\u566a\u58f0\u3001\u4e3b\u9898\u504f\u5dee\u548c\u95ee\u9898\u91cd\u590d\u7b49\u9650\u5236\uff0c\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u77ed\u6587\u672c\u4e8b\u5b9e\u6027\u8bc4\u4f30\u5de5\u5177", "method": "\u901a\u8fc7\u4e25\u683c\u7684\u591a\u9636\u6bb5\u7b5b\u9009\u8fc7\u7a0b\uff0c\u5305\u62ec\u53bb\u91cd\u3001\u4e3b\u9898\u5e73\u8861\u548c\u6765\u6e90\u534f\u8c03\uff0c\u540c\u65f6\u6539\u8fdb\u4e86\u81ea\u52a8\u8bc4\u5206\u63d0\u793a", "result": "Gemini 2.5 Pro\u5728\u8be5\u65b0\u6807\u51c6\u4e0a\u8fbe\u5230\u4e86\u72b6\u6001\u6700\u4f73\u7684F1\u5206\u657055.6\uff0c\u8d85\u8fc7\u4e86\u5305\u62ecGPT-5\u5728\u5185\u7684\u5176\u4ed6\u524d\u6cbf\u6a21\u578b", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u66f4\u9ad8\u4fdd\u771f\u5ea6\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u8ddf\u8e2a\u53c2\u6570\u6a21\u578b\u4e8b\u5b9e\u6027\u7684\u771f\u5b9e\u8fdb\u5c55\u5e76\u51cf\u5c11\u5e7b\u89c9"}}
{"id": "2509.07980", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07980", "abs": "https://arxiv.org/abs/2509.07980", "authors": ["Tong Zheng", "Hongming Zhang", "Wenhao Yu", "Xiaoyang Wang", "Xinyu Yang", "Runpeng Dai", "Rui Liu", "Huiwen Bao", "Chengsong Huang", "Heng Huang", "Dong Yu"], "title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning", "comment": "Project website: https://zhengkid.github.io/Parallel_R1.github.io/", "summary": "Parallel thinking has emerged as a novel approach for enhancing the reasoning\ncapabilities of large language models (LLMs) by exploring multiple reasoning\npaths concurrently. However, activating such capabilities through training\nremains challenging, as existing methods predominantly rely on supervised\nfine-tuning (SFT) over synthetic data, which encourages teacher-forced\nimitation rather than exploration and generalization. Different from them, we\npropose \\textbf{Parallel-R1}, the first reinforcement learning (RL) framework\nthat enables parallel thinking behaviors for complex real-world reasoning\ntasks. Our framework employs a progressive curriculum that explicitly addresses\nthe cold-start problem in training parallel thinking with RL. We first use SFT\non prompt-generated trajectories from easier tasks to instill the parallel\nthinking ability, then transition to RL to explore and generalize this skill on\nharder problems. Experiments on various math benchmarks, including MATH, AMC23,\nand AIME, show that Parallel-R1 successfully instills parallel thinking,\nleading to 8.4% accuracy improvements over the sequential thinking model\ntrained directly on challenging tasks with RL. Further analysis reveals a clear\nshift in the model's thinking behavior: at an early stage, it uses parallel\nthinking as an exploration strategy, while in a later stage, it uses the same\ncapability for multi-perspective verification. Most significantly, we validate\nparallel thinking as a \\textbf{mid-training exploration scaffold}, where this\ntemporary exploratory phase unlocks a higher performance ceiling after RL,\nyielding a 42.9% improvement over the baseline on AIME25. Our model, data, and\ncode will be open-source at https://github.com/zhengkid/Parallel-R1.", "AI": {"tldr": "Parallel-R1\u662f\u9996\u4e2a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u5e76\u884c\u601d\u7ef4\u80fd\u529b\u7684\u6846\u67b6\uff0c\u91c7\u7528\u6e10\u8fdb\u5f0f\u8bfe\u7a0b\u5b66\u4e60\u89e3\u51b3\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u76f8\u6bd4\u987a\u5e8f\u601d\u7ef4\u6a21\u578b\u63d0\u53478.4%\u51c6\u786e\u7387\uff0c\u5e76\u80fd\u4f5c\u4e3a\u8bad\u7ec3\u4e2d\u7684\u63a2\u7d22\u652f\u67b6\u5e26\u676542.9%\u7684\u6027\u80fd\u63d0\u5347", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u76d1\u7763\u5fae\u8c03\u5408\u6210\u6570\u636e\uff0c\u9f13\u52b1\u6559\u5e08\u5f3a\u5236\u6a21\u4eff\u800c\u975e\u63a2\u7d22\u548c\u6cdb\u5316\uff0c\u96be\u4ee5\u6709\u6548\u8bad\u7ec3\u5e76\u884c\u601d\u7ef4\u80fd\u529b", "method": "\u63d0\u51faParallel-R1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5148\u4f7f\u7528SFT\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u57f9\u517b\u5e76\u884c\u601d\u7ef4\u57fa\u7840\uff0c\u7136\u540e\u901a\u8fc7RL\u5728\u56f0\u96be\u95ee\u9898\u4e0a\u63a2\u7d22\u548c\u6cdb\u5316\u8be5\u80fd\u529b", "result": "\u5728MATH\u3001AMC23\u3001AIME\u7b49\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u76f4\u63a5RL\u8bad\u7ec3\u7684\u5e8f\u5217\u601d\u7ef4\u6a21\u578b\u51c6\u786e\u7387\u63d0\u53478.4%\uff0c\u5728AIME25\u4e0a\u76f8\u6bd4\u57fa\u7ebf\u63d0\u534742.9%", "conclusion": "\u5e76\u884c\u601d\u7ef4\u53ef\u4f5c\u4e3a\u8bad\u7ec3\u4e2d\u7684\u63a2\u7d22\u652f\u67b6\uff0c\u65e9\u671f\u7528\u4e8e\u63a2\u7d22\u7b56\u7565\uff0c\u540e\u671f\u7528\u4e8e\u591a\u89c6\u89d2\u9a8c\u8bc1\uff0c\u4e34\u65f6\u63a2\u7d22\u9636\u6bb5\u80fd\u89e3\u9501\u66f4\u9ad8\u7684\u6027\u80fd\u4e0a\u9650"}}
