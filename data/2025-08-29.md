<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 47]
- [cs.MM](#cs.MM) [Total: 5]
- [cs.SD](#cs.SD) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Social Bias in Multilingual Language Models: A Survey](https://arxiv.org/abs/2508.20201)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

TL;DR: 这份系统评估分析了多语言模型中的社会偏见问题，揭示了当前研究在语言多样性、文化意识和评估方法上的空白，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 预训练多语言模型存在与英语模型相似的社会偏见问题，需要系统性地评估和减缓这些偏见在多语言和非英语语境中的表现。

Method: 通过系统评估方法，分析了多语言偏见评估和减缓技术的研究。重点考察语言多样性、文化意识、评估指标和减缓方法的选择。

Result: 研究发现该领域存在显著的方法论偏吐（如偏好某些语言、多语言减缓实验稀缺），同时记录了在不同语言咏文化中适配偏见基准时遇到的常见问题咏解决方案。

Conclusion: 研究指出了未来研究的方向，应加强多语言偏见研究的包容性、跨文化适度性以及与现最先进NLP技术的对齐。

Abstract: Pretrained multilingual models exhibit the same social bias as models
processing English texts. This systematic review analyzes emerging research
that extends bias evaluation and mitigation approaches into multilingual and
non-English contexts. We examine these studies with respect to linguistic
diversity, cultural awareness, and their choice of evaluation metrics and
mitigation techniques. Our survey illuminates gaps in the field's dominant
methodological design choices (e.g., preference for certain languages, scarcity
of multilingual mitigation experiments) while cataloging common issues
encountered and solutions implemented in adapting bias benchmarks across
languages and cultures. Drawing from the implications of our findings, we chart
directions for future research that can reinforce the multilingual bias
literature's inclusivity, cross-cultural appropriateness, and alignment with
state-of-the-art NLP advancements.

</details>


### [2] [Prompting Strategies for Language Model-Based Item Generation in K-12 Education: Bridging the Gap Between Small and Large Language Models](https://arxiv.org/abs/2508.20217)
*Mohammad Amini,Babak Ahmadi,Xiaomeng Xiong,Yilin Zhang,Christopher Qiao*

Main category: cs.CL

TL;DR: 本研究探索使用语言模型自动生成形态学评估的多项选择题，通过结构化提示策略和微调提升中等规模模型性能，结合自动化指标和专家评分验证效果。


<details>
  <summary>Details</summary>
Motivation: 降低人工测试开发成本和不一致性，探索在有限数据条件下如何有效利用语言模型自动生成高质量的评估题目。

Method: 采用两阶段方法：比较微调的中等模型(Gemma 2B)与未调优的大模型(GPT-3.5 175B)；评估7种结构化提示策略，包括零样本、少样本、思维链、角色扮演等组合。使用自动化指标和专家五维评分进行评估，并用GPT-4.1模拟大规模人类评分。

Result: 结构化提示策略（特别是思维链与顺序设计组合）显著提升Gemma输出质量；Gemma比GPT-3.5的零样本响应产生更多结构对齐和教学适宜的题目；提示设计对中等规模模型性能起关键作用。

Conclusion: 结构化提示和高效微调可在有限数据条件下增强中等规模模型的自动生成能力；结合自动化指标、专家判断和大模型模拟能确保与评估目标的一致性；提出的工作流程为K-12语言评估项目开发提供了实用且可扩展的方法。

Abstract: This study explores automatic generation (AIG) using language models to
create multiple choice questions (MCQs) for morphological assessment, aiming to
reduce the cost and inconsistency of manual test development. The study used a
two-fold approach. First, we compared a fine-tuned medium model (Gemma, 2B)
with a larger untuned one (GPT-3.5, 175B). Second, we evaluated seven
structured prompting strategies, including zero-shot, few-shot,
chain-of-thought, role-based, sequential, and combinations. Generated items
were assessed using automated metrics and expert scoring across five
dimensions. We also used GPT-4.1, trained on expert-rated samples, to simulate
human scoring at scale. Results show that structured prompting, especially
strategies combining chain-of-thought and sequential design, significantly
improved Gemma's outputs. Gemma generally produced more construct-aligned and
instructionally appropriate items than GPT-3.5's zero-shot responses, with
prompt design playing a key role in mid-size model performance. This study
demonstrates that structured prompting and efficient fine-tuning can enhance
midsized models for AIG under limited data conditions. We highlight the value
of combining automated metrics, expert judgment, and large-model simulation to
ensure alignment with assessment goals. The proposed workflow offers a
practical and scalable way to develop and validate language assessment items
for K-12.

</details>


### [3] [Integrating SystemC TLM into FMI 3.0 Co-Simulations with an Open-Source Approach](https://arxiv.org/abs/2508.20223)
*Andrei Mihai Albu,Giovanni Pollo,Alessio Burrello,Daniele Jahier Pagliari,Cristian Tesconi,Alessandra Neri,Dario Soldi,Fabio Autieri,Sara Vinco*

Main category: cs.CL

TL;DR: 提出了一种将SystemC TLM模型集成到FMI协同仿真工作流的开源方法，通过封装为FMI 3.0 FMU实现跨域标准化集成


<details>
  <summary>Details</summary>
Motivation: 随着信息物理系统复杂度的增加，特别是在汽车应用中，需要高效的建模和跨域协同仿真技术。SystemC TLM虽然支持硬件/软件协同设计，但与其他工程领域模型的互操作性有限

Method: 将SystemC TLM组件封装为FMI 3.0协同仿真功能模型单元(FMU)，开发轻量级开源工具链，解决时间同步和数据交换等关键技术挑战

Result: 通过代表性案例研究证明了该集成方法的可行性和有效性

Conclusion: 该方法实现了SystemC TLM模型与FMI协同仿真工作流的无缝标准化集成，解决了跨异构仿真环境的互操作性问题

Abstract: The growing complexity of cyber-physical systems, particularly in automotive
applications, has increased the demand for efficient modeling and cross-domain
co-simulation techniques. While SystemC Transaction-Level Modeling (TLM)
enables effective hardware/software co-design, its limited interoperability
with models from other engineering domains poses integration challenges. This
paper presents a fully open-source methodology for integrating SystemC TLM
models into Functional Mock-up Interface (FMI)-based co-simulation workflows.
By encapsulating SystemC TLM components as FMI 3.0 Co Simulation Functional
Mock-up Units (FMUs), the proposed approach facilitates seamless, standardized
integration across heterogeneous simulation environments. We introduce a
lightweight open-source toolchain, address key technical challenges such as
time synchronization and data exchange, and demonstrate the feasibility and
effectiveness of the integration through representative case studies.

</details>


### [4] [Can Compact Language Models Search Like Agents? Distillation-Guided Policy Optimization for Preserving Agentic RAG Capabilities](https://arxiv.org/abs/2508.20324)
*Rikuto Kotoge,Mai Nishimura,Jiaxin Ma*

Main category: cs.CL

TL;DR: DGPO通过教师演示的冷启动初始化和持续教师指导，解决了紧凑语言模型在强化学习中奖励稀疏和训练不稳定的问题，使小模型能够实现复杂的代理搜索行为。


<details>
  <summary>Details</summary>
Motivation: 紧凑语言模型（如0.5B参数）在强化学习中由于推理能力差导致奖励稀疏和训练不稳定，难以实现代理RAG行为如搜索和规划。

Method: 提出蒸馏引导策略优化（DGPO），包括从教师演示进行冷启动初始化以及在策略优化过程中提供持续的教师指导。

Result: DGPO使紧凑模型能够实现复杂的代理搜索行为，在某些情况下甚至超越了更大的教师模型。

Conclusion: DGPO使得在计算资源受限的环境中实现代理RAG成为可能。

Abstract: Reinforcement Learning has emerged as a post-training approach to elicit
agentic RAG behaviors such as search and planning from language models.
However, compact language models (e.g., 0.5B parameters) struggle due to poor
reasoning ability, resulting in sparse rewards and unstable training. To
overcome these difficulties, we propose Distillation-Guided Policy Optimization
(DGPO), which addresses the challenges through cold-start initialization from
teacher demonstrations and continuous teacher guidance during policy
optimization. To systematically evaluate our approach, we introduce Agentic RAG
Capabilities (ARC), a fine-grained metric analyzing reasoning, search
coordination, and response synthesis. Comprehensive experiments demonstrate
that DGPO enables compact models to achieve sophisticated agentic search
behaviors, even outperforming the larger teacher model in some cases. DGPO
makes agentic RAG feasible in computing resource-constrained environments.

</details>


### [5] [GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs](https://arxiv.org/abs/2508.20325)
*Haibo Jin,Ruoxi Chen,Peiyan Zhang,Andy Zhou,Yang Zhang,Haohan Wang*

Main category: cs.CL

TL;DR: GUARD是一个将政府AI伦理指南转化为可操作测试问题的框架，通过自动生成违反指南的问题来评估LLM合规性，并包含越狱诊断功能来识别潜在安全绕过漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各领域的广泛应用，其生成有害内容的风险引发社会关注。政府发布的伦理指南缺乏具体测试方法，需要将高层要求转化为可操作的测试问题来验证LLM合规性。

Method: GUARD方法包括：1）基于政府指南自动生成违反指南的测试问题；2）对不直接违反指南的响应，集成越狱诊断(GUARD-JD)创建场景诱发不当响应；3）生成合规报告详细说明遵守情况和违规点。

Result: 在7个LLM（Vicuna-13B、LongChat-7B、Llama2-7B、Llama-3-8B、GPT-3.5、GPT-4、GPT-4o、Claude-3.7）上实证验证有效性，测试了三个政府指南的合规性并进行了越狱诊断。GUARD-JD还可迁移到视觉语言模型。

Conclusion: GUARD提供了一种系统化的方法来将高层伦理指南转化为具体测试，有效识别LLM的合规问题和安全漏洞，有助于促进可靠的LLM应用开发。

Abstract: As Large Language Models become increasingly integral to various domains,
their potential to generate harmful responses has prompted significant societal
and regulatory concerns. In response, governments have issued ethics guidelines
to promote the development of trustworthy AI. However, these guidelines are
typically high-level demands for developers and testers, leaving a gap in
translating them into actionable testing questions to verify LLM compliance.
  To address this challenge, we introduce GUARD (\textbf{G}uideline
\textbf{U}pholding Test through \textbf{A}daptive \textbf{R}ole-play and
Jailbreak \textbf{D}iagnostics), a testing method designed to operationalize
guidelines into specific guideline-violating questions that assess LLM
adherence. To implement this, GUARD uses automated generation of
guideline-violating questions based on government-issued guidelines, thereby
testing whether responses comply with these guidelines. When responses directly
violate guidelines, GUARD reports inconsistencies. Furthermore, for responses
that do not directly violate guidelines, GUARD integrates the concept of
``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that
provoke unethical or guideline-violating responses, effectively identifying
potential scenarios that could bypass built-in safety mechanisms. Our method
finally culminates in a compliance report, delineating the extent of adherence
and highlighting any violations.
  We have empirically validated the effectiveness of GUARD on seven LLMs,
including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4,
GPT-4o, and Claude-3.7, by testing compliance under three government-issued
guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can
transfer jailbreak diagnostics to vision-language models, demonstrating its
usage in promoting reliable LLM-based applications.

</details>


### [6] [Joint Enhancement of Relational Reasoning for Long-Context LLMs](https://arxiv.org/abs/2508.20351)
*Zhirui Chen,Wei Shen,Jiashui Huang,Ling Shao*

Main category: cs.CL

TL;DR: JERR是一个基于图推理的框架，通过摘要提取、图构建和关系推理来增强大语言模型的长文本理解能力，在ROUGE和F1指标上优于所有基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在处理长上下文时面临的内存限制、复杂任务处理困难、缺乏透明度和容易产生幻觉等问题。

Method: 集成三个关键组件：策略性文本分块的摘要提取、构建有向无环图(DAG)解决冗余问题、使用蒙特卡洛树搜索(MCTS)导航复杂推理路径。

Result: 在ROUGE和F1指标上一致优于所有基线方法，在LLM-Rater评估中获得最高分数。

Conclusion: JERR框架为大语言模型处理扩展上下文和复杂推理任务提供了新颖解决方案，提高了可靠性和透明度。

Abstract: Despite significant progress, large language models (LLMs) still struggle
with long contexts due to memory limitations and their inability to tackle
complex and long-context tasks. Additionally, LLMs often suffer from a lack of
transparency and are prone to producing hallucinations. To address these
challenges, we propose \textbf{JERR}, a novel framework designed to enhance
long-context comprehension via graph-based reasoning in LLMs. JERR integrates
three key components: synopsis extraction, graph construction, and relational
reasoning. First, synopsis is extracted by chunking text strategically,
allowing the model to summarize and understand information more efficiently.
Second, we build a directed acyclic graph (DAG) to resolve redundancy, ensuring
logical consistency and clarity. Finally, we incorporate Monte Carlo Tree
Search (MCTS) to help the model navigate complex reasoning paths, ensuring more
accurate and interpretable outputs. This framework provides a novel solution
that enables LLMs to handle extended contexts and complex reasoning tasks with
improved reliability and transparency. Experimental results show that JERR
consistently outperforms all baselines on the ROUGE and F1 metrics, achieving
the highest scores on the LLM-Rater evaluation.

</details>


### [7] [Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems](https://arxiv.org/abs/2508.20373)
*Yuyao Wang,Bowen Liu,Jianheng Tang,Nuo Chen,Yuhan Li,Qifan Zhang,Jia Li*

Main category: cs.CL

TL;DR: 这篇论文提出了一种使用NP难图论问题作为合成训练资源的方法，通过两阶段训练框架提升大语言模型的长链式思维推理能力，并在多个领域展现出良好的沿尾性能。


<details>
  <summary>Details</summary>
Motivation: 解决依赖人工精细标注数据集的成本高、可扩展性差的问题，寻找一种可扩展的合成训练资源来提升大语言模型的长链式思维推理能力。

Method: 使用NP难图论问题作为合成训练资源，采用两阶段训练框架：(1)基于拒绝采样的长链式思维有监督精细调整，(2)使用细粒度奖励设计的强化学习来提升推理效率。

Result: 旗舰模型Graph-R1-7B在数学、编程、STEM和逻辑领域都展现出良好的沿尾性能，并在NP难图论问题上在准确性和推理效率方面超过QwQ-32B模型。

Conclusion: NP难图论问题是一种有效且可扩展的资源，可以推动大语言模型的长链式思维推理能力发展，为LLM训练开启了新的前沿。

Abstract: Reasoning Large Language Models (RLLMs) have recently achieved remarkable
progress on complex reasoning tasks, largely enabled by their long
chain-of-thought (Long CoT) capabilities. However, developing these Long CoT
behaviors relies heavily on post-training with high-quality datasets, which are
typically costly and human-curated (e.g., mathematics and code), leaving
scalable alternatives unexplored. In this work, we introduce NP-hard (NPH)
graph problems as a novel synthetic training corpus, as they inherently require
deep reasoning, extensive exploration, and reflective strategies, which are
core characteristics of Long CoT reasoning. Building on this insight, we
develop a two-stage post-training framework: (i) Long CoT Supervised
Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially
enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a
fine-grained reward design, which sharpens reasoning efficiency. Our flagship
model, Graph-R1-7B, demonstrates strong generalization across mathematics,
coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both
accuracy and reasoning efficiency. These results position NPH graph problems as
an effective and scalable resource for advancing Long CoT reasoning in LLMs,
opening a new frontier for LLM post-training. Our implementation is available
at https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted
in our Hugging Face collection HKUST-DSAIL/Graph-R1.

</details>


### [8] [CAPE: Context-Aware Personality Evaluation Framework for Large Language Models](https://arxiv.org/abs/2508.20385)
*Jivnesh Sandhan,Fei Cheng,Tushar Sandhan,Yugo Murawaki*

Main category: cs.CL

TL;DR: 提出了首个上下文感知的人格评估框架CAPE，通过引入对话历史来评估LLM的行为特质，发现上下文增强了响应一致性但也导致人格偏移，不同模型对上下文依赖程度不同


<details>
  <summary>Details</summary>
Motivation: 现有心理测量测试采用无上下文方法孤立回答问题，忽略了真实应用中对话历史对响应的塑造作用，需要开发上下文感知的评估框架

Method: 提出CAPE框架，引入先前的对话交互作为上下文，设计新指标量化LLM响应一致性，在7个LLM上进行实验分析

Result: 对话历史通过上下文学习增强响应一致性，但也引起人格偏移；GPT模型对问题顺序稳健，而Gemini和Llama对顺序敏感；GPT响应源于内在人格特质和先前交互，其他模型更依赖先前交互；在角色扮演代理中上下文依赖的人格偏移改善一致性并更符合人类判断

Conclusion: 上下文在LLM人格评估中至关重要，CAPE框架能更真实地评估LLM行为特质，不同模型对上下文依赖模式不同，这对开发更可靠的AI系统具有重要意义

Abstract: Psychometric tests, traditionally used to assess humans, are now being
applied to Large Language Models (LLMs) to evaluate their behavioral traits.
However, existing studies follow a context-free approach, answering each
question in isolation to avoid contextual influence. We term this the Disney
World test, an artificial setting that ignores real-world applications, where
conversational history shapes responses. To bridge this gap, we propose the
first Context-Aware Personality Evaluation (CAPE) framework for LLMs,
incorporating prior conversational interactions. To thoroughly analyze the
influence of context, we introduce novel metrics to quantify the consistency of
LLM responses, a fundamental trait in human behavior.
  Our exhaustive experiments on 7 LLMs reveal that conversational history
enhances response consistency via in-context learning but also induces
personality shifts, with GPT-3.5-Turbo and GPT-4-Turbo exhibiting extreme
deviations. While GPT models are robust to question ordering, Gemini-1.5-Flash
and Llama-8B display significant sensitivity. Moreover, GPT models response
stem from their intrinsic personality traits as well as prior interactions,
whereas Gemini-1.5-Flash and Llama--8B heavily depend on prior interactions.
Finally, applying our framework to Role Playing Agents (RPAs) shows
context-dependent personality shifts improve response consistency and better
align with human judgments. Our code and datasets are publicly available at:
https://github.com/jivnesh/CAPE

</details>


### [9] [Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction](https://arxiv.org/abs/2508.20395)
*Xu Guo*

Main category: cs.CL

TL;DR: 研究发现推理步骤中的条件熵变化模式与答案正确性相关：熵递减对应正确答案，熵平坦或递增对应错误答案，且错误推理路径往往更长。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs依赖中间推理步骤提高准确性，但缺乏对推理效用如何影响最终答案正确性的研究。由于自回归生成的随机性，更多上下文不一定增加答案置信度。

Method: 在MATH数据集上进行oracle研究，使用Qwen2.5-32B和GPT-4o生成推理链，然后用Qwen3-8B量化推理链对最终准确性的效用，通过条件熵测量模型在每一步对答案的不确定性。

Result: 条件熵随步骤递减的模式与正确答案强相关，而平坦或递增的熵往往导致错误答案。错误推理路径通常比正确路径更长。

Conclusion: 这些发现为设计高效推理管道提供了基础，可以早期检测和避免无效推理，提高推理效率。

Abstract: Recent advancements in large language models (LLMs) often rely on generating
intermediate reasoning steps to enhance accuracy. However, little work has
examined how reasoning utility contributes to the final answer's correctness.
Due to the stochastic nature of autoregressive generation, generating more
context does not guarantee increased confidence in the answer. If we could
predict, during generation, whether a reasoning step will be useful, we could
stop early or prune ineffective steps, avoiding distractions in the final
decision.
  We present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to
generate reasoning chains, and then employing a separate model (Qwen3-8B) to
quantify the utility of these chains for final accuracy. Specifically, we
measure the model's uncertainty on the answer span Y at each reasoning step
using conditional entropy (expected negative log-likelihood over the
vocabulary) with context expanding step by step. Our results show a clear
pattern: conditional entropy that decreases over steps is strongly associated
with correct answers, whereas flat or increasing entropy often results in wrong
answers. We also corroborate that incorrect reasoning paths tend to be longer
than correct ones, suggesting that longer reasoning does not necessarily yield
better outcomes. These findings serve as a foundation to inspire future work on
designing efficient reasoning pipelines that detect and avoid unproductive
reasoning early.

</details>


### [10] [Exploring Machine Learning and Language Models for Multimodal Depression Detection](https://arxiv.org/abs/2508.20805)
*Javier Si Zhao Hong,Timothy Zoe Delaya,Sherwyn Chan Yin Kit,Pai Chet Ng,Xiaoxiao Miao*

Main category: cs.CL

TL;DR: 多模态抑郁检测方法研究，比较XGBoost、Transformer和大语言模型在音频、视频、文本特征上的表现


<details>
  <summary>Details</summary>
Motivation: 研究多模态机器学习和深度学习模型在抑郁检测中的效果，为精神健康预测提供有效的多模态表征策略

Method: 使用XGBoost、Transformer架构和大语言模型(LLMs)，在音频、视频和文本特征上进行多模态抑郁检测对比分析

Result: 识别了不同模型类型在抑郁相关信号捕获方面的优势和局限性

Conclusion: 为精神健康预测领域提供了有价值的多模态表征策略见解

Abstract: This paper presents our approach to the first Multimodal Personality-Aware
Depression Detection Challenge, focusing on multimodal depression detection
using machine learning and deep learning models. We explore and compare the
performance of XGBoost, transformer-based architectures, and large language
models (LLMs) on audio, video, and text features. Our results highlight the
strengths and limitations of each type of model in capturing depression-related
signals across modalities, offering insights into effective multimodal
representation strategies for mental health prediction.

</details>


### [11] [UI-Bench: A Benchmark for Evaluating Design Capabilities of AI Text-to-App Tools](https://arxiv.org/abs/2508.20410)
*Sam Jung,Agustin Garcinuno,Spencer Mateega*

Main category: cs.CL

TL;DR: UI-Bench是首个大规模评估AI文本转应用工具视觉质量的基准测试，通过专家两两比较对10个工具、30个提示词、300个生成网站进行4000+次专家评判，使用TrueSkill模型进行排名。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏公开基准来严格验证AI文本转应用工具声称的高质量应用和网站生成能力，需要建立可复现的标准来推动AI驱动的网页设计发展。

Method: 构建包含10个工具、30个提示词、300个生成网站的大规模数据集，通过专家进行4000+次两两比较评估，使用TrueSkill衍生模型进行系统排名并计算校准置信区间。

Result: 建立了UI-Bench基准测试，提供完整的提示词集、开源评估框架和公开排行榜，为AI文本转应用工具提供了可复现的评估标准。

Conclusion: UI-Bench为AI驱动的网页设计领域建立了首个大规模、可复现的评估基准，通过专家评判和统计模型为工具性能提供了可靠的排名和比较标准。

Abstract: AI text-to-app tools promise high quality applications and websites in
minutes, yet no public benchmark rigorously verifies those claims. We introduce
UI-Bench, the first large-scale benchmark that evaluates visual excellence
across competing AI text-to-app tools through expert pairwise comparison.
Spanning 10 tools, 30 prompts, 300 generated sites, and \textit{4000+} expert
judgments, UI-Bench ranks systems with a TrueSkill-derived model that yields
calibrated confidence intervals. UI-Bench establishes a reproducible standard
for advancing AI-driven web design. We release (i) the complete prompt set,
(ii) an open-source evaluation framework, and (iii) a public leaderboard. The
generated sites rated by participants will be released soon. View the UI-Bench
leaderboard at https://uibench.ai/leaderboard.

</details>


### [12] [DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding](https://arxiv.org/abs/2508.20416)
*Hengchuan Zhu,Yihuan Xu,Yichen Li,Zijie Meng,Zuozhu Liu*

Main category: cs.CL

TL;DR: DentalBench是首个针对牙科领域的双语基准测试，包含问答数据集和大规模语料库，用于评估和提升LLM在牙科专业领域的表现。


<details>
  <summary>Details</summary>
Motivation: 现有医学LLM在牙科等专业医学领域的表现尚未充分探索，缺乏针对性的评估资源，需要专门的基准测试来推动该领域发展。

Method: 构建DentalBench基准，包含DentalQA（36,597个双语问答问题，覆盖4个任务和16个牙科子领域）和DentalCorpus（3.37亿token的大规模高质量语料库），支持监督微调和检索增强生成。

Result: 评估14个LLM发现性能存在显著差距，领域适应实验显示Qwen-2.5-3B在知识密集和术语相关任务上性能大幅提升。

Conclusion: 领域特定基准测试对于开发可信有效的医疗应用LLM至关重要，领域适应能显著提升模型在专业医学任务上的表现。

Abstract: Recent advances in large language models (LLMs) and medical LLMs (Med-LLMs)
have demonstrated strong performance on general medical benchmarks. However,
their capabilities in specialized medical fields, such as dentistry which
require deeper domain-specific knowledge, remain underexplored due to the lack
of targeted evaluation resources. In this paper, we introduce DentalBench, the
first comprehensive bilingual benchmark designed to evaluate and advance LLMs
in the dental domain. DentalBench consists of two main components: DentalQA, an
English-Chinese question-answering (QA) benchmark with 36,597 questions
spanning 4 tasks and 16 dental subfields; and DentalCorpus, a large-scale,
high-quality corpus with 337.35 million tokens curated for dental domain
adaptation, supporting both supervised fine-tuning (SFT) and
retrieval-augmented generation (RAG). We evaluate 14 LLMs, covering
proprietary, open-source, and medical-specific models, and reveal significant
performance gaps across task types and languages. Further experiments with
Qwen-2.5-3B demonstrate that domain adaptation substantially improves model
performance, particularly on knowledge-intensive and terminology-focused tasks,
and highlight the importance of domain-specific benchmarks for developing
trustworthy and effective LLMs tailored to healthcare applications.

</details>


### [13] [KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval](https://arxiv.org/abs/2508.20417)
*Chi Minh Bui,Ngoc Mai Thieu,Van Vinh Nguyen,Json J. Jung,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: KG-CQR是一个基于知识图谱的上下文查询检索框架，通过提取和补全相关KG子图来丰富查询上下文，在RAG系统中显著提升检索性能


<details>
  <summary>Details</summary>
Motivation: 现有方法主要解决语料库级别的上下文丢失问题，但缺乏对复杂查询的结构化关系表示和语义丰富化处理

Method: 包含子图提取、补全和上下文生成三个模块的模型无关流水线，通过知识图谱增强查询的上下文表示

Result: 在RAGBench和MultiHop-RAG数据集上实现mAP提升4-6%，Recall@25提升2-3%，在多跳问答等挑战性任务中表现优异

Conclusion: KG-CQR通过知识图谱的查询丰富化有效提升了RAG系统的检索效果，具有模型无关性和良好的可扩展性

Abstract: The integration of knowledge graphs (KGs) with large language models (LLMs)
offers significant potential to improve the retrieval phase of
retrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR,
a novel framework for Contextual Query Retrieval (CQR) that enhances the
retrieval phase by enriching the contextual representation of complex input
queries using a corpus-centric KG. Unlike existing methods that primarily
address corpus-level context loss, KG-CQR focuses on query enrichment through
structured relation representations, extracting and completing relevant KG
subgraphs to generate semantically rich query contexts. Comprising subgraph
extraction, completion, and contextual generation modules, KG-CQR operates as a
model-agnostic pipeline, ensuring scalability across LLMs of varying sizes
without additional training. Experimental results on RAGBench and MultiHop-RAG
datasets demonstrate KG-CQR's superior performance, achieving a 4-6%
improvement in mAP and a 2-3% improvement in Recall@25 over strong baseline
models. Furthermore, evaluations on challenging RAG tasks such as multi-hop
question answering show that, by incorporating KG-CQR, the performance
consistently outperforms the existing baseline in terms of retrieval
effectiveness

</details>


### [14] [CAMB: A comprehensive industrial LLM benchmark on civil aviation maintenance](https://arxiv.org/abs/2508.20420)
*Feng Zhang,Chengjie Pang,Yuehan Zhang,Chenyu Luo*

Main category: cs.CL

TL;DR: 开发专门用于民航维修领域的工业级LLM评测基准，评估模型基础知识和复杂推理能力，为领域特定优化提供基础


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估主要集中在数学和编程推理任务，缺乏专门面向民航维修这种知识密集领域的标准化评测工具

Method: 开发专门的民航维修评测基准，用于测试LLM在领域知识和复杂推理方面的能力，并利用该基准评估现有知名的向量嵌入模型和LLM

Result: 证明了该评测基准在民航维修领域评估模型性能的有效性，并开源了评测基准和代码

Conclusion: 该工作补充了当前LLM评估的重要空白，为民航维修领域的目标化改进奠定了基础，有助于推动更智能化解决方案的发展

Abstract: Civil aviation maintenance is a domain characterized by stringent industry
standards. Within this field, maintenance procedures and troubleshooting
represent critical, knowledge-intensive tasks that require sophisticated
reasoning. To address the lack of specialized evaluation tools for large
language models (LLMs) in this vertical, we propose and develop an
industrial-grade benchmark specifically designed for civil aviation
maintenance. This benchmark serves a dual purpose: It provides a standardized
tool to measure LLM capabilities within civil aviation maintenance, identifying
specific gaps in domain knowledge and complex reasoning. By pinpointing these
deficiencies, the benchmark establishes a foundation for targeted improvement
efforts (e.g., domain-specific fine-tuning, RAG optimization, or specialized
prompt engineering), ultimately facilitating progress toward more intelligent
solutions within civil aviation maintenance. Our work addresses a significant
gap in the current LLM evaluation, which primarily focuses on mathematical and
coding reasoning tasks. In addition, given that Retrieval-Augmented Generation
(RAG) systems are currently the dominant solutions in practical applications ,
we leverage this benchmark to evaluate existing well-known vector embedding
models and LLMs for civil aviation maintenance scenarios. Through experimental
exploration and analysis, we demonstrate the effectiveness of our benchmark in
assessing model performance within this domain, and we open-source this
evaluation benchmark and code to foster further research and
development:https://github.com/CamBenchmark/cambenchmark

</details>


### [15] [Searching the Title of Practical Work of the Informatics Engineering Bachelor Program with the Case Base Reasoning Method](https://arxiv.org/abs/2508.20442)
*Agung Sukrisna Jaya,Osvari Arsalan,Danny Matthew Saputra*

Main category: cs.CL

TL;DR: 基于案例推理(CBR)和TF-IDF向量化的实践工作标题搜索系统，通过余弦相似度计算匹配度，在705个标题测试中表现良好


<details>
  <summary>Details</summary>
Motivation: 为了解决实践工作标题搜索问题，利用历史案例经验来寻找最相似的实践工作标题，提高搜索效率和准确性

Method: 使用案例推理(CBR)技术，结合TF-IDF进行文本向量化处理，采用余弦相似度计算标题间的相似性值，支持标题和关键词两种搜索方式

Result: 在705个实践工作标题的测试中，通过两个阶段的测试：第一阶段搜索现有标题，第二阶段随机化标题搜索，结果显示第二阶段找到相同数量的标题且获得更高的平均匹配分数

Conclusion: 该系统能够有效搜索实践工作标题，TF-IDF和余弦相似度的结合在标题相似性计算中表现良好，为实践工作标题搜索提供了可靠的解决方案

Abstract: Case Base Reasoning (CBR) is a case solving technique based on experience in
cases that have occurred before with the highest similarity. CBR is used to
search for practical work titles. TF-IDF is applied to process the
vectorization of each practical work title word and Cosine Similarity for the
calculation of similarity values. This system can search either in the form of
titles or keywords. The output of the system is the title of practical work and
the match value of each title. Based on the test results using 705 practical
work titles, testing was carried out with five titles and carried out in two
stages. The first stage searches with existing titles and the second stage
randomizes the title from the first stage. And the results obtained in the
second stage are the same number of titles found and the highest average match
score.

</details>


### [16] [MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers](https://arxiv.org/abs/2508.20453)
*Zhenting Wang,Qi Chang,Hemani Patel,Shashank Biju,Cheng-En Wu,Quan Liu,Aolin Ding,Alireza Rezazadeh,Ankit Shah,Yujia Bao,Eugene Siow*

Main category: cs.CL

TL;DR: MCP-Bench是一个基于Model Context Protocol的基准测试，用于评估大语言模型在多步骤工具使用、跨工具协调和复杂规划推理方面的能力，包含28个真实MCP服务器和250个工具。


<details>
  <summary>Details</summary>
Motivation: 现有API基准测试主要依赖显式工具规范、浅层工作流和孤立域操作，无法充分评估LLM在模糊指令下检索工具、多跳执行规划、中间输出基础和跨域工作流协调等关键能力。

Method: 基于MCP协议构建基准，连接28个代表性实时MCP服务器，涵盖金融、旅行、科学计算和学术搜索等领域，设计包含丰富输入输出耦合的真实多步骤任务。

Result: 对20个先进LLM的实验揭示了在MCP-Bench上的持续挑战，表明当前模型在多步骤工具使用和跨域协调方面仍存在显著困难。

Conclusion: MCP-Bench提供了一个更真实、更全面的评估框架，能够更好地测试LLM在实际工具使用场景中的能力，填补了现有基准测试的空白。

Abstract: We introduce MCP-Bench, a benchmark for evaluating large language models
(LLMs) on realistic, multi-step tasks that demand tool use, cross-tool
coordination, precise parameter control, and planning/reasoning for solving
tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28
representative live MCP servers spanning 250 tools across domains such as
finance, traveling, scientific computing, and academic search. Unlike prior
API-based benchmarks, each MCP server provides a set of complementary tools
designed to work together, enabling the construction of authentic, multi-step
tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability
to retrieve relevant tools from fuzzy instructions without explicit tool names,
plan multi-hop execution trajectories for complex objectives, ground responses
in intermediate tool outputs, and orchestrate cross-domain workflows -
capabilities not adequately evaluated by existing benchmarks that rely on
explicit tool specifications, shallow few-step workflows, and isolated domain
operations. We propose a multi-faceted evaluation framework covering tool-level
schema understanding and usage, trajectory-level planning, and task completion.
Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code
and data: https://github.com/Accenture/mcp-bench.

</details>


### [17] [Prediction of mortality and resource utilization in critical care: a deep learning approach using multimodal electronic health records with natural language processing techniques](https://arxiv.org/abs/2508.20460)
*Yucheng Ruan,Xiang Lan,Daniel J. Tan,Hairil Rizal Abdullah,Mengling Feng*

Main category: cs.CL

TL;DR: 提出了一种基于深度学习和自然语言处理的多模态电子病历分析框架，用于预测ICU患者死亡率和资源使用情况，在三个临床任务上均优于现有方法，并对结构化数据损坏具有强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注结构化电子病历数据，忽略了自由文本中的临床洞察力，且未充分利用结构化数据中的文本信息潜力。需要开发能够整合多模态电子病历数据的预测模型。

Method: 使用两个真实世界电子病历数据集，开发了基于自然语言处理的深度学习框架，包含医学提示、自由文本和预训练句子编码器三个关键组件，并与现有领先方法进行比较评估。

Result: 在三个临床任务上表现优异：死亡率预测BACC/AUROC提升1.6%/0.8%，住院时间预测RMSE/MAE提升0.5%/2.2%，手术时长估计RMSE/MAE提升10.9%/11.0%，且在不同损坏率下均优于基线方法。

Conclusion: 该框架是预测重症监护死亡率和资源利用的有效准确方法，展示了使用提示学习和transformer编码器分析多模态电子病历的成功，对结构化数据损坏具有强韧性。

Abstract: Background Predicting mortality and resource utilization from electronic
health records (EHRs) is challenging yet crucial for optimizing patient
outcomes and managing costs in intensive care unit (ICU). Existing approaches
predominantly focus on structured EHRs, often ignoring the valuable clinical
insights in free-text notes. Additionally, the potential of textual information
within structured data is not fully leveraged. This study aimed to introduce
and assess a deep learning framework using natural language processing
techniques that integrates multimodal EHRs to predict mortality and resource
utilization in critical care settings. Methods Utilizing two real-world EHR
datasets, we developed and evaluated our model on three clinical tasks with
leading existing methods. We also performed an ablation study on three key
components in our framework: medical prompts, free-texts, and pre-trained
sentence encoder. Furthermore, we assessed the model's robustness against the
corruption in structured EHRs. Results Our experiments on two real-world
datasets across three clinical tasks showed that our proposed model improved
performance metrics by 1.6\%/0.8\% on BACC/AUROC for mortality prediction,
0.5%/2.2% on RMSE/MAE for LOS prediction, 10.9%/11.0% on RMSE/MAE for surgical
duration estimation compared to the best existing methods. It consistently
demonstrated superior performance compared to other baselines across three
tasks at different corruption rates. Conclusions The proposed framework is an
effective and accurate deep learning approach for predicting mortality and
resource utilization in critical care. The study also highlights the success of
using prompt learning with a transformer encoder in analyzing multimodal EHRs.
Importantly, the model showed strong resilience to data corruption within
structured data, especially at high corruption levels.

</details>


### [18] [ConspirED: A Dataset for Cognitive Traits of Conspiracy Theories and Large Language Model Safety](https://arxiv.org/abs/2508.20468)
*Luke Bates,Max Glockner,Preslav Nakov,Iryna Gurevych*

Main category: cs.CL

TL;DR: 提出了ConspirED数据集，用于分析阴谋论内容的认知特征，并评估大语言模型对阴谋论输入的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 阴谋论侵蚀公众对科学和机构的信任，且随着AI生成虚假信息的日益复杂化，理解阴谋论内容的修辞模式对于开发干预措施和评估AI漏洞至关重要

Method: 引入ConspirED数据集，使用CONSPIR认知框架标注在线阴谋文章的认知特征，开发计算模型识别阴谋论特征，并评估大语言模型对阴谋论输入的鲁棒性

Result: 发现大语言模型在处理阴谋论内容时会产生偏差，输出会反映输入的推理模式，即使能够成功处理类似的事实核查错误信息

Conclusion: ConspirED数据集为识别阴谋论认知特征和评估AI模型鲁棒性提供了重要工具，揭示了当前大语言模型在处理阴谋论内容时的局限性

Abstract: Conspiracy theories erode public trust in science and institutions while
resisting debunking by evolving and absorbing counter-evidence. As AI-generated
misinformation becomes increasingly sophisticated, understanding rhetorical
patterns in conspiratorial content is important for developing interventions
such as targeted prebunking and assessing AI vulnerabilities. We introduce
ConspirED (CONSPIR Evaluation Dataset), which captures the cognitive traits of
conspiratorial ideation in multi-sentence excerpts (80--120 words) from online
conspiracy articles, annotated using the CONSPIR cognitive framework
(Lewandowsky and Cook, 2020). ConspirED is the first dataset of conspiratorial
content annotated for general cognitive traits. Using ConspirED, we (i) develop
computational models that identify conspiratorial traits and determine dominant
traits in text excerpts, and (ii) evaluate large language/reasoning model
(LLM/LRM) robustness to conspiratorial inputs. We find that both are misaligned
by conspiratorial content, producing output that mirrors input reasoning
patterns, even when successfully deflecting comparable fact-checked
misinformation.

</details>


### [19] [Languages Still Left Behind: Toward a Better Multilingual Machine Translation Benchmark](https://arxiv.org/abs/2508.20511)
*Chihiro Taguchi,Seng Mai,Keita Kurabe,Yusuke Sakai,Georgina Agyei,Soudabeh Eslami,David Chiang*

Main category: cs.CL

TL;DR: FLORES+基准测试在200多种语言的机器翻译评估中存在质量问题，包括翻译质量低于声称标准、源文本领域特定和文化偏见、命名实体复制漏洞等问题，需要更领域通用和文化中立的基准。


<details>
  <summary>Details</summary>
Motivation: 研究FLORES+多语言机器翻译基准测试的实际适用性，发现其在翻译质量、文化偏见和评估协议方面存在严重缺陷，影响了真正的多语言评估效果。

Method: 通过人工评估四种语言（Asante Twi、日语、Jinghpaw和南阿塞拜疆语）的翻译质量，分析源文本的领域特异性和文化偏见，并使用简单启发式方法测试评估协议的漏洞。

Result: 发现许多翻译质量低于声称的90%标准，源文本过于领域特定且偏向英语世界文化，简单复制命名实体就能获得不错的BLEU分数，高质量MT模型在FLORES+上表现差但在领域相关测试集上表现好。

Conclusion: 需要构建使用领域通用、文化中立源文本且减少依赖命名实体的多语言MT基准，以更好地反映真实世界的翻译挑战。

Abstract: Multilingual machine translation (MT) benchmarks play a central role in
evaluating the capabilities of modern MT systems. Among them, the FLORES+
benchmark is widely used, offering English-to-many translation data for over
200 languages, curated with strict quality control protocols. However, we study
data in four languages (Asante Twi, Japanese, Jinghpaw, and South Azerbaijani)
and uncover critical shortcomings in the benchmark's suitability for truly
multilingual evaluation. Human assessments reveal that many translations fall
below the claimed 90% quality standard, and the annotators report that source
sentences are often too domain-specific and culturally biased toward the
English-speaking world. We further demonstrate that simple heuristics, such as
copying named entities, can yield non-trivial BLEU scores, suggesting
vulnerabilities in the evaluation protocol. Notably, we show that MT models
trained on high-quality, naturalistic data perform poorly on FLORES+ while
achieving significant gains on our domain-relevant evaluation set. Based on
these findings, we advocate for multilingual MT benchmarks that use
domain-general and culturally neutral source texts rely less on named entities,
in order to better reflect real-world translation challenges.

</details>


### [20] [SciTopic: Enhancing Topic Discovery in Scientific Literature through Advanced LLM](https://arxiv.org/abs/2508.20514)
*Pengjiang Li,Zaitian Wang,Xinhao Zhang,Ran Zhang,Lu Jiang,Pengfei Wang,Yuanchun Zhou*

Main category: cs.CL

TL;DR: SciTopic是一种基于大语言模型增强的科学文献主题发现方法，通过构建文本编码器、空间优化模块和LLM指导的对比学习，显著提升了科学主题识别的性能。


<details>
  <summary>Details</summary>
Motivation: 现有主题发现方法主要依赖词嵌入技术，缺乏对科学文献的全面理解，难以处理复杂的高维文本关系。大语言模型在文本理解方面的卓越能力为解决这一问题提供了新的思路。

Method: 1) 构建文本编码器捕获科学文献内容（元数据、标题、摘要）；2) 构建空间优化模块，整合基于熵的采样和LLM指导的三元组任务；3) 基于LLM指导微调文本编码器，通过优化三元组对比损失来更好地区分不同主题的实例。

Result: 在三个真实世界科学文献数据集上的广泛实验表明，SciTopic在科学主题发现方面优于现有最先进方法，使研究人员能够获得更深入和更快速的洞察。

Conclusion: SciTopic通过利用大语言模型的强大文本理解能力，有效解决了传统主题发现方法的局限性，为科学文献主题分析提供了更准确和高效的解决方案。

Abstract: Topic discovery in scientific literature provides valuable insights for
researchers to identify emerging trends and explore new avenues for
investigation, facilitating easier scientific information retrieval. Many
machine learning methods, particularly deep embedding techniques, have been
applied to discover research topics. However, most existing topic discovery
methods rely on word embedding to capture the semantics and lack a
comprehensive understanding of scientific publications, struggling with
complex, high-dimensional text relationships. Inspired by the exceptional
comprehension of textual information by large language models (LLMs), we
propose an advanced topic discovery method enhanced by LLMs to improve
scientific topic identification, namely SciTopic. Specifically, we first build
a textual encoder to capture the content from scientific publications,
including metadata, title, and abstract. Next, we construct a space
optimization module that integrates entropy-based sampling and triplet tasks
guided by LLMs, enhancing the focus on thematic relevance and contextual
intricacies between ambiguous instances. Then, we propose to fine-tune the
textual encoder based on the guidance from the LLMs by optimizing the
contrastive loss of the triplets, forcing the text encoder to better
discriminate instances of different topics. Finally, extensive experiments
conducted on three real-world datasets of scientific publications demonstrate
that SciTopic outperforms the state-of-the-art (SOTA) scientific topic
discovery methods, enabling researchers to gain deeper and faster insights.

</details>


### [21] [Overview of BioASQ 2024: The twelfth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20532)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Salvador Lima-López,Eulàlia Farré-Maduell,Martin Krallinger,Natalia Loukachevitch,Vera Davydova,Elena Tutubalina,Georgios Paliouras*

Main category: cs.CL

TL;DR: BioASQ 2024挑战赛概述，包含两个传统任务和两个新任务，37个团队参与，提交超过700次，系统性能持续提升


<details>
  <summary>Details</summary>
Motivation: 促进大规模生物医学语义索引和问答系统的进步，通过挑战赛推动该领域的技术发展

Method: 设立四个共享任务：两个传统任务（b和Synergy）和两个新任务（MultiCardioNER多语言临床实体检测和BIONNE嵌套NER）

Result: 37个竞争团队参与，提交超过700次，大多数系统达到竞争性性能水平

Conclusion: BioASQ挑战赛持续推动生物医学语义处理领域的技术进步，参与团队表现优异，表明该领域state-of-the-art技术不断发展

Abstract: This is an overview of the twelfth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2024. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks b and Synergy, and two
new tasks: a) MultiCardioNER on the adaptation of clinical entity detection to
the cardiology domain in a multilingual setting, and b) BIONNE on nested NER in
Russian and English. In this edition of BioASQ, 37 competing teams participated
with more than 700 distinct submissions in total for the four different shared
tasks of the challenge. Similarly to previous editions, most of the
participating systems achieved competitive performance, suggesting the
continuous advancement of the state-of-the-art in the field.

</details>


### [22] [Overview of BioASQ 2025: The Thirteenth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20554)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Martin Krallinger,Miguel Rodríguez-Ortega,Eduard Rodriguez-López,Natalia Loukachevitch,Andrey Sakhovskiy,Elena Tutubalina,Dimitris Dimitriadis,Grigorios Tsoumakas,George Giannakoulas,Alexandra Bekiaridou,Athanasios Samaras,Giorgio Maria Di Nunzio,Nicola Ferro,Stefano Marchesin,Marco Martinelli,Gianmaria Silvello,Georgios Paliouras*

Main category: cs.CL

TL;DR: BioASQ 2025挑战赛概述，包含两个传统任务和四个新任务，涉及生物医学语义索引、问答、多语言临床摘要、命名实体链接、临床编码和信息提取等领域，共有83个团队参与。


<details>
  <summary>Details</summary>
Motivation: 促进大规模生物医学语义索引和问答技术的进步，通过挑战赛形式推动该领域的前沿发展。

Method: 设立六个共享任务：两个传统任务（b和Synergy）和四个新任务（多语言临床摘要、嵌套命名实体链接、心脏病学临床编码、肠脑相互作用信息提取），邀请研究团队参与并提交解决方案。

Result: 83个竞争团队参与，提交超过1000份不同的解决方案，多个系统表现出色，显示了该领域技术的持续进步。

Conclusion: BioASQ挑战赛成功推动了生物医学信息处理技术的发展，新任务的引入拓展了研究范围，参与团队的表现表明该领域正在不断取得进展。

Abstract: This is an overview of the thirteenth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2025. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks, b and Synergy, and four
new tasks: a) Task MultiClinSum on multilingual clinical summarization. b) Task
BioNNE-L on nested named entity linking in Russian and English. c) Task
ELCardioCC on clinical coding in cardiology. d) Task GutBrainIE on gut-brain
interplay information extraction. In this edition of BioASQ, 83 competing teams
participated with more than 1000 distinct submissions in total for the six
different shared tasks of the challenge. Similar to previous editions, several
participating systems achieved competitive performance, indicating the
continuous advancement of the state-of-the-art in the field.

</details>


### [23] [Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data](https://arxiv.org/abs/2508.20557)
*Jiahao Xiao,Jiangming Liu*

Main category: cs.CL

TL;DR: 本文提出了一个针对多领域非IID数据的自适应联邦蒸馏框架(AdaFD)，解决了现有联邦学习主要关注标签多样性而忽略语言领域多样性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦蒸馏方法主要关注标签(output)多样性，但忽略了自然语言处理中至关重要的语言领域(input)多样性，无法真实反映现实环境中的非IID数据挑战。

Method: 提出了自适应联邦蒸馏(AdaFD)框架，包含多领域非IID场景的统一基准测试框架，能够处理同质和异质设置下的多领域非IID挑战。

Result: 实验结果表明，该模型能够捕捉本地客户端的多样性，相比现有工作取得了更好的性能表现。

Conclusion: AdaFD框架有效解决了多领域非IID数据在联邦学习中的挑战，为真实环境下的联邦学习评估提供了统一的基准测试框架。

Abstract: The widespread success of pre-trained language models has established a new
training paradigm, where a global PLM is fine-tuned using task-specific data
from local clients. The local data are highly different from each other and can
not capture the global distribution of the whole data in real world. To address
the challenges of non-IID data in real environments, privacy-preserving
federated distillation has been proposed and highly investigated. However,
previous experimental non-IID scenarios are primarily identified with the label
(output) diversity, without considering the diversity of language domains
(input) that is crucial in natural language processing. In this paper, we
introduce a comprehensive set of multi-domain non-IID scenarios and propose a
unified benchmarking framework that includes diverse data. The benchmark can be
used to evaluate the federated learning framework in a real environment. To
this end, we propose an Adaptive Federated Distillation (AdaFD) framework
designed to address multi-domain non-IID challenges in both homogeneous and
heterogeneous settings. Experimental results demonstrate that our models
capture the diversity of local clients and achieve better performance compared
to the existing works. The code for this paper is available at:
https://github.com/jiahaoxiao1228/AdaFD.

</details>


### [24] [Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search](https://arxiv.org/abs/2508.20559)
*Zeyu Xiong,Yixuan Nan,Li Gao,Hengzhu Tang,Shuaiqiang Wang,Junfeng Wang,Dawei Yin*

Main category: cs.CL

TL;DR: 本文提出了一种基于生成模型的查询驱动文本摘要新框架，通过大模型蒸馏、监督微调、直接偏好优化和前瞻解码等技术，将轻量级模型转化为专业QDTS专家，在工业网络搜索中实现了优异性能。


<details>
  <summary>Details</summary>
Motivation: 传统抽取式摘要模型存在多阶段流水线导致信息损失和架构瓶颈，以及对用户查询和文档语义理解不足的问题，特别是在处理复杂搜索意图时表现不佳。

Method: 整合大模型蒸馏、监督微调、直接偏好优化和前瞻解码技术，将仅有0.1B参数的轻量级模型转化为领域专业化的查询驱动文本摘要专家模型。

Result: 在多个行业相关指标上超越生产基线并达到新的最先进水平，部署效率优异，仅需334块NVIDIA L20 GPU即可处理约50,000 QPS，平均延迟55ms。

Conclusion: 生成模型在实时查询驱动文本摘要任务中具有显著优势，提出的框架成功解决了传统方法的局限性，为工业网络搜索提供了高效实用的解决方案。

Abstract: In the dynamic landscape of large-scale web search, Query-Driven Text
Summarization (QDTS) aims to generate concise and informative summaries from
textual documents based on a given query, which is essential for improving user
engagement and facilitating rapid decision-making. Traditional extractive
summarization models, based primarily on ranking candidate summary segments,
have been the dominant approach in industrial applications. However, these
approaches suffer from two key limitations: 1) The multi-stage pipeline often
introduces cumulative information loss and architectural bottlenecks due to its
weakest component; 2) Traditional models lack sufficient semantic understanding
of both user queries and documents, particularly when dealing with complex
search intents. In this study, we propose a novel framework to pioneer the
application of generative models to address real-time QDTS in industrial web
search. Our approach integrates large model distillation, supervised
fine-tuning, direct preference optimization, and lookahead decoding to
transform a lightweight model with only 0.1B parameters into a
domain-specialized QDTS expert. Evaluated on multiple industry-relevant
metrics, our model outperforms the production baseline and achieves a new state
of the art. Furthermore, it demonstrates excellent deployment efficiency,
requiring only 334 NVIDIA L20 GPUs to handle \textasciitilde50,000 queries per
second under 55~ms average latency per query.

</details>


### [25] [KCS: Diversify Multi-hop Question Generation with Knowledge Composition Sampling](https://arxiv.org/abs/2508.20567)
*Yangfan Wang,Jie Liu,Chen Tang,Lian Yan,Jingchi Jiang*

Main category: cs.CL

TL;DR: 本文提出了知识组合采样(KCS)框架，通过采样不同的知识组合来增强多跳问题的多样性，解决了数据稀疏性问题，在多个数据集上取得了性能提升。


<details>
  <summary>Details</summary>
Motivation: 多跳问答面临数据稀疏性挑战，导致语言模型容易学习虚假模式。现有方法主要关注简单问题的生成，但忽略了文档中相关句子的知识整合。

Method: 提出知识组合采样(KCS)框架，将知识组合选择建模为句子级条件预测任务，使用概率对比损失预测下一个最相关的知识片段，在推理时采用随机解码策略平衡准确性和多样性。

Result: KCS将知识组合选择的整体准确率提高了3.9%，在HotpotQA和2WikiMultihopQA数据集上的数据增强应用带来了性能改进。

Conclusion: KCS框架通过有效采样知识组合，成功提升了多跳问题生成的多样性和质量，为解决数据稀疏性问题提供了有效解决方案。

Abstract: Multi-hop question answering faces substantial challenges due to data
sparsity, which increases the likelihood of language models learning spurious
patterns. To address this issue, prior research has focused on diversifying
question generation through content planning and varied expression. However,
these approaches often emphasize generating simple questions and neglect the
integration of essential knowledge, such as relevant sentences within
documents. This paper introduces the Knowledge Composition Sampling (KCS), an
innovative framework designed to expand the diversity of generated multi-hop
questions by sampling varied knowledge compositions within a given context. KCS
models the knowledge composition selection as a sentence-level conditional
prediction task and utilizes a probabilistic contrastive loss to predict the
next most relevant piece of knowledge. During inference, we employ a stochastic
decoding strategy to effectively balance accuracy and diversity. Compared to
competitive baselines, our KCS improves the overall accuracy of knowledge
composition selection by 3.9%, and its application for data augmentation yields
improvements on HotpotQA and 2WikiMultihopQA datasets. Our code is available
at: https://github.com/yangfanww/kcs.

</details>


### [26] [A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models](https://arxiv.org/abs/2508.20583)
*Soham Petkar,Hari Aakash K,Anirudh Vempati,Akshit Sinha,Ponnurangam Kumarauguru,Chirag Agarwal*

Main category: cs.CL

TL;DR: 这篇论文提出了新的CLEGR测试标准，指出当前图语言模型的评估存在缺陷，现有测试集无法有效评估多模态理解能力，且现有GLM模型在结构化理解方面表现差强。


<details>
  <summary>Details</summary>
Motivation: 当前图语言模型的评估标准主要是重用节点分类数据集，这些数据集只需单模态信息就能获得强劳性能，无法真正评估多模态理解能力。

Method: 提出CLEGR测试标准，通过合成图生成流水线配合需要结构和语义联合理解的问题，在不同复杂度层面评估多模态理解能力。

Result: 软提示LLM基线模型与包含完整GNN核心的GLM模型表现相似，且GLM模型在需要结构化理解的任务中表现显著下降。

Conclusion: 当前图语言模型在图结构理解能力上存在显著局限性，CLEGR标准为推进多模态理解研究提供了基础。

Abstract: Developments in Graph-Language Models (GLMs) aim to integrate the structural
reasoning capabilities of Graph Neural Networks (GNNs) with the semantic
understanding of Large Language Models (LLMs). However, we demonstrate that
current evaluation benchmarks for GLMs, which are primarily repurposed
node-level classification datasets, are insufficient to assess multimodal
reasoning. Our analysis reveals that strong performance on these benchmarks is
achievable using unimodal information alone, suggesting that they do not
necessitate graph-language integration. To address this evaluation gap, we
introduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed
to evaluate multimodal reasoning at various complexity levels. Our benchmark
employs a synthetic graph generation pipeline paired with questions that
require joint reasoning over structure and textual semantics. We perform a
thorough evaluation of representative GLM architectures and find that
soft-prompted LLM baselines perform on par with GLMs that incorporate a full
GNN backbone. This result calls into question the architectural necessity of
incorporating graph structure into LLMs. We further show that GLMs exhibit
significant performance degradation in tasks that require structural reasoning.
These findings highlight limitations in the graph reasoning capabilities of
current GLMs and provide a foundation for advancing the community toward
explicit multimodal reasoning involving graph structure and language.

</details>


### [27] [Generative Annotation for ASR Named Entity Correction](https://arxiv.org/abs/2508.20700)
*Yuanchang Luo,Daimeng Wei,Shaojun Li,Hengchao Shang,Jiaxin Guo,Zongyao Li,Zhanglin Wu,Xiaoyu Chen,Zhiqiang Rao,Jinlong Yang,Hao Yang*

Main category: cs.CL

TL;DR: 这篇论文提出了一种利用语音声音特征来编正语音识别中域特定名称实体错误的新方法，解决了传统语音编辑距离方法在词形差异较大时失效的问题。


<details>
  <summary>Details</summary>
Motivation: 终端到终端语音识别系统经常无法正确识别域特定名称实体，导致下游任务的灾难性失败。现有的语音编辑距离方法在词形差异较大时无法准确定位错误词汇。

Method: 提出一种新的名称实体编正方法，利用语音声音特征来检索候选实体，然后设计生成方法来注释ASR识别结果中的实体错误并替换为正确实体。

Result: 在开源和自建测试集上的实验结果显示，该方法能够显著提高实体识别的准确性，尤其在词形差异较大的场景中表现优异。

Conclusion: 该研究提出的基于语音声音特征的名称实体编正方法有效解决了传统方法的局限性，为ASR系统在域特定名称实体识别方面提供了重要改进。

Abstract: End-to-end automatic speech recognition systems often fail to transcribe
domain-specific named entities, causing catastrophic failures in downstream
tasks. Numerous fast and lightweight named entity correction (NEC) models have
been proposed in recent years. These models, mainly leveraging phonetic-level
edit distance algorithms, have shown impressive performances. However, when the
forms of the wrongly-transcribed words(s) and the ground-truth entity are
significantly different, these methods often fail to locate the wrongly
transcribed words in hypothesis, thus limiting their usage. We propose a novel
NEC method that utilizes speech sound features to retrieve candidate entities.
With speech sound features and candidate entities, we inovatively design a
generative method to annotate entity errors in ASR transcripts and replace the
text with correct entities. This method is effective in scenarios of word form
difference. We test our method using open-source and self-constructed test
sets. The results demonstrate that our NEC method can bring significant
improvement to entity accuracy. We will open source our self-constructed test
set and training data.

</details>


### [28] [Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label Hierarchical Learning](https://arxiv.org/abs/2508.20712)
*Nelson Filipe Costa,Leila Kosseim*

Main category: cs.CL

TL;DR: 提出了首个多语言多标签隐式篇章关系识别模型HArch，在DiscoGeM 2.0语料上验证，通过层次化依赖关系预测PDTB 3.0框架中的三个意义层级，性能优于GPT-4o和Llama-4-Maverick，并在DiscoGeM 1.0上达到SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 解决隐式篇章关系识别中的多语言和多标签分类问题，利用层次化依赖关系提升识别性能。

Method: 使用HArch模型，结合预训练编码器（RoBERTa和XLM-RoBERTa），通过层次化依赖预测PDTB 3.0的三个意义层级，并与LLM进行少样本提示对比。

Result: RoBERTa-HArch在英语中表现最佳，XLM-RoBERTa-HArch在多语言设置中最佳，微调模型 consistently 优于GPT-4o和Llama-4-Maverick，在DiscoGeM 1.0上达到SOTA。

Conclusion: 任务特异性微调在隐式篇章关系识别中优于提示方法，层次化方法有效，HArch模型在多语言和多标签分类中表现出色。

Abstract: This paper introduces the first multi-lingual and multi-label classification
model for implicit discourse relation recognition (IDRR). Our model, HArch, is
evaluated on the recently released DiscoGeM 2.0 corpus and leverages
hierarchical dependencies between discourse senses to predict probability
distributions across all three sense levels in the PDTB 3.0 framework. We
compare several pre-trained encoder backbones and find that RoBERTa-HArch
achieves the best performance in English, while XLM-RoBERTa-HArch performs best
in the multi-lingual setting. In addition, we compare our fine-tuned models
against GPT-4o and Llama-4-Maverick using few-shot prompting across all
language configurations. Our results show that our fine-tuned models
consistently outperform these LLMs, highlighting the advantages of
task-specific fine-tuning over prompting in IDRR. Finally, we report SOTA
results on the DiscoGeM 1.0 corpus, further validating the effectiveness of our
hierarchical approach.

</details>


### [29] [Addressing Tokenization Inconsistency in Steganography and Watermarking Based on Large Language Models](https://arxiv.org/abs/2508.20718)
*Ruiyi Yan,Yugo Murawaki*

Main category: cs.CL

TL;DR: 该研究针对文本隐写和水印中的tokenization不一致问题，提出了基于问题token特征（低频性和临时性）的两种解决方案，分别在隐写和水印任务中取得了性能提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型提升了文本生成能力，一方面改善了文本隐写质量，另一方面也凸显了水印技术防止恶意滥用的重要性。研究发现tokenization不一致会破坏鲁棒性，需要专门解决。

Method: 提出了两种针对性的TI消除方法：1）针对隐写的逐步验证方法；2）针对水印的事后回滚方法。这些方法基于问题token的两个关键特征：低频性和临时性。

Result: 实验表明：1）与传统消歧方法相比，直接解决TI问题在隐写中提升了流畅性、不可感知性和抗隐写分析能力；2）在水印中解决TI问题增强了可检测性和抗攻击鲁棒性。

Conclusion: 通过识别和解决tokenization不一致问题，可以有效提升文本隐写和水印技术的性能表现，为相关应用提供了实用的解决方案。

Abstract: Large language models have significantly enhanced the capacities and
efficiency of text generation. On the one hand, they have improved the quality
of text-based steganography. On the other hand, they have also underscored the
importance of watermarking as a safeguard against malicious misuse. In this
study, we focus on tokenization inconsistency (TI) between Alice and Bob in
steganography and watermarking, where TI can undermine robustness. Our
investigation reveals that the problematic tokens responsible for TI exhibit
two key characteristics: infrequency and temporariness. Based on these
findings, we propose two tailored solutions for TI elimination: a stepwise
verification method for steganography and a post-hoc rollback method for
watermarking. Experiments show that (1) compared to traditional disambiguation
methods in steganography, directly addressing TI leads to improvements in
fluency, imperceptibility, and anti-steganalysis capacity; (2) for
watermarking, addressing TI enhances detectability and robustness against
attacks.

</details>


### [30] [rStar2-Agent: Agentic Reasoning Technical Report](https://arxiv.org/abs/2508.20722)
*Ning Shang,Yifei Liu,Yi Zhu,Li Lyna Zhang,Weijiang Xu,Xinyu Guan,Buze Zhang,Bingcheng Dong,Xudong Zhou,Bowen Zhang,Ying Xin,Ziming Miao,Scarlett Li,Fan Yang,Mao Yang*

Main category: cs.CL

TL;DR: rStar2-Agent是一个14B参数的数学推理模型，通过智能体强化学习实现前沿性能，在AIME24和AIME25上分别达到80.6%和69.8%的pass@1分数，超越DeepSeek-R1(671B)且响应更短。


<details>
  <summary>Details</summary>
Motivation: 当前的长链思维(CoT)方法存在局限性，需要开发能够自主探索、验证和优化中间步骤的智能推理模型，特别是在复杂数学问题求解中有效使用Python编码工具。

Method: 采用智能体强化学习方法，包含三个关键技术：(1)高效的RL基础设施和可靠的Python代码执行环境；(2)GRPO-RoC算法，通过重采样校正策略处理编码工具的环境噪声；(3)从非推理SFT开始的多阶段RL训练配方。

Result: 在仅510个RL步骤和一周训练时间内，将预训练的14B模型提升到最先进水平，在AIME24和AIME25上分别达到80.6%和69.8%的平均pass@1分数，超越了DeepSeek-R1(671B)模型。

Conclusion: rStar2-Agent不仅展示了在数学推理方面的卓越性能，还在对齐、科学推理和智能体工具使用任务上表现出强大的泛化能力，证明了智能体RL在大规模应用中的有效性。

Abstract: We introduce rStar2-Agent, a 14B math reasoning model trained with agentic
reinforcement learning to achieve frontier-level performance. Beyond current
long CoT, the model demonstrates advanced cognitive behaviors, such as thinking
carefully before using Python coding tools and reflecting on code execution
feedback to autonomously explore, verify, and refine intermediate steps in
complex problem-solving. This capability is enabled through three key
innovations that makes agentic RL effective at scale: (i) an efficient RL
infrastructure with a reliable Python code environment that supports
high-throughput execution and mitigates the high rollout costs, enabling
training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic
RL algorithm with a Resample-on-Correct rollout strategy that addresses the
inherent environment noises from coding tools, allowing the model to reason
more effectively in a code environment; (iii) An efficient agent training
recipe that starts with non-reasoning SFT and progresses through multi-RL
stages, yielding advanced cognitive abilities with minimal compute cost. To
this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in
only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on
AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly
shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates
strong generalization to alignment, scientific reasoning, and agentic tool-use
tasks. Code and training recipes are available at
https://github.com/microsoft/rStar.

</details>


### [31] [Leveraging Semantic Triples for Private Document Generation with Local Differential Privacy Guarantees](https://arxiv.org/abs/2508.20736)
*Stephen Meisenbacher,Maulik Chevli,Florian Matthes*

Main category: cs.CL

TL;DR: 提出DP-ST方法，利用语义三元组在本地差分隐私下实现邻域感知的文档生成，通过分治策略和LLM后处理在较低ε值下实现隐私与效用的平衡


<details>
  <summary>Details</summary>
Motivation: 解决本地差分隐私下文本隐私保护的挑战，传统方法在低ε值下效果不佳，需要新的方法来平衡隐私保护和文本效用

Method: DP-ST方法，基于语义三元组进行邻域感知的私有文档生成，采用分治策略，结合LLM后处理来提升文本连贯性

Result: 在较低ε值下实现了连贯的文本生成，有效平衡了隐私保护和文本效用，证明了分治范式的有效性

Conclusion: 语义连贯性对于在合理ε水平下实现平衡的隐私化输出至关重要，DP-ST方法为本地差分隐私下的文本处理提供了有效解决方案

Abstract: Many works at the intersection of Differential Privacy (DP) in Natural
Language Processing aim to protect privacy by transforming texts under DP
guarantees. This can be performed in a variety of ways, from word perturbations
to full document rewriting, and most often under local DP. Here, an input text
must be made indistinguishable from any other potential text, within some bound
governed by the privacy parameter $\varepsilon$. Such a guarantee is quite
demanding, and recent works show that privatizing texts under local DP can only
be done reasonably under very high $\varepsilon$ values. Addressing this
challenge, we introduce DP-ST, which leverages semantic triples for
neighborhood-aware private document generation under local DP guarantees.
Through the evaluation of our method, we demonstrate the effectiveness of the
divide-and-conquer paradigm, particularly when limiting the DP notion (and
privacy guarantees) to that of a privatization neighborhood. When combined with
LLM post-processing, our method allows for coherent text generation even at
lower $\varepsilon$ values, while still balancing privacy and utility. These
findings highlight the importance of coherence in achieving balanced
privatization outputs at reasonable $\varepsilon$ levels.

</details>


### [32] [Specializing General-purpose LLM Embeddings for Implicit Hate Speech Detection across Datasets](https://arxiv.org/abs/2508.20750)
*Vassiliy Cheremetiev,Quang Long Ho Ngo,Chau Ying Kot,Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CL

TL;DR: 通过细调基于LLM的通用嵌入模型，在隐式敌意言语检测中实现了独立的新的最高性能绩效


<details>
  <summary>Details</summary>
Motivation: 隐式敌意言语通过隐蕴提示、谬刺或代码术语传递偏见，因不包含明显谬辱性词汇而难以检测，需要更有效的解决方案

Method: 仅通过细调最新的通用嵌入模型（Stella、Jasper、NV-Embed、E5等），不依赖外部知识或额外信息

Result: 在多个IHS数据集上实现了最高性能：数据集内评估F1-macro提升0.55-1.10百分点，跨数据集评估提升达20.35百分点

Conclusion: 简单的细调通用嵌入模型方法可以较传统流水线更有效地检测隐式敌意言语，特别是在跨数据集情况下表现更优

Abstract: Implicit hate speech (IHS) is indirect language that conveys prejudice or
hatred through subtle cues, sarcasm or coded terminology. IHS is challenging to
detect as it does not include explicit derogatory or inflammatory words. To
address this challenge, task-specific pipelines can be complemented with
external knowledge or additional information such as context, emotions and
sentiment data. In this paper, we show that, by solely fine-tuning recent
general-purpose embedding models based on large language models (LLMs), such as
Stella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance.
Experiments on multiple IHS datasets show up to 1.10 percentage points
improvements for in-dataset, and up to 20.35 percentage points improvements in
cross-dataset evaluation, in terms of F1-macro score.

</details>


### [33] [GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and Efficient Open-Ended Text Generation](https://arxiv.org/abs/2508.20757)
*Yuanhao Ding,Esteban Garces Arias,Meimingwei Li,Julian Rodemann,Matthias Aßenmacher,Danlu Chen,Gaojuan Fan,Christian Heumann,Chongsheng Zhang*

Main category: cs.CL

TL;DR: GUARD是一种自适应的解码方法，通过全局和局部不确定性信号平衡文本生成的连贯性和多样性，同时提高生成速度


<details>
  <summary>Details</summary>
Motivation: 解决开放文本生成中连贯性与多样性平衡的挑战，现有对比搜索方法存在超参数依赖和计算成本高的问题

Method: 提出GUARD方法，结合全局熵估计和局部熵偏差的"Glocal"不确定性框架，并加入基于token计数的惩罚机制来降低计算开销

Result: 实验表明GUARD在文本多样性和连贯性之间取得良好平衡，生成速度显著提升，人类和LLM评估者均验证其优异性能

Conclusion: GUARD提供了一种有效且高效的自适应解码方案，为开放文本生成提供了新的解决方案

Abstract: Open-ended text generation faces a critical challenge: balancing coherence
with diversity in LLM outputs. While contrastive search-based decoding
strategies have emerged to address this trade-off, their practical utility is
often limited by hyperparameter dependence and high computational costs. We
introduce GUARD, a self-adaptive decoding method that effectively balances
these competing objectives through a novel "Glocal" uncertainty-driven
framework. GUARD combines global entropy estimates with local entropy
deviations to integrate both long-term and short-term uncertainty signals. We
demonstrate that our proposed global entropy formulation effectively mitigates
abrupt variations in uncertainty, such as sudden overconfidence or high entropy
spikes, and provides theoretical guarantees of unbiasedness and consistency. To
reduce computational overhead, we incorporate a simple yet effective
token-count-based penalty into GUARD. Experimental results demonstrate that
GUARD achieves a good balance between text diversity and coherence, while
exhibiting substantial improvements in generation speed. In a more nuanced
comparison study across different dimensions of text quality, both human and
LLM evaluators validated its remarkable performance. Our code is available at
https://github.com/YecanLee/GUARD.

</details>


### [34] [Feel the Difference? A Comparative Analysis of Emotional Arcs in Real and LLM-Generated CBT Sessions](https://arxiv.org/abs/2508.20764)
*Xiaoyi Wang,Jiwei Zhang,Guangtao Zhang,Honglei Guo*

Main category: cs.CL

TL;DR: 这篇论文通过情感弧分析比较了真实和LLM生成的认知行为疗法对话，发现合成对话在情感变化、语言丰富度和反应模式上与真实对话存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成的合成疗法对话在精神健康NLP中应用日渐增多，需要评估这些合成对话是否能够准确摘据真实疗法中的微妙情感动态。

Method: 采用Utterance Emotion Dynamics框架分析了价值(valence)、舌动(arousal)和主导(dominance)三个维度的情感轨迹，对比公开视频转写的真实CBT会议和CACTUS数据集中的合成对话。

Result: 合成对话虽然流畅且结构一致，但在情感特性上与真实对话存在显著差异：真实会议呈现更高的情感变化性、更丰富的情感语言以及更真实的反应和调节模式。情感弧相似性低，尤其是客户端。

Conclusion: 当前LLM生成的疗法数据在情感保真度方面存在限制，精神健康应用中情感准确性至关重要。研究人员提供了RealCBT数据集支持未来研究。

Abstract: Synthetic therapy dialogues generated by large language models (LLMs) are
increasingly used in mental health NLP to simulate counseling scenarios, train
models, and supplement limited real-world data. However, it remains unclear
whether these synthetic conversations capture the nuanced emotional dynamics of
real therapy. In this work, we conduct the first comparative analysis of
emotional arcs between real and LLM-generated Cognitive Behavioral Therapy
dialogues. We adapt the Utterance Emotion Dynamics framework to analyze
fine-grained affective trajectories across valence, arousal, and dominance
dimensions. Our analysis spans both full dialogues and individual speaker roles
(counselor and client), using real sessions transcribed from public videos and
synthetic dialogues from the CACTUS dataset. We find that while synthetic
dialogues are fluent and structurally coherent, they diverge from real
conversations in key emotional properties: real sessions exhibit greater
emotional variability,more emotion-laden language, and more authentic patterns
of reactivity and regulation. Moreover, emotional arc similarity between real
and synthetic speakers is low, especially for clients. These findings
underscore the limitations of current LLM-generated therapy data and highlight
the importance of emotional fidelity in mental health applications. We
introduce RealCBT, a curated dataset of real CBT sessions, to support future
research in this space.

</details>


### [35] [Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection](https://arxiv.org/abs/2508.20766)
*Harethah Abu Shairah,Hasan Abed Al Kader Hammoud,George Turkiyyah,Bernard Ghanem*

Main category: cs.CL

TL;DR: ROSI是一种白盒方法，通过秩一权重修改来增强LLM的安全对齐，提高拒绝有害请求的能力，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有安全机制容易被绕过，需要一种简单有效的方法来增强LLM的安全拒绝能力。

Method: 通过计算有害/无害指令对的安全方向，对所有残差流写入矩阵进行秩一权重修改，永久引导激活向量朝向拒绝子空间。

Result: ROSI显著提高了安全拒绝率（Llama Guard 3评估），同时在MMLU、HellaSwag、Arc等基准测试中保持模型性能。

Conclusion: 定向、可解释的权重引导是改善LLM安全性的廉价而有效的机制，可作为最后一公里安全程序。

Abstract: Safety alignment in Large Language Models (LLMs) often involves mediating
internal representations to refuse harmful requests. Recent research has
demonstrated that these safety mechanisms can be bypassed by ablating or
removing specific representational directions within the model. In this paper,
we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box
method that amplifies a model's safety alignment by permanently steering its
activations toward the refusal-mediating subspace. ROSI operates as a simple,
fine-tuning-free rank-one weight modification applied to all residual stream
write matrices. The required safety direction can be computed from a small set
of harmful and harmless instruction pairs. We show that ROSI consistently
increases safety refusal rates - as evaluated by Llama Guard 3 - while
preserving the utility of the model on standard benchmarks such as MMLU,
HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align
'uncensored' models by amplifying their own latent safety directions,
demonstrating its utility as an effective last-mile safety procedure. Our
results suggest that targeted, interpretable weight steering is a cheap and
potent mechanism to improve LLM safety, complementing more resource-intensive
fine-tuning paradigms.

</details>


### [36] [Signs of Struggle: Spotting Cognitive Distortions across Language and Register](https://arxiv.org/abs/2508.20771)
*Abhishek Kuber,Enrico Liscio,Ruixuan Zhang,Caroline Figueroa,Pradeep K. Murukannaiah*

Main category: cs.CL

TL;DR: 这篇论文研究了跨语言和语域的认知扭曲检测问题，分析了荷兰青少年论坛文本中的心理疾病早期警示信号


<details>
  <summary>Details</summary>
Motivation: 青少年心理健康问题日益增多，需要自动化方法来检测数字文本中的认知扭曲（不合理思维模式），以实现时及、低成本干预

Method: 进行跨语言和语域的泛化研究，分析荷兰青少年论坛发帖，使用域适配方法来提高模型表现

Result: 语言和写作风格的变化会显著影响模型性能，但域适配方法显示出最大的潜力

Conclusion: 认知扭曲检测需要考虑跨语言和语域的挑战，域适配技术是提高模型泛化能力的有效途径

Abstract: Rising mental health issues among youth have increased interest in automated
approaches for detecting early signs of psychological distress in digital text.
One key focus is the identification of cognitive distortions, irrational
thought patterns that have a role in aggravating mental distress. Early
detection of these distortions may enable timely, low-cost interventions. While
prior work has focused on English clinical data, we present the first in-depth
study of cross-lingual and cross-register generalization of cognitive
distortion detection, analyzing forum posts written by Dutch adolescents. Our
findings show that while changes in language and writing style can
significantly affect model performance, domain adaptation methods show the most
promise.

</details>


### [37] [GDLLM: A Global Distance-aware Modeling Approach Based on Large Language Models for Event Temporal Relation Extraction](https://arxiv.org/abs/2508.20828)
*Jie Zhao,Wanting Ning,Yuxiao Fei,Yubo Feng,Lishuang Li*

Main category: cs.CL

TL;DR: 提出GDLLM方法，通过距离感知图结构和软推理时序特征学习，解决小语言模型处理少数类关系和大语言模型长距离依赖判断的问题，在TB-Dense和MATRES数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在类别不平衡数据集中处理少数类关系能力有限，大语言模型使用人工设计的提示可能引入噪声，干扰对事件间长距离依赖关系的判断。

Method: 基于大语言模型的全局距离感知建模方法(GDLLM)，使用图注意力网络构建距离感知图结构来捕获长距离依赖特征，设计基于软推理的时序特征学习范式增强短距离邻近带的关系识别。

Result: 在两个公开数据集TB-Dense和MATRES上实现了最先进的性能，显著提升了少数关系类的性能并改善了整体学习能力。

Conclusion: GDLLM框架通过有效捕获全局特征，能够显著增强少数关系类的性能并提升整体学习能力，在事件时序关系提取任务中表现出色。

Abstract: In Natural Language Processing(NLP), Event Temporal Relation Extraction
(ETRE) is to recognize the temporal relations of two events. Prior studies have
noted the importance of language models for ETRE. However, the restricted
pre-trained knowledge of Small Language Models(SLMs) limits their capability to
handle minority class relations in imbalanced classification datasets. For
Large Language Models(LLMs), researchers adopt manually designed prompts or
instructions, which may introduce extra noise, leading to interference with the
model's judgment of the long-distance dependencies between events. To address
these issues, we propose GDLLM, a Global Distance-aware modeling approach based
on LLMs. We first present a distance-aware graph structure utilizing Graph
Attention Network(GAT) to assist the LLMs in capturing long-distance dependency
features. Additionally, we design a temporal feature learning paradigm based on
soft inference to augment the identification of relations with a short-distance
proximity band, which supplements the probabilistic information generated by
LLMs into the multi-head attention mechanism. Since the global feature can be
captured effectively, our framework substantially enhances the performance of
minority relation classes and improves the overall learning ability.
Experiments on two publicly available datasets, TB-Dense and MATRES,
demonstrate that our approach achieves state-of-the-art (SOTA) performance.

</details>


### [38] [MSRS: Evaluating Multi-Source Retrieval-Augmented Generation](https://arxiv.org/abs/2508.20867)
*Rohan Phanse,Yijie Zhou,Kejian Shi,Wencai Zhang,Yixin Liu,Yilun Zhao,Arman Cohan*

Main category: cs.CL

TL;DR: 这篇论文提出了一个可扩展的评测框架，用于构建多源检索和综合的RAG系统评测基准，解决实际应用中需要从多个来源整合信息的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统评测通常假设信息可从单个来源获得或答案短小，而实际应用需要从多个分散来源整合和汇总信息。

Method: 设计了一个可扩展的评测框架，并构建了两个新的多源检索和综合基准：MSRS-Story（故事综合）和MSRS-Meet（会议摘要），这些任务需要从大规模集合中检索信息。

Result: 实验结果显示，生成质量很大程度依赖于检索效果，而检索效果因任务而异。即使在理想检索条件下，多源综合仍然具有挑战性，但理解模型在这一步骤表现显著超过标准LLM。

Conclusion: 该研究为多源信息整合的RAG系统提供了重要的评测框架，并证明了检索效果对生成质量的关键影响，以及理解模型在多源综合任务中的优势。

Abstract: Retrieval-augmented systems are typically evaluated in settings where
information required to answer the query can be found within a single source or
the answer is short-form or factoid-based. However, many real-world
applications demand the ability to integrate and summarize information
scattered across multiple sources, where no single source is sufficient to
respond to the user's question. In such settings, the retrieval component of a
RAG pipeline must recognize a variety of relevance signals, and the generation
component must connect and synthesize information across multiple sources. We
present a scalable framework for constructing evaluation benchmarks that
challenge RAG systems to integrate information across distinct sources and
generate long-form responses. Using our framework, we build two new benchmarks
on Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representing
narrative synthesis and summarization tasks, respectively, that require
retrieval from large collections. Our extensive experiments with various RAG
pipelines -- including sparse and dense retrievers combined with frontier LLMs
-- reveal that generation quality is highly dependent on retrieval
effectiveness, which varies greatly by task. While multi-source synthesis
proves challenging even in an oracle retrieval setting, we find that reasoning
models significantly outperform standard LLMs at this distinct step.

</details>


### [39] [The Uneven Impact of Post-Training Quantization in Machine Translation](https://arxiv.org/abs/2508.20893)
*Benjamin Marie,Atsushi Fujita*

Main category: cs.CL

TL;DR: 大规模评估显示，4位量化对高资源语言和大模型翻译质量影响较小，但2位量化对低资源和类型多样语言造成显著质量下降，GGUF方法在低比特场景下表现最稳定


<details>
  <summary>Details</summary>
Motivation: 量化技术对多语言任务的影响尚未充分研究，特别是在机器翻译领域需要系统评估不同量化方法对多种语言的影响

Method: 使用5个参数量从17亿到700亿的大语言模型，在55种语言上进行后训练量化评估，比较了AWQ、BitsAndBytes、GGUF和AutoRound四种量化技术

Result: 4位量化通常能保持高资源语言和大模型的翻译质量，但低资源语言和2位量化设置下出现显著质量下降；GGUF变体在2位精度下提供最一致的性能；语言匹配的校准主要在低比特场景下有益

Conclusion: 研究为在多语言机器翻译部署中应用量化技术提供了实用指导，特别是在低资源环境下，算法选择和模型大小共同决定了量化鲁棒性

Abstract: Quantization is essential for deploying large language models (LLMs) on
resource-constrained hardware, but its implications for multilingual tasks
remain underexplored. We conduct the first large-scale evaluation of
post-training quantization (PTQ) on machine translation across 55 languages
using five LLMs ranging from 1.7B to 70B parameters. Our analysis reveals that
while 4-bit quantization often preserves translation quality for high-resource
languages and large models, significant degradation occurs for low-resource and
typologically diverse languages, particularly in 2-bit settings. We compare
four quantization techniques (AWQ, BitsAndBytes, GGUF, and AutoRound), showing
that algorithm choice and model size jointly determine robustness. GGUF
variants provide the most consistent performance, even at 2-bit precision.
Additionally, we quantify the interactions between quantization, decoding
hyperparameters, and calibration languages, finding that language-matched
calibration offers benefits primarily in low-bit scenarios. Our findings offer
actionable insights for deploying multilingual LLMs for machine translation
under quantization constraints, especially in low-resource settings.

</details>


### [40] [SageLM: A Multi-aspect and Explainable Large Language Model for Speech Judgement](https://arxiv.org/abs/2508.20916)
*Yuan Ge,Junxiang Zhang,Xiaoqian Liu,Bei Li,Xiangnan Ma,Chenglong Wang,Kaiyang Ye,Yangfan Du,Linfeng Zhang,Yuxin Huang,Tong Xiao,Zhengtao Yu,JingBo Zhu*

Main category: cs.CL

TL;DR: SageLM是一个端到端、多维度、可解释的语音大语言模型，用于全面评估语音到语音的LLM系统，在语义和声学维度上联合评估，与人类评估者的一致性达到82.79%。


<details>
  <summary>Details</summary>
Motivation: 当前语音到语音大语言模型的评估存在根本性挑战，传统级联方法忽视声学特征，缺乏端到端的综合评估方法，且语音偏好数据稀缺。

Method: 提出SageLM模型，采用基于原理的监督增强可解释性，引入SpeechFeedback合成偏好数据集，使用两阶段训练范式，在语义和声学维度上联合训练。

Result: SageLM与人类评估者的一致性达到82.79%，相比级联方法和SLM基线分别提升至少7.42%和26.20%。

Conclusion: SageLM提供了一个有效的端到端语音LLM评估框架，通过多维度联合评估和可解释性监督，显著提升了评估性能。

Abstract: Speech-to-Speech (S2S) Large Language Models (LLMs) are foundational to
natural human-computer interaction, enabling end-to-end spoken dialogue
systems. However, evaluating these models remains a fundamental challenge. We
propose \texttt{SageLM}, an end-to-end, multi-aspect, and explainable speech
LLM for comprehensive S2S LLMs evaluation. First, unlike cascaded approaches
that disregard acoustic features, SageLM jointly assesses both semantic and
acoustic dimensions. Second, it leverages rationale-based supervision to
enhance explainability and guide model learning, achieving superior alignment
with evaluation outcomes compared to rule-based reinforcement learning methods.
Third, we introduce \textit{SpeechFeedback}, a synthetic preference dataset,
and employ a two-stage training paradigm to mitigate the scarcity of speech
preference data. Trained on both semantic and acoustic dimensions, SageLM
achieves an 82.79\% agreement rate with human evaluators, outperforming
cascaded and SLM-based baselines by at least 7.42\% and 26.20\%, respectively.

</details>


### [41] [How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on $τ$-bench](https://arxiv.org/abs/2508.20931)
*Venkatesh Mishra,Amir Saeidi,Satyam Raj,Mutsumi Nakamura,Jayanth Srinivasa,Gaowen Liu,Ali Payani,Chitta Baral*

Main category: cs.CL

TL;DR: IRMA框架通过自动重构用户查询并添加领域规则和工具建议，显著提升LLM在多轮对话环境中的工具使用性能，在τ-bench基准上比其他方法提升12.7-19.1%


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多轮对话环境中存在推理不一致、违反领域策略和长序列信息提取困难等问题，需要改进工具调用代理的决策能力

Method: 提出IRMA多智能体框架，自动重构用户查询并添加相关领域规则和工具建议，帮助工具调用代理更好地聚焦任务

Result: IRMA在整体通过率上显著优于ReAct、函数调用和自我反思方法，分别提升16.1%、12.7%和19.1%

Conclusion: IRMA框架在动态环境中展现出比其他方法更优越的可靠性和一致性，有效解决了LLM在多轮工具调用对话中的挑战

Abstract: Recent advances in reasoning and planning capabilities of large language
models (LLMs) have enabled their potential as autonomous agents capable of tool
use in dynamic environments. However, in multi-turn conversational environments
like $\tau$-bench, these agents often struggle with consistent reasoning,
adherence to domain-specific policies, and extracting correct information over
a long horizon of tool-calls and conversation. To capture and mitigate these
failures, we conduct a comprehensive manual analysis of the common errors
occurring in the conversation trajectories. We then experiment with
reformulations of inputs to the tool-calling agent for improvement in agent
decision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)
framework, which automatically reformulates user queries augmented with
relevant domain rules and tool suggestions for the tool-calling agent to focus
on. The results show that IRMA significantly outperforms ReAct, Function
Calling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in
overall pass^5 scores. These findings highlight the superior reliability and
consistency of IRMA compared to other methods in dynamic environments.

</details>


### [42] [STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment](https://arxiv.org/abs/2508.20944)
*Jiaqian Li,Qisheng Hu,Jing Li,Wenya Wang*

Main category: cs.CL

TL;DR: 这篇论文提出了一种两阶段的示例选择策略，通过结构意识的监督和插件模块提升了语义解析任务中上下文学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的ICL示例选择策略在结构化预测任务中忽视了结构对齐问题，导致性能不佳和泛化能力差。

Method: 第一阶段使用结构意识监督细调BERT检索器，第二阶段添加插件模块增强隐藏表征中的语法意义信息。

Result: 在四个语义解析任务的基准测试中，方法持续超越现有基线，适用于多个最新LLM推理模型。

Conclusion: 该方法实现了效率、泛化能力和性能之间的良好平衡，能无缝集成到现有流程中。

Abstract: In-Context Learning (ICL) has become a powerful paradigm that enables LLMs to
perform a wide range of tasks without task-specific fine-tuning. However, the
effectiveness of ICL heavily depends on the quality of exemplar selection. In
particular, for structured prediction tasks such as semantic parsing, existing
ICL selection strategies often overlook structural alignment, leading to
suboptimal performance and poor generalization. To address this issue, we
propose a novel two-stage exemplar selection strategy that achieves a strong
balance between efficiency, generalizability, and performance. First, we
fine-tune a BERT-based retriever using structure-aware supervision, guiding it
to select exemplars that are both semantically relevant and structurally
aligned. Then, we enhance the retriever with a plug-in module, which amplifies
syntactically meaningful information in the hidden representations. This
plug-in is model-agnostic, requires minimal overhead, and can be seamlessly
integrated into existing pipelines. Experiments on four benchmarks spanning
three semantic parsing tasks demonstrate that our method consistently
outperforms existing baselines with multiple recent LLMs as inference-time
models.

</details>


### [43] [ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents](https://arxiv.org/abs/2508.20973)
*Tianjian Liu,Fanqi Wan,Jiajian Guo,Xiaojun Quan*

Main category: cs.CL

TL;DR: 提出了ProactiveEval统一评估框架，用于评估大语言模型的主动对话能力，包含目标规划和对话引导两个维度，并在6个领域构建了328个评估环境。


<details>
  <summary>Details</summary>
Motivation: 现有主动对话研究主要集中在特定领域或任务导向场景，导致评估碎片化，限制了全面探索模型的主动对话能力。

Method: 提出ProactiveEval框架，将主动对话分解为目标规划和对话引导，建立跨领域评估指标，并支持自动生成多样化评估数据。

Result: 在22种不同类型的大语言模型实验中，DeepSeek-R1在目标规划任务上表现优异，Claude-3.7-Sonnet在对话引导任务上表现最佳。

Conclusion: 研究了推理能力对主动行为的影响，并讨论了其对未来模型发展的启示。

Abstract: Proactive dialogue has emerged as a critical and challenging research problem
in advancing large language models (LLMs). Existing works predominantly focus
on domain-specific or task-oriented scenarios, which leads to fragmented
evaluations and limits the comprehensive exploration of models' proactive
conversation abilities. In this work, we propose ProactiveEval, a unified
framework designed for evaluating proactive dialogue capabilities of LLMs. This
framework decomposes proactive dialogue into target planning and dialogue
guidance, establishing evaluation metrics across various domains. Moreover, it
also enables the automatic generation of diverse and challenging evaluation
data. Based on the proposed framework, we develop 328 evaluation environments
spanning 6 distinct domains. Through experiments with 22 different types of
LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional
performance on target planning and dialogue guidance tasks, respectively.
Finally, we investigate how reasoning capabilities influence proactive
behaviors and discuss their implications for future model development.

</details>


### [44] [Lethe: Purifying Backdoored Large Language Models with Knowledge Dilution](https://arxiv.org/abs/2508.21004)
*Chen Chen,Yuchen Sun,Jiaxin Gao,Xueluan Gong,Qian Wang,Ziyao Wang,Yongsen Zheng,Kwok-Yan Lam*

Main category: cs.CL

TL;DR: LETHE是一种通过知识稀释消除LLM后门行为的新方法，结合内部模型合并和外部提示增强，在多个LLM上有效防御8种后门攻击，攻击成功率降低达98%


<details>
  <summary>Details</summary>
Motivation: 现有后门防御方法存在局限性，无法应对模型编辑、多触发器和无触发器等高级攻击场景，需要更全面的防御方案

Method: 内部使用轻量数据集训练干净模型并与后门模型合并来稀释后门影响；外部在提示中添加良性相关证据来分散LLM对后门特征的注意力

Result: 在5个主流LLM的分类和生成任务上，LETHE优于8个SOTA防御基线，将高级后门攻击成功率降低高达98%，同时保持模型效用

Conclusion: LETHE是一种成本效益高且鲁棒的防御方法，能够有效消除LLM中的后门行为，抵御自适应后门攻击

Abstract: Large language models (LLMs) have seen significant advancements, achieving
superior performance in various Natural Language Processing (NLP) tasks.
However, they remain vulnerable to backdoor attacks, where models behave
normally for standard queries but generate harmful responses or unintended
output when specific triggers are activated. Existing backdoor defenses either
lack comprehensiveness, focusing on narrow trigger settings, detection-only
mechanisms, and limited domains, or fail to withstand advanced scenarios like
model-editing-based, multi-trigger, and triggerless attacks. In this paper, we
present LETHE, a novel method to eliminate backdoor behaviors from LLMs through
knowledge dilution using both internal and external mechanisms. Internally,
LETHE leverages a lightweight dataset to train a clean model, which is then
merged with the backdoored model to neutralize malicious behaviors by diluting
the backdoor impact within the model's parametric memory. Externally, LETHE
incorporates benign and semantically relevant evidence into the prompt to
distract LLM's attention from backdoor features. Experimental results on
classification and generation domains across 5 widely used LLMs demonstrate
that LETHE outperforms 8 state-of-the-art defense baselines against 8 backdoor
attacks. LETHE reduces the attack success rate of advanced backdoor attacks by
up to 98% while maintaining model utility. Furthermore, LETHE has proven to be
cost-efficient and robust against adaptive backdoor attacks.

</details>


### [45] [An Agile Method for Implementing Retrieval Augmented Generation Tools in Industrial SMEs](https://arxiv.org/abs/2508.21024)
*Mathieu Bourdin,Anas Neumann,Thomas Paviot,Robert Pellerin,Samir Lamouri*

Main category: cs.CL

TL;DR: EASI-RAG是一个为中小企业设计的结构化敏捷方法，旨在简化RAG系统在工业环境中的部署，通过案例验证显示能够快速实施并获得高用户采纳率。


<details>
  <summary>Details</summary>
Motivation: 解决中小企业在部署检索增强生成(RAG)系统时面临的资源有限和NLP专业知识缺乏的问题，帮助它们克服大语言模型的幻觉和知识过时等局限性。

Method: 基于方法工程原则，设计了包含明确角色、活动和技术的EASI-RAG方法，通过在环境测试实验室的实际案例研究进行验证，团队在没有RAG经验的情况下一个月内完成部署。

Result: 系统实现了快速实施、高用户采纳率，提供准确答案并增强了底层数据的可靠性，证明了RAG在工业中小企业中的部署潜力。

Conclusion: EASI-RAG方法有效支持中小企业的RAG部署，未来工作需要扩展到更多用例并与微调模型进一步集成。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful solution to
mitigate the limitations of Large Language Models (LLMs), such as
hallucinations and outdated knowledge. However, deploying RAG-based tools in
Small and Medium Enterprises (SMEs) remains a challenge due to their limited
resources and lack of expertise in natural language processing (NLP). This
paper introduces EASI-RAG, Enterprise Application Support for Industrial RAG, a
structured, agile method designed to facilitate the deployment of RAG systems
in industrial SME contexts. EASI-RAG is based on method engineering principles
and comprises well-defined roles, activities, and techniques. The method was
validated through a real-world case study in an environmental testing
laboratory, where a RAG tool was implemented to answer operators queries using
data extracted from operational procedures. The system was deployed in under a
month by a team with no prior RAG experience and was later iteratively improved
based on user feedback. Results demonstrate that EASI-RAG supports fast
implementation, high user adoption, delivers accurate answers, and enhances the
reliability of underlying data. This work highlights the potential of RAG
deployment in industrial SMEs. Future works include the need for generalization
across diverse use cases and further integration with fine-tuned models.

</details>


### [46] [Re-Representation in Sentential Relation Extraction with Sequence Routing Algorithm](https://arxiv.org/abs/2508.21049)
*Ramazan Ali Bahrami,Ramin Yahyapour*

Main category: cs.CL

TL;DR: 本文提出使用胶囊网络中的动态路由进行句子级关系抽取，在多个数据集上表现优于现有方法，但发现在大规模数据集Wikidata上性能较低，主要原因是标签噪声和再表示能力不足。


<details>
  <summary>Details</summary>
Motivation: 句子级关系抽取是NLP中的重要任务，现有方法在处理大规模噪声数据时存在性能下降问题，需要探索更鲁棒的方法。

Method: 采用胶囊网络中的动态路由机制进行句子级关系抽取，通过胶囊间的动态信息传递来提升关系表示能力。

Result: 在Tacred、Tacredrev、Retacred和Conll04数据集上达到state-of-the-art性能，但在Wikidata数据集上表现较差，发现标签噪声是主要原因。

Conclusion: 除了标签噪声问题外，再表示能力（re-representation）是句子级关系抽取的一个重要挑战，提出的胶囊网络方法在再表示方面表现优于传统方法。

Abstract: Sentential relation extraction (RE) is an important task in natural language
processing (NLP). In this paper we propose to do sentential RE with dynamic
routing in capsules. We first show that the proposed approach outperform state
of the art on common sentential relation extraction datasets Tacred, Tacredrev,
Retacred, and Conll04. We then investigate potential reasons for its good
performance on the mentioned datasets, and yet low performance on another
similar, yet larger sentential RE dataset, Wikidata. As such, we identify noise
in Wikidata labels as one of the reasons that can hinder performance.
Additionally, we show associativity of better performance with better
re-representation, a term from neuroscience referred to change of
representation in human brain to improve the match at comparison time. As
example, in the given analogous terms King:Queen::Man:Woman, at comparison
time, and as a result of re-representation, the similarity between related head
terms (King,Man), and tail terms (Queen,Woman) increases. As such, our
observation show that our proposed model can do re-representation better than
the vanilla model compared with. To that end, beside noise in the labels of the
distantly supervised RE datasets, we propose re-representation as a challenge
in sentential RE.

</details>


### [47] [Enabling Equitable Access to Trustworthy Financial Reasoning](https://arxiv.org/abs/2508.21051)
*William Jurayj,Nils Holzenberger,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 结合大型语言模型与符号求解器的神经符号方法，用于自动计算税务义务，在SARA数据集上表现优异且部署成本低于现实平均水平


<details>
  <summary>Details</summary>
Motivation: 税务申报需要复杂推理和精确计算，传统LLMs无法满足高准确性和可审计性要求，错误会导致高额罚款

Method: 将LLMs与符号求解器集成，将文本规则转换为形式逻辑程序，结合智能检索的形式案例表示示例

Result: 在SARA数据集上显著提升性能，部署成本远低于现实世界平均水平

Conclusion: 神经符号架构有望实现可靠税务援助的公平获取，具有经济可行性

Abstract: According to the United States Internal Revenue Service, ''the average
American spends $\$270$ and 13 hours filing their taxes''. Even beyond the
U.S., tax filing requires complex reasoning, combining application of
overlapping rules with numerical calculations. Because errors can incur costly
penalties, any automated system must deliver high accuracy and auditability,
making modern large language models (LLMs) poorly suited for this task. We
propose an approach that integrates LLMs with a symbolic solver to calculate
tax obligations. We evaluate variants of this system on the challenging
StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for
estimating the cost of deploying such a system based on real-world penalties
for tax errors. We further show how combining up-front translation of
plain-text rules into formal logic programs, combined with intelligently
retrieved exemplars for formal case representations, can dramatically improve
performance on this task and reduce costs to well below real-world averages.
Our results demonstrate the promise and economic feasibility of neuro-symbolic
architectures for increasing equitable access to reliable tax assistance.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [48] [MM-HSD: Multi-Modal Hate Speech Detection in Videos](https://arxiv.org/abs/2508.20546)
*Berta Céspedes-Sarrias,Carlos Collado-Capell,Pablo Rodenas-Ruiz,Olena Hrynenko,Andrea Cavallaro*

Main category: cs.MM

TL;DR: MM-HSD是一个用于视频仇恨言论检测的多模态模型，整合了视频帧、音频、语音转录文本和屏幕文本，采用跨模态注意力机制，在HateMM数据集上取得了0.874的M-F1分数，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频仇恨言论检测方法存在局限性，简单融合方法无法充分捕捉模态间依赖关系，且往往忽略屏幕文本和音频等关键模态，这些模态可能包含细微的仇恨内容并提供重要线索。

Method: 提出MM-HSD模型，整合视频帧、音频、语音转录文本和屏幕文本，使用跨模态注意力(CMA)作为早期特征提取器，系统比较查询/键配置，评估不同模态在CMA块中的交互作用。

Result: 在HateMM数据集上，当使用屏幕文本作为查询、其他模态作为键时性能最佳，M-F1分数达到0.874，超越了现有最先进方法。

Conclusion: 跨模态注意力机制在视频仇恨言论检测中有效，屏幕文本作为查询模态特别重要，多模态融合能够显著提升检测性能。

Abstract: While hate speech detection (HSD) has been extensively studied in text,
existing multi-modal approaches remain limited, particularly in videos. As
modalities are not always individually informative, simple fusion methods fail
to fully capture inter-modal dependencies. Moreover, previous work often omits
relevant modalities such as on-screen text and audio, which may contain subtle
hateful content and thus provide essential cues, both individually and in
combination with others. In this paper, we present MM-HSD, a multi-modal model
for HSD in videos that integrates video frames, audio, and text derived from
speech transcripts and from frames (i.e.~on-screen text) together with features
extracted by Cross-Modal Attention (CMA). We are the first to use CMA as an
early feature extractor for HSD in videos, to systematically compare query/key
configurations, and to evaluate the interactions between different modalities
in the CMA block. Our approach leads to improved performance when on-screen
text is used as a query and the rest of the modalities serve as a key.
Experiments on the HateMM dataset show that MM-HSD outperforms state-of-the-art
methods on M-F1 score (0.874), using concatenation of transcript, audio, video,
on-screen text, and CMA for feature extraction on raw embeddings of the
modalities. The code is available at https://github.com/idiap/mm-hsd

</details>


### [49] [diveXplore at the Video Browser Showdown 2024](https://arxiv.org/abs/2508.20560)
*Klaus Schoeffmann,Sahar Nasirihaghighi*

Main category: cs.MM

TL;DR: diveXplore系统在VBS2024中进行了重大修订，集成了基于LAION-2B训练的OpenCLIP进行图像/文本嵌入搜索，优化了查询服务器和用户界面，并新增了视频聚类探索功能


<details>
  <summary>Details</summary>
Motivation: 基于VBS2023经验和CBMI2023会议反馈，需要改进视频检索系统的性能和用户体验

Method: 集成OpenCLIP模型进行多模态嵌入，构建分布式查询服务器，优化用户界面，添加视频聚类探索视图

Result: 系统支持自由文本和视觉相似性搜索，能够高效处理不同类型查询并合并结果，提供快速浏览和大规模视频聚类探索能力

Conclusion: 修订后的diveXplore系统在视频检索性能和用户体验方面得到显著提升，特别适合处理婚礼、滑翔伞活动等特定场景的大规模视频聚类

Abstract: According to our experience from VBS2023 and the feedback from the IVR4B
special session at CBMI2023, we have largely revised the diveXplore system for
VBS2024. It now integrates OpenCLIP trained on the LAION-2B dataset for
image/text embeddings that are used for free-text and visual similarity search,
a query server that is able to distribute different queries and merge the
results, a user interface optimized for fast browsing, as well as an
exploration view for large clusters of similar videos (e.g., weddings,
paraglider events, snow and ice scenery, etc.).

</details>


### [50] [Less is More - diveXplore 5.0 at VBS 2021](https://arxiv.org/abs/2508.20569)
*Andreas Leibetseder,Klaus Schoeffmann*

Main category: cs.MM

TL;DR: diveXplore系统在VBS和LSC竞赛中表现先成功后下降，原因是新增功能增加了系统复杂性。VBS 2021版本5.0进行了彻底重构，采用模块化设计降低复杂性并保留有用功能。


<details>
  <summary>Details</summary>
Motivation: 解决diveXplore系统因功能增加导致复杂性上升和性能下降的问题，通过重构实现更好的功能扩展性和系统性能。

Method: 完全重新构建系统版本5.0，采用模块化设计方法，从零开始实现，减少系统复杂性同时保留经过验证的有用功能。

Result: 开发了新的VBS 2021版本5.0，系统复杂性大幅降低，功能模块化设计改善了系统的可扩展性和维护性。

Conclusion: 通过彻底重构和模块化设计，成功解决了系统复杂性导致性能下降的问题，为未来的功能扩展提供了更好的基础。

Abstract: As a longstanding participating system in the annual Video Browser Showdown
(VBS2017-VBS2020) as well as in two iterations of the more recently established
Lifelog Search Challenge (LSC2018-LSC2019), diveXplore is developed as a
feature-rich Deep Interactive Video Exploration system. After its initial
successful employment as a competitive tool at the challenges, its performance,
however, declined as new features were introduced increasing its overall
complexity. We mainly attribute this to the fact that many additions to the
system needed to revolve around the system's core element - an interactive
self-organizing browseable featuremap, which, as an integral component did not
accommodate the addition of new features well. Therefore, counteracting said
performance decline, the VBS 2021 version constitutes a completely rebuilt
version 5.0, implemented from scratch with the aim of greatly reducing the
system's complexity as well as keeping proven useful features in a modular
manner.

</details>


### [51] [diveXplore 6.0: ITEC's Interactive Video Exploration System at VBS 2022](https://arxiv.org/abs/2508.20687)
*Andreas Leibetseder,Klaus Schoeffmann*

Main category: cs.MM

TL;DR: diveXplore系统从VBS2017开始参与视频检索竞赛，经过VBS2021重大重构后，版本5.0虽然功能减少但更现代、轻量和快速。版本6.0重新考虑镜头分割和地图搜索，并引入新功能来改进概念和时序上下文搜索。


<details>
  <summary>Details</summary>
Motivation: 通过系统重构和功能优化，提升视频检索系统的性能和效率，在Video Browser Showdown竞赛中取得更好的表现。

Method: 对系统进行重大重构，减少功能但使系统更现代、轻量和快速；重新设计镜头分割和地图搜索；引入新的概念搜索和时序上下文搜索功能。

Result: 重构后的系统在VBS2021中相比之前的竞赛表现出性能提升。

Conclusion: 系统重构和功能精简是明智的决定，新版系统在保持核心功能的同时提高了性能和效率，为后续版本的功能扩展奠定了基础。

Abstract: Continuously participating since the sixth Video Browser Showdown (VBS2017),
diveXplore is a veteran interactive search system that throughout its lifetime
has offered and evaluated numerous features. After undergoing major refactoring
for the most recent VBS2021, however, the system since version 5.0 is less
feature rich, yet, more modern, leaner and faster than the original system.
This proved to be a sensible decision as the new system showed increasing
performance in VBS2021 when compared to the most recent former competitions.
With version 6.0 we reconsider shot segmentation, map search and introduce new
features for improving concept as well as temporal context search.

</details>


### [52] [AdaDPCC: Adaptive Rate Control and Rate-Distortion-Complexity Optimization for Dynamic Point Cloud Compression](https://arxiv.org/abs/2508.20741)
*Chenhao Zhang,Wei Gao*

Main category: cs.MM

TL;DR: 这篇论文提出了一种新的动态点云压缩框架，通过可变码率和计算复杂度控制，在保持高效率的同时实现了更优的压缩性能。


<details>
  <summary>Details</summary>
Motivation: 当前的动态点云压缩方法在复杂性管理和码率控制方面遇到挑战，特别是在自动驾驶和AR/VR等实时应用中。

Method: 提出了一种包含多重编码路径的可缩减框架，支持变码率和计算复杂度。包括粗细粒度运动估计和补偿模块，以及内容自适应的精确码率控制模块。

Result: 实验结果显示，该方法平均BD-Rate降低5.81%，BD-PSNR提高0.42 dB，平均码率误差仅0.40%，编码时间减少44.6%。

Conclusion: 该方法在实时性和码率约束的动态点云压缩场景中表现出艺出的效率和性能，为相关应用提供了有效的解决方案。

Abstract: Dynamic point cloud compression (DPCC) is crucial in applications like
autonomous driving and AR/VR. Current compression methods face challenges with
complexity management and rate control. This paper introduces a novel dynamic
coding framework that supports variable bitrate and computational complexities.
Our approach includes a slimmable framework with multiple coding routes,
allowing for efficient Rate-Distortion-Complexity Optimization (RDCO) within a
single model. To address data sparsity in inter-frame prediction, we propose
the coarse-to-fine motion estimation and compensation module that deconstructs
geometric information while expanding the perceptive field. Additionally, we
propose a precise rate control module that content-adaptively navigates point
cloud frames through various coding routes to meet target bitrates. The
experimental results demonstrate that our approach reduces the average BD-Rate
by 5.81% and improves the BD-PSNR by 0.42 dB compared to the state-of-the-art
method, while keeping the average bitrate error at 0.40%. Moreover, the average
coding time is reduced by up to 44.6% compared to D-DPCC, underscoring its
efficiency in real-time and bitrate-constrained DPCC scenarios. Our code is
available at https://git.openi.org.cn/OpenPointCloud/Ada_DPCC.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [53] [MoTAS: MoE-Guided Feature Selection from TTS-Augmented Speech for Enhanced Multimodal Alzheimer's Early Screening](https://arxiv.org/abs/2508.20513)
*Yongqi Shao,Binxin Mei,Cong Tan,Hong Huo,Tao Fang*

Main category: cs.SD

TL;DR: MoTAS框架通过TTS数据增强和MoE特征选择机制，在ADReSSo数据集上实现了85.71%的阿尔茨海默病筛查准确率，优于现有基线方法


<details>
  <summary>Details</summary>
Motivation: 解决阿尔茨海默病早期筛查中数据有限和缺乏细粒度自适应特征选择的问题

Method: 使用ASR获取准确转录，TTS合成语音增强数据集，MoE机制动态选择最有信息量的声学和文本特征进行优化融合

Result: 在ADReSSo数据集上达到85.71%的准确率，超越现有基线方法

Conclusion: MoTAS在数据有限的实际AD筛查场景中具有重要实用价值，TTS增强和MoE机制均对性能提升有显著贡献

Abstract: Early screening for Alzheimer's Disease (AD) through speech presents a
promising non-invasive approach. However, challenges such as limited data and
the lack of fine-grained, adaptive feature selection often hinder performance.
To address these issues, we propose MoTAS, a robust framework designed to
enhance AD screening efficiency. MoTAS leverages Text-to-Speech (TTS)
augmentation to increase data volume and employs a Mixture of Experts (MoE)
mechanism to improve multimodal feature selection, jointly enhancing model
generalization. The process begins with automatic speech recognition (ASR) to
obtain accurate transcriptions. TTS is then used to synthesize speech that
enriches the dataset. After extracting acoustic and text embeddings, the MoE
mechanism dynamically selects the most informative features, optimizing feature
fusion for improved classification. Evaluated on the ADReSSo dataset, MoTAS
achieves a leading accuracy of 85.71\%, outperforming existing baselines.
Ablation studies further validate the individual contributions of TTS
augmentation and MoE in boosting classification performance. These findings
highlight the practical value of MoTAS in real-world AD screening scenarios,
particularly in data-limited settings.

</details>


### [54] [Flowing Straighter with Conditional Flow Matching for Accurate Speech Enhancement](https://arxiv.org/abs/2508.20584)
*Mattias Cross,Anton Ragni*

Main category: cs.SD

TL;DR: 本文研究概率路径的直线性对语音增强质量的影响，比较了Schrodinger桥（弯曲路径）和条件流匹配（直线路径）方法，发现直线路径在语音质量指标上表现更好。


<details>
  <summary>Details</summary>
Motivation: 当前基于流的生成式语音增强方法学习弯曲的概率路径来建模干净和噪声语音之间的映射，但弯曲路径的影响未知。机器学习研究表明直线路径更容易训练且泛化能力更好。

Method: 通过Schrodinger桥实验研究不同配置对路径直线性的影响，并提出独立条件流匹配方法用于语音增强，该方法在噪声和干净语音之间建模直线路径。

Result: 实验表明时间无关方差比梯度对样本质量影响更大。条件流匹配改善了多个语音质量指标，但需要多步推理。通过将训练好的流模型直接用于预测，实现了一步推理解决方案。

Conclusion: 直线的时间无关概率路径比弯曲的时间相关路径能更好地改善生成式语音增强性能。

Abstract: Current flow-based generative speech enhancement methods learn curved
probability paths which model a mapping between clean and noisy speech. Despite
impressive performance, the implications of curved probability paths are
unknown. Methods such as Schrodinger bridges focus on curved paths, where
time-dependent gradients and variance do not promote straight paths. Findings
in machine learning research suggest that straight paths, such as conditional
flow matching, are easier to train and offer better generalisation. In this
paper we quantify the effect of path straightness on speech enhancement
quality. We report experiments with the Schrodinger bridge, where we show that
certain configurations lead to straighter paths. Conversely, we propose
independent conditional flow-matching for speech enhancement, which models
straight paths between noisy and clean speech. We demonstrate empirically that
a time-independent variance has a greater effect on sample quality than the
gradient. Although conditional flow matching improves several speech quality
metrics, it requires multiple inference steps. We rectify this with a one-step
solution by inferring the trained flow-based model as if it was directly
predictive. Our work suggests that straighter time-independent probability
paths improve generative speech enhancement over curved time-dependent paths.

</details>


### [55] [Amadeus: Autoregressive Model with Bidirectional Attribute Modelling for Symbolic Music](https://arxiv.org/abs/2508.20665)
*Hongju Su,Ke Li,Lan Yang,Honggang Zhang,Yi-Zhe Song*

Main category: cs.SD

TL;DR: Amadeus是一个创新的符号音乐生成框架，采用自回归音符序列和双向离散扩散属性模型的两层架构，通过对比学习和注意力机制增强表示能力，在多个指标上显著超越现有SOTA模型并实现4倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有符号音乐生成模型假设音符属性存在固定严格的时间依赖关系，但研究发现不同属性作为初始token都能获得相似性能，表明音符属性本质上是并发无序的集合而非时间依赖序列。

Method: 提出Amadeus框架：1）自回归模型处理音符序列；2）双向离散扩散模型处理属性；3）MLSDES策略通过对比学习增强中间表示判别性；4）CIEM模块通过注意力机制强化音符潜在向量表示。

Result: 在无条件和文本条件生成任务中，Amadeus在多个指标上显著优于SOTA模型，同时实现至少4倍速度提升，并展示无需训练即可实现细粒度音符属性控制的可行性。

Conclusion: Amadeus框架通过重新思考音符属性的本质依赖关系，采用创新的两层架构和增强策略，在性能和效率上都取得了显著突破，为符号音乐生成提供了新的研究方向。

Abstract: Existing state-of-the-art symbolic music generation models predominantly
adopt autoregressive or hierarchical autoregressive architectures, modelling
symbolic music as a sequence of attribute tokens with unidirectional temporal
dependencies, under the assumption of a fixed, strict dependency structure
among these attributes. However, we observe that using different attributes as
the initial token in these models leads to comparable performance. This
suggests that the attributes of a musical note are, in essence, a concurrent
and unordered set, rather than a temporally dependent sequence. Based on this
insight, we introduce Amadeus, a novel symbolic music generation framework.
Amadeus adopts a two-level architecture: an autoregressive model for note
sequences and a bidirectional discrete diffusion model for attributes. To
enhance performance, we propose Music Latent Space Discriminability Enhancement
Strategy(MLSDES), incorporating contrastive learning constraints that amplify
discriminability of intermediate music representations. The Conditional
Information Enhancement Module (CIEM) simultaneously strengthens note latent
vector representation via attention mechanisms, enabling more precise note
decoding. We conduct extensive experiments on unconditional and
text-conditioned generation tasks. Amadeus significantly outperforms SOTA
models across multiple metrics while achieving at least 4$\times$ speed-up.
Furthermore, we demonstrate training-free, fine-grained note attribute control
feasibility using our model. To explore the upper performance bound of the
Amadeus architecture, we compile the largest open-source symbolic music dataset
to date, AMD (Amadeus MIDI Dataset), supporting both pre-training and
fine-tuning.

</details>


### [56] [Unified Multi-task Learning for Voice-Based Detection of Diverse Clinical Conditions](https://arxiv.org/abs/2508.20717)
*Ran Piao,Yuan Lu,Hareld Kemps,Tong Xia,Aaqib Saeed*

Main category: cs.SD

TL;DR: MARVEL是一个多任务学习框架，通过语音分析同时检测9种神经、呼吸和声音疾病，使用派生声学特征保护隐私，在Bridge2AI-Voice数据集上表现优异，AUROC达到0.78，特别在阿尔茨海默病检测上达到0.97。


<details>
  <summary>Details</summary>
Motivation: 现有语音健康评估方法通常只关注单一疾病，未能充分利用语音中丰富的多层面信息，需要开发能够同时检测多种疾病且保护隐私的统一框架。

Method: 采用双分支架构，使用专门编码器和任务特定头部共享共同声学骨干网络，实现跨条件知识转移，仅使用派生声学特征避免原始音频传输。

Result: 在9个任务中的7个上超越最先进的自监督模型，整体AUROC为0.78，神经疾病检测AUROC达0.89，阿尔茨海默病检测AUROC达0.97，比单模态基线提升5-19%。

Conclusion: 该工作证明单一统一模型可有效筛查多种疾病，为资源受限和远程医疗环境中的可部署语音诊断奠定了基础，学习到的表征与临床认可的声学模式一致。

Abstract: Voice-based health assessment offers unprecedented opportunities for
scalable, non-invasive disease screening, yet existing approaches typically
focus on single conditions and fail to leverage the rich, multi-faceted
information embedded in speech. We present MARVEL (Multi-task Acoustic
Representations for Voice-based Health Analysis), a privacy-conscious multitask
learning framework that simultaneously detects nine distinct neurological,
respiratory, and voice disorders using only derived acoustic features,
eliminating the need for raw audio transmission. Our dual-branch architecture
employs specialized encoders with task-specific heads sharing a common acoustic
backbone, enabling effective cross-condition knowledge transfer. Evaluated on
the large-scale Bridge2AI-Voice v2.0 dataset, MARVEL achieves an overall AUROC
of 0.78, with exceptional performance on neurological disorders (AUROC = 0.89),
particularly for Alzheimer's disease/mild cognitive impairment (AUROC = 0.97).
Our framework consistently outperforms single-modal baselines by 5-19% and
surpasses state-of-the-art self-supervised models on 7 of 9 tasks, while
correlation analysis reveals that the learned representations exhibit
meaningful similarities with established acoustic features, indicating that the
model's internal representations are consistent with clinically recognized
acoustic patterns. By demonstrating that a single unified model can effectively
screen for diverse conditions, this work establishes a foundation for
deployable voice-based diagnostics in resource-constrained and remote
healthcare settings.

</details>


### [57] [Speech Emotion Recognition via Entropy-Aware Score Selection](https://arxiv.org/abs/2508.20796)
*ChenYi Chua,JunKai Wong,Chengxin Chen,Xiaoxiao Miao*

Main category: cs.SD

TL;DR: 提出了一种基于熵感知分数选择的多模态语音情感识别框架，结合语音和文本预测，通过后期分数融合和情感映射策略提升性能


<details>
  <summary>Details</summary>
Motivation: 传统单模态语音情感识别系统存在置信度限制，需要克服主要管道预测的置信约束，通过多模态融合提升识别可靠性

Method: 使用wav2vec2.0的声学模型和基于RoBERTa-XLM的情感分析模型，通过Whisper-large-v3生成转录，采用基于熵和变熵阈值的后期分数融合方法，以及三情感类别到四情感类别的映射策略

Result: 在IEMOCAP和MSP-IMPROV数据集上的实验结果表明，该方法相比传统单模态系统提供了实用且可靠的性能提升

Conclusion: 提出的多模态框架通过熵感知分数选择和情感映射，有效提升了语音情感识别的准确性和可靠性，为多模态情感识别提供了实用解决方案

Abstract: In this paper, we propose a multimodal framework for speech emotion
recognition that leverages entropy-aware score selection to combine speech and
textual predictions. The proposed method integrates a primary pipeline that
consists of an acoustic model based on wav2vec2.0 and a secondary pipeline that
consists of a sentiment analysis model using RoBERTa-XLM, with transcriptions
generated via Whisper-large-v3. We propose a late score fusion approach based
on entropy and varentropy thresholds to overcome the confidence constraints of
primary pipeline predictions. A sentiment mapping strategy translates three
sentiment categories into four target emotion classes, enabling coherent
integration of multimodal predictions. The results on the IEMOCAP and
MSP-IMPROV datasets show that the proposed method offers a practical and
reliable enhancement over traditional single-modality systems.

</details>


### [58] [OLMoASR: Open Models and Data for Training Robust Speech Recognition Models](https://arxiv.org/abs/2508.20869)
*Huong Ngo,Matt Deitke,Martijn Bartelds,Sarah Pratt,Josh Gardner,Matt Jordan,Ludwig Schmidt*

Main category: cs.SD

TL;DR: 这篇论文提出了OLMoASR-Pool大规模语音识别数据集和OLMoASR模型系列，通过质量筛选获得1M小时高质量语音数据，在各种模型规模下实现了与Whisper相当的零样本语音识别性能。


<details>
  <summary>Details</summary>
Motivation: 虽然训练数据的规模和质量提升带来了显著进步，但它在语音识别中的影响仍然很少被深入研究。需要研究和开发健壮的零样本语音识别模型。

Method: 从3M小时的OLMoASR-Pool数据集出发，设计文本含义筛选器去除低质量或误识别的数据，生成包含1M小时高质量音频-文本对的OLMoASR-Mix数据集。使用该数据集训练从39M到1.5B参数的OLMoASR模型系列。

Result: 在所有模型规模下，OLMoASR在短语音和长语音识别性能上与OpenAI的Whisper相当。OLMoASR-medium.en在短语音和长语音识别上分别达到12.8%和11.0%的词错误率，与相同参数规模的Whisper-medium.en的12.4%和10.5%相当。

Conclusion: 通过大规模高质量数据的筛选和训练，OLMoASR模型系列实现了与现有最佳模型相当的零样本语音识别性能，为健壮语音处理研究提供了重要资源。

Abstract: Improvements in training data scale and quality have led to significant
advances, yet its influence in speech recognition remains underexplored. In
this paper, we present a large-scale dataset, OLMoASR-Pool, and series of
models, OLMoASR, to study and develop robust zero-shot speech recognition
models. Beginning from OLMoASR-Pool, a collection of 3M hours of English audio
and 17M transcripts, we design text heuristic filters to remove low-quality or
mistranscribed data. Our curation pipeline produces a new dataset containing 1M
hours of high-quality audio-transcript pairs, which we call OLMoASR-Mix. We use
OLMoASR-Mix to train the OLMoASR-Mix suite of models, ranging from 39M
(tiny.en) to 1.5B (large.en) parameters. Across all model scales, OLMoASR
achieves comparable average performance to OpenAI's Whisper on short and
long-form speech recognition benchmarks. Notably, OLMoASR-medium.en attains a
12.8\% and 11.0\% word error rate (WER) that is on par with Whisper's largest
English-only model Whisper-medium.en's 12.4\% and 10.5\% WER for short and
long-form recognition respectively (at equivalent parameter count).
OLMoASR-Pool, OLMoASR models, and filtering, training and evaluation code will
be made publicly available to further research on robust speech processing.

</details>


### [59] [SincQDR-VAD: A Noise-Robust Voice Activity Detection Framework Leveraging Learnable Filters and Ranking-Aware Optimization](https://arxiv.org/abs/2508.20885)
*Chien-Chun Wang,En-Lun Yu,Jeih-Weih Hung,Shih-Chieh Huang,Berlin Chen*

Main category: cs.SD

TL;DR: SincQDR-VAD是一个紧凑且鲁棒的语音活动检测框架，结合了Sinc-extractor前端和二次差异排序损失，在噪声环境中显著提升性能且参数更少


<details>
  <summary>Details</summary>
Motivation: 现有VAD方法在噪声环境中缺乏鲁棒性，且帧级分类损失与VAD评估指标关联不紧密，需要更有效的解决方案

Method: 使用可学习带通滤波器的Sinc-extractor前端提取抗噪声频谱特征，结合新颖的二次差异排序损失优化语音和非语音帧的成对得分顺序

Result: 在代表性基准数据集上的实验表明，该框架显著提高了AUROC和F2-Score，同时仅使用先前方法69%的参数

Conclusion: SincQDR-VAD框架在效率和实用性方面表现出色，为噪声环境下的语音活动检测提供了有效的解决方案

Abstract: Voice activity detection (VAD) is essential for speech-driven applications,
but remains far from perfect in noisy and resource-limited environments.
Existing methods often lack robustness to noise, and their frame-wise
classification losses are only loosely coupled with the evaluation metric of
VAD. To address these challenges, we propose SincQDR-VAD, a compact and robust
framework that combines a Sinc-extractor front-end with a novel quadratic
disparity ranking loss. The Sinc-extractor uses learnable bandpass filters to
capture noise-resistant spectral features, while the ranking loss optimizes the
pairwise score order between speech and non-speech frames to improve the area
under the receiver operating characteristic curve (AUROC). A series of
experiments conducted on representative benchmark datasets show that our
framework considerably improves both AUROC and F2-Score, while using only 69%
of the parameters compared to prior arts, confirming its efficiency and
practical viability.

</details>


### [60] [Learning Robust Spatial Representations from Binaural Audio through Feature Distillation](https://arxiv.org/abs/2508.20914)
*Holger Severin Bovbjerg,Jan Østergaard,Jesper Jensen,Shinji Watanabe,Zheng-Hua Tan*

Main category: cs.SD

TL;DR: 提出基于特征蒸馏的预训练方法，从双耳语音中学习空间表示，无需数据标签，用于提升噪声和混响环境中的声源方向估计性能


<details>
  <summary>Details</summary>
Motivation: 深度表示学习在音频任务中表现优异，但多声道音频的空间表示学习研究不足，需要探索无监督学习方法

Method: 使用特征蒸馏预训练框架：从干净双耳语音计算空间特征作为预测标签，通过神经网络从增强语音预测这些特征，预训练后丢弃特征预测器，用编码器权重初始化DoA估计模型进行微调

Result: 实验表明，经过微调的预训练模型在噪声和混响环境中的方向估计性能优于全监督模型和传统信号处理方法

Conclusion: 基于特征蒸馏的无监督预训练方法能有效学习鲁棒的空间表示，提升声源方向估计在复杂声学环境中的性能

Abstract: Recently, deep representation learning has shown strong performance in
multiple audio tasks. However, its use for learning spatial representations
from multichannel audio is underexplored. We investigate the use of a
pretraining stage based on feature distillation to learn a robust spatial
representation of binaural speech without the need for data labels. In this
framework, spatial features are computed from clean binaural speech samples to
form prediction labels. These clean features are then predicted from
corresponding augmented speech using a neural network. After pretraining, we
throw away the spatial feature predictor and use the learned encoder weights to
initialize a DoA estimation model which we fine-tune for DoA estimation. Our
experiments demonstrate that the pretrained models show improved performance in
noisy and reverberant environments after fine-tuning for direction-of-arrival
estimation, when compared to fully supervised models and classic signal
processing methods.

</details>


### [61] [WoW-Bench: Evaluating Fine-Grained Acoustic Perception in Audio-Language Models via Marine Mammal Vocalizations](https://arxiv.org/abs/2508.20976)
*Jaeyeon Kim,Heeseung Yun,Sang Hoon Woo,Chao-Han Huck Yang,Gunhee Kim*

Main category: cs.SD

TL;DR: 提出了WoW-Bench基准测试，用于评估大型音频语言模型在低频听觉感知和认知方面的能力，特别是针对海洋哺乳动物叫声的识别和理解。实验显示当前模型性能远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 大型音频语言模型在低频听觉感知（如音高和时长检测）方面的能力尚未充分探索，而这对处理现实世界中的分布外任务至关重要，模型需要基于细粒度声学线索来推理不熟悉的声音。

Method: 引入World-of-Whale基准测试（WoW-Bench），包含感知基准（分类新声音）和认知基准（基于Bloom分类法评估记忆、理解、应用和分析声音事件的能力），并引入干扰问题来验证模型是否真正通过听觉解决问题。

Result: 对最先进的大型音频语言模型的实验显示，其性能远低于人类水平，表明这些模型在听觉基础方面存在不足。

Conclusion: 当前的大型音频语言模型在低频听觉感知和认知方面表现不佳，需要更强的听觉基础能力来提升处理现实世界音频任务的能力。

Abstract: Large audio language models (LALMs) extend language understanding into the
auditory domain, yet their ability to perform low-level listening, such as
pitch and duration detection, remains underexplored. However, low-level
listening is critical for real-world, out-of-distribution tasks where models
must reason about unfamiliar sounds based on fine-grained acoustic cues. To
address this gap, we introduce the World-of-Whale benchmark (WoW-Bench) to
evaluate low-level auditory perception and cognition using marine mammal
vocalizations. WoW-bench is composed of a Perception benchmark for categorizing
novel sounds and a Cognition benchmark, inspired by Bloom's taxonomy, to assess
the abilities to remember, understand, apply, and analyze sound events. For the
Cognition benchmark, we additionally introduce distractor questions to evaluate
whether models are truly solving problems through listening rather than relying
on other heuristics. Experiments with state-of-the-art LALMs show performance
far below human levels, indicating a need for stronger auditory grounding in
LALMs.

</details>
