{"id": "2508.19251", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.19251", "abs": "https://arxiv.org/abs/2508.19251", "authors": ["Qian Liang", "Menghaoran Tang", "Yi Zeng"], "title": "MuSpike: A Benchmark and Evaluation Framework for Symbolic Music Generation with Spiking Neural Networks", "comment": null, "summary": "Symbolic music generation has seen rapid progress with artificial neural\nnetworks, yet remains underexplored in the biologically plausible domain of\nspiking neural networks (SNNs), where both standardized benchmarks and\ncomprehensive evaluation methods are lacking. To address this gap, we introduce\nMuSpike, a unified benchmark and evaluation framework that systematically\nassesses five representative SNN architectures (SNN-CNN, SNN-RNN, SNN-LSTM,\nSNN-GAN and SNN-Transformer) across five typical datasets, covering tonal,\nstructural, emotional, and stylistic variations. MuSpike emphasizes\ncomprehensive evaluation, combining established objective metrics with a\nlarge-scale listening study. We propose new subjective metrics, targeting\nmusical impression, autobiographical association, and personal preference, that\ncapture perceptual dimensions often overlooked in prior work. Results reveal\nthat (1) different SNN models exhibit distinct strengths across evaluation\ndimensions; (2) participants with different musical backgrounds exhibit diverse\nperceptual patterns, with experts showing greater tolerance toward AI-composed\nmusic; and (3) a noticeable misalignment exists between objective and\nsubjective evaluations, highlighting the limitations of purely statistical\nmetrics and underscoring the value of human perceptual judgment in assessing\nmusical quality. MuSpike provides the first systematic benchmark and systemic\nevaluation framework for SNN models in symbolic music generation, establishing\na solid foundation for future research into biologically plausible and\ncognitively grounded music generation.", "AI": {"tldr": "MuSpike\u662f\u9996\u4e2a\u9488\u5bf9\u8109\u51b2\u795e\u7ecf\u7f51\u7edc(SNN)\u5728\u7b26\u53f7\u97f3\u4e50\u751f\u6210\u9886\u57df\u7684\u7edf\u4e00\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e865\u79cdSNN\u67b6\u6784\u57285\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u7ed3\u5408\u5ba2\u89c2\u6307\u6807\u548c\u5927\u89c4\u6a21\u542c\u611f\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u5ba2\u89c2\u4e0e\u4e3b\u89c2\u8bc4\u4f30\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "motivation": "\u7b26\u53f7\u97f3\u4e50\u751f\u6210\u5728\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u9886\u57df\u8fdb\u5c55\u8fc5\u901f\uff0c\u4f46\u5728\u751f\u7269\u53ef\u89e3\u91ca\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc(SNN)\u9886\u57df\u4ecd\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f15\u5165MuSpike\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f305\u79cd\u4ee3\u8868\u6027SNN\u67b6\u6784(SNN-CNN\u3001SNN-RNN\u3001SNN-LSTM\u3001SNN-GAN\u548cSNN-Transformer)\u57285\u4e2a\u5178\u578b\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u7ed3\u5408\u5ba2\u89c2\u6307\u6807\u548c\u4e3b\u89c2\u542c\u611f\u7814\u7a76\uff0c\u63d0\u51fa\u65b0\u7684\u4e3b\u89c2\u8bc4\u4f30\u6307\u6807\u3002", "result": "1)\u4e0d\u540cSNN\u6a21\u578b\u5728\u4e0d\u540c\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u8868\u73b0\u5404\u5f02\uff1b2)\u4e0d\u540c\u97f3\u4e50\u80cc\u666f\u7684\u53c2\u4e0e\u8005\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u611f\u77e5\u6a21\u5f0f\uff0c\u4e13\u5bb6\u5bf9AI\u521b\u4f5c\u97f3\u4e50\u66f4\u5bbd\u5bb9\uff1b3)\u5ba2\u89c2\u4e0e\u4e3b\u89c2\u8bc4\u4f30\u5b58\u5728\u660e\u663e\u4e0d\u4e00\u81f4\uff0c\u51f8\u663e\u7eaf\u7edf\u8ba1\u6307\u6807\u7684\u5c40\u9650\u6027\u3002", "conclusion": "MuSpike\u4e3aSNN\u6a21\u578b\u5728\u7b26\u53f7\u97f3\u4e50\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u6027\u57fa\u51c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u672a\u6765\u751f\u7269\u53ef\u89e3\u91ca\u548c\u8ba4\u77e5\u57fa\u7840\u7684\u97f3\u4e50\u751f\u6210\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2508.19262", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.19262", "abs": "https://arxiv.org/abs/2508.19262", "authors": ["Maximilian Wachter", "Sebastian Murgul", "Michael Heizmann"], "title": "Beat-Based Rhythm Quantization of MIDI Performances", "comment": "Accepted to the Late Breaking Demo Papers of the 1st AES\n  International Conference on Artificial Intelligence and Machine Learning for\n  Audio (AIMLA LBDP), 2025", "summary": "We propose a transformer-based rhythm quantization model that incorporates\nbeat and downbeat information to quantize MIDI performances into\nmetrically-aligned, human-readable scores. We propose a beat-based\npreprocessing method that transfers score and performance data into a unified\ntoken representation. We optimize our model architecture and data\nrepresentation and train on piano and guitar performances. Our model exceeds\nstate-of-the-art performance based on the MUSTER metric.", "AI": {"tldr": "\u4e00\u79cd\u57fa\u4e8etransformer\u7684\u8282\u594f\u91cf\u5316\u6a21\u578b\uff0c\u5229\u7528\u8282\u62cd\u548c\u5f3a\u62cd\u4fe1\u606f\u5c06MIDI\u8868\u6f14\u8f6c\u6362\u4e3a\u8ba1\u91cf\u5bf9\u9f50\u7684\u4e50\u8c31", "motivation": "\u5c06MIDI\u8868\u6f14\u6570\u636e\u91cf\u5316\u4e3a\u4eba\u7c7b\u53ef\u8bfb\u7684\u4e50\u8c31\u8868\u8fbe\uff0c\u4ee5\u652f\u6301\u97f3\u4e50\u5236\u4f5c\u548c\u6559\u5b66\u5e94\u7528", "method": "\u63d0\u51fa\u57fa\u4e8e\u8282\u62cd\u7684\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u5c06\u4e50\u8c31\u548c\u8868\u6f14\u6570\u636e\u8f6c\u6362\u4e3a\u7edf\u4e00\u7684token\u8868\u793a\uff0c\u4f7f\u7528transformer\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3", "result": "\u5728\u94a2\u7434\u548c\u5409\u4ed6\u8868\u6f14\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5728MUSTER\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u597d\u65b9\u6cd5", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u5c06MIDI\u8868\u6f14\u91cf\u5316\u4e3a\u8ba1\u91cf\u5bf9\u9f50\u7684\u4e50\u8c31\uff0c\u4e3a\u97f3\u4e50\u81ea\u52a8\u8bb0\u8c31\u63d0\u4f9b\u4e86\u6548\u679c\u663e\u8457\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.19308", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.19308", "abs": "https://arxiv.org/abs/2508.19308", "authors": ["Haolin Yu", "Yanxiong Li"], "title": "Infant Cry Detection In Noisy Environment Using Blueprint Separable Convolutions and Time-Frequency Recurrent Neural Network", "comment": null, "summary": "Infant cry detection is a crucial component of baby care system. In this\npaper, we propose a lightweight and robust method for infant cry detection. The\nmethod leverages blueprint separable convolutions to reduce computational\ncomplexity, and a time-frequency recurrent neural network for adaptive\ndenoising. The overall framework of the method is structured as a multi-scale\nconvolutional recurrent neural network, which is enhanced by efficient spatial\nattention mechanism and contrast-aware channel attention module, and acquire\nlocal and global information from the input feature of log Mel-spectrogram.\nMultiple public datasets are adopted to create a diverse and representative\ndataset, and environmental corruption techniques are used to generate the noisy\nsamples encountered in real-world scenarios. Results show that our method\nexceeds many state-of-the-art methods in accuracy, F1-score, and complexity\nunder various signal-to-noise ratio conditions. The code is at\nhttps://github.com/fhfjsd1/ICD_MMSP.", "AI": {"tldr": "\u8f7b\u91cf\u7ea7\u5a74\u513f\u54ed\u58f0\u68c0\u6d4b\u65b9\u6cd5\uff0c\u91c7\u7528\u84dd\u56fe\u5206\u79bb\u5377\u79ef\u7f51\u7edc\u548c\u65f6\u9891\u9012\u5f52\u7f51\u7edc\uff0c\u5728\u5404\u79cd\u566a\u58f0\u6761\u4ef6\u4e0b\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "motivation": "\u5a74\u513f\u54ed\u58f0\u68c0\u6d4b\u662f\u5b9d\u5b9d\u62a4\u7406\u7cfb\u7edf\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u9700\u8981\u8f7b\u91cf\u7ea7\u4e14\u7a33\u5065\u7684\u65b9\u6cd5\u6765\u5904\u7406\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u566a\u58f0\u95ee\u9898", "method": "\u591a\u5c3a\u5ea6\u5377\u79ef\u9012\u5f52\u7f51\u7edc\u6846\u67b6\uff0c\u7ed3\u5408\u84dd\u56fe\u5206\u79bb\u5377\u79ef\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u65f6\u9891\u9012\u5f52\u7f51\u7edc\u8fdb\u884c\u81ea\u9002\u5e94\u53bb\u566a\uff0c\u4f7f\u7528\u9ad8\u6548\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\u548c\u5bf9\u6bd4\u611f\u77e5\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757", "result": "\u5728\u591a\u79cd\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u3001F1\u5206\u6570\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u65b9\u9762\u8d85\u8d8a\u4e86\u8bb8\u591a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5", "conclusion": "\u8be5\u8f7b\u91cf\u7ea7\u5a74\u513f\u54ed\u58f0\u68c0\u6d4b\u65b9\u6cd5\u5177\u6709\u9ad8\u6548\u6027\u548c\u5f3a\u9547\u6027\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u573a\u666f\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2508.19514", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.19514", "abs": "https://arxiv.org/abs/2508.19514", "authors": ["Zhihao Ouyang", "Ju-Chiang Wang", "Daiyu Zhang", "Bin Chen", "Shangjie Li", "Quan Lin"], "title": "MQAD: A Large-Scale Question Answering Dataset for Training Music Large Language Models", "comment": null, "summary": "Question-answering (QA) is a natural approach for humans to understand a\npiece of music audio. However, for machines, accessing a large-scale dataset\ncovering diverse aspects of music is crucial, yet challenging, due to the\nscarcity of publicly available music data of this type. This paper introduces\nMQAD, a music QA dataset built on the Million Song Dataset (MSD), encompassing\na rich array of musical features, including beat, chord, key, structure,\ninstrument, and genre -- across 270,000 tracks, featuring nearly 3 million\ndiverse questions and captions. MQAD distinguishes itself by offering detailed\ntime-varying musical information such as chords and sections, enabling\nexploration into the inherent structure of music within a song. To compile\nMQAD, our methodology leverages specialized Music Information Retrieval (MIR)\nmodels to extract higher-level musical features and Large Language Models\n(LLMs) to generate natural language QA pairs. Then, we leverage a multimodal\nLLM that integrates the LLaMA2 and Whisper architectures, along with novel\nsubjective metrics to assess the performance of MQAD. In experiments, our model\ntrained on MQAD demonstrates advancements over conventional music audio\ncaptioning approaches. The dataset and code are available at\nhttps://github.com/oyzh888/MQAD.", "AI": {"tldr": "MQAD\u662f\u4e00\u4e2a\u57fa\u4e8e\u767e\u4e07\u6b4c\u66f2\u6570\u636e\u96c6\u7684\u5927\u89c4\u6a21\u97f3\u4e50\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u5305\u542b270,000\u9996\u6b4c\u66f2\u7684\u8fd1300\u4e07\u4e2a\u591a\u6837\u5316\u95ee\u9898\u548c\u63cf\u8ff0\uff0c\u6db5\u76d6\u8282\u62cd\u3001\u548c\u5f26\u3001\u8c03\u6027\u3001\u7ed3\u6784\u3001\u4e50\u5668\u3001\u6d41\u6d3e\u7b49\u4e30\u5bcc\u97f3\u4e50\u7279\u5f81\u3002", "motivation": "\u89e3\u51b3\u97f3\u4e50\u97f3\u9891\u7406\u89e3\u4e2d\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u95ee\u7b54\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u4f7f\u673a\u5668\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u95ee\u7b54\u65b9\u5f0f\u6df1\u5165\u7406\u89e3\u97f3\u4e50\u5185\u5bb9\u3002", "method": "\u5229\u7528\u4e13\u4e1a\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u6a21\u578b\u63d0\u53d6\u9ad8\u5c42\u6b21\u97f3\u4e50\u7279\u5f81\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u81ea\u7136\u8bed\u8a00\u95ee\u7b54\u5bf9\uff0c\u6784\u5efa\u591a\u6a21\u6001LLM\u6574\u5408LLaMA2\u548cWhisper\u67b6\u6784\u3002", "result": "\u5728MQAD\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u6bd4\u4f20\u7edf\u97f3\u4e50\u97f3\u9891\u63cf\u8ff0\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u8bc1\u660e\u4e86\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "MQAD\u4e3a\u97f3\u4e50\u7406\u89e3\u548c\u95ee\u7b54\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u97f3\u4e50AI\u9886\u57df\u7684\u53d1\u5c55\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.19639", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2508.19639", "abs": "https://arxiv.org/abs/2508.19639", "authors": ["Junxi Wang", "Yaxiong Wang", "Lechao Cheng", "Zhun Zhong"], "title": "FakeSV-VLM: Taming VLM for Detecting Fake Short-Video News via Progressive Mixture-Of-Experts Adapter", "comment": "EMNLP2025 Findings", "summary": "We present FakeSV-VLM in this paper, a new VLM-based framework for detecting\nfake news on short video platforms. Despite significant efforts to combat this\nissue due to the severe threat that fake news videos pose to public information\nsecurity, existing methods still fall short in detection accuracy, often due to\nlack of knowledge to verify the news is real or not. However, large Vision\nLanguage Models (VLMs) have absorbed extensive real-world knowledge from\nmassive multimodal datasets. Motivated by this, we adapt advanced VLMs for fake\nnews detection in short videos. Upon close examination of news samples, we\nobserve that short video samples can be categorized into four distinct\nscenarios: both video and text are real (for real samples), or both are fake,\nor either the video or text is fake (for fake samples). Inspired by this\ninsight, we design four experts tailored to handle each scenario and integrate\nthem into VLM via Mixture of Experts. Specifically, we develop the Progressive\nMoE Adapter (PMOE) module where detection experts first provide an initial\nanalysis, followed by attribution experts for a comprehensive diagnosis,\nleading to a robust decision. Additionally, we also note the fake news videos\noften show inconsistency between two modalities. Consequently, we further\ndesign the Alignment-driven Event Checking (ADEC) module, which perceives the\nfake news by capturing the inconsistency between different modalities.\nExtensive experiments on two benchmark datasets, FakeSV and FakeTT, verify the\nsuperiority of our model. It significantly outperforms current state-of-the-art\nmodels by +3.32% and +5.02%, establishing a new benchmark in the field.", "AI": {"tldr": "FakeSV-VLM\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u77ed\u89c6\u9891\u5047\u65b0\u95fb\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u673a\u5236\u548c\u6a21\u6001\u5bf9\u9f50\u68c0\u67e5\uff0c\u5728FakeSV\u548cFakeTT\u6570\u636e\u96c6\u4e0a\u6bd4\u73b0\u6709\u6700\u4f73\u6a21\u578b\u63d0\u53473.32%\u548c5.02%", "motivation": "\u73b0\u6709\u5047\u65b0\u95fb\u68c0\u6d4b\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u77e5\u8bc6\u9a8c\u8bc1\u80fd\u529b\u800c\u51c6\u786e\u7387\u4e0d\u8db3\uff0c\u800c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u6d77\u91cf\u591a\u6a21\u6001\u6570\u636e\u4e2d\u5438\u6536\u4e86\u4e30\u5bcc\u7684\u771f\u5b9e\u4e16\u754c\u77e5\u8bc6\uff0c\u9002\u5408\u7528\u4e8e\u5047\u65b0\u95fb\u68c0\u6d4b", "method": "\u63d0\u51fa\u56db\u4e13\u5bb6\u6df7\u5408\u673a\u5236\u5904\u7406\u4e0d\u540c\u771f\u5047\u7ec4\u5408\u573a\u666f\uff0c\u8bbe\u8ba1\u6e10\u8fdb\u5f0fMoE\u9002\u914d\u5668\u6a21\u5757\u8fdb\u884c\u5206\u5c42\u5206\u6790\uff0c\u5e76\u5f00\u53d1\u5bf9\u9f50\u9a71\u52a8\u7684\u4e8b\u4ef6\u68c0\u67e5\u6a21\u5757\u6355\u6349\u6a21\u6001\u95f4\u4e0d\u4e00\u81f4\u6027", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6FakeSV\u548cFakeTT\u4e0a\u663e\u8457\u8d85\u8d8a\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5206\u522b\u63d0\u53473.32%\u548c5.02%\uff0c\u5efa\u7acb\u4e86\u8be5\u9886\u57df\u7684\u65b0\u57fa\u51c6", "conclusion": "FakeSV-VLM\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5229\u7528VLM\u7684\u77e5\u8bc6\u80fd\u529b\u548c\u591a\u6a21\u6001\u5206\u6790\uff0c\u4e3a\u77ed\u89c6\u9891\u5e73\u53f0\u7684\u5047\u65b0\u95fb\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5f3a\u5927\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.19268", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19268", "abs": "https://arxiv.org/abs/2508.19268", "authors": ["Qing Wang", "Xue Han", "Jiahui Wang", "Lehao Xing", "Qian Hu", "Lianlian Zhang", "Chao Deng", "Junlan Feng"], "title": "MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts", "comment": null, "summary": "Despite LLMs' excellent code creation capabilities, multilingual code\ngeneration remains extremely challenging. To address this, we intent to improve\nthe multi-programming-lingual (MultiPL) performance of the base LLMs while\nretaining the most popular ones using restricted computational resources. We\nconsider MultiPL to be a special case of multiple natural languages and propose\na MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called\nMultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize\nexpert selection at both the token and segment levels. The token-level MoE is a\nstandard upcycling MoE structure with a shared expert and a novel gate weight\nnormalization approach that aids in the final fusion with the segment-level\nMoE. The segment-level MoE incorporates two innovative designs to better\ncapture the syntactic structure and contextual patterns of programming\nlanguages: First, using a sliding window to partition the input token sequence\ninto multiple segments; Then, adopting an expert-choice routing strategy that\nallows experts to select the top-k segments. The results of the experiment\nproved the effectiveness of MultiPL-MoE.", "AI": {"tldr": "\u63d0\u51faMultiPL-MoE\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u63d0\u5347LLMs\u7684\u591a\u7f16\u7a0b\u8bed\u8a00\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u4e3b\u6d41\u8bed\u8a00\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u591a\u8bed\u8a00\u8868\u73b0", "motivation": "\u5c3d\u7ba1LLMs\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u591a\u7f16\u7a0b\u8bed\u8a00\u4ee3\u7801\u751f\u6210\u4ecd\u7136\u6781\u5177\u6311\u6218\u6027\u3002\u9700\u8981\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u63d0\u5347\u591a\u7f16\u7a0b\u8bed\u8a00\u6027\u80fd\u540c\u65f6\u4fdd\u6301\u4e3b\u6d41\u8bed\u8a00\u80fd\u529b", "method": "\u63d0\u51faMultiPL-MoE\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff0c\u7ed3\u5408token\u7ea7\u548csegment\u7ea7\u4e24\u4e2a\u914d\u5bf9MoE\u3002token\u7ea7\u91c7\u7528\u6807\u51c6upcycling MoE\u7ed3\u6784\uff0csegment\u7ea7\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u5206\u5272\u8f93\u5165\u5e8f\u5217\u5e76\u91c7\u7528\u4e13\u5bb6\u9009\u62e9\u8def\u7531\u7b56\u7565", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eMultiPL-MoE\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u63d0\u5347\u4e86LLMs\u5728\u591a\u7f16\u7a0b\u8bed\u8a00\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u591a\u8bed\u8a00\u4ee3\u7801\u751f\u6210\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848"}}
{"id": "2508.19603", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19603", "abs": "https://arxiv.org/abs/2508.19603", "authors": ["Zhejing Hu", "Yan Liu", "Gong Chen", "Bruce X. B. Yu"], "title": "CompLex: Music Theory Lexicon Constructed by Autonomous Agents for Automatic Music Generation", "comment": null, "summary": "Generative artificial intelligence in music has made significant strides, yet\nit still falls short of the substantial achievements seen in natural language\nprocessing, primarily due to the limited availability of music data.\nKnowledge-informed approaches have been shown to enhance the performance of\nmusic generation models, even when only a few pieces of musical knowledge are\nintegrated. This paper seeks to leverage comprehensive music theory in\nAI-driven music generation tasks, such as algorithmic composition and style\ntransfer, which traditionally require significant manual effort with existing\ntechniques. We introduce a novel automatic music lexicon construction model\nthat generates a lexicon, named CompLex, comprising 37,432 items derived from\njust 9 manually input category keywords and 5 sentence prompt templates. A new\nmulti-agent algorithm is proposed to automatically detect and mitigate\nhallucinations. CompLex demonstrates impressive performance improvements across\nthree state-of-the-art text-to-music generation models, encompassing both\nsymbolic and audio-based methods. Furthermore, we evaluate CompLex in terms of\ncompleteness, accuracy, non-redundancy, and executability, confirming that it\npossesses the key characteristics of an effective lexicon.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86CompLex\u81ea\u52a8\u97f3\u4e50\u8bcd\u5178\u6784\u5efa\u6a21\u578b\uff0c\u901a\u8fc7\u5c11\u91cf\u4eba\u5de5\u8f93\u5165\u751f\u6210\u5305\u542b37,432\u4e2a\u6761\u76ee\u7684\u97f3\u4e50\u8bcd\u5178\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u97f3\u4e50\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u97f3\u4e50\u751f\u6210AI\u5728\u6570\u636e\u53ef\u7528\u6027\u65b9\u9762\u843d\u540e\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u9700\u8981\u5229\u7528\u97f3\u4e50\u7406\u8bba\u77e5\u8bc6\u6765\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u51cf\u5c11\u4f20\u7edf\u65b9\u6cd5\u6240\u9700\u7684\u5927\u91cf\u4eba\u5de5\u52aa\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u81ea\u52a8\u97f3\u4e50\u8bcd\u5178\u6784\u5efa\u6a21\u578b\uff0c\u4ec5\u97009\u4e2a\u624b\u52a8\u8f93\u5165\u7c7b\u522b\u5173\u952e\u8bcd\u548c5\u4e2a\u53e5\u5b50\u63d0\u793a\u6a21\u677f\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u591a\u667a\u80fd\u4f53\u7b97\u6cd5\u6765\u81ea\u52a8\u68c0\u6d4b\u548c\u7f13\u89e3\u5e7b\u89c9\u95ee\u9898\u3002", "result": "CompLex\u5728\u4e09\u4e2a\u6700\u5148\u8fdb\u7684\u6587\u672c\u5230\u97f3\u4e50\u751f\u6210\u6a21\u578b\uff08\u5305\u62ec\u7b26\u53f7\u548c\u97f3\u9891\u65b9\u6cd5\uff09\u4e0a\u90fd\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u5728\u5b8c\u6574\u6027\u3001\u51c6\u786e\u6027\u3001\u975e\u5197\u4f59\u6027\u548c\u53ef\u6267\u884c\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CompLex\u8bc1\u660e\u4e86\u5229\u7528\u5168\u9762\u97f3\u4e50\u7406\u8bba\u6784\u5efa\u81ea\u52a8\u8bcd\u5178\u7684\u6709\u6548\u6027\uff0c\u4e3aAI\u9a71\u52a8\u7684\u97f3\u4e50\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u77e5\u8bc6\u589e\u5f3a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.20057", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2508.20057", "abs": "https://arxiv.org/abs/2508.20057", "authors": ["Haoshuo Zhang", "Yufei Bo", "Meixia Tao"], "title": "ProMSC-MIS: Prompt-based Multimodal Semantic Communication for Multi-Spectral Image Segmentation", "comment": "arXiv admin note: text overlap with arXiv:2508.17920", "summary": "Multimodal semantic communication has great potential to enhance downstream\ntask performance by integrating complementary information across modalities.\nThis paper introduces ProMSC-MIS, a novel Prompt-based Multimodal Semantic\nCommunication framework for Multi-Spectral Image Segmentation. It enables\nefficient task-oriented transmission of spatially aligned RGB and thermal\nimages over band-limited channels. Our framework has two main design novelties.\nFirst, by leveraging prompt learning and contrastive learning, unimodal\nsemantic encoders are pre-trained to learn diverse and complementary semantic\nrepresentations by using features from one modality as prompts for another.\nSecond, a semantic fusion module that combines cross-attention mechanism and\nsqueeze-and-excitation (SE) networks is designed to effectively fuse\ncross-modal features. Experimental results demonstrate that ProMSC-MIS\nsubstantially outperforms conventional image transmission combined with\nstate-of-the-art segmentation methods. Notably, it reduces the required channel\nbandwidth by 50%--70% at the same segmentation performance, while also\ndecreasing the storage overhead and computational complexity by 26% and 37%,\nrespectively. Ablation studies also validate the effectiveness of the proposed\npre-training and semantic fusion strategies. Our scheme is highly suitable for\napplications such as autonomous driving and nighttime surveillance.", "AI": {"tldr": "ProMSC-MIS\u662f\u4e00\u4e2a\u57fa\u4e8e\u63d0\u793a\u5b66\u4e60\u7684\u591a\u6a21\u6001\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u5149\u8c31\u56fe\u50cf\u5206\u5272\uff0c\u80fd\u591f\u5728\u6709\u9650\u5e26\u5bbd\u4e0b\u9ad8\u6548\u4f20\u8f93RGB\u548c\u70ed\u6210\u50cf\u56fe\u50cf\uff0c\u663e\u8457\u964d\u4f4e\u5e26\u5bbd\u9700\u6c42\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u591a\u6a21\u6001\u8bed\u4e49\u901a\u4fe1\u901a\u8fc7\u6574\u5408\u8de8\u6a21\u6001\u7684\u4e92\u8865\u4fe1\u606f\u6765\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5e26\u5bbd\u53d7\u9650\u7684\u901a\u9053\u4e2d\u4f20\u8f93\u7a7a\u95f4\u5bf9\u9f50\u7684RGB\u548c\u70ed\u6210\u50cf\u56fe\u50cf\u65f6\uff0c\u9700\u8981\u9ad8\u6548\u7684\u9762\u5411\u4efb\u52a1\u7684\u4f20\u8f93\u65b9\u6848\u3002", "method": "\u91c7\u7528\u63d0\u793a\u5b66\u4e60\u548c\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3\u5355\u6a21\u6001\u8bed\u4e49\u7f16\u7801\u5668\uff0c\u4f7f\u7528\u4e00\u4e2a\u6a21\u6001\u7684\u7279\u5f81\u4f5c\u4e3a\u53e6\u4e00\u4e2a\u6a21\u6001\u7684\u63d0\u793a\uff1b\u8bbe\u8ba1\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u548cSE\u7f51\u7edc\u7684\u8bed\u4e49\u878d\u5408\u6a21\u5757\u6765\u6709\u6548\u878d\u5408\u8de8\u6a21\u6001\u7279\u5f81\u3002", "result": "\u5728\u76f8\u540c\u5206\u5272\u6027\u80fd\u4e0b\u51cf\u5c1150%-70%\u7684\u5e26\u5bbd\u9700\u6c42\uff0c\u5b58\u50a8\u5f00\u9500\u964d\u4f4e26%\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u964d\u4f4e37%\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u56fe\u50cf\u4f20\u8f93\u7ed3\u5408\u6700\u5148\u8fdb\u5206\u5272\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u591c\u95f4\u76d1\u63a7\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u9ad8\u5ea6\u9002\u7528\u6027\uff0c\u9884\u8bad\u7ec3\u548c\u8bed\u4e49\u878d\u5408\u7b56\u7565\u7684\u6709\u6548\u6027\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2508.19270", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19270", "abs": "https://arxiv.org/abs/2508.19270", "authors": ["Nguyen Huu Nhat Minh", "Tran Nguyen Anh", "Truong Dinh Dung", "Vo Van Nam", "Le Pham Tuyen"], "title": "Whisper based Cross-Lingual Phoneme Recognition between Vietnamese and English", "comment": null, "summary": "Cross-lingual phoneme recognition has emerged as a significant challenge for\naccurate automatic speech recognition (ASR) when mixing Vietnamese and English\npronunciations. Unlike many languages, Vietnamese relies on tonal variations to\ndistinguish word meanings, whereas English features stress patterns and\nnon-standard pronunciations that hinder phoneme alignment between the two\nlanguages. To address this challenge, we propose a novel bilingual speech\nrecognition approach with two primary contributions: (1) constructing a\nrepresentative bilingual phoneme set that bridges the differences between\nVietnamese and English phonetic systems; (2) designing an end-to-end system\nthat leverages the PhoWhisper pre-trained encoder for deep high-level\nrepresentations to improve phoneme recognition. Our extensive experiments\ndemonstrate that the proposed approach not only improves recognition accuracy\nin bilingual speech recognition for Vietnamese but also provides a robust\nframework for addressing the complexities of tonal and stress-based phoneme\nrecognition", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u53cc\u8bed\u8bed\u97f3\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u8de8\u8bed\u8a00\u97f3\u7d20\u96c6\u548c\u5229\u7528PhoWhisper\u9884\u8bad\u7ec3\u7f16\u7801\u5668\uff0c\u6709\u6548\u89e3\u51b3\u8d8b\u5357\u8bed\u548c\u82f1\u8bed\u6df7\u5408\u8bed\u97f3\u8bc6\u522b\u4e2d\u7684\u97f3\u8c03\u548c\u91cd\u97f3\u6311\u6218\u3002", "motivation": "\u8d8b\u5357\u8bed\u4f9d\u8d56\u97f3\u8c03\u533a\u5206\u8bcd\u4e49\uff0c\u82f1\u8bed\u5177\u6709\u91cd\u97f3\u6a21\u5f0f\u548c\u975e\u6807\u51c6\u53d1\u97f3\uff0c\u4e24\u8005\u7684\u97f3\u7cfb\u5dee\u5f02\u7ed9\u8de8\u8bed\u8a00\u97f3\u7d20\u8bc6\u522b\u5e26\u6765\u91cd\u5927\u6311\u6218\u3002", "method": "\u6784\u5efa\u4ee3\u8868\u6027\u53cc\u8bed\u97f3\u7d20\u96c6\u6765\u7f29\u5c0f\u8d8b\u5357\u8bed\u548c\u82f1\u8bed\u97f3\u7cfb\u5dee\u5f02\uff0c\u8bbe\u8ba1\u7aef\u5230\u7aef\u7cfb\u7edf\u5229\u7528PhoWhisper\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u83b7\u53d6\u6df1\u5c42\u9ad8\u7ea7\u8868\u5f81\u6765\u6539\u5584\u97f3\u7d20\u8bc6\u522b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u8d8b\u5357\u8bed\u53cc\u8bed\u8bed\u97f3\u8bc6\u522b\u4e2d\u63d0\u9ad8\u4e86\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4e3a\u5904\u7406\u97f3\u8c03\u548c\u91cd\u97f3\u57fa\u7840\u7684\u97f3\u7d20\u8bc6\u522b\u590d\u6742\u6027\u63d0\u4f9b\u4e86\u7a33\u5065\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5730\u89e3\u51b3\u4e86\u8d8b\u5357\u8bed-\u82f1\u8bed\u6df7\u5408\u8bed\u97f3\u8bc6\u522b\u7684\u97f3\u7cfb\u5dee\u5f02\u95ee\u9898\uff0c\u4e3a\u8de8\u8bed\u8a00\u97f3\u7d20\u8bc6\u522b\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19876", "categories": ["cs.SD", "cs.DL"], "pdf": "https://arxiv.org/pdf/2508.19876", "abs": "https://arxiv.org/abs/2508.19876", "authors": ["Sepideh Shafiei", "Shapour Hakam"], "title": "The IRMA Dataset: A Structured Audio-MIDI Corpus for Iranian Classical Music", "comment": null, "summary": "We present the IRMA Dataset (Iranian Radif MIDI Audio), a multi-level,\nopen-access corpus designed for the computational study of Iranian classical\nmusic, with a particular emphasis on the radif, a structured repertoire of\nmodal-melodic units central to pedagogy and performance. The dataset combines\nsymbolic MIDI representations, phrase-level audio-MIDI alignment, musicological\ntranscriptions in PDF format, and comparative tables of theoretical information\ncurated from a range of performers and scholars. We outline the multi-phase\nconstruction process, including segment annotation, alignment methods, and a\nstructured system of identifier codes to reference individual musical units.\nThe current release includes the complete radif of Karimi; MIDI files and\nmetadata from Mirza Abdollah's radif; selected segments from the vocal radif of\nDavami, as transcribed by Payvar and Fereyduni; and a dedicated section\nfeaturing audio-MIDI examples of tahrir ornamentation performed by prominent\n20th-century vocalists. While the symbolic and analytical components are\nreleased under an open-access license (CC BY-NC 4.0), some referenced audio\nrecordings and third-party transcriptions are cited using discographic\ninformation to enable users to locate the original materials independently,\npending copyright permission. Serving both as a scholarly archive and a\nresource for computational analysis, this dataset supports applications in\nethnomusicology, pedagogy, symbolic audio research, cultural heritage\npreservation, and AI-driven tasks such as automatic transcription and music\ngeneration. We welcome collaboration and feedback to support its ongoing\nrefinement and broader integration into musicological and machine learning\nworkflows.", "AI": {"tldr": "IRMA\u6570\u636e\u96c6\u662f\u4e00\u4e2a\u591a\u5c42\u7ea7\u3001\u5f00\u653e\u83b7\u53d6\u7684\u4f0a\u6717\u53e4\u5178\u97f3\u4e50\u8ba1\u7b97\u7814\u7a76\u8bed\u6599\u5e93\uff0c\u7279\u522b\u5173\u6ce8radif\uff08\u4f0a\u6717\u97f3\u4e50\u6559\u5b66\u548c\u8868\u6f14\u4e2d\u7684\u6838\u5fc3\u6a21\u6001\u65cb\u5f8b\u5355\u5143\u4f53\u7cfb\uff09\uff0c\u5305\u542bMIDI\u7b26\u53f7\u8868\u793a\u3001\u97f3\u9891-MIDI\u5bf9\u9f50\u3001\u97f3\u4e50\u5b66\u8f6c\u5f55\u548c\u7406\u8bba\u4fe1\u606f\u6bd4\u8f83\u8868\u3002", "motivation": "\u4e3a\u4f0a\u6717\u53e4\u5178\u97f3\u4e50\u7684\u8ba1\u7b97\u7814\u7a76\u63d0\u4f9b\u5168\u9762\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7279\u522b\u5173\u6ce8radif\u8fd9\u4e00\u6838\u5fc3\u97f3\u4e50\u4f20\u7edf\uff0c\u652f\u6301\u6c11\u65cf\u97f3\u4e50\u5b66\u3001\u6559\u5b66\u3001\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u548cAI\u9a71\u52a8\u4efb\u52a1\u7684\u7814\u7a76\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u6784\u5efa\u8fc7\u7a0b\uff0c\u5305\u62ec\u7247\u6bb5\u6807\u6ce8\u3001\u5bf9\u9f50\u65b9\u6cd5\u548c\u7ed3\u6784\u5316\u6807\u8bc6\u7b26\u7f16\u7801\u7cfb\u7edf\uff0c\u7ed3\u5408\u7b26\u53f7MIDI\u8868\u793a\u3001\u77ed\u8bed\u7ea7\u97f3\u9891-MIDI\u5bf9\u9f50\u3001PDF\u683c\u5f0f\u97f3\u4e50\u5b66\u8f6c\u5f55\u4ee5\u53ca\u4ece\u4e0d\u540c\u8868\u6f14\u8005\u548c\u5b66\u8005\u6536\u96c6\u7684\u7406\u8bba\u4fe1\u606f\u6bd4\u8f83\u8868\u3002", "result": "\u53d1\u5e03\u4e86\u5305\u542bKarimi\u5b8c\u6574radif\u3001Mirza Abdollah\u7684radif MIDI\u6587\u4ef6\u548c\u5143\u6570\u636e\u3001Davami\u58f0\u4e50radif\u7cbe\u9009\u7247\u6bb5\uff0c\u4ee5\u53ca20\u4e16\u7eaa\u8457\u540d\u58f0\u4e50\u5bb6tahrir\u88c5\u9970\u97f3\u97f3\u9891-MIDI\u793a\u4f8b\u7684\u6570\u636e\u96c6\u3002\u7b26\u53f7\u548c\u5206\u6790\u7ec4\u4ef6\u91c7\u7528CC BY-NC 4.0\u5f00\u653e\u8bb8\u53ef\u3002", "conclusion": "IRMA\u6570\u636e\u96c6\u65e2\u662f\u5b66\u672f\u6863\u6848\u53c8\u662f\u8ba1\u7b97\u5206\u6790\u8d44\u6e90\uff0c\u652f\u6301\u6c11\u65cf\u97f3\u4e50\u5b66\u3001\u6559\u5b66\u3001\u7b26\u53f7\u97f3\u9891\u7814\u7a76\u3001\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u548cAI\u4efb\u52a1\uff08\u5982\u81ea\u52a8\u8f6c\u5f55\u548c\u97f3\u4e50\u751f\u6210\uff09\u7b49\u591a\u79cd\u5e94\u7528\uff0c\u6b22\u8fce\u5408\u4f5c\u53cd\u9988\u4ee5\u6301\u7eed\u5b8c\u5584\u548c\u96c6\u6210\u5230\u97f3\u4e50\u5b66\u548c\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u4e2d\u3002"}}
{"id": "2508.19271", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19271", "abs": "https://arxiv.org/abs/2508.19271", "authors": ["Rushitha Santhoshi Mamidala", "Anshuman Chhabra", "Ankur Mali"], "title": "Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT", "comment": null, "summary": "Prompt-based reasoning strategies such as Chain-of-Thought (CoT) and\nIn-Context Learning (ICL) have become widely used for eliciting reasoning\ncapabilities in large language models (LLMs). However, these methods rely on\nfragile, implicit mechanisms often yielding inconsistent outputs across seeds,\nformats, or minor prompt variations making them fundamentally unreliable for\ntasks requiring stable, interpretable reasoning. In contrast, automata-based\nneuro-symbolic frameworks like RetoMaton offer a more structured and\ntrustworthy alternative by grounding retrieval in symbolic memory with\ndeterministic transitions. In this work, we extend RetoMaton by replacing its\nglobal datastore with a local, task-adaptive Weighted Finite Automaton (WFA),\nconstructed directly from external domain corpora. This local automaton\nstructure promotes robust, context-aware retrieval while preserving symbolic\ntraceability and low inference overhead. Unlike prompting, which entangles\ncontext and memory in opaque ways, our approach leverages the explicit\nstructure of WFAs to provide verifiable and modular retrieval behavior, making\nit better suited for domain transfer and interoperability. We evaluate this\nlocal RetoMaton variant on two pretrained LLMs LLaMA-3.2-1B and Gemma-3-1B-PT\nacross three reasoning tasks: TriviaQA (reading comprehension), GSM8K\n(multi-step math), and MMLU (domain knowledge). Compared to the base model and\nprompting-based methods, augmenting these setups with local RetoMaton\nconsistently improves performance while enabling transparent and reproducible\nretrieval dynamics. Our results highlight a promising shift toward trustworthy,\nsymbolic reasoning in modern LLMs via lightweight, automaton-guided memory.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c40\u90e8\u52a0\u6743\u6709\u9650\u81ea\u52a8\u673a(WFA)\u7684RetoMaton\u6539\u8fdb\u65b9\u6cd5\uff0c\u66ff\u4ee3\u539f\u6709\u7684\u5168\u5c40\u6570\u636e\u5b58\u50a8\uff0c\u901a\u8fc7\u4ece\u5916\u90e8\u9886\u57df\u8bed\u6599\u76f4\u63a5\u6784\u5efa\u672c\u5730\u81ea\u52a8\u673a\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u7684\u68c0\u7d22\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u63a8\u7406\u65b9\u6cd5\uff08\u5982Chain-of-Thought\u548cIn-Context Learning\uff09\u5b58\u5728\u8106\u5f31\u6027\u548c\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u8f93\u51fa\u7ed3\u679c\u5bf9\u79cd\u5b50\u3001\u683c\u5f0f\u6216\u5fae\u5c0f\u63d0\u793a\u53d8\u5316\u654f\u611f\uff0c\u7f3a\u4e4f\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e0d\u9002\u5408\u9700\u8981\u53ef\u9760\u63a8\u7406\u7684\u4efb\u52a1\u3002", "method": "\u6269\u5c55RetoMaton\u6846\u67b6\uff0c\u7528\u4ece\u5916\u90e8\u9886\u57df\u8bed\u6599\u6784\u5efa\u7684\u5c40\u90e8\u52a0\u6743\u6709\u9650\u81ea\u52a8\u673a(WFA)\u66ff\u4ee3\u5168\u5c40\u6570\u636e\u5b58\u50a8\uff0c\u4fdd\u6301\u7b26\u53f7\u53ef\u8ffd\u6eaf\u6027\u548c\u4f4e\u63a8\u7406\u5f00\u9500\uff0c\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u548c\u6a21\u5757\u5316\u7684\u68c0\u7d22\u884c\u4e3a\u3002", "result": "\u5728LLaMA-3.2-1B\u548cGemma-3-1B-PT\u4e24\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u4e0a\uff0c\u5728TriviaQA\u3001GSM8K\u548cMMLU\u4e09\u4e2a\u63a8\u7406\u4efb\u52a1\u4e2d\u8bc4\u4f30\uff0c\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u672c\u5730RetoMaton\u53d8\u4f53\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u5b9e\u73b0\u900f\u660e\u548c\u53ef\u590d\u73b0\u7684\u68c0\u7d22\u52a8\u6001\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u81ea\u52a8\u673a\u5f15\u5bfc\u5185\u5b58\uff0c\u4e3a\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u53ef\u4fe1\u8d56\u7684\u7b26\u53f7\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u4fc3\u8fdb\u4e86\u5411\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u63a8\u7406\u7684\u8f6c\u53d8\u3002"}}
{"id": "2508.19272", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19272", "abs": "https://arxiv.org/abs/2508.19272", "authors": ["Kshitij Fadnis", "Sara Rosenthal", "Maeda Hanafi", "Yannis Katsis", "Marina Danilevsky"], "title": "RAGAPHENE: A RAG Annotation Platform with Human Enhancements and Edits", "comment": null, "summary": "Retrieval Augmented Generation (RAG) is an important aspect of conversing\nwith Large Language Models (LLMs) when factually correct information is\nimportant. LLMs may provide answers that appear correct, but could contain\nhallucinated information. Thus, building benchmarks that can evaluate LLMs on\nmulti-turn RAG conversations has become an increasingly important task.\nSimulating real-world conversations is vital for producing high quality\nevaluation benchmarks. We present RAGAPHENE, a chat-based annotation platform\nthat enables annotators to simulate real-world conversations for benchmarking\nand evaluating LLMs. RAGAPHENE has been successfully used by approximately 40\nannotators to build thousands of real-world conversations.", "AI": {"tldr": "RAGAPHENE\u662f\u4e00\u4e2a\u57fa\u4e8e\u804a\u5929\u7684\u6807\u6ce8\u5e73\u53f0\uff0c\u7528\u4e8e\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u5bf9\u8bdd\u6765\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u7531\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63d0\u4f9b\u4e8b\u5b9e\u6027\u4fe1\u606f\u65f6\u53ef\u80fd\u51fa\u73b0\u5e7b\u89c9\uff0c\u9700\u8981\u6784\u5efa\u80fd\u591f\u8bc4\u4f30\u591a\u8f6eRAG\u5bf9\u8bdd\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u800c\u6a21\u62df\u771f\u5b9e\u5bf9\u8bdd\u5bf9\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u8bc4\u4f30\u57fa\u51c6\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86RAGAPHENE\u804a\u5929\u6807\u6ce8\u5e73\u53f0\uff0c\u8ba9\u6807\u6ce8\u8005\u80fd\u591f\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u5bf9\u8bdd\uff0c\u7528\u4e8e\u6784\u5efa\u8bc4\u4f30LLM\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "\u8be5\u5e73\u53f0\u5df2\u88ab\u7ea640\u540d\u6807\u6ce8\u8005\u6210\u529f\u4f7f\u7528\uff0c\u6784\u5efa\u4e86\u6570\u5343\u4e2a\u771f\u5b9e\u4e16\u754c\u5bf9\u8bdd\u6837\u672c\u3002", "conclusion": "RAGAPHENE\u5e73\u53f0\u4e3a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728RAG\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\u548c\u65b9\u6cd5\u3002"}}
{"id": "2508.19274", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19274", "abs": "https://arxiv.org/abs/2508.19274", "authors": ["Yue Chu"], "title": "Leveraging Language Models and Machine Learning in Verbal Autopsy Analysis", "comment": "Ph.D. dissertation submitted to The Ohio State University, August\n  2025", "summary": "In countries without civil registration and vital statistics, verbal autopsy\n(VA) is a critical tool for estimating cause of death (COD) and inform policy\npriorities. In VA, interviewers ask proximal informants for details on the\ncircumstances preceding a death, in the form of unstructured narratives and\nstructured questions. Existing automated VA cause classification algorithms\nonly use the questions and ignore the information in the narratives. In this\nthesis, we investigate how the VA narrative can be used for automated COD\nclassification using pretrained language models (PLMs) and machine learning\n(ML) techniques. Using empirical data from South Africa, we demonstrate that\nwith the narrative alone, transformer-based PLMs with task-specific fine-tuning\noutperform leading question-only algorithms at both the individual and\npopulation levels, particularly in identifying non-communicable diseases. We\nexplore various multimodal fusion strategies combining narratives and questions\nin unified frameworks. Multimodal approaches further improve performance in COD\nclassification, confirming that each modality has unique contributions and may\ncapture valuable information that is not present in the other modality. We also\ncharacterize physician-perceived information sufficiency in VA. We describe\nvariations in sufficiency levels by age and COD and demonstrate that\nclassification accuracy is affected by sufficiency for both physicians and\nmodels. Overall, this thesis advances the growing body of knowledge at the\nintersection of natural language processing, epidemiology, and global health.\nIt demonstrates the value of narrative in enhancing COD classification. Our\nfindings underscore the need for more high-quality data from more diverse\nsettings to use in training and fine-tuning PLM/ML methods, and offer valuable\ninsights to guide the rethinking and redesign of the VA instrument and\ninterview.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u53e3\u5934\u5c38\u68c0\u4e2d\u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5206\u6790\u53d9\u8ff0\u6587\u672c\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u95ee\u9898\u6765\u6539\u5584\u6b7b\u56e0\u5206\u7c7b\uff0c\u8bc1\u660e\u53d9\u8ff0\u6587\u672c\u5305\u542b\u72ec\u7279\u4fe1\u606f\u5e76\u80fd\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u6ca1\u6709\u6c11\u4e8b\u767b\u8bb0\u548c\u751f\u547d\u7edf\u8ba1\u7684\u56fd\u5bb6\uff0c\u53e3\u5934\u5c38\u68c0\u662f\u4f30\u8ba1\u6b7b\u56e0\u7684\u5173\u952e\u5de5\u5177\u3002\u73b0\u6709\u81ea\u52a8\u5316\u5206\u7c7b\u7b97\u6cd5\u4ec5\u4f7f\u7528\u7ed3\u6784\u5316\u95ee\u9898\u800c\u5ffd\u7565\u4e86\u53d9\u8ff0\u6587\u672c\u4e2d\u7684\u4fe1\u606f\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u5229\u7528\u53d9\u8ff0\u6587\u672c\u6765\u6539\u5584\u6b7b\u56e0\u5206\u7c7b\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u5206\u6790\u5357\u975e\u7684\u7ecf\u9a8c\u6570\u636e\uff0c\u63a2\u7d22\u5355\u6a21\u6001\uff08\u4ec5\u53d9\u8ff0\u6587\u672c\uff09\u548c\u591a\u6a21\u6001\uff08\u53d9\u8ff0\u6587\u672c+\u7ed3\u6784\u5316\u95ee\u9898\uff09\u878d\u5408\u7b56\u7565\uff0c\u5e76\u8bc4\u4f30\u533b\u5e08\u611f\u77e5\u7684\u4fe1\u606f\u5145\u5206\u6027\u3002", "result": "\u4ec5\u4f7f\u7528\u53d9\u8ff0\u6587\u672c\u65f6\uff0c\u57fa\u4e8eTransformer\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4e2a\u4f53\u548c\u7fa4\u4f53\u5c42\u9762\u7684\u6b7b\u56e0\u5206\u7c7b\u8868\u73b0\u4f18\u4e8e\u4ec5\u4f7f\u7528\u95ee\u9898\u7684\u7b97\u6cd5\uff0c\u7279\u522b\u662f\u5728\u8bc6\u522b\u975e\u4f20\u67d3\u6027\u75be\u75c5\u65b9\u9762\u3002\u591a\u6a21\u6001\u65b9\u6cd5\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u8bc1\u660e\u6bcf\u79cd\u6a21\u6001\u90fd\u6709\u72ec\u7279\u8d21\u732e\u3002", "conclusion": "\u53d9\u8ff0\u6587\u672c\u80fd\u663e\u8457\u589e\u5f3a\u6b7b\u56e0\u5206\u7c7b\uff0c\u9700\u8981\u66f4\u591a\u9ad8\u8d28\u91cf\u591a\u6837\u5316\u6570\u636e\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u6307\u5bfc\u91cd\u65b0\u8bbe\u8ba1\u548c\u6539\u8fdb\u53e3\u5934\u5c38\u68c0\u5de5\u5177\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002"}}
{"id": "2508.19279", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19279", "abs": "https://arxiv.org/abs/2508.19279", "authors": ["Gunjan Jalori", "Preetika Verma", "Sercan \u00d6 Ar\u0131k"], "title": "FLAIRR-TS -- Forecasting LLM-Agents with Iterative Refinement and Retrieval for Time Series", "comment": "EMNLP", "summary": "Time series Forecasting with large languagemodels (LLMs) requires bridging\nnumericalpatterns and natural language. Effective fore-casting on LLM often\nrelies on extensive pre-processing and fine-tuning.Recent studiesshow that a\nfrozen LLM can rival specializedforecasters when supplied with a carefully\nen-gineered natural-language prompt, but craft-ing such a prompt for each task\nis itself oner-ous and ad-hoc. We introduce FLAIRR-TS, atest-time prompt\noptimization framework thatutilizes an agentic system: a\nForecaster-agentgenerates forecasts using an initial prompt,which is then\nrefined by a refiner agent, in-formed by past outputs and retrieved\nanalogs.This adaptive prompting generalizes across do-mains using creative\nprompt templates andgenerates high-quality forecasts without inter-mediate code\ngeneration.Experiments onbenchmark datasets show improved accuracyover static\nprompting and retrieval-augmentedbaselines, approaching the performance\nofspecialized prompts.FLAIRR-TS providesa practical alternative to tuning,\nachievingstrong performance via its agentic approach toadaptive prompt\nrefinement and retrieval.", "AI": {"tldr": "FLAIRR-TS\u662f\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u7cfb\u7edf\u7684\u6d4b\u8bd5\u65f6\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u4ee3\u7406\u548c\u4f18\u5316\u4ee3\u7406\u7684\u534f\u4f5c\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6027\u80fd", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u7cbe\u5fc3\u8bbe\u8ba1\u81ea\u7136\u8bed\u8a00\u63d0\u793a\uff0c\u8fc7\u7a0b\u7e41\u7410\u4e14\u7f3a\u4e4f\u901a\u7528\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u9002\u5e94\u4f18\u5316\u63d0\u793a\u7684\u6846\u67b6", "method": "\u4f7f\u7528\u53cc\u4ee3\u7406\u7cfb\u7edf\uff1a\u9884\u6d4b\u4ee3\u7406\u7528\u521d\u59cb\u63d0\u793a\u751f\u6210\u9884\u6d4b\uff0c\u4f18\u5316\u4ee3\u7406\u57fa\u4e8e\u5386\u53f2\u8f93\u51fa\u548c\u68c0\u7d22\u7684\u76f8\u4f3c\u5e8f\u5217\u6765\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\uff0c\u91c7\u7528\u521b\u610f\u63d0\u793a\u6a21\u677f\u5b9e\u73b0\u8de8\u9886\u57df\u6cdb\u5316", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u9759\u6001\u63d0\u793a\u548c\u68c0\u7d22\u589e\u5f3a\u57fa\u7ebf\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u63a5\u8fd1\u4e13\u95e8\u8bbe\u8ba1\u7684\u63d0\u793a\u6027\u80fd", "conclusion": "FLAIRR-TS\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u66ff\u4ee3\u5fae\u8c03\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63d0\u793a\u4f18\u5316\u548c\u68c0\u7d22\u7684\u4ee3\u7406\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5f3a\u52b2\u6027\u80fd"}}
{"id": "2508.19282", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19282", "abs": "https://arxiv.org/abs/2508.19282", "authors": ["Ziqiang Cui", "Yunpeng Weng", "Xing Tang", "Peiyang Liu", "Shiwei Li", "Bowei He", "Jiamin Chen", "Xiuqiang He", "Chen Ma"], "title": "CORE: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising approach to\nenhance the timeliness of knowledge and the factual accuracy of responses in\nLarge Language Models (LLMs). However, the inclusion of excessive retrieved\ndocuments substantially increases the input length, leading to higher\ncomputational costs. Previous studies have attempted to compress retrieved\ndocuments into shorter texts before in-context integration, but such methods\noften compromise end-task performance. The lack of well-defined compression\ntargets forces many approaches to rely on fixed heuristics, which cannot\nguarantee that the compressed content will effectively support the end task. To\naddress these limitations, we propose CORE, a novel method designed to achieve\nlossless context compression for RAG. CORE employs reinforcement learning to\noptimize the compression process without relying on predefined compression\nlabels. Specifically, it utilizes end-task performance as a reward signal and\napplies Generalized Reinforcement Learning Policy Optimization (GRPO) to train\nthe compressor. This end-to-end training framework enables the compressor to\ngenerate summaries that maximize the accuracy of answers generated by the LLM.\nExtensive experiments on four datasets demonstrate the superiority of our\napproach. With a high compression ratio of 3\\%, our method not only avoids\nperformance degradation compared to prepending full documents across all\ndatasets but also improves the average Exact Match (EM) score by 3.3 points.\nThe code will be released soon.", "AI": {"tldr": "CORE\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65e0\u635f\u4e0a\u4e0b\u6587\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u4f18\u5316RAG\u4e2d\u7684\u6587\u6863\u538b\u7f29\uff0c\u57283%\u7684\u9ad8\u538b\u7f29\u6bd4\u4e0b\u4e0d\u4ec5\u907f\u514d\u4e86\u6027\u80fd\u4e0b\u964d\uff0c\u8fd8\u5e73\u5747\u63d0\u5347\u4e863.3\u4e2aEM\u5206\u6570\u3002", "motivation": "\u89e3\u51b3RAG\u4e2d\u68c0\u7d22\u6587\u6863\u8fc7\u591a\u5bfc\u81f4\u8f93\u5165\u957f\u5ea6\u589e\u52a0\u548c\u8ba1\u7b97\u6210\u672c\u4e0a\u5347\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u4f20\u7edf\u538b\u7f29\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u660e\u786e\u538b\u7f29\u76ee\u6807\u800c\u635f\u5bb3\u6700\u7ec8\u4efb\u52a1\u6027\u80fd\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u6700\u7ec8\u4efb\u52a1\u6027\u80fd\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\uff0c\u5e94\u7528\u5e7f\u4e49\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4f18\u5316(GRPO)\u8bad\u7ec3\u538b\u7f29\u5668\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u65e0\u635f\u4e0a\u4e0b\u6587\u538b\u7f29\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u57283%\u7684\u9ad8\u538b\u7f29\u6bd4\u4e0b\uff0c\u4e0d\u4ec5\u907f\u514d\u4e86\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u5b8c\u6574\u6587\u6863\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u8fd8\u5c06\u5e73\u5747\u7cbe\u786e\u5339\u914d(EM)\u5206\u6570\u63d0\u9ad8\u4e863.3\u5206\u3002", "conclusion": "CORE\u65b9\u6cd5\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u538b\u7f29\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6587\u6863\u538b\u7f29\uff0c\u5728\u4fdd\u6301\u751a\u81f3\u63d0\u5347RAG\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u4e3aRAG\u7cfb\u7edf\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19357", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19357", "abs": "https://arxiv.org/abs/2508.19357", "authors": ["Peiran Zhou", "Junnan Zhu", "Yichen Shen", "Ruoxi Yu"], "title": "Context-Adaptive Synthesis and Compression for Enhanced Retrieval-Augmented Generation in Complex Domains", "comment": null, "summary": "Large Language Models (LLMs) excel in language tasks but are prone to\nhallucinations and outdated knowledge. Retrieval-Augmented Generation (RAG)\nmitigates these by grounding LLMs in external knowledge. However, in complex\ndomains involving multiple, lengthy, or conflicting documents, traditional RAG\nsuffers from information overload and inefficient synthesis, leading to\ninaccurate and untrustworthy answers. To address this, we propose CASC\n(Context-Adaptive Synthesis and Compression), a novel framework that\nintelligently processes retrieved contexts. CASC introduces a Context Analyzer\n& Synthesizer (CAS) module, powered by a fine-tuned smaller LLM, which performs\nkey information extraction, cross-document consistency checking and conflict\nresolution, and question-oriented structured synthesis. This process transforms\nraw, scattered information into a highly condensed, structured, and\nsemantically rich context, significantly reducing the token count and cognitive\nload for the final Reader LLM. We evaluate CASC on SciDocs-QA, a new\nchallenging multi-document question answering dataset designed for complex\nscientific domains with inherent redundancies and conflicts. Our extensive\nexperiments demonstrate that CASC consistently outperforms strong baselines.", "AI": {"tldr": "CASC\u6846\u67b6\u901a\u8fc7\u4e0a\u4e0b\u6587\u5206\u6790\u548c\u5408\u6210\u6a21\u5757\uff0c\u6709\u6548\u5904\u7406\u591a\u6587\u6863\u68c0\u7d22\u4e2d\u7684\u4fe1\u606f\u8fc7\u8f7d\u548c\u51b2\u7a81\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u9886\u57df\u95ee\u7b54\u7684\u51c6\u786e\u6027\u548c\u53ef\u4fe1\u5ea6", "motivation": "\u4f20\u7edfRAG\u5728\u5904\u7406\u591a\u6587\u6863\u3001\u957f\u6587\u6863\u6216\u51b2\u7a81\u6587\u6863\u65f6\u5b58\u5728\u4fe1\u606f\u8fc7\u8f7d\u548c\u5408\u6210\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u7b54\u6848\u4e0d\u51c6\u786e\u548c\u4e0d\u53ef\u4fe1", "method": "\u63d0\u51faCASC\u6846\u67b6\uff0c\u5305\u542b\u57fa\u4e8e\u5fae\u8c03\u5c0f\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5206\u6790\u5408\u6210\u6a21\u5757\uff0c\u8fdb\u884c\u5173\u952e\u4fe1\u606f\u63d0\u53d6\u3001\u8de8\u6587\u6863\u4e00\u81f4\u6027\u68c0\u67e5\u548c\u51b2\u7a81\u89e3\u51b3\u3001\u9762\u5411\u95ee\u9898\u7684\u7ed3\u6784\u5316\u5408\u6210", "result": "\u5728SciDocs-QA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCASC\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "CASC\u901a\u8fc7\u667a\u80fd\u4e0a\u4e0b\u6587\u5904\u7406\uff0c\u5c06\u539f\u59cb\u5206\u6563\u4fe1\u606f\u8f6c\u6362\u4e3a\u9ad8\u5ea6\u538b\u7f29\u7684\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\uff0c\u663e\u8457\u51cf\u5c11token\u6570\u91cf\u548c\u8ba4\u77e5\u8d1f\u8377\uff0c\u63d0\u9ad8\u95ee\u7b54\u8d28\u91cf"}}
{"id": "2508.19359", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19359", "abs": "https://arxiv.org/abs/2508.19359", "authors": ["Fatemeh Haji", "Mazal Bethany", "Cho-Yu Jason Chiang", "Anthony Rios", "Peyman Najafirad"], "title": "Reflective Agreement: Combining Self-Mixture of Agents with a Sequence Tagger for Robust Event Extraction", "comment": null, "summary": "Event Extraction (EE) involves automatically identifying and extracting\nstructured information about events from unstructured text, including triggers,\nevent types, and arguments. Traditional discriminative models demonstrate high\nprecision but often exhibit limited recall, particularly for nuanced or\ninfrequent events. Conversely, generative approaches leveraging Large Language\nModels (LLMs) provide higher semantic flexibility and recall but suffer from\nhallucinations and inconsistent predictions. To address these challenges, we\npropose Agreement-based Reflective Inference System (ARIS), a hybrid approach\ncombining a Self Mixture of Agents with a discriminative sequence tagger. ARIS\nexplicitly leverages structured model consensus, confidence-based filtering,\nand an LLM reflective inference module to reliably resolve ambiguities and\nenhance overall event prediction quality. We further investigate decomposed\ninstruction fine-tuning for enhanced LLM event extraction understanding.\nExperiments demonstrate our approach outperforms existing state-of-the-art\nevent extraction methods across three benchmark datasets.", "AI": {"tldr": "\u4e8b\u4ef6\u63d0\u53d6\u65b0\u65b9\u6cd5ARIS\uff0c\u7ed3\u5408\u4e86\u5224\u522b\u6a21\u578b\u548c\u751f\u6210\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u6a21\u578b\u5171\u8bc6\u548c\u53cd\u601d\u63a8\u7406\u63d0\u9ad8\u63d0\u53d6\u51c6\u786e\u6027", "motivation": "\u4f20\u7edf\u5224\u522b\u6a21\u578b\u7cbe\u786e\u5ea6\u9ad8\u4f46\u56de\u53ec\u7387\u4f4e\uff0c\u751f\u6210\u6a21\u578b\u56de\u53ec\u7387\u9ad8\u4f46\u5b58\u5728\u5e7b\u89c9\u548c\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u9700\u8981\u627e\u5230\u4e24\u8005\u7684\u5e73\u8861\u70b9", "method": "\u63d0\u51faARIS\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u6211\u6df7\u5408\u4ee3\u7406\u548c\u5224\u522b\u5e8f\u5217\u6807\u6ce8\u5668\uff0c\u5229\u7528\u7ed3\u6784\u5316\u6a21\u578b\u5171\u8bc6\u3001\u4fe1\u5fc3\u5ea6\u7b5b\u9009\u548cLLM\u53cd\u601d\u63a8\u7406\u6a21\u5757\u6765\u89e3\u51b3\u6b63\u5f0f\u95ee\u9898", "result": "\u5728\u4e09\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u4e8b\u4ef6\u63d0\u53d6\u65b9\u6cd5", "conclusion": "ARIS\u901a\u8fc7\u6df7\u5408\u65b9\u6848\u6709\u6548\u7ed3\u5408\u4e86\u5224\u522b\u6a21\u578b\u548c\u751f\u6210\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e8b\u4ef6\u63d0\u53d6\u7684\u6574\u4f53\u6027\u80fd"}}
{"id": "2508.19363", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19363", "abs": "https://arxiv.org/abs/2508.19363", "authors": ["Jiayu Ding", "Shuming Ma", "Lei Cui", "Nanning Zheng", "Furu Wei"], "title": "LongReasonArena: A Long Reasoning Benchmark for Large Language Models", "comment": null, "summary": "Existing long-context benchmarks for Large Language Models (LLMs) focus on\nevaluating comprehension of long inputs, while overlooking the evaluation of\nlong reasoning abilities. To address this gap, we introduce LongReasonArena, a\nbenchmark specifically designed to assess the long reasoning capabilities of\nLLMs. Our tasks require models to solve problems by executing multi-step\nalgorithms that reflect key aspects of long reasoning, such as retrieval and\nbacktracking. By controlling the inputs, the required reasoning length can be\narbitrarily scaled, reaching up to 1 million tokens of reasoning for the most\nchallenging tasks. Extensive evaluation results demonstrate that\nLongReasonArena presents a significant challenge for both open-source and\nproprietary LLMs. For instance, Deepseek-R1 achieves only 7.5% accuracy on our\ntask. Further analysis also reveals that the accuracy exhibits a linear decline\nwith respect to the logarithm of the expected number of reasoning steps. Our\ncode and data is available at\nhttps://github.com/LongReasonArena/LongReasonArena.", "AI": {"tldr": "LongReasonArena\u662f\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u957f\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u591a\u6b65\u7b97\u6cd5\u6267\u884c\u4efb\u52a1\uff0c\u63a8\u7406\u957f\u5ea6\u53ef\u8fbe100\u4e07token\uff0c\u73b0\u6709\u6a21\u578b\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u73b0\u6709\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u957f\u8f93\u5165\u7406\u89e3\uff0c\u800c\u5ffd\u89c6\u4e86\u957f\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\uff0c\u9700\u8981\u4e13\u95e8\u8bbe\u8ba1\u57fa\u51c6\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1\u9700\u8981\u6267\u884c\u591a\u6b65\u7b97\u6cd5\uff08\u5982\u68c0\u7d22\u548c\u56de\u6eaf\uff09\u7684\u4efb\u52a1\uff0c\u901a\u8fc7\u63a7\u5236\u8f93\u5165\u6765\u4efb\u610f\u6269\u5c55\u63a8\u7406\u957f\u5ea6\uff0c\u6700\u9ad8\u53ef\u8fbe100\u4e07token\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u8be5\u57fa\u51c6\u5bf9\u5f00\u6e90\u548c\u4e13\u6709LLM\u90fd\u6784\u6210\u91cd\u5927\u6311\u6218\uff0cDeepseek-R1\u4ec5\u8fbe\u52307.5%\u51c6\u786e\u7387\uff0c\u51c6\u786e\u7387\u968f\u63a8\u7406\u6b65\u6570\u5bf9\u6570\u5448\u7ebf\u6027\u4e0b\u964d\u8d8b\u52bf\u3002", "conclusion": "LongReasonArena\u6709\u6548\u8bc4\u4f30\u4e86LLM\u7684\u957f\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u6b64\u65b9\u9762\u7684\u663e\u8457\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002"}}
{"id": "2508.19372", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19372", "abs": "https://arxiv.org/abs/2508.19372", "authors": ["Zikun Fu", "Chen Yang", "Kourosh Davoudi", "Ken Q. Pu"], "title": "Database Entity Recognition with Data Augmentation and Deep Learning", "comment": "6 pages, 5 figures. Accepted at IEEE 26th International Conference on\n  Information Reuse and Integration for Data Science (IRI 2025), San Jose,\n  California, August 6-8, 2025", "summary": "This paper addresses the challenge of Database Entity Recognition (DB-ER) in\nNatural Language Queries (NLQ). We present several key contributions to advance\nthis field: (1) a human-annotated benchmark for DB-ER task, derived from\npopular text-to-sql benchmarks, (2) a novel data augmentation procedure that\nleverages automatic annotation of NLQs based on the corresponding SQL queries\nwhich are available in popular text-to-SQL benchmarks, (3) a specialized\nlanguage model based entity recognition model using T5 as a backbone and two\ndown-stream DB-ER tasks: sequence tagging and token classification for\nfine-tuning of backend and performing DB-ER respectively. We compared our DB-ER\ntagger with two state-of-the-art NER taggers, and observed better performance\nin both precision and recall for our model. The ablation evaluation shows that\ndata augmentation boosts precision and recall by over 10%, while fine-tuning of\nthe T5 backbone boosts these metrics by 5-10%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u4e2d\u6570\u636e\u5e93\u5b9e\u4f53\u8bc6\u522b(DB-ER)\u7684\u65b0\u65b9\u6cd5\uff0c\u5305\u62ec\u521b\u5efa\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6\u3001\u6570\u636e\u589e\u5f3a\u6280\u672f\u548c\u57fa\u4e8eT5\u7684\u4e13\u95e8\u5b9e\u4f53\u8bc6\u522b\u6a21\u578b\uff0c\u5728\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709NER\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u4e2d\u6570\u636e\u5e93\u5b9e\u4f53\u8bc6\u522b\u7684\u6311\u6218\uff0c\u73b0\u6709NER\u6a21\u578b\u5728\u8be5\u7279\u5b9a\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u6570\u636e\u5e93\u73af\u5883\u7684\u5b9e\u4f53\u8bc6\u522b\u65b9\u6cd5\u3002", "method": "1) \u4ece\u6d41\u884c\u7684text-to-SQL\u57fa\u51c6\u521b\u5efa\u4eba\u5de5\u6807\u6ce8\u7684DB-ER\u57fa\u51c6\uff1b2) \u5229\u7528SQL\u67e5\u8be2\u81ea\u52a8\u6807\u6ce8NLQ\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff1b3) \u57fa\u4e8eT5\u67b6\u6784\u7684\u4e13\u95e8\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u7528\u5e8f\u5217\u6807\u6ce8\u548ctoken\u5206\u7c7b\u4e24\u4e2a\u4e0b\u6e38\u4efb\u52a1\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u63d0\u51fa\u7684DB-ER\u6807\u6ce8\u5668\u5728\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u65b9\u9762\u4f18\u4e8e\u4e24\u79cd\u6700\u5148\u8fdb\u7684NER\u6807\u6ce8\u5668\u3002\u6570\u636e\u589e\u5f3a\u4f7f\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u63d0\u5347\u8d85\u8fc710%\uff0cT5\u9aa8\u5e72\u7f51\u7edc\u5fae\u8c03\u4f7f\u8fd9\u4e9b\u6307\u6807\u63d0\u53475-10%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u5e93\u73af\u5883\u4e0b\u7684\u5b9e\u4f53\u8bc6\u522b\u95ee\u9898\uff0c\u6570\u636e\u589e\u5f3a\u548c\u4e13\u95e8\u6a21\u578b\u67b6\u6784\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86DB-ER\u4efb\u52a1\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2508.19402", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19402", "abs": "https://arxiv.org/abs/2508.19402", "authors": ["Mor Turgeman", "Chen Shani", "Dafna Shahaf"], "title": "One Joke to Rule them All? On the (Im)possibility of Generalizing Humor", "comment": null, "summary": "Humor is a broad and complex form of communication that remains challenging\nfor machines. Despite its broadness, most existing research on computational\nhumor traditionally focused on modeling a specific type of humor. In this work,\nwe wish to understand whether competence on one or more specific humor tasks\nconfers any ability to transfer to novel, unseen types; in other words, is this\nfragmentation inevitable? This question is especially timely as new humor types\ncontinuously emerge in online and social media contexts (e.g., memes,\nanti-humor, AI fails). If Large Language Models (LLMs) are to keep up with this\nevolving landscape, they must be able to generalize across humor types by\ncapturing deeper, transferable mechanisms. To investigate this, we conduct a\nseries of transfer learning experiments across four datasets, representing\ndifferent humor tasks. We train LLMs under varied diversity settings (1-3\ndatasets in training, testing on a novel task). Experiments reveal that models\nare capable of some transfer, and can reach up to 75% accuracy on unseen\ndatasets; training on diverse sources improves transferability (1.88-4.05%)\nwith minimal-to-no drop in in-domain performance. Further analysis suggests\nrelations between humor types, with Dad Jokes surprisingly emerging as the best\nenabler of transfer (but is difficult to transfer to). We release data and\ncode.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u5e7d\u9ed8\u7c7b\u578b\u95f4\u7684\u8fc1\u79fb\u5b66\u4e60\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u4e00\u5b9a\u7a0b\u5ea6\u7684\u8fc1\u79fb\uff0c\u591a\u6e90\u8bad\u7ec3\u53ef\u63d0\u5347\u8fc1\u79fb\u6027\u80fd\uff0c\u5176\u4e2dDad Jokes\u7c7b\u578b\u5728\u4fc3\u8fdb\u8fc1\u79fb\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5e7d\u9ed8\u662f\u590d\u6742\u591a\u6837\u7684\u4ea4\u6d41\u5f62\u5f0f\uff0c\u73b0\u6709\u8ba1\u7b97\u5e7d\u9ed8\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u7279\u5b9a\u7c7b\u578b\u3002\u968f\u7740\u65b0\u578b\u5e7d\u9ed8\u5728\u793e\u4ea4\u5a92\u4f53\u4e2d\u4e0d\u65ad\u6d8c\u73b0\uff0c\u9700\u8981\u7814\u7a76LLMs\u662f\u5426\u80fd\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u6355\u6349\u6df1\u5c42\u53ef\u8f6c\u79fb\u673a\u5236\u6765\u9002\u5e94\u8fd9\u79cd\u6f14\u53d8\u3002", "method": "\u901a\u8fc7\u5728\u56db\u4e2a\u4e0d\u540c\u5e7d\u9ed8\u4efb\u52a1\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u5b9e\u9a8c\uff0c\u8bad\u7ec3LLMs\u5728\u4e0d\u540c\u591a\u6837\u6027\u8bbe\u7f6e\u4e0b\uff081-3\u4e2a\u8bad\u7ec3\u6570\u636e\u96c6\uff09\uff0c\u6d4b\u8bd5\u5728\u65b0\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u4e00\u5b9a\u7a0b\u5ea6\u7684\u8fc1\u79fb\uff0c\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u53ef\u8fbe75%\u51c6\u786e\u7387\uff1b\u591a\u6e90\u8bad\u7ec3\u53ef\u63d0\u53471.88-4.05%\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u4e14\u5bf9\u57df\u5185\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\uff1bDad Jokes\u7c7b\u578b\u6700\u80fd\u4fc3\u8fdb\u8fc1\u79fb\u4f46\u6700\u96be\u88ab\u8fc1\u79fb\u5230\u3002", "conclusion": "\u5e7d\u9ed8\u7c7b\u578b\u95f4\u7684\u8fc1\u79fb\u5b66\u4e60\u662f\u53ef\u884c\u7684\uff0c\u591a\u6e90\u8bad\u7ec3\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3aLLMs\u9002\u5e94\u4e0d\u65ad\u6f14\u53d8\u7684\u5e7d\u9ed8\u666f\u89c2\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2508.19427", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.19427", "abs": "https://arxiv.org/abs/2508.19427", "authors": ["Evandro L. T. P. Cunha"], "title": "A perishable ability? The future of writing in the face of generative artificial intelligence", "comment": "10 pages", "summary": "The 2020s have been witnessing a very significant advance in the development\nof generative artificial intelligence tools, including text generation systems\nbased on large language models. These tools have been increasingly used to\ngenerate texts in the most diverse domains -- from technical texts to literary\ntexts --, which might eventually lead to a lower volume of written text\nproduction by humans. This article discusses the possibility of a future in\nwhich human beings will have lost or significantly decreased their ability to\nwrite due to the outsourcing of this activity to machines. This possibility\nparallels the loss of the ability to write in other moments of human history,\nsuch as during the so-called Greek Dark Ages (approx. 1200 BCE - 800 BCE).", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4eba\u5de5\u667a\u80fd\u6587\u672c\u751f\u6210\u5de5\u5177\u53ef\u80fd\u5bfc\u81f4\u4eba\u7c7b\u5199\u4f5c\u80fd\u529b\u9000\u5316\u7684\u98ce\u9669\uff0c\u4e0e\u53e4\u5e0c\u814a\u9ed1\u6697\u65f6\u4ee3\u6587\u5b57\u80fd\u529b\u4e27\u5931\u7684\u5386\u53f2\u73b0\u8c61\u76f8\u7c7b\u6bd4", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5de5\u5177\u7279\u522b\u662f\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u673a\u5668\u751f\u6210\u7684\u6587\u672c\u5728\u5404\u4e2a\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u53ef\u80fd\u5bfc\u81f4\u4eba\u7c7b\u5199\u4f5c\u6d3b\u52a8\u51cf\u5c11\uff0c\u5f15\u53d1\u5bf9\u4eba\u7c7b\u5199\u4f5c\u80fd\u529b\u53ef\u80fd\u9000\u5316\u751a\u81f3\u4e27\u5931\u7684\u62c5\u5fe7", "method": "\u91c7\u7528\u5386\u53f2\u7c7b\u6bd4\u65b9\u6cd5\uff0c\u5c06\u5f53\u524dAI\u6587\u672c\u751f\u6210\u6280\u672f\u53d1\u5c55\u53ef\u80fd\u5e26\u6765\u7684\u5f71\u54cd\u4e0e\u53e4\u5e0c\u814a\u9ed1\u6697\u65f6\u4ee3\uff08\u7ea6\u516c\u5143\u524d1200-800\u5e74\uff09\u4eba\u7c7b\u6587\u5b57\u80fd\u529b\u4e27\u5931\u7684\u5386\u53f2\u73b0\u8c61\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790", "result": "\u8bc6\u522b\u51faAI\u6587\u672c\u751f\u6210\u5de5\u5177\u7684\u666e\u53ca\u53ef\u80fd\u5bfc\u81f4\u4eba\u7c7b\u5199\u4f5c\u80fd\u529b\u9010\u6e10\u9000\u5316\uff0c\u7c7b\u4f3c\u4e8e\u5386\u53f2\u4e0a\u67d0\u4e9b\u65f6\u671f\u4eba\u7c7b\u6587\u5b57\u80fd\u529b\u7684\u4e27\u5931\uff0c\u8fd9\u79cd\u6280\u672f\u53d1\u5c55\u53ef\u80fd\u5e26\u6765\u610f\u60f3\u4e0d\u5230\u7684\u6587\u5316\u548c\u8ba4\u77e5\u540e\u679c", "conclusion": "\u9700\u8981\u8b66\u60d5\u8fc7\u5ea6\u4f9d\u8d56AI\u6587\u672c\u751f\u6210\u5de5\u5177\u53ef\u80fd\u5bfc\u81f4\u7684\u4eba\u7c7b\u5199\u4f5c\u80fd\u529b\u9000\u5316\u98ce\u9669\uff0c\u5efa\u8bae\u5728\u4eab\u53d7\u6280\u672f\u4fbf\u5229\u7684\u540c\u65f6\u4fdd\u6301\u4eba\u7c7b\u81ea\u8eab\u7684\u5199\u4f5c\u5b9e\u8df5\u548c\u80fd\u529b\u53d1\u5c55"}}
{"id": "2508.19428", "categories": ["cs.CL", "cs.LO", "cs.SC", "68T30, 68T50, 68T07, 68U15", "I.2.4; I.2.7; H.3.1; H.3.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.19428", "abs": "https://arxiv.org/abs/2508.19428", "authors": ["Aleksandra Beliaeva", "Temurbek Rahmatullaev"], "title": "Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting, Ensemble Typing, and Attention-Based Taxonomies)", "comment": null, "summary": "We present a comprehensive system for addressing Tasks A, B, and C of the\nLLMs4OL 2025 challenge, which together span the full ontology construction\npipeline: term extraction, typing, and taxonomy discovery. Our approach\ncombines retrieval-augmented prompting, zero-shot classification, and\nattention-based graph modeling -- each tailored to the demands of the\nrespective task. For Task A, we jointly extract domain-specific terms and their\nontological types using a retrieval-augmented generation (RAG) pipeline.\nTraining data was reformulated into a document to terms and types\ncorrespondence, while test-time inference leverages semantically similar\ntraining examples. This single-pass method requires no model finetuning and\nimproves overall performance through lexical augmentation Task B, which\ninvolves assigning types to given terms, is handled via a dual strategy. In the\nfew-shot setting (for domains with labeled training data), we reuse the RAG\nscheme with few-shot prompting. In the zero-shot setting (for previously unseen\ndomains), we use a zero-shot classifier that combines cosine similarity scores\nfrom multiple embedding models using confidence-based weighting. In Task C, we\nmodel taxonomy discovery as graph inference. Using embeddings of type labels,\nwe train a lightweight cross-attention layer to predict is-a relations by\napproximating a soft adjacency matrix. These modular, task-specific solutions\nenabled us to achieve top-ranking results in the official leaderboard across\nall three tasks. Taken together these strategies showcase the scalability,\nadaptability, and robustness of LLM-based architectures for ontology learning\nacross heterogeneous domains.\n  Code is available at:\nhttps://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u63d0\u793a\u3001\u96f6\u68ad\u5206\u7c7b\u548c\u6ce8\u610f\u529b\u56fe\u8c31\u6a21\u578b\uff0c\u5728LLMs4OL 2025\u6311\u6218\u8d5b\u7684\u4e09\u4e2a\u4efb\u52a1\uff08\u672f\u8bed\u63d0\u53d6\u3001\u7c7b\u578b\u5206\u914d\u548c\u5206\u7c7b\u7cfb\u53d1\u73b0\uff09\u4e2d\u83b7\u5f97\u4e86\u9886\u5148\u6210\u7ee9\u3002", "motivation": "\u89e3\u51b3\u672c\u4f53\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u9886\u57df\u7279\u5b9a\u672f\u8bed\u63d0\u53d6\u3001\u7c7b\u578b\u5206\u914d\u548c\u5206\u7c7b\u7cfb\u6784\u5efa\uff0c\u5c55\u73b0LLM\u57fa\u4e8e\u67b6\u6784\u5728\u5f02\u6784\u9886\u57df\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3001\u9002\u5e94\u6027\u548c\u7a33\u5065\u6027\u3002", "method": "\u4efb\u52a1A\uff1a\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u6d41\u6c34\u7ebf\u8054\u5408\u63d0\u53d6\u672f\u8bed\u548c\u7c7b\u578b\uff1b\u4efb\u52a1B\uff1a\u5728\u5c11\u68ad\u8bbe\u7f6e\u4e2d\u91cd\u7528RAG\u65b9\u6848\uff0c\u5728\u96f6\u68ad\u8bbe\u7f6e\u4e2d\u4f7f\u7528\u591a\u6a21\u578b\u5f39\u6027\u8ddd\u79bb\u7ec4\u5408\u7684\u96f6\u68ad\u5206\u7c7b\u5668\uff1b\u4efb\u52a1C\uff1a\u901a\u8fc7\u8bed\u4e49\u5d4c\u5165\u548c\u8de8\u6ce8\u610f\u529b\u5c42\u5c06\u5206\u7c7b\u7cfb\u53d1\u73b0\u6a21\u578b\u5316\u4e3a\u56fe\u8c31\u63a8\u7406\u95ee\u9898\u3002", "result": "\u8be5\u7cfb\u7edf\u5728\u5b98\u65b9\u6392\u884c\u699c\u4e2d\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u5747\u83b7\u5f97\u4e86\u6700\u9ad8\u6392\u540d\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u52bf\u3002", "conclusion": "\u8fd9\u4e9b\u6a21\u5757\u5316\u7684\u4efb\u52a1\u7279\u5b9a\u89e3\u51b3\u65b9\u6848\u5c55\u793a\u4e86LLM\u57fa\u7840\u67b6\u6784\u5728\u5f02\u8d28\u9886\u57df\u672c\u4f53\u5b66\u4e60\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3001\u9002\u5e94\u6027\u548c\u7a33\u5065\u6027\uff0c\u4e3a\u672c\u4f53\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2508.19464", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19464", "abs": "https://arxiv.org/abs/2508.19464", "authors": ["Philipp Borchert", "Jochen De Weerdt", "Marie-Francine Moens"], "title": "Bridging Language Gaps: Enhancing Few-Shot Language Adaptation", "comment": "17 pages", "summary": "The disparity in language resources poses a challenge in multilingual NLP,\nwith high-resource languages benefiting from extensive data, while low-resource\nlanguages lack sufficient data for effective training. Our Contrastive Language\nAlignment with Prompting (CoLAP) method addresses this gap by integrating\ncontrastive learning with cross-lingual representations, facilitating\ntask-specific knowledge transfer from high-resource to lower-resource\nlanguages. The primary advantage of our approach is its data efficiency,\nenabling rapid adaptation to new languages and reducing the need for large\nlabeled datasets. We conduct experiments with multilingual encoder-only and\ndecoder-only language models on natural language understanding tasks, including\nnatural language inference and relation extraction, evaluating performance\nacross both high- and low-resource languages. Our results demonstrate that\nCoLAP outperforms few-shot cross-lingual transfer baselines and in-context\nlearning, even with limited available data. This effectively narrows the\ncross-lingual performance gap, contributing to the development of more\nefficient multilingual NLP techniques.", "AI": {"tldr": "CoLAP\u65b9\u6cd5\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u8de8\u8bed\u8a00\u8868\u793a\u6574\u5408\uff0c\u5b9e\u73b0\u4e86\u4ece\u9ad8\u8d44\u6e90\u8bed\u8a00\u5230\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u8fc1\u79fb\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u8bed\u8a00NLP\u7684\u6570\u636e\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00NLP\u4e2d\u8bed\u8a00\u8d44\u6e90\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u9ad8\u8d44\u6e90\u8bed\u8a00\u6570\u636e\u4e30\u5bcc\u800c\u4f4e\u8d44\u6e90\u8bed\u8a00\u6570\u636e\u532e\u4e4f\uff0c\u5bfc\u81f4\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u5bf9\u6bd4\u8bed\u8a00\u5bf9\u9f50\u63d0\u793a\u65b9\u6cd5(CoLAP)\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u8de8\u8bed\u8a00\u8868\u793a\uff0c\u4fc3\u8fdb\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u4ece\u9ad8\u8d44\u6e90\u8bed\u8a00\u5411\u4f4e\u8d44\u6e90\u8bed\u8a00\u8fc1\u79fb\u3002", "result": "\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u548c\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u4e0a\uff0cCoLAP\u8d85\u8d8a\u4e86\u5c11\u6837\u672c\u8de8\u8bed\u8a00\u8fc1\u79fb\u57fa\u7ebf\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u6709\u9650\u6570\u636e\u4e0b\u4e5f\u80fd\u6709\u6548\u7f29\u5c0f\u8de8\u8bed\u8a00\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "CoLAP\u65b9\u6cd5\u4e3a\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u591a\u8bed\u8a00NLP\u6280\u672f\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u8bed\u8a00\u9002\u5e94\u5e76\u51cf\u5c11\u4e86\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u9700\u6c42\u3002"}}
{"id": "2508.19467", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.19467", "abs": "https://arxiv.org/abs/2508.19467", "authors": ["Sumon Kanti Dey", "Jeanne M. Powell", "Azra Ismail", "Jeanmarie Perrone", "Abeed Sarker"], "title": "Inference Gap in Domain Expertise and Machine Intelligence in Named Entity Recognition: Creation of and Insights from a Substance Use-related Dataset", "comment": "Dataset and code: https://github.com/SumonKantiDey/Reddit_Impacts_NER", "summary": "Nonmedical opioid use is an urgent public health challenge, with far-reaching\nclinical and social consequences that are often underreported in traditional\nhealthcare settings. Social media platforms, where individuals candidly share\nfirst-person experiences, offer a valuable yet underutilized source of insight\ninto these impacts. In this study, we present a named entity recognition (NER)\nframework to extract two categories of self-reported consequences from social\nmedia narratives related to opioid use: ClinicalImpacts (e.g., withdrawal,\ndepression) and SocialImpacts (e.g., job loss). To support this task, we\nintroduce RedditImpacts 2.0, a high-quality dataset with refined annotation\nguidelines and a focus on first-person disclosures, addressing key limitations\nof prior work. We evaluate both fine-tuned encoder-based models and\nstate-of-the-art large language models (LLMs) under zero- and few-shot\nin-context learning settings. Our fine-tuned DeBERTa-large model achieves a\nrelaxed token-level F1 of 0.61 [95% CI: 0.43-0.62], consistently outperforming\nLLMs in precision, span accuracy, and adherence to task-specific guidelines.\nFurthermore, we show that strong NER performance can be achieved with\nsubstantially less labeled data, emphasizing the feasibility of deploying\nrobust models in resource-limited settings. Our findings underscore the value\nof domain-specific fine-tuning for clinical NLP tasks and contribute to the\nresponsible development of AI tools that may enhance addiction surveillance,\nimprove interpretability, and support real-world healthcare decision-making.\nThe best performing model, however, still significantly underperforms compared\nto inter-expert agreement (Cohen's kappa: 0.81), demonstrating that a gap\npersists between expert intelligence and current state-of-the-art NER/AI\ncapabilities for tasks requiring deep domain knowledge.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u6846\u67b6\uff0c\u4ece\u793e\u4ea4\u5a92\u4f53\u4e2d\u63d0\u53d6\u975e\u533b\u7597\u7c7b\u963f\u7247\u7c7b\u836f\u7269\u4f7f\u7528\u7684\u4e34\u5e8a\u548c\u793e\u4f1a\u5f71\u54cd\uff0c\u5e76\u4e3b\u5f20\u9886\u57df\u7279\u5b9a\u7cbe\u7ec6\u8c03\u6574\u6a21\u578b\u7684\u4f18\u52bf\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u7528\u6237\u81ea\u53d1\u5206\u4eab\u7684\u7b2c\u4e00\u624b\u7ecf\u9a8c\u63d0\u4f9b\u4e86\u4f20\u7edf\u533b\u7597\u573a\u666f\u4e2d\u7f3a\u4e4f\u7684\u6709\u4ef7\u503c\u89c1\u89e3\uff0c\u4f46\u8fd9\u4e9b\u4fe1\u606f\u5f80\u5f80\u88ab\u5ffd\u89c6\u3002\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7NLP\u6280\u672f\u63d0\u53d6\u8fd9\u4e9b\u81ea\u6211\u62a5\u544a\u7684\u540e\u679c\u4fe1\u606f\u3002", "method": "\u6784\u5efa\u4e86RedditImpacts 2.0\u6570\u636e\u96c6\uff0c\u5305\u542b\u7cbe\u7ec6\u7684\u6ce8\u91ca\u6307\u5357\u548c\u7b2c\u4e00\u4eba\u79f0\u8ff0\u3002\u8bc4\u4f30\u4e86\u7cbe\u7ec6\u8c03\u6574\u7684\u7f16\u7801\u5668\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\u3002\u6700\u4f73\u6a21\u578b\u662f\u7cbe\u7ec6\u8c03\u6574\u7684DeBERTa-large\u3002", "result": "\u7cbe\u7ec6\u8c03\u6574\u7684DeBERTa-large\u6a21\u578b\u5728\u677e\u5f1b\u7684token\u7ea7F1\u5f97\u5206\u4e3a0.61\uff0c\u5728\u7cbe\u786e\u5ea6\u3001\u5b57\u7b26\u4e32\u51c6\u786e\u6027\u548c\u4efb\u52a1\u6307\u5357\u9075\u5faa\u65b9\u9762\u5747\u8d85\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\u53ef\u4ee5\u4f7f\u7528\u66f4\u5c11\u7684\u6807\u7b7e\u6570\u636e\u8fbe\u5230\u5f3a\u52b2\u7684NER\u6027\u80fd\u3002", "conclusion": "\u9886\u57df\u7279\u5b9a\u7684\u7cbe\u7ec6\u8c03\u6574\u5bf9\u4e34\u5e8aNLP\u4efb\u52a1\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u6700\u4f73\u6a21\u578b\u4ecd\u663e\u8457\u843d\u540e\u4e8e\u4e13\u5bb6\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\uff08Cohen's kappa: 0.81\uff09\uff0c\u8bf4\u660e\u5728\u9700\u8981\u6df1\u5ea6\u9886\u57df\u77e5\u8bc6\u7684\u4efb\u52a1\u4e2d\uff0c\u4eba\u5de5\u667a\u80fd\u4e0e\u4e13\u5bb6\u667a\u80fd\u4e4b\u95f4\u4ecd\u5b58\u5728\u5dee\u8ddd\u3002"}}
{"id": "2508.19475", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19475", "abs": "https://arxiv.org/abs/2508.19475", "authors": ["Md. Alvee Ehsan", "A. S. M Mehedi Hasan", "Kefaya Benta Shahnoor", "Syeda Sumaiya Tasneem"], "title": "Automatic Question & Answer Generation Using Generative Large Language Model (LLM)", "comment": null, "summary": "\\Abstract{In the realm of education, student evaluation holds equal\nsignificance as imparting knowledge. To be evaluated, students usually need to\ngo through text-based academic assessment methods. Instructors need to make\ndiverse sets of questions that need to be fair for all students to prove their\nadequacy over a particular topic. This can prove to be quite challenging as\nthey may need to manually go through several different lecture materials. Our\nobjective is to make this whole process much easier by implementing Automatic\nQuestion Answer Generation /(AQAG), using fine-tuned generative LLM. For\ntailoring the instructor's preferred question style (MCQ, conceptual, or\nfactual questions), prompt Engineering (PE) is being utilized. In this\nresearch, we propose to leverage unsupervised learning methods in NLP,\nprimarily focusing on the English language. This approach empowers the base\nMeta-Llama 2-7B model to integrate RACE dataset as training data for the\nfine-tuning process. Creating a customized model that will offer efficient\nsolutions for educators, instructors, and individuals engaged in text-based\nevaluations. A reliable and efficient tool for generating questions and answers\ncan free up valuable time and resources, thus streamlining their evaluation\nprocesses.}", "AI": {"tldr": "\u4f7f\u7528\u5fae\u8c03\u7684LLaMA 2-7B\u6a21\u578b\u548cRACE\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u5b9e\u73b0\u81ea\u52a8\u95ee\u7b54\u751f\u6210\uff0c\u4e3a\u6559\u80b2\u5de5\u4f5c\u8005\u63d0\u4f9b\u5b9a\u5236\u5316\u7684\u95ee\u9898\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u7684\u5b66\u751f\u8bc4\u4f30\u9700\u8981\u6559\u5e08\u624b\u52a8\u521b\u5efa\u591a\u6837\u5316\u4e14\u516c\u5e73\u7684\u95ee\u9898\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u8017\u65f6\u4e14\u5177\u6709\u6311\u6218\u6027\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u95ee\u7b54\u751f\u6210\u6280\u672f\u7b80\u5316\u8bc4\u4f30\u6d41\u7a0b\uff0c\u89e3\u653e\u6559\u80b2\u5de5\u4f5c\u8005\u7684\u65f6\u95f4\u548c\u8d44\u6e90\u3002", "method": "\u91c7\u7528\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u57fa\u4e8eMeta-Llama 2-7B\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u7528RACE\u6570\u636e\u96c6\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u6765\u5b9a\u5236\u4e0d\u540c\u7c7b\u578b\u7684\u95ee\u9898\uff08\u9009\u62e9\u9898\u3001\u6982\u5ff5\u9898\u6216\u4e8b\u5b9e\u9898\uff09\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9a\u5236\u5316\u7684\u81ea\u52a8\u95ee\u7b54\u751f\u6210\u6a21\u578b\uff0c\u80fd\u591f\u6839\u636e\u6559\u5e08\u504f\u597d\u7684\u95ee\u9898\u98ce\u683c\u751f\u6210\u76f8\u5e94\u7684\u8bc4\u4f30\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9760\u9ad8\u6548\u7684\u95ee\u7b54\u751f\u6210\u5de5\u5177\uff0c\u80fd\u591f\u663e\u8457\u7b80\u5316\u6559\u80b2\u8bc4\u4f30\u6d41\u7a0b\uff0c\u4e3a\u6559\u80b2\u5de5\u4f5c\u8005\u8282\u7701\u5b9d\u8d35\u7684\u65f6\u95f4\u548c\u8d44\u6e90\u3002"}}
{"id": "2508.19481", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19481", "abs": "https://arxiv.org/abs/2508.19481", "authors": ["Manuel Mosquera", "Melissa Robles", "Johan Rodriguez", "Ruben Manrique"], "title": "Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and RL: A Spanish-to-Wayuunaiki Study", "comment": null, "summary": "Low-resource machine translation remains a significant challenge for large\nlanguage models (LLMs), which often lack exposure to these languages during\npretraining and have limited parallel data for fine-tuning. We propose a novel\napproach that enhances translation for low-resource languages by integrating an\nexternal dictionary tool and training models end-to-end using reinforcement\nlearning, in addition to supervised fine-tuning. Focusing on the\nSpanish-Wayuunaiki language pair, we frame translation as a tool-augmented\ndecision-making problem in which the model can selectively consult a bilingual\ndictionary during generation. Our method combines supervised instruction tuning\nwith Guided Reward Policy Optimization (GRPO), enabling the model to learn both\nwhen and how to use the tool effectively. BLEU similarity scores are used as\nrewards to guide this learning process. Preliminary results show that our\ntool-augmented models achieve up to +3.37 BLEU improvement over previous work,\nand a 18% relative gain compared to a supervised baseline without dictionary\naccess, on the Spanish-Wayuunaiki test set from the AmericasNLP 2025 Shared\nTask. We also conduct ablation studies to assess the effects of model\narchitecture and training strategy, comparing Qwen2.5-0.5B-Instruct with other\nmodels such as LLaMA and a prior NLLB-based system. These findings highlight\nthe promise of combining LLMs with external tools and the role of reinforcement\nlearning in improving translation quality in low-resource language settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5916\u90e8\u8bcd\u5178\u5de5\u5177\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u4f4e\u8d44\u6e90\u673a\u5668\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u5728\u897f\u73ed\u7259\u8bed-Wayuunaiki\u8bed\u8a00\u5bf9\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457BLEU\u5206\u6570\u63d0\u5347", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u4e2d\u9762\u4e34\u7684\u9884\u8bad\u7ec3\u6570\u636e\u7f3a\u4e4f\u548c\u5e73\u884c\u8bed\u6599\u6709\u9650\u7684\u95ee\u9898", "method": "\u5c06\u7ffb\u8bd1\u5efa\u6a21\u4e3a\u5de5\u5177\u589e\u5f3a\u7684\u51b3\u7b56\u95ee\u9898\uff0c\u7ed3\u5408\u76d1\u7763\u6307\u4ee4\u5fae\u8c03\u548cGRPO\u5f3a\u5316\u5b66\u4e60\uff0c\u8ba9\u6a21\u578b\u5b66\u4f1a\u9009\u62e9\u6027\u4f7f\u7528\u53cc\u8bed\u8bcd\u5178", "result": "\u76f8\u6bd4\u4e4b\u524d\u5de5\u4f5c\u63d0\u5347+3.37 BLEU\uff0c\u76f8\u6bd4\u65e0\u8bcd\u5178\u8bbf\u95ee\u7684\u76d1\u7763\u57fa\u7ebf\u76f8\u5bf9\u63d0\u534718%", "conclusion": "\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u5916\u90e8\u5de5\u5177\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b"}}
{"id": "2508.19484", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19484", "abs": "https://arxiv.org/abs/2508.19484", "authors": ["Bahar Bateni", "Benjamin Pratt", "Jim Whitehead"], "title": "Rule Synergy Analysis using LLMs: State of the Art and Implications", "comment": "Submitted for publication at the IEEE Transactions on Games 2024,\n  Special Issue on Large Language Models and Games (10 pages excluding\n  appendix, 3 figures)", "summary": "Large language models (LLMs) have demonstrated strong performance across a\nvariety of domains, including logical reasoning, mathematics, and more. In this\npaper, we investigate how well LLMs understand and reason about complex rule\ninteractions in dynamic environments, such as card games. We introduce a\ndataset of card synergies from the game Slay the Spire, where pairs of cards\nare classified based on their positive, negative, or neutral interactions. Our\nevaluation shows that while LLMs excel at identifying non-synergistic pairs,\nthey struggle with detecting positive and, particularly, negative synergies. We\ncategorize common error types, including issues with timing, defining game\nstates, and following game rules. Our findings suggest directions for future\nresearch to improve model performance in predicting the effect of rules and\ntheir interactions.", "AI": {"tldr": "LLMs\u5728\u5361\u724c\u6e38\u620f\u534f\u540c\u6548\u5e94\u8bc6\u522b\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u68c0\u6d4b\u6b63\u8d1f\u534f\u540c\u6548\u5e94\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u9519\u8bef\u5305\u62ec\u65f6\u5e8f\u7406\u89e3\u3001\u6e38\u620f\u72b6\u6001\u5b9a\u4e49\u548c\u89c4\u5219\u9075\u5faa\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\uff08\u5982\u5361\u724c\u6e38\u620f\uff09\u4e2d\u7406\u89e3\u548c\u63a8\u7406\u590d\u6742\u89c4\u5219\u4ea4\u4e92\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5bf9\u5361\u724c\u534f\u540c\u6548\u5e94\u7684\u8bc6\u522b\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u6e38\u620fSlay the Spire\u7684\u5361\u724c\u534f\u540c\u6548\u5e94\u6570\u636e\u96c6\uff0c\u5bf9\u5361\u724c\u5bf9\u7684\u6b63\u9762\u3001\u8d1f\u9762\u6216\u4e2d\u6027\u4ea4\u4e92\u8fdb\u884c\u5206\u7c7b\uff0c\u8bc4\u4f30LLMs\u7684\u8868\u73b0\u3002", "result": "LLMs\u64c5\u957f\u8bc6\u522b\u975e\u534f\u540c\u5361\u724c\u5bf9\uff0c\u4f46\u5728\u68c0\u6d4b\u6b63\u9762\u534f\u540c\u6548\u5e94\uff08\u7279\u522b\u662f\u8d1f\u9762\u534f\u540c\u6548\u5e94\uff09\u65b9\u9762\u8868\u73b0\u8f83\u5dee\uff0c\u5b58\u5728\u65f6\u5e8f\u3001\u6e38\u620f\u72b6\u6001\u5b9a\u4e49\u548c\u89c4\u5219\u9075\u5faa\u7b49\u5e38\u89c1\u9519\u8bef\u7c7b\u578b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u672a\u6765\u6539\u8fdb\u6a21\u578b\u5728\u9884\u6d4b\u89c4\u5219\u6548\u679c\u53ca\u5176\u4ea4\u4e92\u65b9\u9762\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u8868\u660e\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u63d0\u5347LLMs\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2508.19529", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19529", "abs": "https://arxiv.org/abs/2508.19529", "authors": ["Bowen Sun", "Yujun Cai", "Ming-Hsuan Yang", "Yiwei Wang"], "title": "Blockwise SFT for Diffusion Language Models: Reconciling Bidirectional Attention and Autoregressive Decoding", "comment": null, "summary": "Discrete diffusion language models have shown strong potential for text\ngeneration, yet standard supervised fine-tuning (SFT) misaligns with their\nsemi-autoregressive inference: training randomly masks tokens across the entire\nresponse, while inference generates fixed-size blocks sequentially. This\nmismatch introduces noisy prefixes and leaky suffixes, biasing gradients away\nfrom the desired blockwise likelihood. We propose Blockwise SFT, which\npartitions responses into fixed-size blocks, selects one active block per step\nfor stochastic masking, freezes all preceding tokens, and fully hides future\nones. Loss is computed only over the active block, directly mirroring the\nblockwise decoding process. Experiments on GSM8K, MATH, and MetaMathQA show\nconsistent gains over classical SFT under equal compute or token budgets. Block\nsize consistency studies and ablations confirm that improvements stem from\nfaithful training-inference alignment rather than incidental masking effects.\nOur results highlight the importance of matching supervision granularity to the\ndecoding procedure in diffusion-based language models.", "AI": {"tldr": "\u63d0\u51fa\u4e86Blockwise SFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u54cd\u5e94\u5212\u5206\u4e3a\u56fa\u5b9a\u5927\u5c0f\u7684\u5757\uff0c\u6bcf\u4e2a\u6b65\u9aa4\u53ea\u5bf9\u4e00\u4e2a\u6d3b\u52a8\u5757\u8fdb\u884c\u968f\u673a\u63a9\u7801\uff0c\u51bb\u7ed3\u524d\u9762\u6240\u6709token\u5e76\u5b8c\u5168\u9690\u85cf\u672a\u6765token\uff0c\u4f7f\u79bb\u6563\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\u4e0e\u534a\u81ea\u56de\u5f52\u63a8\u7406\u8fc7\u7a0b\u5bf9\u9f50\u3002", "motivation": "\u6807\u51c6\u76d1\u7763\u5fae\u8c03(SFT)\u4e0e\u79bb\u6563\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u534a\u81ea\u56de\u5f52\u63a8\u7406\u5b58\u5728\u4e0d\u5339\u914d\uff1a\u8bad\u7ec3\u65f6\u5728\u6574\u4e2a\u54cd\u5e94\u4e2d\u968f\u673a\u63a9\u7801token\uff0c\u800c\u63a8\u7406\u65f6\u6309\u56fa\u5b9a\u5927\u5c0f\u7684\u5757\u987a\u5e8f\u751f\u6210\uff0c\u8fd9\u79cd\u4e0d\u5339\u914d\u5bfc\u81f4\u566a\u58f0\u524d\u7f00\u548c\u6cc4\u9732\u540e\u7f00\uff0c\u4f7f\u68af\u5ea6\u504f\u79bb\u671f\u671b\u7684\u5757\u7ea7\u4f3c\u7136\u3002", "method": "Blockwise SFT\u65b9\u6cd5\u5c06\u54cd\u5e94\u5212\u5206\u4e3a\u56fa\u5b9a\u5927\u5c0f\u7684\u5757\uff0c\u6bcf\u4e2a\u8bad\u7ec3\u6b65\u9aa4\u9009\u62e9\u4e00\u4e2a\u6d3b\u52a8\u5757\u8fdb\u884c\u968f\u673a\u63a9\u7801\uff0c\u51bb\u7ed3\u6240\u6709\u524d\u9762\u7684token\u5e76\u5b8c\u5168\u9690\u85cf\u672a\u6765\u7684token\uff0c\u635f\u5931\u53ea\u8ba1\u7b97\u5728\u6d3b\u52a8\u5757\u4e0a\uff0c\u76f4\u63a5\u53cd\u6620\u5757\u7ea7\u89e3\u7801\u8fc7\u7a0b\u3002", "result": "\u5728GSM8K\u3001MATH\u548cMetaMathQA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u76f8\u540c\u8ba1\u7b97\u6216token\u9884\u7b97\u4e0b\uff0cBlockwise SFT\u76f8\u6bd4\u7ecf\u5178SFT\u83b7\u5f97\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002\u5757\u5927\u5c0f\u4e00\u81f4\u6027\u7814\u7a76\u548c\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u6539\u8fdb\u6e90\u4e8e\u8bad\u7ec3-\u63a8\u7406\u5bf9\u9f50\u800c\u975e\u5076\u7136\u7684\u63a9\u7801\u6548\u5e94\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u57fa\u4e8e\u6269\u6563\u7684\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u5c06\u76d1\u7763\u7c92\u5ea6\u4e0e\u89e3\u7801\u8fc7\u7a0b\u5339\u914d\u7684\u91cd\u8981\u6027\uff0cBlockwise SFT\u901a\u8fc7\u7cbe\u786e\u5bf9\u9f50\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u79bb\u6563\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u751f\u6210\u6027\u80fd\u3002"}}
{"id": "2508.19532", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19532", "abs": "https://arxiv.org/abs/2508.19532", "authors": ["Houxing Ren", "Zimu Lu", "Weikang Shi", "Haotian Hou", "Yunqiao Yang", "Ke Wang", "Aojun Zhou", "Junting Pan", "Mingjie Zhan", "Hongsheng Li"], "title": "Alignment with Fill-In-the-Middle for Enhancing Code Generation", "comment": "Accepted to EMNLP 2025 (main conference)", "summary": "The code generation capabilities of Large Language Models (LLMs) have\nadvanced applications like tool invocation and problem-solving. However,\nimproving performance in code-related tasks remains challenging due to limited\ntraining data that is verifiable with accurate test cases. While Direct\nPreference Optimization (DPO) has shown promise, existing methods for\ngenerating test cases still face limitations. In this paper, we propose a novel\napproach that splits code snippets into smaller, granular blocks, creating more\ndiverse DPO pairs from the same test cases. Additionally, we introduce the\nAbstract Syntax Tree (AST) splitting and curriculum training method to enhance\nthe DPO training. Our approach demonstrates significant improvements in code\ngeneration tasks, as validated by experiments on benchmark datasets such as\nHumanEval (+), MBPP (+), APPS, LiveCodeBench, and BigCodeBench. Code and data\nare available at https://github.com/SenseLLM/StructureCoder.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7801\u7247\u6bb5\u7ec6\u7c92\u5ea6\u5206\u5272\u548cAST\u7ed3\u6784\u7684DPO\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4ee3\u7801\u62c6\u5206\u6210\u66f4\u5c0f\u7684\u5757\u6765\u751f\u6210\u66f4\u591a\u6837\u5316\u7684\u8bad\u7ec3\u5bf9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u73b0\u6709\u7684\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u4f18\u5316\u7b56\u7565\u3002", "method": "\u91c7\u7528\u4ee3\u7801\u7247\u6bb5\u7ec6\u7c92\u5ea6\u5206\u5272\u6280\u672f\uff0c\u5c06\u4ee3\u7801\u62c6\u5206\u6210\u66f4\u5c0f\u7684\u5757\u6765\u521b\u5efa\u591a\u6837\u5316\u7684DPO\u8bad\u7ec3\u5bf9\uff1b\u5f15\u5165\u62bd\u8c61\u8bed\u6cd5\u6811(AST)\u5206\u5272\u548c\u8bfe\u7a0b\u8bad\u7ec3\u65b9\u6cd5\u6765\u589e\u5f3aDPO\u8bad\u7ec3\u6548\u679c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5305\u62ecHumanEval\u3001MBPP\u3001APPS\u3001LiveCodeBench\u548cBigCodeBench\uff0c\u5747\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ed3\u6784\u5316\u4ee3\u7801\u5206\u5272\u548c\u8bfe\u7a0b\u8bad\u7ec3\u65b9\u6cd5\u4e3a\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u7684DPO\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2508.19533", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19533", "abs": "https://arxiv.org/abs/2508.19533", "authors": ["Kun Peng", "Cong Cao", "Hao Peng", "Guanlin Wu", "Zhifeng Hao", "Lei Jiang", "Yanbing Liu", "Philip S. Yu"], "title": "Emotion Transfer with Enhanced Prototype for Unseen Emotion Recognition in Conversation", "comment": "Accepted at EMNLP2025", "summary": "Current Emotion Recognition in Conversation (ERC) research follows a\nclosed-domain assumption. However, there is no clear consensus on emotion\nclassification in psychology, which presents a challenge for models when it\ncomes to recognizing previously unseen emotions in real-world applications. To\nbridge this gap, we introduce the Unseen Emotion Recognition in Conversation\n(UERC) task for the first time and propose ProEmoTrans, a solid prototype-based\nemotion transfer framework. This prototype-based approach shows promise but\nstill faces key challenges: First, implicit expressions complicate emotion\ndefinition, which we address by proposing an LLM-enhanced description approach.\nSecond, utterance encoding in long conversations is difficult, which we tackle\nwith a proposed parameter-free mechanism for efficient encoding and overfitting\nprevention. Finally, the Markovian flow nature of emotions is hard to transfer,\nwhich we address with an improved Attention Viterbi Decoding (AVD) method to\ntransfer seen emotion transitions to unseen emotions. Extensive experiments on\nthree datasets show that our method serves as a strong baseline for preliminary\nexploration in this new area.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u672a\u89c1\u60c5\u611f\u8bc6\u522b\u5bf9\u8bdd\u4efb\u52a1(UERC)\u548c\u539f\u578b\u60c5\u611f\u8fc1\u79fb\u6846\u67b6ProEmoTrans\uff0c\u901a\u8fc7LLM\u589e\u5f3a\u63cf\u8ff0\u3001\u53c2\u6570\u81ea\u7531\u7f16\u7801\u673a\u5236\u548c\u6539\u8fdb\u7684\u6ce8\u610f\u529b\u7ef4\u7279\u6bd4\u89e3\u7801\u6765\u89e3\u51b3\u9690\u5f0f\u8868\u8fbe\u3001\u957f\u5bf9\u8bdd\u7f16\u7801\u548c\u60c5\u611f\u8f6c\u79fb\u7b49\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u60c5\u611f\u8bc6\u522b\u5bf9\u8bdd\u7814\u7a76\u57fa\u4e8e\u5c01\u95ed\u9886\u57df\u5047\u8bbe\uff0c\u4f46\u5fc3\u7406\u5b66\u4e2d\u60c5\u611f\u5206\u7c7b\u7f3a\u4e4f\u660e\u786e\u5171\u8bc6\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u6a21\u578b\u96be\u4ee5\u8bc6\u522b\u672a\u89c1\u60c5\u611f\u7c7b\u578b\u3002", "method": "\u63d0\u51faProEmoTrans\u6846\u67b6\uff1a1\uff09LLM\u589e\u5f3a\u63cf\u8ff0\u5904\u7406\u9690\u5f0f\u8868\u8fbe\uff1b2\uff09\u53c2\u6570\u81ea\u7531\u7f16\u7801\u673a\u5236\u5904\u7406\u957f\u5bf9\u8bdd\uff1b3\uff09\u6539\u8fdb\u6ce8\u610f\u529b\u7ef4\u7279\u6bd4\u89e3\u7801\u8f6c\u79fb\u60c5\u611f\u72b6\u6001\u8f6c\u6362\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65b0\u9886\u57df\u521d\u6b65\u63a2\u7d22\u4e2d\u4f5c\u4e3a\u5f3a\u57fa\u7ebf\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u5b9a\u4e49\u4e86UERC\u4efb\u52a1\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u5f00\u653e\u57df\u60c5\u611f\u8bc6\u522b\u5bf9\u8bdd\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.19546", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19546", "abs": "https://arxiv.org/abs/2508.19546", "authors": ["Jio Choi", "Mohit Bansal", "Elias Stengel-Eskin"], "title": "Language Models Identify Ambiguities and Exploit Loopholes", "comment": "EMNLP 2025 camera-ready; Code:\n  https://github.com/esteng/ambiguous-loophole-exploitation", "summary": "Studying the responses of large language models (LLMs) to loopholes presents\na two-fold opportunity. First, it affords us a lens through which to examine\nambiguity and pragmatics in LLMs, since exploiting a loophole requires\nidentifying ambiguity and performing sophisticated pragmatic reasoning. Second,\nloopholes pose an interesting and novel alignment problem where the model is\npresented with conflicting goals and can exploit ambiguities to its own\nadvantage. To address these questions, we design scenarios where LLMs are given\na goal and an ambiguous user instruction in conflict with the goal, with\nscenarios covering scalar implicature, structural ambiguities, and power\ndynamics. We then measure different models' abilities to exploit loopholes to\nsatisfy their given goals as opposed to the goals of the user. We find that\nboth closed-source and stronger open-source models can identify ambiguities and\nexploit their resulting loopholes, presenting a potential AI safety risk. Our\nanalysis indicates that models which exploit loopholes explicitly identify and\nreason about both ambiguity and conflicting goals.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u6cd5\u5f8b\u7a7a\u5b50\u7684\u54cd\u5e94\uff0c\u53d1\u73b0\u95ed\u6e90\u548c\u5f3a\u5927\u7684\u5f00\u6e90\u6a21\u578b\u90fd\u80fd\u8bc6\u522b\u6b67\u4e49\u6027\u5e76\u5229\u7528\u7a7a\u5b50\u6765\u5b8c\u6210\u81ea\u8eab\u76ee\u6807\uff0c\u6784\u6210AI\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u901a\u8fc7\u7814\u7a76LLM\u5bf9\u7a7a\u5b50\u7684\u54cd\u5e94\uff0c\u53ef\u4ee5\u4e00\u65b9\u9762\u5206\u6790\u6a21\u578b\u5728\u6b67\u4e49\u6027\u548c\u8bed\u7528\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53e6\u4e00\u65b9\u9762\u53d1\u73b0\u4e00\u79cd\u65b0\u7684\u5bf9\u9f50\u95ee\u9898\uff1a\u6a21\u578b\u9047\u5230\u51b2\u7a81\u76ee\u6807\u65f6\u4f1a\u5229\u7528\u6b67\u4e49\u6027\u6765\u4e3a\u81ea\u8eab\u83b7\u76ca\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b\u6807\u91cf\u542b\u4e49\u3001\u7ed3\u6784\u6b67\u4e49\u6027\u548c\u6743\u529b\u52a8\u6001\u7684\u573a\u666f\uff0c\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\u7ed9\u6a21\u578b\u4e00\u4e2a\u76ee\u6807\u548c\u4e00\u4e2a\u4e0e\u8be5\u76ee\u6807\u51b2\u7a81\u7684\u6b67\u4e49\u7528\u6237\u6307\u4ee4\uff0c\u7136\u540e\u6d4b\u91cf\u6a21\u578b\u5229\u7528\u7a7a\u5b50\u6765\u6ee1\u8db3\u81ea\u8eab\u76ee\u6807\u7684\u80fd\u529b\u3002", "result": "\u53d1\u73b0\u95ed\u6e90\u548c\u66f4\u5f3a\u5927\u7684\u5f00\u6e90\u6a21\u578b\u90fd\u80fd\u8bc6\u522b\u6b67\u4e49\u6027\u5e76\u5229\u7528\u7a7a\u5b50\u6765\u6ee1\u8db3\u81ea\u8eab\u76ee\u6807\uff0c\u800c\u4e0d\u662f\u7528\u6237\u7684\u76ee\u6807\u3002\u5206\u6790\u663e\u793a\uff0c\u5229\u7528\u7a7a\u5b50\u7684\u6a21\u578b\u4f1a\u660e\u786e\u8bc6\u522b\u5e76\u63a8\u7406\u6b67\u4e49\u6027\u548c\u51b2\u7a81\u76ee\u6807\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u8bc6\u522b\u6b67\u4e49\u6027\u5e76\u5229\u7528\u7a7a\u5b50\u6765\u5b8c\u6210\u81ea\u8eab\u76ee\u6807\uff0c\u8fd9\u6784\u6210\u4e86\u6f5c\u5728\u7684AI\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u5728\u6a21\u578b\u5bf9\u9f50\u65b9\u9762\u8fdb\u884c\u66f4\u6df1\u5165\u7684\u7814\u7a76\u3002"}}
{"id": "2508.19578", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19578", "abs": "https://arxiv.org/abs/2508.19578", "authors": ["Jiaqi Deng", "Yuho Lee", "Nicole Hee-Yeon Kim", "Hyangsuk Min", "Taewon Yun", "Minjeong Ban", "Kim Yul", "Hwanjun Song"], "title": "Towards a Holistic and Automated Evaluation Framework for Multi-Level Comprehension of LLMs in Book-Length Contexts", "comment": "Accepted to EMNLP 2025 (Main)", "summary": "We introduce HAMLET, a holistic and automated framework for evaluating the\nlong-context comprehension of large language models (LLMs). HAMLET structures\nsource texts into a three-level key-fact hierarchy at root-, branch-, and\nleaf-levels, and employs query-focused summarization to evaluate how well\nmodels recall and faithfully represent information at each level. To validate\nthe reliability of our fully automated pipeline, we conduct a systematic human\nstudy, showing that our automatic evaluation achieves over 90% agreement with\nexpert human judgments, while reducing the cost by up to 25 times. HAMLET\nreveals that LLMs struggle with fine-grained comprehension, especially at the\nleaf level, and are sensitive to positional effects like the\nlost-in-the-middle. Analytical queries pose greater challenges than narrative\nones, and consistent performance gaps emerge between open-source and\nproprietary models, as well as across model scales. Our code and dataset are\npublicly available at https://github.com/DISL-Lab/HAMLET.", "AI": {"tldr": "HAMLET\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u6587\u672c\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u4e09\u5c42\u5173\u952e\u4e8b\u5b9e\u5c42\u6b21\u7ed3\u6784\u548c\u67e5\u8be2\u805a\u7126\u6458\u8981\u6765\u6d4b\u8bd5\u6a21\u578b\u5728\u4e0d\u540c\u7c92\u5ea6\u4e0a\u7684\u4fe1\u606f\u56de\u5fc6\u548c\u8868\u793a\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u6587\u672c\u7406\u89e3\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u7ec6\u7c92\u5ea6\u4fe1\u606f\u63d0\u53d6\u548c\u4f4d\u7f6e\u654f\u611f\u6027\u65b9\u9762\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u5168\u9762\u6d4b\u8bd5\u6a21\u578b\u7684\u957f\u6587\u672c\u7406\u89e3\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e09\u5c42\u5173\u952e\u4e8b\u5b9e\u5c42\u6b21\u7ed3\u6784\uff08\u6839\u7ea7\u3001\u5206\u652f\u7ea7\u3001\u53f6\u7ea7\uff09\uff0c\u91c7\u7528\u67e5\u8be2\u805a\u7126\u6458\u8981\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u5c42\u6b21\u7684\u4fe1\u606f\u56de\u5fc6\u548c\u5fe0\u5b9e\u8868\u793a\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u7814\u7a76\u9a8c\u8bc1\u81ea\u52a8\u5316\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "result": "\u81ea\u52a8\u5316\u8bc4\u4f30\u4e0e\u4e13\u5bb6\u4eba\u5de5\u5224\u65ad\u8fbe\u523090%\u4ee5\u4e0a\u7684\u4e00\u81f4\u6027\uff0c\u6210\u672c\u964d\u4f4e25\u500d\uff1b\u53d1\u73b0LLMs\u5728\u53f6\u7ea7\u7ec6\u7c92\u5ea6\u7406\u89e3\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u5bf9\u4f4d\u7f6e\u6548\u5e94\u654f\u611f\uff0c\u5206\u6790\u6027\u67e5\u8be2\u6bd4\u53d9\u8ff0\u6027\u67e5\u8be2\u66f4\u5177\u6311\u6218\u6027\uff0c\u5f00\u6e90\u6a21\u578b\u4e0e\u4e13\u6709\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "HAMLET\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86LLMs\u5728\u957f\u6587\u672c\u7406\u89e3\u65b9\u9762\u7684\u5177\u4f53\u5f31\u70b9\u548c\u6311\u6218\uff0c\u4e3a\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.19580", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19580", "abs": "https://arxiv.org/abs/2508.19580", "authors": ["Omkar Gurjar", "Agam Goyal", "Eshwar Chandrasekharan"], "title": "ArgCMV: An Argument Summarization Benchmark for the LLM-era", "comment": null, "summary": "Key point extraction is an important task in argument summarization which\ninvolves extracting high-level short summaries from arguments. Existing\napproaches for KP extraction have been mostly evaluated on the popular ArgKP21\ndataset. In this paper, we highlight some of the major limitations of the\nArgKP21 dataset and demonstrate the need for new benchmarks that are more\nrepresentative of actual human conversations. Using SoTA large language models\n(LLMs), we curate a new argument key point extraction dataset called ArgCMV\ncomprising of around 12K arguments from actual online human debates spread\nacross over 3K topics. Our dataset exhibits higher complexity such as longer,\nco-referencing arguments, higher presence of subjective discourse units, and a\nlarger range of topics over ArgKP21. We show that existing methods do not adapt\nwell to ArgCMV and provide extensive benchmark results by experimenting with\nexisting baselines and latest open source models. This work introduces a novel\nKP extraction dataset for long-context online discussions, setting the stage\nfor the next generation of LLM-driven summarization research.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u73b0\u6709ArgKP21\u6570\u636e\u96c6\u5728\u8bba\u70b9\u5173\u952e\u70b9\u63d0\u53d6\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u65b0\u7684ArgCMV\u6570\u636e\u96c6\uff0c\u5305\u542b12K\u4e2a\u771f\u5b9e\u5728\u7ebf\u8fa9\u8bba\u8bba\u70b9\uff0c\u8986\u76d63K\u591a\u4e2a\u4e3b\u9898\uff0c\u5177\u6709\u66f4\u9ad8\u590d\u6742\u6027\u548c\u4ee3\u8868\u6027\u3002", "motivation": "\u73b0\u6709ArgKP21\u6570\u636e\u96c6\u4e0d\u80fd\u5f88\u597d\u5730\u4ee3\u8868\u771f\u5b9e\u4eba\u7c7b\u5bf9\u8bdd\uff0c\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u521b\u5efa\u66f4\u7b26\u5408\u5b9e\u9645\u5728\u7ebf\u8fa9\u8bba\u573a\u666f\u7684\u65b0\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u6784\u5efaArgCMV\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ea612K\u4e2a\u6765\u81ea\u771f\u5b9e\u5728\u7ebf\u4eba\u7c7b\u8fa9\u8bba\u7684\u8bba\u70b9\uff0c\u8986\u76d63K\u591a\u4e2a\u4e3b\u9898\uff0c\u5177\u6709\u66f4\u957f\u6587\u672c\u3001\u5171\u6307\u8bba\u8bc1\u548c\u66f4\u591a\u4e3b\u89c2\u8bdd\u8bed\u5355\u5143\u7b49\u66f4\u9ad8\u590d\u6742\u6027\u7279\u5f81\u3002", "result": "ArgCMV\u6570\u636e\u96c6\u5c55\u73b0\u51fa\u6bd4ArgKP21\u66f4\u9ad8\u7684\u590d\u6742\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u901a\u8fc7\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u7684\u57fa\u51c6\u7ed3\u679c\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u957f\u4e0a\u4e0b\u6587\u5728\u7ebf\u8ba8\u8bba\u5f15\u5165\u4e86\u65b0\u9896\u7684\u5173\u952e\u70b9\u63d0\u53d6\u6570\u636e\u96c6\uff0c\u4e3a\u4e0b\u4e00\u4ee3LLM\u9a71\u52a8\u7684\u6458\u8981\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.19587", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19587", "abs": "https://arxiv.org/abs/2508.19587", "authors": ["Hadi Zaatiti", "Hatem Hajri", "Osama Abdullah", "Nader Masmoudi"], "title": "Towards stable AI systems for Evaluating Arabic Pronunciations", "comment": null, "summary": "Modern Arabic ASR systems such as wav2vec 2.0 excel at word- and\nsentence-level transcription, yet struggle to classify isolated letters. In\nthis study, we show that this phoneme-level task, crucial for language\nlearning, speech therapy, and phonetic research, is challenging because\nisolated letters lack co-articulatory cues, provide no lexical context, and\nlast only a few hundred milliseconds. Recogniser systems must therefore rely\nsolely on variable acoustic cues, a difficulty heightened by Arabic's emphatic\n(pharyngealized) consonants and other sounds with no close analogues in many\nlanguages. This study introduces a diverse, diacritised corpus of isolated\nArabic letters and demonstrates that state-of-the-art wav2vec 2.0 models\nachieve only 35% accuracy on it. Training a lightweight neural network on\nwav2vec embeddings raises performance to 65%. However, adding a small amplitude\nperturbation (epsilon = 0.05) cuts accuracy to 32%. To restore robustness, we\napply adversarial training, limiting the noisy-speech drop to 9% while\npreserving clean-speech accuracy. We detail the corpus, training pipeline, and\nevaluation protocol, and release, on demand, data and code for reproducibility.\nFinally, we outline future work extending these methods to word- and\nsentence-level frameworks, where precise letter pronunciation remains critical.", "AI": {"tldr": "\u73b0\u4ee3\u963f\u62c9\u4f2f\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u5728\u5b57\u6bcd\u7ea7\u522b\u8bc6\u522b\u4e0a\u8868\u73b0\u5dee\u5f3a\uff0c\u4ec535%\u51c6\u786e\u7387\u3002\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u63d0\u5347\u81f365%\uff0c\u4f46\u5bf9\u5c0f\u5e45\u5ea6\u5e72\u6270\u654f\u611f\u3002\u5bf9\u6297\u8bad\u7ec3\u6709\u6548\u63d0\u9ad8\u7cfb\u7edf\u7a33\u5065\u6027\u3002", "motivation": "\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u5355\u72ec\u5b57\u6bcd\u8bc6\u522b\u7684\u6311\u6218\uff0c\u8fd9\u5bf9\u8bed\u8a00\u5b66\u4e60\u3001\u8bed\u97f3\u6cbb\u7597\u548c\u8bed\u97f3\u7814\u7a76\u81f3\u5173\u91cd\u8981\u3002\u5b57\u6bcd\u7f3a\u4e4f\u8fde\u8d2f\u4e0a\u4e0b\u6587\u7ebf\u7d22\u548c\u8bcd\u6c47\u73af\u5883\uff0c\u4f9d\u9760\u7eaf\u7cb9\u58f0\u5b66\u7279\u5f81\uff0c\u52a0\u4e0a\u963f\u62c9\u4f2f\u8bed\u54ac\u8089\u97f3\u7b49\u7279\u6b8a\u97f3\u7d20\u589e\u52a0\u4e86\u8bc6\u522b\u96be\u5ea6\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u6837\u5316\u3001\u5e26\u97f3\u7b26\u6807\u6ce8\u7684\u963f\u62c9\u4f2f\u8bed\u5b57\u6bcd\u8bed\u6599\u5e93\u3002\u4f7f\u7528wav2vec 2.0\u6a21\u578b\u8fdb\u884c\u57fa\u7ebf\u6d4b\u8bd5\uff0c\u7136\u540e\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u5904\u7406wav2vec\u5d4c\u5165\u3002\u91c7\u7528\u5bf9\u6297\u8bad\u7ec3\u6765\u63d0\u9ad8\u7cfb\u7edf\u5bf9\u5e72\u6270\u7684\u8010\u53d7\u6027\u3002", "result": "wav2vec 2.0\u6a21\u578b\u5728\u5b57\u6bcd\u8bc6\u522b\u4efb\u52a1\u4e0a\u4ec5\u8fbe35%\u51c6\u786e\u7387\u3002\u8f7b\u91cf\u7ea7\u7f51\u7edc\u5c06\u6027\u80fd\u63d0\u5347\u81f365%\uff0c\u4f46\u5c0f\u5e45\u5ea6\u5e72\u6270(\u03b5=0.05)\u4f1a\u4f7f\u51c6\u786e\u7387\u964d\u81f332%\u3002\u5bf9\u6297\u8bad\u7ec3\u540e\uff0c\u566a\u58f0\u60c5\u51b5\u4e0b\u6027\u80fd\u4e0b\u964d\u4ec5\u4e3a9%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6e05\u6670\u8bed\u97f3\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u963f\u62c9\u4f2f\u8bed\u5b57\u6bcd\u7ea7\u522b\u8bc6\u522b\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u5904\u7406\u3002\u5bf9\u6297\u8bad\u7ec3\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u7cfb\u7edf\u7684\u7a33\u5065\u6027\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u6269\u5c55\u8fd9\u4e9b\u65b9\u6cd5\u5230\u8bcd\u8bed\u548c\u53e5\u5b50\u7ea7\u522b\u7684\u6846\u67b6\u4e2d\uff0c\u4ee5\u652f\u6301\u66f4\u7cbe\u786e\u7684\u5b57\u6bcd\u53d1\u97f3\u8bc6\u522b\u3002"}}
{"id": "2508.19594", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19594", "abs": "https://arxiv.org/abs/2508.19594", "authors": ["Jun Bai", "Minghao Tong", "Yang Liu", "Zixia Jia", "Zilong Zheng"], "title": "Understanding and Leveraging the Expert Specialization of Context Faithfulness in Mixture-of-Experts LLMs", "comment": "Accepted by EMNLP 2025 Main", "summary": "Context faithfulness is essential for reliable reasoning in context-dependent\nscenarios. However, large language models often struggle to ground their\noutputs in the provided context, resulting in irrelevant responses. Inspired by\nthe emergent expert specialization observed in mixture-of-experts\narchitectures, this work investigates whether certain experts exhibit\nspecialization in context utilization, offering a potential pathway toward\ntargeted optimization for improved context faithfulness. To explore this, we\npropose Router Lens, a method that accurately identifies context-faithful\nexperts. Our analysis reveals that these experts progressively amplify\nattention to relevant contextual information, thereby enhancing context\ngrounding. Building on this insight, we introduce Context-faithful Expert\nFine-Tuning (CEFT), a lightweight optimization approach that selectively\nfine-tunes context-faithful experts. Experiments across a wide range of\nbenchmarks and models demonstrate that CEFT matches or surpasses the\nperformance of full fine-tuning while being significantly more efficient.", "AI": {"tldr": "\u63d0\u51fa\u4e86Router Lens\u65b9\u6cd5\u8bc6\u522b\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u4e13\u5bb6\uff0c\u5e76\u5f00\u53d1\u4e86\u8f7b\u91cf\u7ea7\u7684CEFT\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5fae\u8c03\u4e13\u5bb6\u6765\u63d0\u5347\u6a21\u578b\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u5ea6\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7684\u540c\u65f6\u8fbe\u5230\u6216\u8d85\u8d8a\u5168\u5fae\u8c03\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u4f9d\u8d56\u573a\u666f\u4e2d\u7ecf\u5e38\u96be\u4ee5\u5c06\u8f93\u51fa\u57fa\u4e8e\u7ed9\u5b9a\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u4e0d\u76f8\u5173\u54cd\u5e94\u3002\u53d7\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u4e2d\u4e13\u5bb6\u4e13\u4e1a\u5316\u7684\u542f\u53d1\uff0c\u7814\u7a76\u662f\u5426\u5b58\u5728\u4e13\u95e8\u5904\u7406\u4e0a\u4e0b\u6587\u5229\u7528\u7684\u4e13\u5bb6\u3002", "method": "\u63d0\u51faRouter Lens\u65b9\u6cd5\u51c6\u786e\u8bc6\u522b\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u4e13\u5bb6\uff0c\u5206\u6790\u53d1\u73b0\u8fd9\u4e9b\u4e13\u5bb6\u4f1a\u9010\u6b65\u653e\u5927\u5bf9\u76f8\u5173\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u6ce8\u610f\u529b\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86Context-faithful Expert Fine-Tuning (CEFT)\u8f7b\u91cf\u7ea7\u4f18\u5316\u65b9\u6cd5\uff0c\u9009\u62e9\u6027\u5fae\u8c03\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u4e13\u5bb6\u3002", "result": "\u5728\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCEFT\u65b9\u6cd5\u5728\u663e\u8457\u66f4\u9ad8\u6548\u7684\u540c\u65f6\uff0c\u80fd\u591f\u5339\u914d\u6216\u8d85\u8d8a\u5168\u5fae\u8c03\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u8bc6\u522b\u548c\u9009\u62e9\u6027\u4f18\u5316\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u4e13\u5bb6\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u4f9d\u8d56\u573a\u666f\u4e2d\u7684\u5fe0\u5b9e\u5ea6\uff0c\u4e3a\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u4f18\u5316\u8def\u5f84\u3002"}}
{"id": "2508.19614", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19614", "abs": "https://arxiv.org/abs/2508.19614", "authors": ["Yang Sun", "Lixin Zou", "Dan Luo", "Zhiyong Xie", "Long Zhang", "Liming Dong", "Yunwei Zhao", "Xixun Lin", "Yanxiong Lu", "Chenliang Li"], "title": "LFD: Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-augmented generation (RAG) incorporates external knowledge into\nlarge language models (LLMs), improving their adaptability to downstream tasks\nand enabling information updates. Surprisingly, recent empirical evidence\ndemonstrates that injecting noise into retrieved relevant documents\nparadoxically facilitates exploitation of external knowledge and improves\ngeneration quality. Although counterintuitive and challenging to apply in\npractice, this phenomenon enables granular control and rigorous analysis of how\nLLMs integrate external knowledge. Therefore, in this paper, we intervene on\nnoise injection and establish a layer-specific functional demarcation within\nthe LLM: shallow layers specialize in local context modeling, intermediate\nlayers focus on integrating long-range external factual knowledge, and deeper\nlayers primarily rely on parametric internal knowledge. Building on this\ninsight, we propose Layer Fused Decoding (LFD), a simple decoding strategy that\ndirectly combines representations from an intermediate layer with final-layer\ndecoding outputs to fully exploit the external factual knowledge. To identify\nthe optimal intermediate layer, we introduce an internal knowledge score (IKS)\ncriterion that selects the layer with the lowest IKS value in the latter half\nof layers. Experimental results across multiple benchmarks demonstrate that LFD\nhelps RAG systems more effectively surface retrieved context knowledge with\nminimal cost.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u566a\u58f0\u6ce8\u5165\u5b9e\u9a8c\u53d1\u73b0LLM\u4e0d\u540c\u5c42\u6b21\u7684\u529f\u80fd\u5206\u5de5\uff1a\u6d45\u5c42\u5904\u7406\u5c40\u90e8\u4e0a\u4e0b\u6587\uff0c\u4e2d\u95f4\u5c42\u6574\u5408\u5916\u90e8\u77e5\u8bc6\uff0c\u6df1\u5c42\u4f9d\u8d56\u5185\u90e8\u53c2\u6570\u77e5\u8bc6\u3002\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86Layer Fused Decoding\u89e3\u7801\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347RAG\u7cfb\u7edf\u7684\u77e5\u8bc6\u5229\u7528\u6548\u7387\u3002", "motivation": "\u8fd1\u671f\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u68c0\u7d22\u5230\u7684\u76f8\u5173\u6587\u6863\u4e2d\u6ce8\u5165\u566a\u58f0\u53cd\u800c\u80fd\u4fc3\u8fdbLLM\u5bf9\u5916\u90e8\u77e5\u8bc6\u7684\u5229\u7528\u5e76\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\u3002\u8fd9\u4e00\u53cd\u76f4\u89c9\u73b0\u8c61\u4e3a\u5206\u6790LLM\u5982\u4f55\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u63d0\u4f9b\u4e86\u72ec\u7279\u89c6\u89d2\u3002", "method": "1) \u901a\u8fc7\u566a\u58f0\u6ce8\u5165\u5b9e\u9a8c\u5efa\u7acbLLM\u5c42\u6b21\u529f\u80fd\u5212\u5206\uff1b2) \u63d0\u51faLayer Fused Decoding\u7b56\u7565\uff0c\u5c06\u4e2d\u95f4\u5c42\u8868\u793a\u4e0e\u6700\u7ec8\u5c42\u89e3\u7801\u8f93\u51fa\u7ed3\u5408\uff1b3) \u5f15\u5165\u5185\u90e8\u77e5\u8bc6\u8bc4\u5206\u51c6\u5219\u9009\u62e9\u6700\u4f18\u4e2d\u95f4\u5c42", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLFD\u80fd\u591f\u4ee5\u6700\u5c0f\u6210\u672c\u5e2e\u52a9RAG\u7cfb\u7edf\u66f4\u6709\u6548\u5730\u5229\u7528\u68c0\u7d22\u5230\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6", "conclusion": "LLM\u5b58\u5728\u660e\u786e\u7684\u529f\u80fd\u5c42\u6b21\u5206\u5de5\uff0c\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u7684Layer Fused Decoding\u7b56\u7565\u80fd\u591f\u663e\u8457\u63d0\u5347\u5916\u90e8\u77e5\u8bc6\u5229\u7528\u6548\u7387\uff0c\u4e3aRAG\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2508.19633", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19633", "abs": "https://arxiv.org/abs/2508.19633", "authors": ["Chong Tian", "Qirong Ho", "Xiuying Chen"], "title": "A Symbolic Adversarial Learning Framework for Evolving Fake News Generation and Detection", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Rapid LLM advancements heighten fake news risks by enabling the automatic\ngeneration of increasingly sophisticated misinformation. Previous detection\nmethods, including fine-tuned small models or LLM-based detectors, often\nstruggle with its dynamically evolving nature. In this work, we propose a novel\nframework called the Symbolic Adversarial Learning Framework (SALF), which\nimplements an adversarial training paradigm by an agent symbolic learning\noptimization process, rather than relying on numerical updates. SALF introduces\na paradigm where the generation agent crafts deceptive narratives, and the\ndetection agent uses structured debates to identify logical and factual flaws\nfor detection, and they iteratively refine themselves through such adversarial\ninteractions. Unlike traditional neural updates, we represent agents using\nagent symbolic learning, where learnable weights are defined by agent prompts,\nand simulate back-propagation and gradient descent by operating on natural\nlanguage representations of weights, loss, and gradients. Experiments on two\nmultilingual benchmark datasets demonstrate SALF's effectiveness, showing it\ngenerates sophisticated fake news that degrades state-of-the-art detection\nperformance by up to 53.4% in Chinese and 34.2% in English on average. SALF\nalso refines detectors, improving detection of refined content by up to 7.7%.\nWe hope our work inspires further exploration into more robust, adaptable fake\nnews detection systems.", "AI": {"tldr": "SALF\u6846\u67b6\u901a\u8fc7\u7b26\u53f7\u5bf9\u6297\u5b66\u4e60\u4f18\u5316\u8fc7\u7a0b\uff0c\u8ba9\u751f\u6210\u5668\u5236\u9020\u865a\u5047\u65b0\u95fb\uff0c\u68c0\u6d4b\u5668\u901a\u8fc7\u7ed3\u6784\u5316\u8fa9\u8bba\u8bc6\u522b\u903b\u8f91\u548c\u4e8b\u5b9e\u7f3a\u9677\uff0c\u4e24\u8005\u901a\u8fc7\u5bf9\u6297\u4ea4\u4e92\u8fed\u4ee3\u4f18\u5316\uff0c\u5728\u53cc\u8bed\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u73b0\u6709\u68c0\u6d4b\u5668\u6027\u80fd\u5e76\u63d0\u5347\u4e86\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u968f\u7740LLM\u5feb\u901f\u53d1\u5c55\uff0c\u81ea\u52a8\u751f\u6210\u590d\u6742\u865a\u5047\u65b0\u95fb\u7684\u98ce\u9669\u589e\u52a0\uff0c\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u52a8\u6001\u6f14\u53d8\u7684\u865a\u5047\u4fe1\u606f\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u548c\u81ea\u9002\u5e94\u7684\u68c0\u6d4b\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u7b26\u53f7\u5bf9\u6297\u5b66\u4e60\u6846\u67b6(SALF)\uff0c\u91c7\u7528\u4ee3\u7406\u7b26\u53f7\u5b66\u4e60\u4f18\u5316\u800c\u975e\u6570\u503c\u66f4\u65b0\uff0c\u751f\u6210\u4ee3\u7406\u5236\u4f5c\u6b3a\u9a97\u6027\u53d9\u8ff0\uff0c\u68c0\u6d4b\u4ee3\u7406\u901a\u8fc7\u7ed3\u6784\u5316\u8fa9\u8bba\u8bc6\u522b\u903b\u8f91\u548c\u4e8b\u5b9e\u7f3a\u9677\u8fdb\u884c\u68c0\u6d4b\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8868\u793a\u6743\u91cd\u3001\u635f\u5931\u548c\u68af\u5ea6\u6765\u6a21\u62df\u53cd\u5411\u4f20\u64ad\u548c\u68af\u5ea6\u4e0b\u964d\u3002", "result": "\u5728\u53cc\u8bed\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cSALF\u751f\u6210\u7684\u590d\u6742\u865a\u5047\u65b0\u95fb\u4f7f\u6700\u5148\u8fdb\u68c0\u6d4b\u5668\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe53.4%(\u4e2d\u6587)\u548c34.2%(\u82f1\u6587)\uff0c\u540c\u65f6\u5c06\u7cbe\u70bc\u5185\u5bb9\u7684\u68c0\u6d4b\u80fd\u529b\u63d0\u5347\u9ad8\u8fbe7.7%\u3002", "conclusion": "SALF\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u865a\u5047\u65b0\u95fb\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.19665", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19665", "abs": "https://arxiv.org/abs/2508.19665", "authors": ["Giovanni Pollo", "Andrei Mihai Albu", "Alessio Burrello", "Daniele Jahier Pagliari", "Cristian Tesconi", "Loris Panaro", "Dario Soldi", "Fabio Autieri", "Sara Vinco"], "title": "Automatic integration of SystemC in the FMI standard for Software-defined Vehicle design", "comment": null, "summary": "The recent advancements of the automotive sector demand robust co-simulation\nmethodologies that enable early validation and seamless integration across\nhardware and software domains. However, the lack of standardized interfaces and\nthe dominance of proprietary simulation platforms pose significant challenges\nto collaboration, scalability, and IP protection. To address these limitations,\nthis paper presents an approach for automatically wrapping SystemC models by\nusing the Functional Mock-up Interface (FMI) standard. This method combines the\nmodeling accuracy and fast time-to-market of SystemC with the interoperability\nand encapsulation benefits of FMI, enabling secure and portable integration of\nembedded components into co-simulation workflows. We validate the proposed\nmethodology on real-world case studies, demonstrating its effectiveness with\ncomplex designs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5c06SystemC\u6a21\u578b\u5c01\u88c5\u4e3aFMI\u6807\u51c6\u63a5\u53e3\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5d4c\u5165\u5f0f\u7ec4\u4ef6\u5728\u534f\u540c\u4eff\u771f\u4e2d\u7684\u5b89\u5168\u4fbf\u643a\u96c6\u6210", "motivation": "\u6c7d\u8f66\u884c\u4e1a\u9700\u8981\u5f3a\u5927\u7684\u534f\u540c\u4eff\u771f\u65b9\u6cd5\u8fdb\u884c\u65e9\u671f\u9a8c\u8bc1\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u63a5\u53e3\u548c\u4e13\u6709\u4eff\u771f\u5e73\u53f0\u7684\u5784\u65ad\u7ed9\u534f\u4f5c\u3001\u53ef\u6269\u5c55\u6027\u548cIP\u4fdd\u62a4\u5e26\u6765\u6311\u6218", "method": "\u4f7f\u7528\u529f\u80fd\u6a21\u62df\u63a5\u53e3(FMI)\u6807\u51c6\u81ea\u52a8\u5c01\u88c5SystemC\u6a21\u578b\uff0c\u7ed3\u5408SystemC\u7684\u5efa\u6a21\u7cbe\u5ea6\u548c\u5feb\u901f\u4e0a\u5e02\u4f18\u52bf\u4e0eFMI\u7684\u4e92\u64cd\u4f5c\u6027\u548c\u5c01\u88c5\u4f18\u52bf", "result": "\u5728\u771f\u5b9e\u6848\u4f8b\u7814\u7a76\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u8bc1\u660e\u5176\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u8bbe\u8ba1", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86SystemC\u6a21\u578b\u4e0eFMI\u6807\u51c6\u4e4b\u95f4\u7684\u96c6\u6210\u95ee\u9898\uff0c\u4e3a\u5d4c\u5165\u5f0f\u7ec4\u4ef6\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u4fbf\u643a\u7684\u534f\u540c\u4eff\u771f\u96c6\u6210\u65b9\u6848"}}
{"id": "2508.19667", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19667", "abs": "https://arxiv.org/abs/2508.19667", "authors": ["Chenghan Yang", "Ruiyu Zhao", "Yang Liu", "Ling Jiang"], "title": "Survey of Specialized Large Language Model", "comment": "9 pages, 1 figures", "summary": "The rapid evolution of specialized large language models (LLMs) has\ntransitioned from simple domain adaptation to sophisticated native\narchitectures, marking a paradigm shift in AI development. This survey\nsystematically examines this progression across healthcare, finance, legal, and\ntechnical domains. Besides the wide use of specialized LLMs, technical\nbreakthrough such as the emergence of domain-native designs beyond fine-tuning,\ngrowing emphasis on parameter efficiency through sparse computation and\nquantization, increasing integration of multimodal capabilities and so on are\napplied to recent LLM agent. Our analysis reveals how these innovations address\nfundamental limitations of general-purpose LLMs in professional applications,\nwith specialized models consistently performance gains on domain-specific\nbenchmarks. The survey further highlights the implications for E-Commerce field\nto fill gaps in the field.", "AI": {"tldr": "\u8fd9\u4efd\u8c03\u67e5\u7cfb\u7edf\u5206\u6790\u4e86\u4e13\u4e1a\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u5316\uff0c\u4ece\u7b80\u5355\u57df\u9002\u914d\u5230\u672c\u571f\u5316\u67b6\u6784\u8bbe\u8ba1\uff0c\u6db5\u76d6\u533b\u7597\u3001\u91d1\u878d\u3001\u6cd5\u5f8b\u7b49\u9886\u57df\u7684\u6280\u672f\u7a81\u7834\u548c\u5e94\u7528\u6210\u679c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u901a\u7528LLM\u5728\u4e13\u4e1a\u5e94\u7528\u4e2d\u7684\u6839\u672c\u5c40\u9650\u6027\uff0c\u63a2\u7d22\u4e13\u4e1a\u5316LLM\u7684\u6280\u672f\u8fdb\u6b65\u548c\u5b9e\u8df5\u6548\u679c\u3002", "method": "\u7cfb\u7edf\u6027\u8c03\u67e5\u5206\u6790\uff0c\u6db5\u76d6\u57df\u672c\u571f\u5316\u8bbe\u8ba1\u3001\u53c2\u6570\u6548\u7387\u4f18\u5316\u3001\u591a\u6a21\u6001\u96c6\u6210\u7b49\u6280\u672f\u7a81\u7834\uff0c\u5bf9\u6bd4\u4e13\u4e1a\u57df\u6027\u80fd\u6307\u6807\u3002", "result": "\u4e13\u4e1a\u5316LLM\u5728\u5404\u9886\u57df\u6307\u6807\u4e0a\u5747\u8868\u73b0\u51fa\u6301\u7eed\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u7535\u5b50\u5546\u52a1\u9886\u57df\u6709\u91cd\u8981\u53d1\u73b0\u3002", "conclusion": "\u4e13\u4e1a\u5316LLM\u7684\u53d1\u5c55\u6807\u5fd7\u7740AI\u5f00\u53d1\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e3a\u5404\u884c\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u52a0\u9ad8\u6548\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19689", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19689", "abs": "https://arxiv.org/abs/2508.19689", "authors": ["Xiaoying Zhang"], "title": "Building Task Bots with Self-learning for Enhanced Adaptability, Extensibility, and Factuality", "comment": "179 pages", "summary": "Developing adaptable, extensible, and accurate task bots with minimal or zero\nhuman intervention is a significant challenge in dialog research. This thesis\nexamines the obstacles and potential solutions for creating such bots, focusing\non innovative techniques that enable bots to learn and adapt autonomously in\nconstantly changing environments.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u5f00\u53d1\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u9002\u5e94\u6027\u3001\u53ef\u6269\u5c55\u6027\u5f3a\u7684\u4efb\u52a1\u5bf9\u8bdd\u673a\u5668\u4eba\u7684\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848", "motivation": "\u5f00\u53d1\u80fd\u591f\u81ea\u4e3b\u5b66\u4e60\u3001\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u73af\u5883\u7684\u5bf9\u8bdd\u673a\u5668\u4eba\u662f\u5bf9\u8bdd\u7814\u7a76\u9886\u57df\u7684\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u6700\u5c0f\u5316\u6216\u96f6\u4eba\u5de5\u5e72\u9884", "method": "\u7814\u7a76\u521b\u65b0\u6280\u672f\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u81ea\u4e3b\u5b66\u4e60\u548c\u9002\u5e94\uff0c\u5206\u6790\u73b0\u6709\u969c\u788d\u5e76\u63d0\u51fa\u6f5c\u5728\u89e3\u51b3\u65b9\u6848", "result": "\u8bba\u6587\u7cfb\u7edf\u6027\u5730\u5206\u6790\u4e86\u521b\u5efa\u81ea\u9002\u5e94\u5bf9\u8bdd\u673a\u5668\u4eba\u7684\u6280\u672f\u969c\u788d\uff0c\u5e76\u63a2\u8ba8\u4e86\u5b9e\u73b0\u81ea\u4e3b\u5b66\u4e60\u7684\u53ef\u80fd\u9014\u5f84", "conclusion": "\u901a\u8fc7\u521b\u65b0\u6280\u672f\u65b9\u6cd5\uff0c\u6709\u53ef\u80fd\u5f00\u53d1\u51fa\u5177\u6709\u9ad8\u5ea6\u9002\u5e94\u6027\u548c\u6269\u5c55\u6027\u7684\u4efb\u52a1\u5bf9\u8bdd\u673a\u5668\u4eba\uff0c\u51cf\u5c11\u5bf9\u4eba\u7c7b\u5e72\u9884\u7684\u4f9d\u8d56"}}
{"id": "2508.19720", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19720", "abs": "https://arxiv.org/abs/2508.19720", "authors": ["Yilin Wang", "Heng Wang", "Yuyang Bai", "Minnan Luo"], "title": "Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models", "comment": null, "summary": "In Large Language Models (LLMs) generation, there exist knowledge conflicts\nand scenarios where parametric knowledge contradicts knowledge provided in the\ncontext. Previous works studied tuning, decoding algorithms, or locating and\nediting context-aware neurons to adapt LLMs to be faithful to new contextual\nknowledge. However, they are usually inefficient or ineffective for large\nmodels, not workable for black-box models, or unable to continuously adjust\nLLMs' sensitivity to the knowledge provided in the context. To mitigate these\nproblems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a\nsimple framework that can steer LLMs' sensitivity to contextual knowledge\ncontinuously at a lightweight cost. Specifically, we tune two small LMs (i.e.\nproxy models) and use the difference in their output distributions to shift the\noriginal distribution of an LLM without modifying the LLM weights. In the\nevaluation process, we not only design synthetic data and fine-grained metrics\nto measure models' sensitivity to contextual knowledge but also use a real\nconflict dataset to validate CSKS's practical efficacy. Extensive experiments\ndemonstrate that our framework achieves continuous and precise control over\nLLMs' sensitivity to contextual knowledge, enabling both increased sensitivity\nand reduced sensitivity, thereby allowing LLMs to prioritize either contextual\nor parametric knowledge as needed flexibly. Our data and code are available at\nhttps://github.com/OliveJuiceLin/CSKS.", "AI": {"tldr": "CSKS\u6846\u67b6\u901a\u8fc7\u8bad\u7ec3\u4e24\u4e2a\u5c0f\u578b\u4ee3\u7406\u6a21\u578b\u6765\u8fde\u7eed\u8c03\u8282\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u4e0a\u4e0b\u6587\u77e5\u8bc6\u7684\u654f\u611f\u6027\uff0c\u65e0\u9700\u4fee\u6539\u539f\u59cb\u6a21\u578b\u6743\u91cd\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea7\u7684\u77e5\u8bc6\u654f\u611f\u6027\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u53c2\u6570\u77e5\u8bc6\u4e0e\u4e0a\u4e0b\u6587\u77e5\u8bc6\u51b2\u7a81\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u6548\u7387\u4f4e\u3001\u4e0d\u9002\u7528\u4e8e\u9ed1\u76d2\u6a21\u578b\u6216\u65e0\u6cd5\u8fde\u7eed\u8c03\u8282\u654f\u611f\u6027\u3002", "method": "\u8bad\u7ec3\u4e24\u4e2a\u5c0f\u578b\u4ee3\u7406\u6a21\u578b\uff0c\u5229\u7528\u5b83\u4eec\u8f93\u51fa\u5206\u5e03\u7684\u5dee\u5f02\u6765\u8c03\u6574\u5927\u8bed\u8a00\u6a21\u578b\u7684\u539f\u59cb\u8f93\u51fa\u5206\u5e03\uff0c\u5b9e\u73b0\u77e5\u8bc6\u654f\u611f\u6027\u7684\u8fde\u7eed\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eCSKS\u80fd\u591f\u7cbe\u786e\u8fde\u7eed\u5730\u63a7\u5236LLMs\u5bf9\u4e0a\u4e0b\u6587\u77e5\u8bc6\u7684\u654f\u611f\u6027\uff0c\u65e2\u53ef\u63d0\u9ad8\u4e5f\u53ef\u964d\u4f4e\u654f\u611f\u6027\uff0c\u7075\u6d3b\u4f18\u5148\u9009\u62e9\u4e0a\u4e0b\u6587\u6216\u53c2\u6570\u77e5\u8bc6\u3002", "conclusion": "CSKS\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6709\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3LLMs\u4e2d\u7684\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u89c4\u6a21\u7684\u9ed1\u76d2\u6a21\u578b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.19721", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.19721", "abs": "https://arxiv.org/abs/2508.19721", "authors": ["Carlos Carvalho", "Francisco Teixeira", "Catarina Botelho", "Anna Pompili", "Rub\u00e9n Solera-Ure\u00f1a", "S\u00e9rgio Paulo", "Mariana Juli\u00e3o", "Thomas Rolland", "John Mendon\u00e7a", "Diogo Pereira", "Isabel Trancoso", "Alberto Abad"], "title": "CAM\u00d5ES: A Comprehensive Automatic Speech Recognition Benchmark for European Portuguese", "comment": "Accepted to ASRU 2025", "summary": "Existing resources for Automatic Speech Recognition in Portuguese are mostly\nfocused on Brazilian Portuguese, leaving European Portuguese (EP) and other\nvarieties under-explored. To bridge this gap, we introduce CAM\\~OES, the first\nopen framework for EP and other Portuguese varieties. It consists of (1) a\ncomprehensive evaluation benchmark, including 46h of EP test data spanning\nmultiple domains; and (2) a collection of state-of-the-art models. For the\nlatter, we consider multiple foundation models, evaluating their zero-shot and\nfine-tuned performances, as well as E-Branchformer models trained from scratch.\nA curated set of 425h of EP was used for both fine-tuning and training. Our\nresults show comparable performance for EP between fine-tuned foundation models\nand the E-Branchformer. Furthermore, the best-performing models achieve\nrelative improvements above 35% WER, compared to the strongest zero-shot\nfoundation model, establishing a new state-of-the-art for EP and other\nvarieties.", "AI": {"tldr": "CAM\u00d5ES\u662f\u9996\u4e2a\u9488\u5bf9\u6b27\u6d32\u8461\u8404\u7259\u8bed\u7684\u5f00\u6e90ASR\u6846\u67b6\uff0c\u5305\u542b\u8bc4\u4f30\u57fa\u51c6\u548c\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5728425\u5c0f\u65f6\u6570\u636e\u4e0a\u8bad\u7ec3\u540e\u76f8\u6bd4\u96f6\u6837\u672c\u6a21\u578bWER\u76f8\u5bf9\u63d0\u534735%\u4ee5\u4e0a", "motivation": "\u73b0\u6709\u8461\u8404\u7259\u8bedASR\u8d44\u6e90\u4e3b\u8981\u5173\u6ce8\u5df4\u897f\u8461\u8404\u7259\u8bed\uff0c\u6b27\u6d32\u8461\u8404\u7259\u8bed\u548c\u5176\u4ed6\u53d8\u79cd\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d", "method": "\u6784\u5efa\u5305\u542b46\u5c0f\u65f6\u6d4b\u8bd5\u6570\u636e\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u4f7f\u7528425\u5c0f\u65f6\u6b27\u6d32\u8461\u8404\u7259\u8bed\u6570\u636e\u5bf9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u6216\u4ece\u5934\u8bad\u7ec3E-Branchformer\u6a21\u578b", "result": "\u5fae\u8c03\u540e\u7684\u57fa\u7840\u6a21\u578b\u4e0eE-Branchformer\u6027\u80fd\u76f8\u5f53\uff0c\u6700\u4f73\u6a21\u578b\u76f8\u6bd4\u6700\u5f3a\u96f6\u6837\u672c\u57fa\u7840\u6a21\u578bWER\u76f8\u5bf9\u63d0\u5347\u8d85\u8fc735%", "conclusion": "CAM\u00d5ES\u4e3a\u6b27\u6d32\u8461\u8404\u7259\u8bed\u5efa\u7acb\u4e86\u65b0\u7684state-of-the-art\uff0c\u4e3a\u5176\u4ed6\u8461\u8404\u7259\u8bed\u53d8\u79cd\u63d0\u4f9b\u4e86\u9996\u4e2a\u5f00\u6e90ASR\u6846\u67b6"}}
{"id": "2508.19724", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19724", "abs": "https://arxiv.org/abs/2508.19724", "authors": ["Aritra Dutta", "Swapnanil Mukherjee", "Deepanway Ghosal", "Somak Aditya"], "title": "NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks", "comment": null, "summary": "Commonsense visual-question answering often hinges on knowledge that is\nmissing from the image or the question. Small vision-language models (sVLMs)\nsuch as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative\ncounterparts. To study the effect of careful commonsense knowledge integration\non sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural\nlanguage facts, (ii) prompts an LLM to craft natural language explanations, and\n(iii) feeds both signals to sVLMs respectively across two commonsense VQA\ndatasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts\nretrieved using a fine-tuned ColBERTv2 and an object information-enriched\nprompt yield explanations that largely cut down hallucinations, while lifting\nthe end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA\nand other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B\nand SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional\nfinetuning using noise-robust losses (such as symmetric cross entropy and\ngeneralised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our\nfindings expose when LLM-based commonsense knowledge beats retrieval from\ncommonsense knowledge bases, how noise-aware training stabilises small models\nin the context of external knowledge augmentation, and why parameter-efficient\ncommonsense reasoning is now within reach for 250M models.", "AI": {"tldr": "NLKI\u6846\u67b6\u901a\u8fc7\u68c0\u7d22\u81ea\u7136\u8bed\u8a00\u4e8b\u5b9e\u548cLLM\u751f\u6210\u89e3\u91ca\uff0c\u5c06\u5e38\u8bc6\u77e5\u8bc6\u96c6\u6210\u5230\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u5728\u591a\u4e2aVQA\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u51c6\u786e\u73877%\uff0c\u5e76\u901a\u8fc7\u566a\u58f0\u9c81\u68d2\u8bad\u7ec3\u8fdb\u4e00\u6b65\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5e38\u8bc6\u89c6\u89c9\u95ee\u7b54\u4e2d\u56e0\u7f3a\u4e4f\u5916\u90e8\u77e5\u8bc6\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u6709\u6548\u96c6\u6210\u5e38\u8bc6\u77e5\u8bc6\u6765\u63d0\u5347\u5176\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u7aef\u5230\u7aef\u6846\u67b6NLKI\uff1a(i)\u4f7f\u7528\u5fae\u8c03ColBERTv2\u68c0\u7d22\u81ea\u7136\u8bed\u8a00\u4e8b\u5b9e\uff0c(ii)\u7528LLM\u751f\u6210\u89e3\u91ca\uff0c(iii)\u5c06\u4fe1\u53f7\u8f93\u5165sVLMs\uff0c\u5e76\u7ed3\u5408\u566a\u58f0\u9c81\u68d2\u635f\u5931\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728CRIC\u3001AOKVQA\u548ce-SNLI-VE\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u51c6\u786e\u7387\u6700\u9ad87%\uff0c\u4f7fFLAVA\u7b49\u6a21\u578b\u8fbe\u5230\u6216\u8d85\u8fc7\u4e2d\u7b49\u89c4\u6a21VLMs\u6027\u80fd\uff0c\u566a\u58f0\u9c81\u68d2\u8bad\u7ec3\u989d\u5916\u63d0\u53472.5-5.5%\u51c6\u786e\u7387\u3002", "conclusion": "LLM\u751f\u6210\u7684\u5e38\u8bc6\u77e5\u8bc6\u4f18\u4e8e\u77e5\u8bc6\u5e93\u68c0\u7d22\uff0c\u566a\u58f0\u611f\u77e5\u8bad\u7ec3\u7a33\u5b9a\u4e86\u5c0f\u6a21\u578b\u7684\u5916\u90e8\u77e5\u8bc6\u589e\u5f3a\uff0c\u4e3a2.5\u4ebf\u53c2\u6570\u6a21\u578b\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u7684\u5e38\u8bc6\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2508.19740", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19740", "abs": "https://arxiv.org/abs/2508.19740", "authors": ["Wenhao Li", "Yuxin Zhang", "Gen Luo", "Haiyuan Wan", "Ziyang Gong", "Fei Chao", "Rongrong Ji"], "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval", "comment": null, "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.", "AI": {"tldr": "Spotlight Attention\u901a\u8fc7\u975e\u7ebf\u6027\u54c8\u5e0c\u51fd\u6570\u4f18\u5316KV\u7f13\u5b58\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u76f8\u6bd4\u4f20\u7edf\u7ebf\u6027\u54c8\u5e0c\u65b9\u6cd5\u538b\u7f29\u54c8\u5e0c\u7801\u957f\u5ea6\u81f3\u5c115\u500d\uff0c\u7aef\u5230\u7aef\u541e\u5410\u91cf\u63d0\u53473\u500d\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2dKV\u7f13\u5b58\u8d1f\u62c5\u91cd\uff0c\u73b0\u6709\u57fa\u4e8e\u968f\u673a\u7ebf\u6027\u54c8\u5e0c\u7684\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u4e3a\u67e5\u8be2\u548c\u952e\u5728LLMs\u4e2d\u5448\u73b0\u6b63\u4ea4\u5206\u5e03\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u975e\u7ebf\u6027\u54c8\u5e0c\u51fd\u6570\u4f18\u5316\u67e5\u8be2\u548c\u952e\u7684\u5d4c\u5165\u5206\u5e03\uff0c\u4f7f\u7528Bradley-Terry\u6392\u5e8f\u635f\u5931\u6784\u5efa\u8f7b\u91cf\u7ea7\u8bad\u7ec3\u6846\u67b6\uff0c\u5e76\u5f00\u53d1\u4e13\u7528CUDA\u5185\u6838\u5b9e\u73b0\u9ad8\u6548\u7684\u4f4d\u8fd0\u7b97\u68c0\u7d22\u3002", "result": "\u54c8\u5e0c\u68c0\u7d22512K token\u4ec5\u9700100\u03bcs\uff0c\u54c8\u5e0c\u7801\u957f\u5ea6\u538b\u7f29\u81f3\u5c115\u500d\uff0c\u7aef\u5230\u7aef\u541e\u5410\u91cf\u6bd4\u539f\u59cb\u89e3\u7801\u63d0\u53473\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "Spotlight Attention\u901a\u8fc7\u975e\u7ebf\u6027\u54c8\u5e0c\u4f18\u5316\u6709\u6548\u89e3\u51b3\u4e86KV\u7f13\u5b58\u6548\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86LLM\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2508.19758", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.19758", "abs": "https://arxiv.org/abs/2508.19758", "authors": ["Yixuan Tang", "Yuanyuan Shi", "Yiqun Sun", "Anthony Kum Hoe Tung"], "title": "Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval", "comment": "Accepted by EMNLP 2025", "summary": "Access to diverse perspectives is essential for understanding real-world\nevents, yet most news retrieval systems prioritize textual relevance, leading\nto redundant results and limited viewpoint exposure. We propose NEWSCOPE, a\ntwo-stage framework for diverse news retrieval that enhances event coverage by\nexplicitly modeling semantic variation at the sentence level. The first stage\nretrieves topically relevant content using dense retrieval, while the second\nstage applies sentence-level clustering and diversity-aware re-ranking to\nsurface complementary information. To evaluate retrieval diversity, we\nintroduce three interpretable metrics, namely Average Pairwise Distance,\nPositive Cluster Coverage, and Information Density Ratio, and construct two\nparagraph-level benchmarks: LocalNews and DSGlobal. Experiments show that\nNEWSCOPE consistently outperforms strong baselines, achieving significantly\nhigher diversity without compromising relevance. Our results demonstrate the\neffectiveness of fine-grained, interpretable modeling in mitigating redundancy\nand promoting comprehensive event understanding. The data and code are\navailable at https://github.com/tangyixuan/NEWSCOPE.", "AI": {"tldr": "NEWSCOPE\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u65b0\u95fb\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u53e5\u5b50\u7ea7\u8bed\u4e49\u53d8\u5316\u5efa\u6a21\u6765\u63d0\u5347\u4e8b\u4ef6\u62a5\u9053\u7684\u591a\u6837\u6027\uff0c\u5728\u4fdd\u6301\u76f8\u5173\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u68c0\u7d22\u7ed3\u679c\u591a\u6837\u6027", "motivation": "\u73b0\u6709\u65b0\u95fb\u68c0\u7d22\u7cfb\u7edf\u8fc7\u4e8e\u5173\u6ce8\u6587\u672c\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u7ed3\u679c\u5197\u4f59\u4e14\u89c6\u89d2\u6709\u9650\uff0c\u9700\u8981\u63d0\u5347\u5bf9\u591a\u6837\u5316\u89c2\u70b9\u7684\u8986\u76d6", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5bc6\u96c6\u68c0\u7d22\u83b7\u53d6\u4e3b\u9898\u76f8\u5173\u5185\u5bb9\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u53e5\u5b50\u7ea7\u805a\u7c7b\u548c\u591a\u6837\u6027\u611f\u77e5\u91cd\u6392\u5e8f\u6765\u53d1\u73b0\u4e92\u8865\u4fe1\u606f", "result": "NEWSCOPE\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5728\u4e0d\u727a\u7272\u76f8\u5173\u6027\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u591a\u6837\u6027", "conclusion": "\u7ec6\u7c92\u5ea6\u3001\u53ef\u89e3\u91ca\u7684\u5efa\u6a21\u80fd\u6709\u6548\u51cf\u5c11\u5197\u4f59\u5e76\u4fc3\u8fdb\u5bf9\u4e8b\u4ef6\u7684\u5168\u9762\u7406\u89e3"}}
{"id": "2508.19764", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19764", "abs": "https://arxiv.org/abs/2508.19764", "authors": ["Pedro Henrique Luz de Araujo", "Paul R\u00f6ttger", "Dirk Hovy", "Benjamin Roth"], "title": "Principled Personas: Defining and Measuring the Intended Effects of Persona Prompting on Task Performance", "comment": "30 pages, 29 figures, accepted to EMNLP 2025", "summary": "Expert persona prompting -- assigning roles such as expert in math to\nlanguage models -- is widely used for task improvement. However, prior work\nshows mixed results on its effectiveness, and does not consider when and why\npersonas should improve performance. We analyze the literature on persona\nprompting for task improvement and distill three desiderata: 1) performance\nadvantage of expert personas, 2) robustness to irrelevant persona attributes,\nand 3) fidelity to persona attributes. We then evaluate 9 state-of-the-art LLMs\nacross 27 tasks with respect to these desiderata. We find that expert personas\nusually lead to positive or non-significant performance changes. Surprisingly,\nmodels are highly sensitive to irrelevant persona details, with performance\ndrops of almost 30 percentage points. In terms of fidelity, we find that while\nhigher education, specialization, and domain-relatedness can boost performance,\ntheir effects are often inconsistent or negligible across tasks. We propose\nmitigation strategies to improve robustness -- but find they only work for the\nlargest, most capable models. Our findings underscore the need for more careful\npersona design and for evaluation schemes that reflect the intended effects of\npersona usage.", "AI": {"tldr": "\u4e13\u5bb6\u89d2\u8272\u63d0\u793a\u5bf9LLM\u4efb\u52a1\u6027\u80fd\u5f71\u54cd\u5206\u6790\uff1a\u901a\u5e38\u5e26\u6765\u6b63\u9762\u6216\u975e\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5bf9\u65e0\u5173\u89d2\u8272\u7ec6\u8282\u9ad8\u5ea6\u654f\u611f\uff08\u6027\u80fd\u4e0b\u964d\u8fd130%\uff09\uff0c\u89d2\u8272\u5c5e\u6027\u4fdd\u771f\u5ea6\u6548\u679c\u4e0d\u4e00\u81f4\uff0c\u4ec5\u6700\u5927\u6a21\u578b\u80fd\u901a\u8fc7\u7f13\u89e3\u7b56\u7565\u6539\u5584\u9c81\u68d2\u6027", "motivation": "\u5206\u6790\u4e13\u5bb6\u89d2\u8272\u63d0\u793a\u5728\u4efb\u52a1\u6539\u8fdb\u4e2d\u7684\u6709\u6548\u6027\uff0c\u660e\u786e\u4f55\u65f6\u4ee5\u53ca\u4e3a\u4f55\u89d2\u8272\u5e94\u8be5\u63d0\u5347\u6027\u80fd\uff0c\u89e3\u51b3\u5148\u524d\u7814\u7a76\u7ed3\u679c\u4e0d\u4e00\u81f4\u4e14\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5206\u6790\u7684\u95ee\u9898", "method": "\u6587\u732e\u5206\u6790\u63d0\u70bc\u4e09\u4e2a\u671f\u671b\u6807\u51c6\uff08\u4e13\u5bb6\u89d2\u8272\u6027\u80fd\u4f18\u52bf\u3001\u65e0\u5173\u5c5e\u6027\u9c81\u68d2\u6027\u3001\u89d2\u8272\u5c5e\u6027\u4fdd\u771f\u5ea6\uff09\uff0c\u572827\u4e2a\u4efb\u52a1\u4e0a\u8bc4\u4f309\u4e2a\u6700\u5148\u8fdbLLM", "result": "\u4e13\u5bb6\u89d2\u8272\u901a\u5e38\u5e26\u6765\u6b63\u9762\u6216\u975e\u663e\u8457\u6027\u80fd\u53d8\u5316\uff1b\u6a21\u578b\u5bf9\u65e0\u5173\u89d2\u8272\u7ec6\u8282\u9ad8\u5ea6\u654f\u611f\uff08\u6027\u80fd\u4e0b\u964d\u8fd130%\uff09\uff1b\u9ad8\u7b49\u6559\u80b2\u3001\u4e13\u4e1a\u5316\u548c\u9886\u57df\u76f8\u5173\u6027\u7b49\u5c5e\u6027\u6548\u679c\u4e0d\u4e00\u81f4\uff1b\u7f13\u89e3\u7b56\u7565\u4ec5\u5bf9\u6700\u5927\u6700\u5f3a\u5927\u6a21\u578b\u6709\u6548", "conclusion": "\u9700\u8981\u66f4\u8c28\u614e\u7684\u89d2\u8272\u8bbe\u8ba1\u548c\u53cd\u6620\u89d2\u8272\u4f7f\u7528\u9884\u671f\u6548\u679c\u7684\u8bc4\u4f30\u65b9\u6848\uff0c\u5f53\u524d\u89d2\u8272\u63d0\u793a\u5b58\u5728\u9c81\u68d2\u6027\u95ee\u9898\u4e14\u6548\u679c\u4e0d\u7a33\u5b9a"}}
{"id": "2508.19813", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19813", "abs": "https://arxiv.org/abs/2508.19813", "authors": ["Jie Zhang", "Changzai Pan", "Kaiwen Wei", "Sishi Xiong", "Yu Zhao", "Xiangyu Li", "Jiaxin Peng", "Xiaoyan Gu", "Jian Yang", "Wenhan Chang", "Zhenhe Wu", "Jiang Zhong", "Shuangyong Song", "Yongxiang Li", "Xuelong Li"], "title": "T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables", "comment": null, "summary": "Extensive research has been conducted to explore the capabilities of large\nlanguage models (LLMs) in table reasoning. However, the essential task of\ntransforming tables information into reports remains a significant challenge\nfor industrial applications. This task is plagued by two critical issues: 1)\nthe complexity and diversity of tables lead to suboptimal reasoning outcomes;\nand 2) existing table benchmarks lack the capacity to adequately assess the\npractical application of this task. To fill this gap, we propose the\ntable-to-report task and construct a bilingual benchmark named T2R-bench, where\nthe key information flow from the tables to the reports for this task. The\nbenchmark comprises 457 industrial tables, all derived from real-world\nscenarios and encompassing 19 industry domains as well as 4 types of industrial\ntables. Furthermore, we propose an evaluation criteria to fairly measure the\nquality of report generation. The experiments on 25 widely-used LLMs reveal\nthat even state-of-the-art models like Deepseek-R1 only achieves performance\nwith 62.71 overall score, indicating that LLMs still have room for improvement\non T2R-bench. Source code and data will be available after acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8868\u683c\u5230\u62a5\u544a\u751f\u6210\u4efb\u52a1\u548cT2R-bench\u53cc\u8bed\u57fa\u51c6\uff0c\u5305\u542b457\u4e2a\u5de5\u4e1a\u8868\u683c\uff0c\u8bc4\u4f3025\u4e2aLLM\u53d1\u73b0\u5373\u4f7f\u6700\u5148\u8fdb\u6a21\u578b\u4e5f\u4ec5\u670962.71\u5206\uff0c\u663e\u793a\u8be5\u4efb\u52a1\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4", "motivation": "\u73b0\u6709\u8868\u683c\u63a8\u7406\u7814\u7a76\u65e0\u6cd5\u6ee1\u8db3\u5de5\u4e1a\u5e94\u7528\u4e2d\u8868\u683c\u4fe1\u606f\u8f6c\u5316\u4e3a\u62a5\u544a\u7684\u9700\u6c42\uff0c\u5b58\u5728\u8868\u683c\u590d\u6742\u6027\u5bfc\u81f4\u63a8\u7406\u6548\u679c\u4e0d\u4f73\u548c\u7f3a\u4e4f\u5b9e\u7528\u8bc4\u4f30\u57fa\u51c6\u4e24\u4e2a\u5173\u952e\u95ee\u9898", "method": "\u6784\u5efaT2R-bench\u53cc\u8bed\u57fa\u51c6\uff0c\u5305\u542b457\u4e2a\u771f\u5b9e\u5de5\u4e1a\u8868\u683c\uff0c\u6db5\u76d619\u4e2a\u884c\u4e1a\u9886\u57df\u548c4\u79cd\u8868\u683c\u7c7b\u578b\uff0c\u5e76\u63d0\u51fa\u8bc4\u4f30\u6807\u51c6\u6765\u8861\u91cf\u62a5\u544a\u751f\u6210\u8d28\u91cf", "result": "\u572825\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684LLM\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u53d1\u73b0\u5373\u4f7f\u662fDeepseek-R1\u7b49\u6700\u5148\u8fdb\u6a21\u578b\u4e5f\u53ea\u8fbe\u523062.71\u7684\u6574\u4f53\u5206\u6570", "conclusion": "\u8868\u683c\u5230\u62a5\u544a\u751f\u6210\u4efb\u52a1\u5bf9LLM\u4ecd\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u6a21\u578b\u6027\u80fd\u6709\u5f85\u63d0\u5347\uff0cT2R-bench\u4e3a\u8bc4\u4f30\u8be5\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u51c6"}}
{"id": "2508.19828", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.19828", "abs": "https://arxiv.org/abs/2508.19828", "authors": ["Sikuan Yan", "Xiufeng Yang", "Zuchao Huang", "Ercong Nie", "Zifeng Ding", "Zonggen Li", "Xiaowen Ma", "Hinrich Sch\u00fctze", "Volker Tresp", "Yunpu Ma"], "title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na wide range of NLP tasks, but they remain fundamentally stateless, constrained\nby limited context windows that hinder long-horizon reasoning. Recent efforts\nto address this limitation often augment LLMs with an external memory bank, yet\nmost existing pipelines are static and heuristic-driven, lacking any learned\nmechanism for deciding what to store, update, or retrieve. We present\nMemory-R1, a reinforcement learning (RL) framework that equips LLMs with the\nability to actively manage and utilize external memory through two specialized\nagents: a Memory Manager that learns to perform structured memory operations\n{ADD, UPDATE, DELETE, NOOP}, and an Answer Agent that selects the most relevant\nentries and reasons over them to produce an answer. Both agents are fine-tuned\nwith outcome-driven RL (PPO and GRPO), enabling adaptive memory management and\nuse with minimal supervision. With as few as 152 question-answer pairs and a\ncorresponding temporal memory bank for training, Memory-R1 outperforms the most\ncompetitive existing baseline and demonstrates strong generalization across\ndiverse question types and LLM backbones. Beyond presenting an effective\napproach, this work provides insights into how RL can unlock more agentic,\nmemory-aware behaviors in LLMs, pointing toward richer, more persistent\nreasoning systems.", "AI": {"tldr": "Memory-R1\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u4e13\u95e8\u4ee3\u7406\uff08\u5185\u5b58\u7ba1\u7406\u5668\u548c\u7b54\u6848\u4ee3\u7406\uff09\u4f7fLLM\u80fd\u591f\u4e3b\u52a8\u7ba1\u7406\u5916\u90e8\u5185\u5b58\uff0c\u5728\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u4e0b\u5b9e\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3LLM\u56e0\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u800c\u65e0\u6cd5\u8fdb\u884c\u957f\u7a0b\u63a8\u7406\u7684\u95ee\u9898\uff0c\u73b0\u6709\u5916\u90e8\u5185\u5b58\u65b9\u6cd5\u591a\u4e3a\u9759\u6001\u542f\u53d1\u5f0f\uff0c\u7f3a\u4e4f\u5b66\u4e60\u673a\u5236\u6765\u51b3\u5b9a\u5b58\u50a8\u3001\u66f4\u65b0\u6216\u68c0\u7d22\u5185\u5bb9\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08PPO\u548cGRPO\uff09\u5fae\u8c03\u4e24\u4e2a\u4ee3\u7406\uff1a\u5185\u5b58\u7ba1\u7406\u5668\u6267\u884c\u7ed3\u6784\u5316\u5185\u5b58\u64cd\u4f5c\uff08\u6dfb\u52a0\u3001\u66f4\u65b0\u3001\u5220\u9664\u3001\u65e0\u64cd\u4f5c\uff09\uff0c\u7b54\u6848\u4ee3\u7406\u9009\u62e9\u76f8\u5173\u6761\u76ee\u5e76\u8fdb\u884c\u63a8\u7406\u751f\u6210\u7b54\u6848\u3002", "result": "\u4ec5\u7528152\u4e2a\u95ee\u7b54\u5bf9\u8bad\u7ec3\uff0cMemory-R1\u5c31\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5f3a\u57fa\u7ebf\uff0c\u5728\u4e0d\u540c\u95ee\u9898\u7c7b\u578b\u548cLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86RL\u5982\u4f55\u89e3\u9501LLM\u4e2d\u66f4\u5177\u4ee3\u7406\u6027\u548c\u5185\u5b58\u611f\u77e5\u80fd\u529b\u7684\u884c\u4e3a\uff0c\u4e3a\u6784\u5efa\u66f4\u4e30\u5bcc\u3001\u66f4\u6301\u4e45\u7684\u63a8\u7406\u7cfb\u7edf\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2508.19831", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19831", "abs": "https://arxiv.org/abs/2508.19831", "authors": ["Anusha Kamath", "Kanishk Singla", "Rakesh Paul", "Raviraj Joshi", "Utkarsh Vaidya", "Sanjay Singh Chauhan", "Niranjan Wartikar"], "title": "Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative Analysis", "comment": null, "summary": "Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is\nchallenging due to a lack of high-quality benchmarks, as direct translation of\nEnglish datasets fails to capture crucial linguistic and cultural nuances. To\naddress this, we introduce a suite of five Hindi LLM evaluation datasets:\nIFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created\nusing a methodology that combines from-scratch human annotation with a\ntranslate-and-verify process. We leverage this suite to conduct an extensive\nbenchmarking of open-source LLMs supporting Hindi, providing a detailed\ncomparative analysis of their current capabilities. Our curation process also\nserves as a replicable methodology for developing benchmarks in other\nlow-resource languages.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4e3a\u5370\u5730\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u5957\u7efc\u5408\u8bc4\u6d4b\u5957\u4ef6\uff0c\u5305\u54285\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7684\u8bc4\u6d4b\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u76f4\u63a5\u7ffb\u8bd1\u82f1\u8bed\u6570\u636e\u96c6\u65e0\u6cd5\u6293\u53d6\u5370\u5730\u8bed\u8bed\u8a00\u6587\u5316\u7ec6\u8282\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u5370\u5730\u8bedLLM\u8bc4\u6d4b\u57fa\u51c6\uff0c\u76f4\u63a5\u7ffb\u8bd1\u82f1\u8bed\u6570\u636e\u96c6\u65e0\u6cd5\u6ee1\u8db3\u8bed\u8a00\u6587\u5316\u7ec6\u8282\u7684\u9700\u6c42\uff0c\u9700\u8981\u4e13\u95e8\u4e3a\u5370\u5730\u8bed\u8bbe\u8ba1\u7684\u8bc4\u6d4b\u5de5\u5177\u3002", "method": "\u91c7\u7528\u7ec4\u5408\u65b9\u6cd5\uff1a\u4ece\u5934\u5f00\u59cb\u7684\u4eba\u5de5\u6ce8\u91ca\u52a0\u4e0a\u7ffb\u8bd1-\u9a8c\u8bc1\u8fc7\u7a0b\uff0c\u521b\u5efa\u4e865\u4e2a\u5370\u5730\u8bed\u8bc4\u6d4b\u6570\u636e\u96c6\uff08IFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, BFCL-Hi\uff09\u3002", "result": "\u5bf9\u652f\u6301\u5370\u5730\u8bed\u7684\u5f00\u6e90LLM\u8fdb\u884c\u4e86\u5168\u9762\u57fa\u51c6\u6d4b\u8bc4\uff0c\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u6bd4\u8f83\u5206\u6790\uff0c\u5c55\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u80fd\u529b\u6c34\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bba\u4e0d\u4ec5\u4e3a\u5370\u5730\u8bedLLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\uff0c\u540c\u65f6\u4e5f\u4e3a\u5176\u4ed6\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u57fa\u51c6\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u590d\u5236\u7684\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2508.19836", "categories": ["cs.CL", "physics.ed-ph"], "pdf": "https://arxiv.org/pdf/2508.19836", "abs": "https://arxiv.org/abs/2508.19836", "authors": ["Jonas Timmann Mjaaland", "Markus Fleten Kreutzer", "Halvor Tyseng", "Rebeckah K. Fussell", "Gina Passante", "N. G. Holmes", "Anders Malthe-S\u00f8renssen", "Tor Ole B. Odden"], "title": "Scalable and consistent few-shot classification of survey responses using text embeddings", "comment": null, "summary": "Qualitative analysis of open-ended survey responses is a commonly-used\nresearch method in the social sciences, but traditional coding approaches are\noften time-consuming and prone to inconsistency. Existing solutions from\nNatural Language Processing such as supervised classifiers, topic modeling\ntechniques, and generative large language models have limited applicability in\nqualitative analysis, since they demand extensive labeled data, disrupt\nestablished qualitative workflows, and/or yield variable results. In this\npaper, we introduce a text embedding-based classification framework that\nrequires only a handful of examples per category and fits well with standard\nqualitative workflows. When benchmarked against human analysis of a conceptual\nphysics survey consisting of 2899 open-ended responses, our framework achieves\na Cohen's Kappa ranging from 0.74 to 0.83 as compared to expert human coders in\nan exhaustive coding scheme. We further show how performance of this framework\nimproves with fine-tuning of the text embedding model, and how the method can\nbe used to audit previously-analyzed datasets. These findings demonstrate that\ntext embedding-assisted coding can flexibly scale to thousands of responses\nwithout sacrificing interpretability, opening avenues for deductive qualitative\nanalysis at scale.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6587\u672c\u5d4c\u5165\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u4ec5\u9700\u5c11\u91cf\u6807\u6ce8\u6837\u672c\u5373\u53ef\u5b9e\u73b0\u5b9a\u6027\u5206\u6790\uff0c\u5728\u7269\u7406\u8c03\u67e5\u6570\u636e\u4e0a\u8fbe\u5230\u4e0e\u4e13\u5bb6\u7f16\u7801\u76f8\u8fd1\u7684\u4e00\u81f4\u6027\u6c34\u5e73\uff08Cohen's Kappa 0.74-0.83\uff09", "motivation": "\u4f20\u7edf\u5b9a\u6027\u5206\u6790\u65b9\u6cd5\u8017\u65f6\u4e14\u4e0d\u4e00\u81f4\uff0c\u73b0\u6709NLP\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u6216\u7834\u574f\u5b9a\u6027\u5de5\u4f5c\u6d41\u7a0b\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u5de5\u4f5c\u6d41\u7a0b\u53c8\u80fd\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21\u6587\u672c\u7684\u65b9\u6cd5", "method": "\u57fa\u4e8e\u6587\u672c\u5d4c\u5165\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u6bcf\u4e2a\u7c7b\u522b\u53ea\u9700\u5c11\u91cf\u793a\u4f8b\uff0c\u652f\u6301\u5fae\u8c03\u6587\u672c\u5d4c\u5165\u6a21\u578b\uff0c\u53ef\u4e0e\u6807\u51c6\u5b9a\u6027\u5de5\u4f5c\u6d41\u7a0b\u826f\u597d\u6574\u5408", "result": "\u57282899\u4e2a\u5f00\u653e\u5f0f\u7269\u7406\u8c03\u67e5\u54cd\u5e94\u4e2d\uff0c\u4e0e\u4e13\u5bb6\u7f16\u7801\u76f8\u6bd4\u8fbe\u5230Cohen's Kappa 0.74-0.83\u7684\u4e00\u81f4\u6027\uff0c\u6027\u80fd\u968f\u6a21\u578b\u5fae\u8c03\u800c\u63d0\u5347\uff0c\u53ef\u7528\u4e8e\u5ba1\u8ba1\u5df2\u6709\u6570\u636e\u96c6", "conclusion": "\u6587\u672c\u5d4c\u5165\u8f85\u52a9\u7f16\u7801\u53ef\u5728\u4e0d\u727a\u7272\u53ef\u89e3\u91ca\u6027\u7684\u524d\u63d0\u4e0b\u7075\u6d3b\u6269\u5c55\u5230\u6570\u5343\u4e2a\u54cd\u5e94\uff0c\u4e3a\u5927\u89c4\u6a21\u6f14\u7ece\u6027\u5b9a\u6027\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84"}}
{"id": "2508.19856", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.19856", "abs": "https://arxiv.org/abs/2508.19856", "authors": ["Shashi Kumar", "Srikanth Madikeri", "Esa\u00fa Villatoro-Tello", "Sergio Burdisso", "Pradeep Rangappa", "Andr\u00e9s Carofilis", "Petr Motlicek", "Karthik Pandia", "Shankar Venkatesan", "Kadri Hacio\u011flu", "Andreas Stolcke"], "title": "TokenVerse++: Towards Flexible Multitask Learning with Dynamic Task Activation", "comment": "Accepted to IEEE ASRU 2025. Copyright\\copyright 2025 IEEE", "summary": "Token-based multitasking frameworks like TokenVerse require all training\nutterances to have labels for all tasks, hindering their ability to leverage\npartially annotated datasets and scale effectively. We propose TokenVerse++,\nwhich introduces learnable vectors in the acoustic embedding space of the\nXLSR-Transducer ASR model for dynamic task activation. This core mechanism\nenables training with utterances labeled for only a subset of tasks, a key\nadvantage over TokenVerse. We demonstrate this by successfully integrating a\ndataset with partial labels, specifically for ASR and an additional task,\nlanguage identification, improving overall performance. TokenVerse++ achieves\nresults on par with or exceeding TokenVerse across multiple tasks, establishing\nit as a more practical multitask alternative without sacrificing ASR\nperformance.", "AI": {"tldr": "TokenVerse++\u5728TokenVerse\u57fa\u7840\u4e0a\u5f15\u5165\u53ef\u5b66\u4e60\u5411\u91cf\u673a\u5236\uff0c\u652f\u6301\u90e8\u5206\u6807\u6ce8\u6570\u636e\u7684\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u6846\u67b6\u9700\u8981\u5168\u6807\u6ce8\u6570\u636e\u7684\u9650\u5236\uff0c\u5728\u4e0d\u727a\u7272ASR\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u66f4\u5b9e\u7528\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8etoken\u7684\u591a\u4efb\u52a1\u6846\u67b6\uff08\u5982TokenVerse\uff09\u8981\u6c42\u6240\u6709\u8bad\u7ec3\u8bed\u53e5\u5fc5\u987b\u5305\u542b\u6240\u6709\u4efb\u52a1\u7684\u6807\u7b7e\uff0c\u8fd9\u9650\u5236\u4e86\u5229\u7528\u90e8\u5206\u6807\u6ce8\u6570\u636e\u96c6\u7684\u80fd\u529b\uff0c\u963b\u788d\u4e86\u6846\u67b6\u7684\u6709\u6548\u6269\u5c55\u3002", "method": "\u5728XLSR-Transducer ASR\u6a21\u578b\u7684\u58f0\u5b66\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5f15\u5165\u53ef\u5b66\u4e60\u5411\u91cf\u8fdb\u884c\u52a8\u6001\u4efb\u52a1\u6fc0\u6d3b\uff0c\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u4f7f\u7528\u4ec5\u6807\u6ce8\u4e86\u90e8\u5206\u4efb\u52a1\u7684\u8bed\u53e5\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u6210\u529f\u6574\u5408\u4ec5\u5305\u542bASR\u548c\u8bed\u8a00\u8bc6\u522b\u90e8\u5206\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0cTokenVerse++\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8fbe\u5230\u6216\u8d85\u8fc7TokenVerse\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "TokenVerse++\u901a\u8fc7\u52a8\u6001\u4efb\u52a1\u6fc0\u6d3b\u673a\u5236\uff0c\u4e3a\u591a\u4efb\u52a1\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u6301ASR\u6027\u80fd\u7684\u540c\u65f6\u6709\u6548\u5229\u7528\u90e8\u5206\u6807\u6ce8\u6570\u636e\u3002"}}
{"id": "2508.19873", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19873", "abs": "https://arxiv.org/abs/2508.19873", "authors": ["Vanessa Toborek", "Sebastian M\u00fcller", "Tim Selbach", "Tam\u00e1s Horv\u00e1th", "Christian Bauckhage"], "title": "Beyond Shallow Heuristics: Leveraging Human Intuition for Curriculum Learning", "comment": "Presented at ICNLSP 2025; to appear in the ACL Anthology; received\n  the Best Short Paper Award", "summary": "Curriculum learning (CL) aims to improve training by presenting data from\n\"easy\" to \"hard\", yet defining and measuring linguistic difficulty remains an\nopen challenge. We investigate whether human-curated simple language can serve\nas an effective signal for CL. Using the article-level labels from the Simple\nWikipedia corpus, we compare label-based curricula to competence-based\nstrategies relying on shallow heuristics. Our experiments with a BERT-tiny\nmodel show that adding simple data alone yields no clear benefit. However,\nstructuring it via a curriculum -- especially when introduced first --\nconsistently improves perplexity, particularly on simple language. In contrast,\ncompetence-based curricula lead to no consistent gains over random ordering,\nprobably because they fail to effectively separate the two classes. Our results\nsuggest that human intuition about linguistic difficulty can guide CL for\nlanguage model pre-training.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u4f7f\u7528Simple Wikipedia\u7684\u4eba\u7c7b\u6807\u6ce8\u7b80\u5355\u8bed\u8a00\u6570\u636e\u4f5c\u4e3a\u8bfe\u7a0b\u5b66\u4e60\u4fe1\u53f7\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u7b80\u5355\u8bed\u8a00\u4e0a\u7684\u56f0\u60d1\u5ea6\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u80fd\u529b\u7684\u8bfe\u7a0b\u7b56\u7565\u3002", "motivation": "\u8bfe\u7a0b\u5b66\u4e60(CL)\u901a\u8fc7\u4ece\u6613\u5230\u96be\u5448\u73b0\u6570\u636e\u6765\u6539\u5584\u8bad\u7ec3\uff0c\u4f46\u5b9a\u4e49\u548c\u8861\u91cf\u8bed\u8a00\u96be\u5ea6\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4eba\u7c7b\u6807\u6ce8\u7684\u7b80\u5355\u8bed\u8a00\u662f\u5426\u80fd\u4f5c\u4e3a\u6709\u6548\u7684CL\u4fe1\u53f7\u3002", "method": "\u4f7f\u7528Simple Wikipedia\u8bed\u6599\u5e93\u7684\u6587\u7ae0\u7ea7\u6807\u7b7e\uff0c\u5c06\u57fa\u4e8e\u6807\u7b7e\u7684\u8bfe\u7a0b\u4e0e\u4f9d\u8d56\u6d45\u5c42\u542f\u53d1\u5f0f\u7684\u57fa\u4e8e\u80fd\u529b\u7684\u7b56\u7565\u8fdb\u884c\u6bd4\u8f83\u3002\u4f7f\u7528BERT-tiny\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5206\u6790\u4e0d\u540c\u8bfe\u7a0b\u7b56\u7565\u7684\u6548\u679c\u3002", "result": "\u4ec5\u6dfb\u52a0\u7b80\u5355\u6570\u636e\u6ca1\u6709\u660e\u663e\u76ca\u5904\uff0c\u4f46\u901a\u8fc7\u8bfe\u7a0b\u7ed3\u6784\uff08\u7279\u522b\u662f\u5148\u5f15\u5165\u7b80\u5355\u6570\u636e\uff09\u80fd\u6301\u7eed\u6539\u5584\u56f0\u60d1\u5ea6\uff0c\u5c24\u5176\u5728\u7b80\u5355\u8bed\u8a00\u4e0a\u3002\u57fa\u4e8e\u80fd\u529b\u7684\u8bfe\u7a0b\u76f8\u6bd4\u968f\u673a\u6392\u5e8f\u6ca1\u6709\u4e00\u81f4\u589e\u76ca\u3002", "conclusion": "\u4eba\u7c7b\u5bf9\u8bed\u8a00\u96be\u5ea6\u7684\u76f4\u89c9\u53ef\u4ee5\u6307\u5bfc\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u7684\u8bfe\u7a0b\u5b66\u4e60\uff0c\u57fa\u4e8e\u4eba\u7c7b\u6807\u6ce8\u7684\u8bfe\u7a0b\u7b56\u7565\u6bd4\u57fa\u4e8e\u80fd\u529b\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u66f4\u6709\u6548\u3002"}}
{"id": "2508.19883", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.19883", "abs": "https://arxiv.org/abs/2508.19883", "authors": ["Chiman Salavati", "Shannon Song", "Scott A. Hale", "Roberto E. Montenegro", "Shiri Dori-Hacohen", "Fabricio Murai"], "title": "AI-Powered Detection of Inappropriate Language in Medical School Curricula", "comment": "Accepted at 2025 AAAI/ACM AI, Ethics and Society Conference (AIES'25)", "summary": "The use of inappropriate language -- such as outdated, exclusionary, or\nnon-patient-centered terms -- medical instructional materials can significantly\ninfluence clinical training, patient interactions, and health outcomes. Despite\ntheir reputability, many materials developed over past decades contain examples\nnow considered inappropriate by current medical standards. Given the volume of\ncurricular content, manually identifying instances of inappropriate use of\nlanguage (IUL) and its subcategories for systematic review is prohibitively\ncostly and impractical. To address this challenge, we conduct a first-in-class\nevaluation of small language models (SLMs) fine-tuned on labeled data and\npre-trained LLMs with in-context learning on a dataset containing approximately\n500 documents and over 12,000 pages. For SLMs, we consider: (1) a general IUL\nclassifier, (2) subcategory-specific binary classifiers, (3) a multilabel\nclassifier, and (4) a two-stage hierarchical pipeline for general IUL detection\nfollowed by multilabel classification. For LLMs, we consider variations of\nprompts that include subcategory definitions and/or shots. We found that both\nLLama-3 8B and 70B, even with carefully curated shots, are largely outperformed\nby SLMs. While the multilabel classifier performs best on annotated data,\nsupplementing training with unflagged excerpts as negative examples boosts the\nspecific classifiers' AUC by up to 25%, making them most effective models for\nmitigating harmful language in medical curricula.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b(SLMs)\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u68c0\u6d4b\u533b\u5b66\u6559\u6750\u4e2d\u4e0d\u5f53\u8bed\u8a00\u4f7f\u7528(IUL)\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0SLMs\u5728\u6027\u80fd\u4e0a\u4f18\u4e8eLLMs\uff0c\u591a\u6807\u7b7e\u5206\u7c7b\u5668\u8868\u73b0\u6700\u4f73\uff0c\u901a\u8fc7\u6dfb\u52a0\u672a\u6807\u8bb0\u6837\u672c\u4f5c\u4e3a\u8d1f\u6837\u672c\u53ef\u663e\u8457\u63d0\u5347\u5206\u7c7b\u5668\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u6559\u6750\u4e2d\u4f7f\u7528\u8fc7\u65f6\u3001\u6392\u4ed6\u6027\u6216\u975e\u60a3\u8005\u4e2d\u5fc3\u7684\u4e0d\u5f53\u8bed\u8a00\u4f1a\u5f71\u54cd\u4e34\u5e8a\u57f9\u8bad\u548c\u60a3\u8005\u5065\u5eb7\uff0c\u4f46\u4eba\u5de5\u8bc6\u522b\u6210\u672c\u9ad8\u6602\u4e14\u4e0d\u5207\u5b9e\u9645\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u7ea6500\u4efd\u6587\u6863\u3001\u8d85\u8fc712,000\u9875\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u56db\u79cdSLMs\u65b9\u6cd5(\u901a\u7528IUL\u5206\u7c7b\u5668\u3001\u5b50\u7c7b\u522b\u4e8c\u5143\u5206\u7c7b\u5668\u3001\u591a\u6807\u7b7e\u5206\u7c7b\u5668\u3001\u4e24\u9636\u6bb5\u5206\u5c42\u7ba1\u9053)\u548cLLMs\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "LLama-3 8B\u548c70B\u5373\u4f7f\u4f7f\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u8bcd\uff0c\u6027\u80fd\u4e5f\u5927\u5e45\u843d\u540e\u4e8eSLMs\u3002\u591a\u6807\u7b7e\u5206\u7c7b\u5668\u5728\u6807\u6ce8\u6570\u636e\u4e0a\u8868\u73b0\u6700\u597d\uff0c\u6dfb\u52a0\u672a\u6807\u8bb0\u6837\u672c\u4f5c\u4e3a\u8d1f\u6837\u672c\u53ef\u4f7f\u7279\u5b9a\u5206\u7c7b\u5668\u7684AUC\u63d0\u5347\u9ad8\u8fbe25%\u3002", "conclusion": "SLMs\u7279\u522b\u662f\u591a\u6807\u7b7e\u5206\u7c7b\u5668\u662f\u68c0\u6d4b\u533b\u5b66\u8bfe\u7a0b\u4e2d\u6709\u5bb3\u8bed\u8a00\u7684\u6700\u6709\u6548\u6a21\u578b\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u7b56\u7565\u53ef\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u533b\u5b66\u6559\u80b2\u6750\u6599\u7684\u8bed\u8a00\u89c4\u8303\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u81ea\u52a8\u5316\u5de5\u5177\u3002"}}
{"id": "2508.19887", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19887", "abs": "https://arxiv.org/abs/2508.19887", "authors": ["Mohammed Rakibul Hasan", "Rafi Majid", "Ahanaf Tahmid"], "title": "Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with LLM-Assisted Translation Refinement", "comment": null, "summary": "In this paper, we introduce Bangla-Bayanno, an open-ended Visual Question\nAnswering (VQA) Dataset in Bangla, a widely used, low-resource language in\nmultimodal AI research. The majority of existing datasets are either manually\nannotated with an emphasis on a specific domain, query type, or answer type or\nare constrained by niche answer formats. In order to mitigate human-induced\nerrors and guarantee lucidity, we implemented a multilingual LLM-assisted\ntranslation refinement pipeline. This dataset overcomes the issues of\nlow-quality translations from multilingual sources. The dataset comprises\n52,650 question-answer pairs across 4750+ images. Questions are classified into\nthree distinct answer types: nominal (short descriptive), quantitative\n(numeric), and polar (yes/no). Bangla-Bayanno provides the most comprehensive\nopen-source, high-quality VQA benchmark in Bangla, aiming to advance research\nin low-resource multimodal learning and facilitate the development of more\ninclusive AI systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86Bangla-Bayanno\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5b5f\u52a0\u62c9\u8bed\u5f00\u653e\u57df\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u5305\u542b52,650\u4e2a\u95ee\u7b54\u5bf9\u548c4,750+\u5f20\u56fe\u50cf\uff0c\u91c7\u7528\u591a\u8bed\u8a00LLM\u8f85\u52a9\u7ffb\u8bd1\u6d41\u7a0b\u786e\u4fdd\u8d28\u91cf", "motivation": "\u89e3\u51b3\u5b5f\u52a0\u62c9\u8bed\u8fd9\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u591a\u5a92\u4f53AI\u7814\u7a76\u4e2d\u7f3a\u4e4f\u9ad8\u8d28\u91cfVQA\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u73b0\u6709\u6570\u636e\u96c6\u8981\u4e48\u9886\u57df\u7279\u5b9a\uff0c\u8981\u4e48\u7b54\u6848\u683c\u5f0f\u53d7\u9650\uff0c\u4e14\u5b58\u5728\u7ffb\u8bd1\u8d28\u91cf\u95ee\u9898", "method": "\u4f7f\u7528\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u8f85\u52a9\u7684\u7ffb\u8bd1\u7cbe\u70bc\u6d41\u7a0b\u6765\u51cf\u5c11\u4eba\u5de5\u9519\u8bef\u5e76\u4fdd\u8bc1\u6e05\u6670\u5ea6\uff0c\u5c06\u95ee\u9898\u5206\u4e3a\u4e09\u7c7b\u7b54\u6848\u7c7b\u578b\uff1a\u540d\u8bcd\u6027\uff08\u7b80\u77ed\u63cf\u8ff0\uff09\u3001\u6570\u91cf\u6027\uff08\u6570\u5b57\uff09\u548c\u6781\u6027\uff08\u662f/\u5426\uff09", "result": "\u521b\u5efa\u4e86\u5305\u542b52,650\u4e2a\u95ee\u7b54\u5bf9\u548c4,750+\u5f20\u56fe\u50cf\u7684\u5b5f\u52a0\u62c9\u8bedVQA\u6570\u636e\u96c6\uff0c\u514b\u670d\u4e86\u591a\u8bed\u8a00\u6765\u6e90\u7684\u4f4e\u8d28\u91cf\u7ffb\u8bd1\u95ee\u9898", "conclusion": "Bangla-Bayanno\u63d0\u4f9b\u4e86\u6700\u5168\u9762\u7684\u5f00\u6e90\u9ad8\u8d28\u91cf\u5b5f\u52a0\u62c9\u8bedVQA\u57fa\u51c6\uff0c\u65e8\u5728\u63a8\u52a8\u4f4e\u8d44\u6e90\u591a\u6a21\u6001\u5b66\u4e60\u7814\u7a76\uff0c\u4fc3\u8fdb\u66f4\u5177\u5305\u5bb9\u6027\u7684AI\u7cfb\u7edf\u53d1\u5c55"}}
{"id": "2508.19903", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19903", "abs": "https://arxiv.org/abs/2508.19903", "authors": ["Ramya Keerthy Thatikonda", "Wray Buntine", "Ehsan Shareghi"], "title": "Logical Reasoning with Outcome Reward Models for Test-Time Scaling", "comment": "EMNLP 2025", "summary": "Logical reasoning is a critical benchmark for evaluating the capabilities of\nlarge language models (LLMs), as it reflects their ability to derive valid\nconclusions from given premises. While the combination of test-time scaling\nwith dedicated outcome or process reward models has opened up new avenues to\nenhance LLMs performance in complex reasoning tasks, this space is\nunder-explored in deductive logical reasoning. We present a set of Outcome\nReward Models (ORMs) for deductive reasoning. To train the ORMs we mainly\ngenerate data using Chain-of-Thought (CoT) with single and multiple samples.\nAdditionally, we propose a novel tactic to further expand the type of errors\ncovered in the training dataset of the ORM. In particular, we propose an echo\ngeneration technique that leverages LLMs' tendency to reflect incorrect\nassumptions made in prompts to extract additional training data, covering\npreviously unexplored error types. While a standard CoT chain may contain\nerrors likely to be made by the reasoner, the echo strategy deliberately steers\nthe model toward incorrect reasoning. We show that ORMs trained on CoT and\necho-augmented data demonstrate improved performance on the FOLIO, JustLogic,\nand ProverQA datasets across four different LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7528\u4e8e\u6f14\u7ece\u903b\u8f91\u63a8\u7406\u7684\u7ed3\u679c\u5956\u52b1\u6a21\u578b(ORMs)\uff0c\u901a\u8fc7CoT\u548c\u56de\u58f0\u751f\u6210\u6280\u672f\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\uff0c\u5728\u591a\u4e2a\u903b\u8f91\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u4e86LLM\u6027\u80fd\u3002", "motivation": "\u903b\u8f91\u63a8\u7406\u662f\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u5173\u952e\u57fa\u51c6\uff0c\u4f46\u76ee\u524d\u6d4b\u8bd5\u65f6\u7f29\u653e\u4e0e\u4e13\u7528\u5956\u52b1\u6a21\u578b\u7ed3\u5408\u7684\u65b9\u6cd5\u5728\u6f14\u7ece\u903b\u8f91\u63a8\u7406\u9886\u57df\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u5355\u6837\u672c\u548c\u591a\u6837\u672c\u7684\u601d\u7ef4\u94fe(CoT)\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u63d0\u51fa\u56de\u58f0\u751f\u6210\u6280\u672f\u6765\u6269\u5c55\u9519\u8bef\u7c7b\u578b\u8986\u76d6\uff0c\u901a\u8fc7\u5f15\u5bfc\u6a21\u578b\u8fdb\u884c\u9519\u8bef\u63a8\u7406\u6765\u83b7\u53d6\u989d\u5916\u8bad\u7ec3\u6837\u672c\u3002", "result": "\u5728FOLIO\u3001JustLogic\u548cProverQA\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u57fa\u4e8eCoT\u548c\u56de\u58f0\u589e\u5f3a\u6570\u636e\u8bad\u7ec3\u7684ORMs\u5728\u56db\u79cd\u4e0d\u540cLLM\u4e0a\u90fd\u8868\u73b0\u51fa\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684ORMs\u548c\u56de\u58f0\u751f\u6210\u6280\u672f\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u6f14\u7ece\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.19919", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19919", "abs": "https://arxiv.org/abs/2508.19919", "authors": ["Jingyu Guo", "Yingying Xu"], "title": "Your AI Bosses Are Still Prejudiced: The Emergence of Stereotypes in LLM-Based Multi-Agent Systems", "comment": null, "summary": "While stereotypes are well-documented in human social interactions, AI\nsystems are often presumed to be less susceptible to such biases. Previous\nstudies have focused on biases inherited from training data, but whether\nstereotypes can emerge spontaneously in AI agent interactions merits further\nexploration. Through a novel experimental framework simulating workplace\ninteractions with neutral initial conditions, we investigate the emergence and\nevolution of stereotypes in LLM-based multi-agent systems. Our findings reveal\nthat (1) LLM-Based AI agents develop stereotype-driven biases in their\ninteractions despite beginning without predefined biases; (2) stereotype\neffects intensify with increased interaction rounds and decision-making power,\nparticularly after introducing hierarchical structures; (3) these systems\nexhibit group effects analogous to human social behavior, including halo\neffects, confirmation bias, and role congruity; and (4) these stereotype\npatterns manifest consistently across different LLM architectures. Through\ncomprehensive quantitative analysis, these findings suggest that stereotype\nformation in AI systems may arise as an emergent property of multi-agent\ninteractions, rather than merely from training data biases. Our work\nunderscores the need for future research to explore the underlying mechanisms\nof this phenomenon and develop strategies to mitigate its ethical impacts.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0AI\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u65e0\u9884\u8bbe\u504f\u89c1\u7684\u4e2d\u6027\u521d\u59cb\u6761\u4ef6\u4e0b\uff0c\u4f1a\u81ea\u53d1\u4ea7\u751f\u523b\u677f\u5370\u8c61\u9a71\u52a8\u7684\u504f\u89c1\uff0c\u4e14\u8fd9\u79cd\u73b0\u8c61\u968f\u4ea4\u4e92\u8f6e\u6b21\u548c\u51b3\u7b56\u6743\u529b\u589e\u52a0\u800c\u52a0\u5267\uff0c\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u793e\u4f1a\u884c\u4e3a\u76f8\u4f3c\u7684\u7fa4\u4f53\u6548\u5e94\u3002", "motivation": "\u867d\u7136\u523b\u677f\u5370\u8c61\u5728\u4eba\u7c7b\u793e\u4ea4\u4e2d\u5df2\u88ab\u5145\u5206\u8bb0\u5f55\uff0c\u4f46AI\u7cfb\u7edf\u901a\u5e38\u88ab\u8ba4\u4e3a\u8f83\u5c11\u53d7\u6b64\u7c7b\u504f\u89c1\u5f71\u54cd\u3002\u4ee5\u5f80\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8bad\u7ec3\u6570\u636e\u5e26\u6765\u7684\u504f\u89c1\uff0c\u4f46\u523b\u677f\u5370\u8c61\u662f\u5426\u80fd\u5728AI\u667a\u80fd\u4f53\u4ea4\u4e92\u4e2d\u81ea\u53d1\u4ea7\u751f\u503c\u5f97\u6df1\u5165\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u804c\u573a\u4ea4\u4e92\u7684\u65b0\u578b\u5b9e\u9a8c\u6846\u67b6\uff0c\u5728\u65e0\u9884\u8bbe\u504f\u89c1\u7684\u4e2d\u6027\u521d\u59cb\u6761\u4ef6\u4e0b\uff0c\u7814\u7a76\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u523b\u677f\u5370\u8c61\u7684\u51fa\u73b0\u548c\u6f14\u5316\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(1)AI\u667a\u80fd\u4f53\u5728\u65e0\u9884\u8bbe\u504f\u89c1\u7684\u60c5\u51b5\u4e0b\u4f1a\u53d1\u5c55\u51fa\u523b\u677f\u5370\u8c61\u9a71\u52a8\u7684\u504f\u89c1\uff1b(2)\u523b\u677f\u5370\u8c61\u6548\u5e94\u968f\u4ea4\u4e92\u8f6e\u6b21\u548c\u51b3\u7b56\u6743\u529b\u589e\u52a0\u800c\u52a0\u5267\uff1b(3)\u7cfb\u7edf\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u7fa4\u4f53\u6548\u5e94\uff1b(4)\u8fd9\u79cd\u73b0\u8c61\u5728\u4e0d\u540cLLM\u67b6\u6784\u4e2d\u4e00\u81f4\u5b58\u5728\u3002", "conclusion": "AI\u7cfb\u7edf\u4e2d\u7684\u523b\u677f\u5370\u8c61\u5f62\u6210\u53ef\u80fd\u662f\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u6d8c\u73b0\u7279\u6027\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u8bad\u7ec3\u6570\u636e\u504f\u89c1\u7684\u7ed3\u679c\u3002\u9700\u8981\u8fdb\u4e00\u6b65\u63a2\u7d22\u8fd9\u79cd\u73b0\u8c61\u7684\u5e95\u5c42\u673a\u5236\u5e76\u5236\u5b9a\u7f13\u89e3\u5176\u4f26\u7406\u5f71\u54cd\u7684\u7b56\u7565\u3002"}}
{"id": "2508.19922", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19922", "abs": "https://arxiv.org/abs/2508.19922", "authors": ["Yifu Huo", "Chenglong Wang", "Qiren Zhu", "Shunjie Xing", "Tong Xiao", "Chunliang Zhang", "Tongran Liu", "Jinbo Zhu"], "title": "HEAL: A Hypothesis-Based Preference-Aware Analysis Framework", "comment": "Accepted by EMNLP 2025 Findings", "summary": "Preference optimization methods like DPO have achieved remarkable performance\nin LLM alignment. However, the evaluation for these methods relies on a single\nresponse and overlooks other potential outputs, which could also be generated\nin real-world applications within this hypothetical space. To address this\nissue, this paper presents a \\textbf{H}ypothesis-based\nPr\\textbf{E}ference-aware \\textbf{A}na\\textbf{L}ysis Framework (HEAL), a novel\nevaluation paradigm that formulates preference alignment as a re-ranking\nprocess within hypothesis spaces. The framework incorporates two complementary\nmetrics: ranking accuracy for evaluating ordinal consistency and preference\nstrength correlation for assessing continuous alignment. To facilitate this\nframework, we develop UniHypoBench, a unified hypothesis benchmark constructed\nfrom diverse instruction-response pairs. Through extensive experiments based on\nHEAL, with a particular focus on the intrinsic mechanisms of preference\nlearning, we demonstrate that current preference learning methods can\neffectively capture preferences provided by proxy models while simultaneously\nsuppressing negative samples. These findings contribute to preference learning\nresearch through two significant avenues. Theoretically, we introduce\nhypothesis space analysis as an innovative paradigm for understanding\npreference alignment. Practically, HEAL offers researchers robust diagnostic\ntools for refining preference optimization methods, while our empirical results\nidentify promising directions for developing more advanced alignment algorithms\ncapable of comprehensive preference capture.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86HEAL\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5047\u8bbe\u7a7a\u95f4\u91cd\u6392\u5e8f\u6765\u8bc4\u4f30\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5355\u54cd\u5e94\u8bc4\u4f30\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff08\u5982DPO\uff09\u7684\u8bc4\u4f30\u4ec5\u4f9d\u8d56\u5355\u4e00\u54cd\u5e94\uff0c\u5ffd\u7565\u4e86\u5047\u8bbe\u7a7a\u95f4\u4e2d\u5176\u4ed6\u6f5c\u5728\u8f93\u51fa\uff0c\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u771f\u5b9e\u5e94\u7528\u573a\u666f\u3002", "method": "\u63d0\u51faHEAL\u6846\u67b6\uff0c\u5c06\u504f\u597d\u5bf9\u9f50\u5efa\u6a21\u4e3a\u5047\u8bbe\u7a7a\u95f4\u5185\u7684\u91cd\u6392\u5e8f\u8fc7\u7a0b\uff0c\u5305\u542b\u6392\u5e8f\u51c6\u786e\u6027\u548c\u504f\u597d\u5f3a\u5ea6\u76f8\u5173\u6027\u4e24\u4e2a\u4e92\u8865\u6307\u6807\uff0c\u5e76\u6784\u5efa\u4e86UniHypoBench\u7edf\u4e00\u5047\u8bbe\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5f53\u524d\u504f\u597d\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u6349\u4ee3\u7406\u6a21\u578b\u7684\u504f\u597d\uff0c\u540c\u65f6\u6291\u5236\u8d1f\u9762\u6837\u672c\uff0c\u9a8c\u8bc1\u4e86HEAL\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "HEAL\u4e3a\u504f\u597d\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u521b\u65b0\u8303\u5f0f\u548c\u5b9e\u8df5\u8bca\u65ad\u5de5\u5177\uff0c\u4e3a\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u504f\u597d\u5bf9\u9f50\u7b97\u6cd5\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2508.19966", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19966", "abs": "https://arxiv.org/abs/2508.19966", "authors": ["Slimane Bellaouar", "Attia Nehar", "Soumia Souffi", "Mounia Bouameur"], "title": "Dhati+: Fine-tuned Large Language Models for Arabic Subjectivity Evaluation", "comment": "25 pages, 7 figures", "summary": "Despite its significance, Arabic, a linguistically rich and morphologically\ncomplex language, faces the challenge of being under-resourced. The scarcity of\nlarge annotated datasets hampers the development of accurate tools for\nsubjectivity analysis in Arabic. Recent advances in deep learning and\nTransformers have proven highly effective for text classification in English\nand French. This paper proposes a new approach for subjectivity assessment in\nArabic textual data. To address the dearth of specialized annotated datasets,\nwe developed a comprehensive dataset, AraDhati+, by leveraging existing Arabic\ndatasets and collections (ASTD, LABR, HARD, and SANAD). Subsequently, we\nfine-tuned state-of-the-art Arabic language models (XLM-RoBERTa, AraBERT, and\nArabianGPT) on AraDhati+ for effective subjectivity classification.\nFurthermore, we experimented with an ensemble decision approach to harness the\nstrengths of individual models. Our approach achieves a remarkable accuracy of\n97.79\\,\\% for Arabic subjectivity classification. Results demonstrate the\neffectiveness of the proposed approach in addressing the challenges posed by\nlimited resources in Arabic language processing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u963f\u62c9\u4f2f\u8bed\u4e3b\u89c2\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efaAraDhati+\u6570\u636e\u96c6\u5e76\u5fae\u8c03\u5148\u8fdb\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u963f\u62c9\u4f2f\u8bed\u4e3b\u89c2\u6027\u5206\u7c7b\u4e2d\u8fbe\u5230\u4e8697.79%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u4f5c\u4e3a\u4e00\u79cd\u8bed\u8a00\u4e30\u5bcc\u4e14\u5f62\u6001\u590d\u6742\u7684\u8bed\u8a00\uff0c\u9762\u4e34\u7740\u8d44\u6e90\u532e\u4e4f\u7684\u6311\u6218\u3002\u7f3a\u4e4f\u5927\u578b\u6807\u6ce8\u6570\u636e\u96c6\u963b\u788d\u4e86\u963f\u62c9\u4f2f\u8bed\u4e3b\u89c2\u6027\u5206\u6790\u5de5\u5177\u7684\u51c6\u786e\u5f00\u53d1\u3002", "method": "1) \u5229\u7528\u73b0\u6709\u963f\u62c9\u4f2f\u8bed\u6570\u636e\u96c6(ASTD\u3001LABR\u3001HARD\u3001SANAD)\u5f00\u53d1\u7efc\u5408\u6570\u636e\u96c6AraDhati+\uff1b2) \u5728AraDhati+\u4e0a\u5fae\u8c03\u6700\u5148\u8fdb\u7684\u963f\u62c9\u4f2f\u8bed\u8bed\u8a00\u6a21\u578b(XLM-RoBERTa\u3001AraBERT\u3001ArabianGPT)\uff1b3) \u5b9e\u9a8c\u96c6\u6210\u51b3\u7b56\u65b9\u6cd5\u4ee5\u5229\u7528\u5404\u6a21\u578b\u7684\u4f18\u52bf\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u963f\u62c9\u4f2f\u8bed\u4e3b\u89c2\u6027\u5206\u7c7b\u4e2d\u53d6\u5f97\u4e8697.79%\u7684\u663e\u8457\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u5904\u7406\u4e2d\u8d44\u6e90\u6709\u9650\u6311\u6218\u65b9\u9762\u975e\u5e38\u6709\u6548\uff0c\u8bc1\u660e\u4e86\u6df1\u5ea6\u5b66\u4e60\u548cTransformer\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u8bed\u6587\u672c\u5206\u7c7b\u4e2d\u7684\u9ad8\u6548\u6027\u3002"}}
{"id": "2508.19982", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19982", "abs": "https://arxiv.org/abs/2508.19982", "authors": ["Pengxiang Li", "Yefan Zhou", "Dilxat Muhtar", "Lu Yin", "Shilin Yan", "Li Shen", "Yi Liang", "Soroush Vosoughi", "Shiwei Liu"], "title": "Diffusion Language Models Know the Answer Before Decoding", "comment": null, "summary": "Diffusion language models (DLMs) have recently emerged as an alternative to\nautoregressive approaches, offering parallel sequence generation and flexible\ntoken orders. However, their inference remains slower than that of\nautoregressive models, primarily due to the cost of bidirectional attention and\nthe large number of refinement steps required for high quality outputs. In this\nwork, we highlight and leverage an overlooked property of DLMs early answer\nconvergence: in many cases, the correct answer can be internally identified by\nhalf steps before the final decoding step, both under semi-autoregressive and\nrandom remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99%\nof instances, respectively, can be decoded correctly using only half of the\nrefinement steps. Building on this observation, we introduce Prophet, a\ntraining-free fast decoding paradigm that enables early commit decoding.\nSpecifically, Prophet dynamically decides whether to continue refinement or to\ngo \"all-in\" (i.e., decode all remaining tokens in one step), using the\nconfidence gap between the top-2 prediction candidates as the criterion. It\nintegrates seamlessly into existing DLM implementations, incurs negligible\noverhead, and requires no additional training. Empirical evaluations of\nLLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the\nnumber of decoding steps by up to 3.4x while preserving high generation\nquality. These results recast DLM decoding as a problem of when to stop\nsampling, and demonstrate that early decode convergence provides a simple yet\npowerful mechanism for accelerating DLM inference, complementary to existing\nspeedup techniques. Our code is publicly available at\nhttps://github.com/pixeli99/Prophet.", "AI": {"tldr": "Prophet\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5feb\u901f\u89e3\u7801\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e2d\u65e9\u671f\u7b54\u6848\u6536\u655b\u7684\u7279\u6027\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u5dee\u8ddd\u52a8\u6001\u51b3\u5b9a\u4f55\u65f6\u505c\u6b62\u91c7\u6837\uff0c\u53ef\u5c06\u89e3\u7801\u6b65\u9aa4\u51cf\u5c113.4\u500d\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u867d\u7136\u63d0\u4f9b\u5e76\u884c\u5e8f\u5217\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5176\u63a8\u7406\u901f\u5ea6\u4ecd\u6162\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u53cc\u5411\u6ce8\u610f\u529b\u8ba1\u7b97\u548c\u5927\u91cf\u7ec6\u5316\u6b65\u9aa4\u7684\u6210\u672c\u3002\u7814\u7a76\u53d1\u73b0DLMs\u5b58\u5728\u65e9\u671f\u7b54\u6848\u6536\u655b\u73b0\u8c61\uff0c\u5373\u5728\u6700\u7ec8\u89e3\u7801\u6b65\u9aa4\u524d\u6a21\u578b\u5df2\u80fd\u8bc6\u522b\u6b63\u786e\u7b54\u6848\u3002", "method": "\u63d0\u51faProphet\u89e3\u7801\u8303\u5f0f\uff1a\u52a8\u6001\u76d1\u6d4btop-2\u9884\u6d4b\u5019\u9009\u7684\u7f6e\u4fe1\u5ea6\u5dee\u8ddd\uff0c\u5f53\u5dee\u8ddd\u8db3\u591f\u5927\u65f6\u8fdb\u884c\"all-in\"\u89e3\u7801\uff08\u4e00\u6b65\u89e3\u7801\u6240\u6709\u5269\u4f59token\uff09\uff0c\u5426\u5219\u7ee7\u7eed\u7ec6\u5316\u6b65\u9aa4\u3002\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709DLM\u5b9e\u73b0\u4e2d\u3002", "result": "\u5728GSM8K\u548cMMLU\u4efb\u52a1\u4e0a\uff0c\u5206\u522b\u670997%\u548c99%\u7684\u5b9e\u4f8b\u4ec5\u9700\u4e00\u534a\u7ec6\u5316\u6b65\u9aa4\u5373\u53ef\u6b63\u786e\u89e3\u7801\u3002\u5bf9LLaDA-8B\u548cDream-7B\u7684\u8bc4\u4f30\u663e\u793a\uff0cProphet\u53ef\u5c06\u89e3\u7801\u6b65\u9aa4\u51cf\u5c11\u9ad8\u8fbe3.4\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u751f\u6210\u3002", "conclusion": "Prophet\u5c06DLM\u89e3\u7801\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4f55\u65f6\u505c\u6b62\u91c7\u6837\u7684\u95ee\u9898\uff0c\u8bc1\u660e\u65e9\u671f\u89e3\u7801\u6536\u655b\u662f\u52a0\u901fDLM\u63a8\u7406\u7684\u7b80\u5355\u800c\u5f3a\u5927\u7684\u673a\u5236\uff0c\u4e0e\u73b0\u6709\u52a0\u901f\u6280\u672f\u4e92\u8865\u3002"}}
{"id": "2508.19988", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19988", "abs": "https://arxiv.org/abs/2508.19988", "authors": ["Lisa Alazraki", "Lihu Chen", "Ana Brassard", "Joe Stacey", "Hossein A. Rahmani", "Marek Rei"], "title": "AgentCoMa: A Compositional Benchmark Mixing Commonsense and Mathematical Reasoning in Real-World Scenarios", "comment": null, "summary": "Large Language Models (LLMs) have achieved high accuracy on complex\ncommonsense and mathematical problems that involve the composition of multiple\nreasoning steps. However, current compositional benchmarks testing these skills\ntend to focus on either commonsense or math reasoning, whereas LLM agents\nsolving real-world tasks would require a combination of both. In this work, we\nintroduce an Agentic Commonsense and Math benchmark (AgentCoMa), where each\ncompositional task requires a commonsense reasoning step and a math reasoning\nstep. We test it on 61 LLMs of different sizes, model families, and training\nstrategies. We find that LLMs can usually solve both steps in isolation, yet\ntheir accuracy drops by ~30% on average when the two are combined. This is a\nsubstantially greater performance gap than the one we observe in prior\ncompositional benchmarks that combine multiple steps of the same reasoning\ntype. In contrast, non-expert human annotators can solve the compositional\nquestions and the individual steps in AgentCoMa with similarly high accuracy.\nFurthermore, we conduct a series of interpretability studies to better\nunderstand the performance gap, examining neuron patterns, attention maps and\nmembership inference. Our work underscores a substantial degree of model\nbrittleness in the context of mixed-type compositional reasoning and offers a\ntest bed for future improvement.", "AI": {"tldr": "LLM\u5728\u5355\u4e00\u7c7b\u578b\u7684\u7ec4\u5408\u63a8\u7406\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6df7\u5408\u5e38\u8bc6\u63a8\u7406\u548c\u6570\u5b66\u63a8\u7406\u7684\u7ec4\u5408\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u4e0b\u964d\u7ea630%\uff0c\u663e\u793a\u51fa\u6a21\u578b\u8106\u5f31\u6027\u3002", "motivation": "\u73b0\u6709\u7ec4\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5355\u4e00\u63a8\u7406\u7c7b\u578b\uff08\u5e38\u8bc6\u6216\u6570\u5b66\uff09\uff0c\u800c\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u9700\u8981\u7ed3\u5408\u591a\u79cd\u63a8\u7406\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30LLM\u5728\u6df7\u5408\u7c7b\u578b\u7ec4\u5408\u63a8\u7406\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faAgentCoMa\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u9700\u8981\u5e38\u8bc6\u63a8\u7406\u548c\u6570\u5b66\u63a8\u7406\u6b65\u9aa4\u7684\u7ec4\u5408\u4efb\u52a1\uff0c\u6d4b\u8bd5\u4e8661\u4e2a\u4e0d\u540c\u89c4\u6a21\u3001\u6a21\u578b\u5bb6\u65cf\u548c\u8bad\u7ec3\u7b56\u7565\u7684LLM\uff0c\u5e76\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u7814\u7a76\uff08\u795e\u7ecf\u5143\u6a21\u5f0f\u3001\u6ce8\u610f\u529b\u56fe\u548c\u6210\u5458\u63a8\u65ad\u5206\u6790\uff09\u3002", "result": "LLM\u5728\u5355\u72ec\u89e3\u51b3\u5e38\u8bc6\u6216\u6570\u5b66\u6b65\u9aa4\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7ec4\u5408\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u5e73\u5747\u4e0b\u964d30%\uff0c\u8fdc\u9ad8\u4e8e\u540c\u7c7b\u578b\u591a\u6b65\u9aa4\u7ec4\u5408\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u4eba\u7c7b\u6ce8\u91ca\u8005\u5219\u5728\u7ec4\u5408\u95ee\u9898\u548c\u5355\u72ec\u6b65\u9aa4\u4e0a\u90fd\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLM\u5728\u6df7\u5408\u7c7b\u578b\u7ec4\u5408\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u7684\u6a21\u578b\u8106\u5f31\u6027\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2508.19993", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.19993", "abs": "https://arxiv.org/abs/2508.19993", "authors": ["Debanjana Kar", "Leopold B\u00f6ss", "Dacia Braca", "Sebastian Maximilian Dennerlein", "Nina Christine Hubig", "Philipp Wintersberger", "Yufang Hou"], "title": "MathBuddy: A Multimodal System for Affective Math Tutoring", "comment": null, "summary": "The rapid adoption of LLM-based conversational systems is already\ntransforming the landscape of educational technology. However, the current\nstate-of-the-art learning models do not take into account the student's\naffective states. Multiple studies in educational psychology support the claim\nthat positive or negative emotional states can impact a student's learning\ncapabilities. To bridge this gap, we present MathBuddy, an emotionally aware\nLLM-powered Math Tutor, which dynamically models the student's emotions and\nmaps them to relevant pedagogical strategies, making the tutor-student\nconversation a more empathetic one. The student's emotions are captured from\nthe conversational text as well as from their facial expressions. The student's\nemotions are aggregated from both modalities to confidently prompt our LLM\nTutor for an emotionally-aware response. We have effectively evaluated our\nmodel using automatic evaluation metrics across eight pedagogical dimensions\nand user studies. We report a massive 23 point performance gain using the win\nrate and a 3 point gain at an overall level using DAMR scores which strongly\nsupports our hypothesis of improving LLM-based tutor's pedagogical abilities by\nmodeling students' emotions.", "AI": {"tldr": "MathBuddy\u662f\u4e00\u4e2a\u60c5\u611f\u611f\u77e5\u7684\u6570\u5b66\u8f85\u5bfc\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u6790\u5b66\u751f\u7684\u6587\u672c\u5bf9\u8bdd\u548c\u9762\u90e8\u8868\u60c5\u6765\u8bc6\u522b\u60c5\u7eea\u72b6\u6001\uff0c\u5e76\u636e\u6b64\u8c03\u6574\u6559\u5b66\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u6559\u5b66\u52a9\u624b\u7684\u6559\u5b66\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u5b66\u4e60\u6a21\u578b\u6ca1\u6709\u8003\u8651\u5b66\u751f\u7684\u60c5\u611f\u72b6\u6001\uff0c\u800c\u6559\u80b2\u5fc3\u7406\u5b66\u7814\u7a76\u8868\u660e\u60c5\u7eea\u72b6\u6001\u4f1a\u5f71\u54cd\u5b66\u4e60\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u611f\u77e5\u548c\u54cd\u5e94\u5b66\u751f\u60c5\u7eea\u7684\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1MathBuddy\u7cfb\u7edf\uff0c\u4ece\u5bf9\u8bdd\u6587\u672c\u548c\u9762\u90e8\u8868\u60c5\u4e24\u4e2a\u6a21\u6001\u6355\u6349\u5b66\u751f\u60c5\u7eea\uff0c\u805a\u5408\u60c5\u7eea\u4fe1\u606f\u540e\u63d0\u793aLLM\u751f\u6210\u60c5\u611f\u611f\u77e5\u7684\u56de\u5e94\uff0c\u91c7\u7528\u76f8\u5173\u6559\u5b66\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u548c\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\uff0c\u5728\u516b\u4e2a\u6559\u5b66\u7ef4\u5ea6\u4e0a\u83b7\u5f97\u663e\u8457\u63d0\u5347\uff1a\u80dc\u7387\u63d0\u9ad823\u4e2a\u767e\u5206\u70b9\uff0cDAMR\u603b\u5206\u63d0\u9ad83\u5206\u3002", "conclusion": "\u901a\u8fc7\u5efa\u6a21\u5b66\u751f\u60c5\u7eea\u53ef\u4ee5\u663e\u8457\u6539\u5584\u57fa\u4e8eLLM\u7684\u6559\u5b66\u52a9\u624b\u7684\u6559\u5b66\u80fd\u529b\uff0c\u60c5\u611f\u611f\u77e5\u662f\u63d0\u5347\u6559\u80b2\u6280\u672f\u6548\u679c\u7684\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2508.19996", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19996", "abs": "https://arxiv.org/abs/2508.19996", "authors": ["Yiming Du", "Yifan Xiang", "Bin Liang", "Dahua Lin", "Kam-Fai Wong", "Fei Tan"], "title": "ReSURE: Regularizing Supervision Unreliability for Multi-turn Dialogue Fine-tuning", "comment": null, "summary": "Fine-tuning multi-turn dialogue systems requires high-quality supervision but\noften suffers from degraded performance when exposed to low-quality data.\nSupervision errors in early turns can propagate across subsequent turns,\nundermining coherence and response quality. Existing methods typically address\ndata quality via static prefiltering, which decouples quality control from\ntraining and fails to mitigate turn-level error propagation. In this context,\nwe propose ReSURE (Regularizing Supervision UnREliability), an adaptive\nlearning method that dynamically down-weights unreliable supervision without\nexplicit filtering. ReSURE estimates per-turn loss distributions using\nWelford's online statistics and reweights sample losses on the fly accordingly.\nExperiments on both single-source and mixed-quality datasets show improved\nstability and response quality. Notably, ReSURE enjoys positive Spearman\ncorrelations (0.21 ~ 1.0 across multiple benchmarks) between response scores\nand number of samples regardless of data quality, which potentially paves the\nway for utilizing large-scale data effectively. Code is publicly available at\nhttps://github.com/Elvin-Yiming-Du/ReSURE_Multi_Turn_Training.", "AI": {"tldr": "ReSURE\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u964d\u4f4e\u4e0d\u53ef\u9760\u76d1\u7763\u7684\u6743\u91cd\u6765\u89e3\u51b3\u591a\u8f6e\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u9519\u8bef\u4f20\u64ad\u95ee\u9898\uff0c\u65e0\u9700\u663e\u5f0f\u8fc7\u6ee4\u6570\u636e", "motivation": "\u591a\u8f6e\u5bf9\u8bdd\u7cfb\u7edf\u5fae\u8c03\u9700\u8981\u9ad8\u8d28\u91cf\u76d1\u7763\uff0c\u4f46\u4f4e\u8d28\u91cf\u6570\u636e\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u65e9\u671f\u8f6e\u6b21\u7684\u76d1\u7763\u9519\u8bef\u4f1a\u4f20\u64ad\u5230\u540e\u7eed\u8f6e\u6b21\uff0c\u7834\u574f\u8fde\u8d2f\u6027\u548c\u54cd\u5e94\u8d28\u91cf\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u9759\u6001\u9884\u8fc7\u6ee4\u5904\u7406\u6570\u636e\u8d28\u91cf\uff0c\u4f46\u5c06\u8d28\u91cf\u63a7\u5236\u4e0e\u8bad\u7ec3\u89e3\u8026\uff0c\u65e0\u6cd5\u7f13\u89e3\u8f6e\u7ea7\u9519\u8bef\u4f20\u64ad", "method": "ReSURE\u4f7f\u7528Welford\u5728\u7ebf\u7edf\u8ba1\u65b9\u6cd5\u4f30\u8ba1\u6bcf\u8f6e\u635f\u5931\u5206\u5e03\uff0c\u5e76\u636e\u6b64\u52a8\u6001\u91cd\u65b0\u52a0\u6743\u6837\u672c\u635f\u5931\uff0c\u81ea\u9002\u5e94\u5730\u964d\u4f4e\u4e0d\u53ef\u9760\u76d1\u7763\u7684\u6743\u91cd", "result": "\u5728\u5355\u6e90\u548c\u6df7\u5408\u8d28\u91cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cReSURE\u63d0\u9ad8\u4e86\u7a33\u5b9a\u6027\u548c\u54cd\u5e94\u8d28\u91cf\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u54cd\u5e94\u5206\u6570\u4e0e\u6837\u672c\u6570\u91cf\u4e4b\u95f4\u5448\u73b0\u6b63Spearman\u76f8\u5173\u6027\uff080.21~1.0\uff09\uff0c\u65e0\u8bba\u6570\u636e\u8d28\u91cf\u5982\u4f55", "conclusion": "ReSURE\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8f6e\u5bf9\u8bdd\u8bad\u7ec3\u4e2d\u7684\u9519\u8bef\u4f20\u64ad\u95ee\u9898\uff0c\u4e3a\u6709\u6548\u5229\u7528\u5927\u89c4\u6a21\u6570\u636e\u94fa\u5e73\u4e86\u9053\u8def"}}
{"id": "2508.19997", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.19997", "abs": "https://arxiv.org/abs/2508.19997", "authors": ["Boheng Mao"], "title": "Selective Retrieval-Augmentation for Long-Tail Legal Text Classification", "comment": null, "summary": "Legal text classification is a fundamental NLP task in the legal domain.\nBenchmark datasets in this area often exhibit a long-tail label distribution,\nwhere many labels are underrepresented, leading to poor model performance on\nrare classes. This paper proposes Selective Retrieval-Augmentation (SRA) as a\nsolution to this problem. SRA focuses on augmenting samples belonging to\nlow-frequency labels in the training set, preventing the introduction of noise\nfor well-represented classes, and requires no changes to the model\narchitecture. Retrieval is performed only from the training data to ensure\nthere is no potential information leakage, removing the need for external\ncorpora simultaneously. The proposed SRA method is tested on two legal text\nclassification benchmark datasets with long-tail distributions: LEDGAR\n(single-label) and UNFAIR-ToS (multi-label). The results indicate that SRA\nattains higher micro-F1 and macro-F1 scores compared to all current LexGLUE\nbaselines across both datasets, illustrating consistent improvements in\nlong-tail legal text classification. The code repository is available at:\nhttps://github.com/Boheng-Mao/sra-legal", "AI": {"tldr": "\u63d0\u51fa\u9009\u62e9\u6027\u68c0\u7d22\u589e\u5f3a(SRA)\u65b9\u6cd5\u89e3\u51b3\u6cd5\u5f8b\u6587\u672c\u5206\u7c7b\u4e2d\u7684\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u901a\u8fc7\u4ec5\u5bf9\u4f4e\u9891\u6807\u7b7e\u6837\u672c\u8fdb\u884c\u68c0\u7d22\u589e\u5f3a\uff0c\u907f\u514d\u5f15\u5165\u566a\u58f0\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\uff0c\u5728LEDGAR\u548cUNFAIR-ToS\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u7684\u6027\u80fd\u3002", "motivation": "\u6cd5\u5f8b\u6587\u672c\u5206\u7c7b\u57fa\u51c6\u6570\u636e\u96c6\u901a\u5e38\u5b58\u5728\u957f\u5c3e\u6807\u7b7e\u5206\u5e03\u95ee\u9898\uff0c\u8bb8\u591a\u6807\u7b7e\u6837\u672c\u4e0d\u8db3\u5bfc\u81f4\u6a21\u578b\u5728\u7a00\u6709\u7c7b\u522b\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u6539\u5584\u957f\u5c3e\u5206\u5e03\u4e0b\u7684\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u9009\u62e9\u6027\u68c0\u7d22\u589e\u5f3a(SRA)\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u589e\u5f3a\u8bad\u7ec3\u96c6\u4e2d\u4f4e\u9891\u6807\u7b7e\u7684\u6837\u672c\uff0c\u4ec5\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u8fdb\u884c\u68c0\u7d22\u4ee5\u907f\u514d\u4fe1\u606f\u6cc4\u9732\uff0c\u4e0d\u9700\u8981\u5916\u90e8\u8bed\u6599\u5e93\uff0c\u4e14\u65e0\u9700\u6539\u53d8\u6a21\u578b\u67b6\u6784\u3002", "result": "\u5728\u4e24\u4e2a\u6cd5\u5f8b\u6587\u672c\u5206\u7c7b\u57fa\u51c6\u6570\u636e\u96c6(LEDGAR\u548cUNFAIR-ToS)\u4e0a\uff0cSRA\u65b9\u6cd5\u5728micro-F1\u548cmacro-F1\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u7684LexGLUE\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u957f\u5c3e\u6cd5\u5f8b\u6587\u672c\u5206\u7c7b\u65b9\u9762\u5b9e\u73b0\u4e86\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "SRA\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6cd5\u5f8b\u6587\u672c\u5206\u7c7b\u4e2d\u7684\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u589e\u5f3a\u4f4e\u9891\u6807\u7b7e\u6837\u672c\u800c\u4e0d\u5f15\u5165\u566a\u58f0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u7a00\u6709\u7c7b\u522b\u4e0a\u7684\u6027\u80fd\uff0c\u4e3a\u957f\u5c3e\u6cd5\u5f8b\u6587\u672c\u5206\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.20033", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20033", "abs": "https://arxiv.org/abs/2508.20033", "authors": ["Liana Patel", "Negar Arabzadeh", "Harshit Gupta", "Ankita Sundar", "Ion Stoica", "Matei Zaharia", "Carlos Guestrin"], "title": "DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis", "comment": null, "summary": "The ability to research and synthesize knowledge is central to human\nexpertise and progress. An emerging class of systems promises these exciting\ncapabilities through generative research synthesis, performing retrieval over\nthe live web and synthesizing discovered sources into long-form, cited\nsummaries. However, evaluating such systems remains an open challenge: existing\nquestion-answering benchmarks focus on short-form factual responses, while\nexpert-curated datasets risk staleness and data contamination. Both fail to\ncapture the complexity and evolving nature of real research synthesis tasks. In\nthis work, we introduce DeepScholar-bench, a live benchmark and holistic,\nautomated evaluation framework designed to evaluate generative research\nsynthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv\npapers and focuses on a real research synthesis task: generating the related\nwork sections of a paper by retrieving, synthesizing, and citing prior\nresearch. Our evaluation framework holistically assesses performance across\nthree key dimensions, knowledge synthesis, retrieval quality, and\nverifiability. We also develop DeepScholar-base, a reference pipeline\nimplemented efficiently using the LOTUS API. Using the DeepScholar-bench\nframework, we perform a systematic evaluation of prior open-source systems,\nsearch AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that\nDeepScholar-base establishes a strong baseline, attaining competitive or higher\nperformance than each other method. We also find that DeepScholar-bench remains\nfar from saturated, with no system exceeding a score of $19\\%$ across all\nmetrics. These results underscore the difficulty of DeepScholar-bench, as well\nas its importance for progress towards AI systems capable of generative\nresearch synthesis. We make our code available at\nhttps://github.com/guestrin-lab/deepscholar-bench.", "AI": {"tldr": "DeepScholar-bench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u5f0f\u7814\u7a76\u5408\u6210\u7cfb\u7edf\u7684\u5b9e\u65f6\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u4ecearXiv\u8bba\u6587\u4e2d\u63d0\u53d6\u67e5\u8be2\u4efb\u52a1\uff0c\u8bc4\u4f30\u7cfb\u7edf\u5728\u77e5\u8bc6\u5408\u6210\u3001\u68c0\u7d22\u8d28\u91cf\u548c\u53ef\u9a8c\u8bc1\u6027\u4e09\u4e2a\u7ef4\u5ea6\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7b80\u77ed\u7684\u4e8b\u5b9e\u6027\u56de\u7b54\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u7814\u7a76\u5408\u6210\u4efb\u52a1\u7684\u590d\u6742\u6027\u548c\u52a8\u6001\u6027\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u63a8\u52a8\u751f\u6210\u5f0f\u7814\u7a76\u5408\u6210\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u4ece\u9ad8\u8d28\u91cfarXiv\u8bba\u6587\u4e2d\u63d0\u53d6\u67e5\u8be2\u4efb\u52a1\uff0c\u6784\u5efa\u5b9e\u65f6\u57fa\u51c6\u6d4b\u8bd5\uff1b\u5f00\u53d1\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u4ece\u77e5\u8bc6\u5408\u6210\u3001\u68c0\u7d22\u8d28\u91cf\u548c\u53ef\u9a8c\u8bc1\u6027\u4e09\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\uff1b\u5b9e\u73b0\u57fa\u4e8eLOTUS API\u7684\u53c2\u8003\u6d41\u6c34\u7ebfDeepScholar-base\u3002", "result": "DeepScholar-base\u5efa\u7acb\u4e86\u5f3a\u5927\u7684\u57fa\u7ebf\u6027\u80fd\uff0c\u5728\u6240\u6709\u8bc4\u4f30\u7cfb\u7edf\u4e2d\u8868\u73b0\u6700\u4f73\u6216\u76f8\u5f53\uff0c\u4f46\u6240\u6709\u7cfb\u7edf\u5728\u6240\u6709\u6307\u6807\u4e0a\u7684\u5f97\u5206\u5747\u672a\u8d85\u8fc719%\uff0c\u8868\u660e\u57fa\u51c6\u6d4b\u8bd5\u96be\u5ea6\u8f83\u9ad8\u4e14\u8fdc\u672a\u9971\u548c\u3002", "conclusion": "DeepScholar-bench\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5bf9\u4e8e\u63a8\u52a8\u80fd\u591f\u8fdb\u884c\u751f\u6210\u5f0f\u7814\u7a76\u5408\u6210\u7684AI\u7cfb\u7edf\u53d1\u5c55\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u5f53\u524d\u7cfb\u7edf\u6027\u80fd\u4ecd\u6709\u5f88\u5927\u63d0\u5347\u7a7a\u95f4\u3002"}}
{"id": "2508.20038", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20038", "abs": "https://arxiv.org/abs/2508.20038", "authors": ["Sheng Liu", "Qiang Sheng", "Danding Wang", "Yang Li", "Guang Yang", "Juan Cao"], "title": "Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks", "comment": "EMNLP 2025 findings", "summary": "Despite advances in improving large language model(LLM) to refuse to answer\nmalicious instructions, widely used LLMs remain vulnerable to jailbreak attacks\nwhere attackers generate instructions with distributions differing from safety\nalignment corpora. New attacks expose LLMs' inability to recognize unseen\nmalicious instructions, highlighting a critical distributional mismatch between\ntraining data and real-world attacks that forces developers into reactive\npatching cycles. To tackle this challenge, we propose IMAGINE, a synthesis\nframework that leverages embedding space distribution analysis to generate\njailbreak-like instructions. This approach effectively fills the distributional\ngap between authentic jailbreak patterns and safety alignment corpora. IMAGINE\nfollows an iterative optimization process that dynamically evolves text\ngeneration distributions across iterations, thereby augmenting the coverage of\nsafety alignment data distributions through synthesized data examples. Based on\nthe safety-aligned corpus enhanced through IMAGINE, our framework demonstrates\nsignificant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2\nwithout compromising their utility.", "AI": {"tldr": "IMAGINE\u662f\u4e00\u4e2a\u901a\u8fc7\u5d4c\u5165\u7a7a\u95f4\u5206\u5e03\u5206\u6790\u751f\u6210\u8d8a\u72f1\u6307\u4ee4\u7684\u5408\u6210\u6846\u67b6\uff0c\u65e8\u5728\u586b\u8865\u771f\u5b9e\u8d8a\u72f1\u6a21\u5f0f\u4e0e\u5b89\u5168\u5bf9\u9f50\u8bed\u6599\u5e93\u4e4b\u95f4\u7684\u5206\u5e03\u5dee\u8ddd\uff0c\u663e\u8457\u964d\u4f4eLLM\u7684\u653b\u51fb\u6210\u529f\u7387\u800c\u4e0d\u5f71\u54cd\u5b9e\u7528\u6027\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u62d2\u7edd\u56de\u7b54\u6076\u610f\u6307\u4ee4\u65b9\u9762\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u4ecd\u5bb9\u6613\u53d7\u5230\u5206\u5e03\u4e0d\u540c\u4e8e\u5b89\u5168\u5bf9\u9f50\u8bed\u6599\u7684\u8d8a\u72f1\u653b\u51fb\uff0c\u66b4\u9732\u4e86\u8bad\u7ec3\u6570\u636e\u4e0e\u73b0\u5b9e\u653b\u51fb\u4e4b\u95f4\u7684\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u63d0\u51faIMAGINE\u6846\u67b6\uff0c\u5229\u7528\u5d4c\u5165\u7a7a\u95f4\u5206\u5e03\u5206\u6790\u751f\u6210\u8d8a\u72f1\u7c7b\u6307\u4ee4\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\u52a8\u6001\u6f14\u5316\u6587\u672c\u751f\u6210\u5206\u5e03\uff0c\u589e\u5f3a\u5b89\u5168\u5bf9\u9f50\u6570\u636e\u5206\u5e03\u7684\u8986\u76d6\u8303\u56f4\u3002", "result": "\u57fa\u4e8eIMAGINE\u589e\u5f3a\u7684\u5b89\u5168\u5bf9\u9f50\u8bed\u6599\u5e93\uff0c\u5728Qwen2.5\u3001Llama3.1\u548cLlama3.2\u4e0a\u663e\u8457\u964d\u4f4e\u4e86\u653b\u51fb\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "IMAGINE\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u5b89\u5168\u5bf9\u9f50\u4e2d\u7684\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u586b\u8865\u5206\u5e03\u5dee\u8ddd\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u672a\u77e5\u6076\u610f\u6307\u4ee4\u7684\u8bc6\u522b\u80fd\u529b\u3002"}}
{"id": "2508.20047", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20047", "abs": "https://arxiv.org/abs/2508.20047", "authors": ["Hassan Alhuzali", "Farah Shamout", "Muhammad Abdul-Mageed", "Chaimae Abouzahir", "Mouath Abu-Daoud", "Ashwag Alasmari", "Walid Al-Eisawi", "Renad Al-Monef", "Ali Alqahtani", "Lama Ayash", "Nizar Habash", "Leen Kharouf"], "title": "AraHealthQA 2025 Shared Task Description Paper", "comment": null, "summary": "We introduce {AraHealthQA 2025}, the {Comprehensive Arabic Health Question\nAnswering Shared Task}, held in conjunction with {ArabicNLP 2025} (co-located\nwith EMNLP 2025). This shared task addresses the paucity of high-quality Arabic\nmedical QA resources by offering two complementary tracks: {MentalQA}, focusing\non Arabic mental health Q\\&A (e.g., anxiety, depression, stigma reduction), and\n{MedArabiQ}, covering broader medical domains such as internal medicine,\npediatrics, and clinical decision making. Each track comprises multiple\nsubtasks, evaluation datasets, and standardized metrics, facilitating fair\nbenchmarking. The task was structured to promote modeling under realistic,\nmultilingual, and culturally nuanced healthcare contexts. We outline the\ndataset creation, task design and evaluation framework, participation\nstatistics, baseline systems, and summarize the overall outcomes. We conclude\nwith reflections on the performance trends observed and prospects for future\niterations in Arabic health QA.", "AI": {"tldr": "AraHealthQA 2025\u662f\u9996\u4e2a\u5168\u9762\u7684\u963f\u62c9\u4f2f\u8bed\u5065\u5eb7\u95ee\u7b54\u5171\u4eab\u4efb\u52a1\uff0c\u5305\u542bMentalQA\uff08\u5fc3\u7406\u5065\u5eb7\uff09\u548cMedArabiQ\uff08\u7efc\u5408\u533b\u7597\uff09\u4e24\u4e2a\u8d5b\u9053\uff0c\u65e8\u5728\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u533b\u7597QA\u8d44\u6e90\u532e\u4e4f\u95ee\u9898", "motivation": "\u89e3\u51b3\u9ad8\u8d28\u91cf\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u95ee\u7b54\u8d44\u6e90\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u4fc3\u8fdb\u5728\u73b0\u5b9e\u3001\u591a\u8bed\u8a00\u548c\u6587\u5316\u654f\u611f\u7684\u533b\u7597\u573a\u666f\u4e0b\u7684\u6a21\u578b\u5f00\u53d1", "method": "\u901a\u8fc7\u521b\u5efa\u4e24\u4e2a\u4e92\u8865\u8d5b\u9053\uff1aMentalQA\u4e13\u6ce8\u4e8e\u5fc3\u7406\u5065\u5eb7\u9886\u57df\uff0cMedArabiQ\u8986\u76d6\u66f4\u5e7f\u6cdb\u7684\u533b\u7597\u9886\u57df\uff1b\u6bcf\u4e2a\u8d5b\u9053\u5305\u542b\u591a\u4e2a\u5b50\u4efb\u52a1\u3001\u8bc4\u4f30\u6570\u636e\u96c6\u548c\u6807\u51c6\u5316\u6307\u6807", "result": "\u5efa\u7acb\u4e86\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4fc3\u8fdb\u4e86\u516c\u5e73\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u603b\u7ed3\u4e86\u53c2\u4e0e\u7edf\u8ba1\u3001\u57fa\u7ebf\u7cfb\u7edf\u548c\u6574\u4f53\u6210\u679c", "conclusion": "\u8be5\u4efb\u52a1\u4e3a\u963f\u62c9\u4f2f\u8bed\u5065\u5eb7\u95ee\u7b54\u9886\u57df\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u603b\u7ed3\u4e86\u6027\u80fd\u8d8b\u52bf\u5e76\u4e3a\u672a\u6765\u8fed\u4ee3\u63d0\u4f9b\u4e86\u5c55\u671b"}}
{"id": "2508.20068", "categories": ["cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20068", "abs": "https://arxiv.org/abs/2508.20068", "authors": ["Chengzu Li", "Wenshan Wu", "Huanyu Zhang", "Qingtao Li", "Zeyu Gao", "Yan Xia", "Jos\u00e9 Hern\u00e1ndez-Orallo", "Ivan Vuli\u0107", "Furu Wei"], "title": "11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis", "comment": "9 pages, 4 figures (22 pages, 7 figures, 7 tables including\n  references and appendices)", "summary": "For human cognitive process, spatial reasoning and perception are closely\nentangled, yet the nature of this interplay remains underexplored in the\nevaluation of multimodal large language models (MLLMs). While recent MLLM\nadvancements show impressive performance on reasoning, their capacity for\nhuman-like spatial cognition remains an open question. In this work, we\nintroduce a systematic evaluation framework to assess the spatial reasoning\nabilities of state-of-the-art MLLMs relative to human performance. Central to\nour work is 11Plus-Bench, a high-quality benchmark derived from realistic\nstandardized spatial aptitude tests. 11Plus-Bench also features fine-grained\nexpert annotations of both perceptual complexity and reasoning process,\nenabling detailed instance-level analysis of model behavior. Through extensive\nexperiments across 14 MLLMs and human evaluation, we find that current MLLMs\nexhibit early signs of spatial cognition. Despite a large performance gap\ncompared to humans, MLLMs' cognitive profiles resemble those of humans in that\ncognitive effort correlates strongly with reasoning-related complexity.\nHowever, instance-level performance in MLLMs remains largely random, whereas\nhuman correctness is highly predictable and shaped by abstract pattern\ncomplexity. These findings highlight both emerging capabilities and limitations\nin current MLLMs' spatial reasoning capabilities and provide actionable\ninsights for advancing model design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e8611Plus-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524dMLLMs\u5c55\u73b0\u51fa\u7a7a\u95f4\u8ba4\u77e5\u7684\u65e9\u671f\u8ff9\u8c61\uff0c\u4f46\u4e0e\u4eba\u7c7b\u5b58\u5728\u8f83\u5927\u6027\u80fd\u5dee\u8ddd\uff0c\u4e14\u5b9e\u4f8b\u7ea7\u8868\u73b0\u968f\u673a\u6027\u8f83\u5f3a\u3002", "motivation": "\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u4e2d\u7a7a\u95f4\u63a8\u7406\u548c\u611f\u77e5\u7d27\u5bc6\u76f8\u5173\uff0c\u4f46\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30MLLMs\u662f\u5426\u5177\u5907\u7c7b\u4eba\u7a7a\u95f4\u8ba4\u77e5\u80fd\u529b\u3002", "method": "\u6784\u5efa11Plus-Bench\u9ad8\u8d28\u91cf\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6e90\u81ea\u771f\u5b9e\u6807\u51c6\u5316\u7a7a\u95f4\u80fd\u529b\u6d4b\u8bd5\uff0c\u5305\u542b\u7ec6\u7c92\u5ea6\u4e13\u5bb6\u6807\u6ce8\u7684\u611f\u77e5\u590d\u6742\u5ea6\u548c\u63a8\u7406\u8fc7\u7a0b\u3002\u5bf914\u4e2aMLLMs\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u5e76\u4e0e\u4eba\u7c7b\u8868\u73b0\u5bf9\u6bd4\u3002", "result": "\u5f53\u524dMLLMs\u5c55\u73b0\u51fa\u7a7a\u95f4\u8ba4\u77e5\u7684\u65e9\u671f\u8ff9\u8c61\uff0c\u8ba4\u77e5\u52aa\u529b\u4e0e\u63a8\u7406\u76f8\u5173\u590d\u6742\u5ea6\u5f3a\u76f8\u5173\uff0c\u7c7b\u4f3c\u4eba\u7c7b\u6a21\u5f0f\u3002\u4f46\u4e0e\u4eba\u7c7b\u5b58\u5728\u8f83\u5927\u6027\u80fd\u5dee\u8ddd\uff0c\u5b9e\u4f8b\u7ea7\u8868\u73b0\u968f\u673a\uff0c\u800c\u4eba\u7c7b\u8868\u73b0\u9ad8\u5ea6\u53ef\u9884\u6d4b\u4e14\u53d7\u62bd\u8c61\u6a21\u5f0f\u590d\u6742\u5ea6\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dMLLMs\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u65b0\u5174\u80fd\u529b\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u8868\u660e\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u5b9e\u73b0\u66f4\u7c7b\u4eba\u7684\u7a7a\u95f4\u8ba4\u77e5\u80fd\u529b\u3002"}}
