{"id": "2508.18673", "categories": ["cs.CL", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.18673", "abs": "https://arxiv.org/abs/2508.18673", "authors": ["Xinglong Yang", "Quan Feng", "Zhongying Pan", "Xiang Chen", "Yu Tian", "Wentong Li", "Shuofei Qiao", "Yuxia Geng", "Xingyu Zhao", "Sheng-Jun Huang"], "title": "Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum", "comment": null, "summary": "The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often\nlimited by the use of randomly or manually selected examples. These examples\nfail to account for both model-specific knowledge distributions and the\nintrinsic complexity of the tasks, resulting in suboptimal and unstable model\nperformance. To address this, we propose a novel framework inspired by the\npedagogical principle of \"tailored teaching with balanced difficulty\". We\nreframe prompt selection as a prompt curriculum design problem: constructing a\nwell ordered set of training examples that align with the model's current\ncapabilities. Our approach integrates two complementary signals: (1)\nmodel-perceived difficulty, quantified through prediction disagreement in an\nactive learning setup, capturing what the model itself finds challenging; and\n(2) intrinsic sample complexity, which measures the inherent difficulty of each\nquestion-image pair independently of any model. By jointly analyzing these\nsignals, we develop a difficulty-balanced sampling strategy that ensures the\nselected prompt examples are diverse across both dimensions. Extensive\nexperiments conducted on five challenging benchmarks and multiple popular\nMultimodal Large Language Models (MLLMs) demonstrate that our method yields\nsubstantial and consistent improvements and greatly reduces performance\ndiscrepancies caused by random sampling, providing a principled and robust\napproach for enhancing multimodal reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96be\u5ea6\u5e73\u8861\u91c7\u6837\u7684\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u63d0\u793a\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5206\u6790\u6a21\u578b\u611f\u77e5\u96be\u5ea6\u548c\u6837\u672c\u5185\u5728\u590d\u6742\u5ea6\u6765\u4f18\u5316\u793a\u4f8b\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u6027\u80fd", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u63d0\u793a\u4e2d\u968f\u673a\u6216\u624b\u52a8\u9009\u62e9\u793a\u4f8b\u7684\u65b9\u6cd5\u65e0\u6cd5\u8003\u8651\u6a21\u578b\u7279\u5b9a\u77e5\u8bc6\u5206\u5e03\u548c\u4efb\u52a1\u5185\u5728\u590d\u6742\u5ea6\uff0c\u5bfc\u81f4\u6027\u80fd\u6b21\u4f18\u4e14\u4e0d\u7a33\u5b9a", "method": "\u5c06\u63d0\u793a\u9009\u62e9\u91cd\u6784\u4e3a\u8bfe\u7a0b\u8bbe\u8ba1\u95ee\u9898\uff0c\u7ed3\u5408\u6a21\u578b\u611f\u77e5\u96be\u5ea6\uff08\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u4e2d\u7684\u9884\u6d4b\u5206\u6b67\u91cf\u5316\uff09\u548c\u6837\u672c\u5185\u5728\u590d\u6742\u5ea6\uff0c\u5f00\u53d1\u96be\u5ea6\u5e73\u8861\u91c7\u6837\u7b56\u7565", "result": "\u5728\u4e94\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u4e2a\u6d41\u884cMLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5e26\u6765\u663e\u8457\u4e14\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5927\u5927\u51cf\u5c11\u968f\u673a\u91c7\u6837\u5bfc\u81f4\u7684\u6027\u80fd\u5dee\u5f02", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u589e\u5f3a\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u548c\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u96be\u5ea6\u5e73\u8861\u7684\u793a\u4f8b\u9009\u62e9\u4f18\u5316\u6a21\u578b\u6027\u80fd"}}
{"id": "2508.18295", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.18295", "abs": "https://arxiv.org/abs/2508.18295", "authors": ["Huangyu Dai", "Lingtao Mao", "Ben Chen", "Zihan Wang", "Zihan Liang", "Ying Han", "Chenyi Lei", "Han Li"], "title": "H-PRM: A Pluggable Hotword Pre-Retrieval Module for Various Speech Recognition Systems", "comment": null, "summary": "Hotword customization is crucial in ASR to enhance the accuracy of\ndomain-specific terms. It has been primarily driven by the advancements in\ntraditional models and Audio large language models (LLMs). However, existing\nmodels often struggle with large-scale hotwords, as the recognition rate drops\ndramatically with the number of hotwords increasing. In this paper, we\nintroduce a novel hotword customization system that utilizes a hotword\npre-retrieval module (H-PRM) to identify the most relevant hotword candidate by\nmeasuring the acoustic similarity between the hotwords and the speech segment.\nThis plug-and-play solution can be easily integrated into traditional models\nsuch as SeACo-Paraformer, significantly enhancing hotwords post-recall rate\n(PRR). Additionally, we incorporate H-PRM into Audio LLMs through a\nprompt-based approach, enabling seamless customization of hotwords. Extensive\ntesting validates that H-PRM can outperform existing methods, showing a new\ndirection for hotword customization in ASR.", "AI": {"tldr": "\u901a\u8fc7\u70ed\u8bcd\u9884\u68c0\u7d22\u6a21\u5757(H-PRM)\u63d0\u5347ASR\u4e2d\u57df\u7279\u5b9a\u70ed\u8bcd\u8bc6\u522b\u51c6\u786e\u5ea6\uff0c\u89e3\u51b3\u5927\u89c4\u6a21\u70ed\u8bcd\u8bc6\u522b\u7387\u4e0b\u964d\u95ee\u9898", "motivation": "\u73b0\u6709ASR\u6a21\u578b\u5728\u5904\u7406\u5927\u89c4\u6a21\u70ed\u8bcd\u65f6\u8bc6\u522b\u7387\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u63d0\u5347\u57df\u7279\u5b9a\u672f\u8bed\u7684\u8bc6\u522b\u51c6\u786e\u6027", "method": "\u8bbe\u8ba1\u70ed\u8bcd\u9884\u68c0\u7d22\u6a21\u5757(H-PRM)\uff0c\u901a\u8fc7\u58f0\u5b66\u76f8\u4f3c\u6027\u8bc4\u4f30\u9009\u51fa\u6700\u76f8\u5173\u70ed\u8bcd\u5019\u9009\uff0c\u652f\u6301\u63d2\u5165\u5f0f\u96c6\u6210\u5230\u4f20\u7edf\u6a21\u578b\u548c\u901a\u8fc7\u63d0\u793a\u65b9\u5f0f\u96c6\u6210\u5230\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b", "result": "\u7ecf\u8fc7\u5927\u91cf\u6d4b\u8bd5\u9a8c\u8bc1\uff0cH-PRM\u80fd\u591f\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u70ed\u8bcd\u540e\u53ec\u56de\u7387(PRR)", "conclusion": "H-PRM\u4e3aASR\u70ed\u8bcd\u5b9a\u5236\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5177\u6709\u63d2\u5165\u5f0f\u4fbf\u6377\u6027\u548c\u826f\u597d\u7684\u6269\u5c55\u6027"}}
{"id": "2508.18290", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18290", "abs": "https://arxiv.org/abs/2508.18290", "authors": ["Hans-Joachim Rudolph"], "title": "Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI", "comment": "10 pages", "summary": "This essay develops a theoretical framework for a semantic Artificial General\nIntelligence (AGI) based on the notion of semantic attractors in complex-valued\nmeaning spaces. Departing from current transformer-based language models, which\noperate on statistical next-token prediction, we explore a model in which\nmeaning is not inferred probabilistically but formed through recursive\ntensorial transformation. Using cyclic operations involving the imaginary unit\n\\emph{i}, we describe a rotational semantic structure capable of modeling\nirony, homonymy, and ambiguity. At the center of this model, however, is a\nsemantic attractor -- a teleological operator that, unlike statistical\ncomputation, acts as an intentional agent (Microvitum), guiding meaning toward\nstability, clarity, and expressive depth. Conceived in terms of gradient flows,\ntensor deformations, and iterative matrix dynamics, the attractor offers a\nmodel of semantic transformation that is not only mathematically suggestive,\nbut also philosophically significant. We argue that true meaning emerges not\nfrom simulation, but from recursive convergence toward semantic coherence, and\nthat this requires a fundamentally new kind of cognitive architecture -- one\ndesigned to shape language, not just predict it.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u590d\u6570\u7a7a\u95f4\u8bed\u4e49\u5438\u5f15\u5b50\u7684AGI\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u5f20\u91cf\u53d8\u6362\u548c\u865a\u6570\u8fd0\u7b97\u6784\u5efa\u8bed\u4e49\u65cb\u8f6c\u7ed3\u6784\uff0c\u7528\u610f\u5411\u6027\u7b97\u5b50\u66ff\u4ee3\u7edf\u8ba1\u9884\u6d4b\u6765\u5b9e\u73b0\u8bed\u4e49\u6536\u655b", "motivation": "\u7a81\u7834\u5f53\u524d\u57fa\u4e8e\u7edf\u8ba1\u7684transformer\u6a21\u578b\uff0c\u63a2\u7d22\u4ece\u6982\u7387\u63a8\u65ad\u8f6c\u5411\u9012\u5f52\u5f20\u91cf\u53d8\u6362\u7684\u8bed\u4e49\u5f62\u6210\u673a\u5236\uff0c\u89e3\u51b3\u53cd\u8bbd\u3001\u540c\u97f3\u5f02\u4e49\u548c\u6b67\u4e49\u7b49\u590d\u6742\u8bed\u4e49\u95ee\u9898", "method": "\u4f7f\u7528\u590d\u6570\u7a7a\u95f4\u4e2d\u7684\u5faa\u73af\u8fd0\u7b97\u548c\u865a\u6570\u5355\u4f4di\uff0c\u6784\u5efa\u65cb\u8f6c\u8bed\u4e49\u7ed3\u6784\uff1b\u91c7\u7528\u68af\u5ea6\u6d41\u3001\u5f20\u91cf\u53d8\u5f62\u548c\u8fed\u4ee3\u77e9\u9635\u52a8\u529b\u5b66\u6765\u63cf\u8ff0\u8bed\u4e49\u5438\u5f15\u5b50\u8fd9\u4e00\u76ee\u7684\u8bba\u7b97\u5b50", "result": "\u5efa\u7acb\u4e86\u80fd\u591f\u5efa\u6a21\u590d\u6742\u8bed\u4e49\u73b0\u8c61\u7684\u6570\u5b66\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u4e0d\u540c\u4e8e\u7edf\u8ba1\u8ba1\u7b97\u7684\u610f\u5411\u6027\u8bed\u4e49\u6536\u655b\u673a\u5236", "conclusion": "\u771f\u6b63\u7684\u610f\u4e49\u6e90\u4e8e\u5411\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u9012\u5f52\u6536\u655b\u800c\u975e\u6a21\u62df\uff0c\u8fd9\u9700\u8981\u8bbe\u8ba1\u65b0\u578b\u8ba4\u77e5\u67b6\u6784\u6765\u5851\u9020\u8bed\u8a00\u800c\u4e0d\u4ec5\u4ec5\u662f\u9884\u6d4b\u8bed\u8a00"}}
{"id": "2508.18440", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.18440", "abs": "https://arxiv.org/abs/2508.18440", "authors": ["Lars Nieradzik"], "title": "SwiftF0: Fast and Accurate Monophonic Pitch Detection", "comment": null, "summary": "Accurate and real-time monophonic pitch estimation in noisy conditions,\nparticularly on resource-constrained devices, remains an open challenge in\naudio processing. We present \\emph{SwiftF0}, a novel, lightweight neural model\nthat sets a new state-of-the-art for monophonic pitch estimation. Through\ntraining on diverse speech, music, and synthetic datasets with extensive data\naugmentation, SwiftF0 achieves robust generalization across acoustic domains\nwhile maintaining computational efficiency. SwiftF0 achieves a 91.80\\% harmonic\nmean (HM) at 10 dB SNR, outperforming baselines like CREPE by over 12\npercentage points and degrading by only 2.3 points from clean audio. SwiftF0\nrequires only 95,842 parameters and runs approximately 42x faster than CREPE on\nCPU, making it ideal for efficient, real-time deployment. To address the\ncritical lack of perfectly accurate ground truth pitch in speech corpora (which\ntypically rely on algorithmic estimators or laryngograph signals), we introduce\n\\emph{SpeechSynth}. This synthetic speech dataset, generated by a phoneme-level\nTTS model, provides exact, on-demand ground-truth pitch curves, enabling more\nrobust model training and evaluation. Furthermore, we propose a unified metric,\ncombining six complementary performance measures for comprehensive and reliable\npitch evaluation, and release an open-source pitch benchmark suite. A live demo\nof SwiftF0 is available at https://swift-f0.github.io/, the source code at\nhttps://github.com/lars76/swift-f0, and the benchmark framework at\nhttps://github.com/lars76/pitch-benchmark.", "AI": {"tldr": "SwiftF0\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5728\u5608\u6742\u73af\u5883\u4e0b\u5b9e\u73b0\u5b9e\u65f6\u5355\u97f3\u9ad8\u97f3\u4f30\u8ba1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8", "motivation": "\u89e3\u51b3\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u5608\u6742\u73af\u5883\u4e0b\u5b9e\u65f6\u5355\u97f3\u9ad8\u97f3\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u73b0\u6709\u8bed\u97f3\u8bed\u6599\u5e93\u7f3a\u4e4f\u7cbe\u786e\u771f\u5b9e\u97f3\u9ad8\u6807\u6ce8\u7684\u95ee\u9898", "method": "\u4f7f\u7528\u591a\u6837\u5316\u8bed\u97f3\u3001\u97f3\u4e50\u548c\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u91c7\u7528\u5e7f\u6cdb\u6570\u636e\u589e\u5f3a\uff0c\u5e76\u5f15\u5165SpeechSynth\u5408\u6210\u8bed\u97f3\u6570\u636e\u96c6\u63d0\u4f9b\u7cbe\u786e\u771f\u5b9e\u97f3\u9ad8\u66f2\u7ebf", "result": "\u572810dB SNR\u4e0b\u8fbe\u523091.80%\u7684\u8c10\u6ce2\u5e73\u5747\u503c\uff0c\u6bd4CREPE\u63d0\u534712\u4e2a\u767e\u5206\u70b9\uff0c\u53c2\u6570\u91cf\u4ec595,842\u4e2a\uff0cCPU\u8fd0\u884c\u901f\u5ea6\u6bd4CREPE\u5feb42\u500d", "conclusion": "SwiftF0\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5355\u97f3\u9ad8\u97f3\u4f30\u8ba1\u6027\u80fd\uff0c\u9002\u5408\u5b9e\u65f6\u90e8\u7f72\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u548c\u5408\u6210\u6570\u636e\u96c6"}}
{"id": "2508.18321", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18321", "abs": "https://arxiv.org/abs/2508.18321", "authors": ["Maojia Song", "Tej Deep Pala", "Weisheng Jin", "Amir Zadeh", "Chuan Li", "Dorien Herremans", "Soujanya Poria"], "title": "LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in multi-agent systems\n(MAS) as components of collaborative intelligence, where peer interactions\ndynamically shape individual decision-making. Although prior work has focused\non conformity bias, we extend the analysis to examine how LLMs form trust from\nprevious impressions, resist misinformation, and integrate peer input during\ninteraction, key factors for achieving collective intelligence under complex\nsocial dynamics. We present KAIROS, a benchmark simulating quiz contests with\npeer agents of varying reliability, offering fine-grained control over\nconditions such as expert-novice roles, noisy crowds, and adversarial peers.\nLLMs receive both historical interactions and current peer responses, allowing\nsystematic investigation into how trust, peer action, and self-confidence\ninfluence decisions. As for mitigation strategies, we evaluate prompting,\nsupervised fine-tuning, and reinforcement learning, Group Relative Policy\nOptimisation (GRPO), across multiple models. Our results reveal that GRPO with\nmulti-agent context combined with outcome-based rewards and unconstrained\nreasoning achieves the best overall performance, but also decreases the\nrobustness to social influence compared to Base models. The code and datasets\nare available at: https://github.com/declare-lab/KAIROS.", "AI": {"tldr": "KAIROS\u57fa\u51c6\u6d4b\u8bd5\u7814\u7a76LLM\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5982\u4f55\u57fa\u4e8e\u5386\u53f2\u5370\u8c61\u5efa\u7acb\u4fe1\u4efb\u3001\u62b5\u6297\u9519\u8bef\u4fe1\u606f\u5e76\u6574\u5408\u540c\u4f34\u8f93\u5165\uff0c\u53d1\u73b0GRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u4e86\u793e\u4ea4\u5f71\u54cd\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4ece\u4f17\u504f\u89c1\uff0c\u4f46\u9700\u8981\u66f4\u5168\u9762\u5730\u5206\u6790LLM\u5728\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e2d\u5982\u4f55\u5f62\u6210\u4fe1\u4efb\u3001\u62b5\u6297\u9519\u8bef\u4fe1\u606f\u4ee5\u53ca\u6574\u5408\u540c\u4f34\u8f93\u5165\uff0c\u8fd9\u4e9b\u56e0\u7d20\u5bf9\u5b9e\u73b0\u590d\u6742\u793e\u4ea4\u52a8\u6001\u4e0b\u7684\u96c6\u4f53\u667a\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faKAIROS\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6a21\u62df\u5177\u6709\u4e0d\u540c\u53ef\u9760\u6027\u540c\u4f34\u7684\u95ee\u7b54\u7ade\u8d5b\uff0c\u63a7\u5236\u4e13\u5bb6-\u65b0\u624b\u89d2\u8272\u3001\u566a\u58f0\u7fa4\u4f53\u548c\u5bf9\u6297\u6027\u540c\u4f34\u7b49\u6761\u4ef6\u3002\u8bc4\u4f30\u63d0\u793a\u5de5\u7a0b\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff08\u7279\u522b\u662fGRPO\uff09\u7b49\u591a\u79cd\u7f13\u89e3\u7b56\u7565\u3002", "result": "GRPO\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u3001\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u548c\u65e0\u7ea6\u675f\u63a8\u7406\u5b9e\u73b0\u4e86\u6700\u4f73\u6574\u4f53\u6027\u80fd\uff0c\u4f46\u4e0e\u57fa\u7840\u6a21\u578b\u76f8\u6bd4\u964d\u4f4e\u4e86\u5bf9\u793e\u4ea4\u5f71\u54cd\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u80fd\u6709\u6548\u63d0\u5347LLM\u5728\u534f\u4f5c\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u6027\u80fd\uff0c\u4f46\u9700\u8981\u5e73\u8861\u6027\u80fd\u63d0\u5347\u4e0e\u793e\u4ea4\u5f71\u54cd\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684\u96c6\u4f53\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2508.18732", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18732", "abs": "https://arxiv.org/abs/2508.18732", "authors": ["Qing Xiao", "Yingshan Peng", "PeiPei Zhang"], "title": "Cross-Learning Fine-Tuning Strategy for Dysarthric Speech Recognition Via CDSD database", "comment": null, "summary": "Dysarthric speech recognition faces challenges from severity variations and\ndisparities relative to normal speech. Conventional approaches individually\nfine-tune ASR models pre-trained on normal speech per patient to prevent\nfeature conflicts. Counter-intuitively, experiments reveal that multi-speaker\nfine-tuning (simultaneously on multiple dysarthric speakers) improves\nrecognition of individual speech patterns. This strategy enhances\ngeneralization via broader pathological feature learning, mitigates\nspeaker-specific overfitting, reduces per-patient data dependence, and improves\ntarget-speaker accuracy - achieving up to 13.15% lower WER versus\nsingle-speaker fine-tuning.", "AI": {"tldr": "\u591a\u8bed\u8005\u7cbe\u8c03\u6bd4\u5355\u8bed\u8005\u7cbe\u8c03\u66f4\u6709\u6548\uff0c\u80fd\u63d0\u5347\u8bed\u97f3\u969c\u788d\u8bed\u97f3\u8bc6\u522b\u7684\u51c6\u786e\u6027", "motivation": "\u89e3\u51b3\u8bed\u97f3\u969c\u788d\u8bed\u97f3\u8bc6\u522b\u4e2d\u7684\u4e25\u91cd\u7a0b\u5ea6\u53d8\u5316\u548c\u4e0e\u6b63\u5e38\u8bed\u97f3\u7684\u5dee\u5f02\u95ee\u9898", "method": "\u91c7\u7528\u591a\u8bed\u8005\u540c\u65f6\u7cbe\u8c03\u7b56\u7565\uff0c\u5728\u591a\u4e2a\u8bed\u97f3\u969c\u788d\u8bed\u8005\u4e0a\u540c\u65f6\u8fdb\u884c\u7cbe\u7ec6\u8c03\u6574", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u591a\u8bed\u8005\u7cbe\u8c03\u80fd\u591f\u83b7\u5f97\u66f4\u4f4e\u7684\u8bcd\u8bef\u7387\uff08WER\uff09\uff0c\u6700\u9ad8\u53ef\u964d\u4f4e13.15%", "conclusion": "\u591a\u8bed\u8005\u7cbe\u8c03\u7b56\u7565\u901a\u8fc7\u5e7f\u6cdb\u7684\u75c5\u7406\u7279\u5f81\u5b66\u4e60\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cbf\u7528\u6027\uff0c\u51cf\u8f7b\u4e86\u8bed\u8005\u7279\u5b9a\u8fc7\u62df\u5408\uff0c\u964d\u4f4e\u4e86\u5bf9\u5355\u4e2a\u75c5\u4eba\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5e76\u63d0\u9ad8\u4e86\u76ee\u6807\u8bed\u8005\u7684\u8bc6\u522b\u51c6\u786e\u6027"}}
{"id": "2508.18328", "categories": ["cs.CL", "cs.CY", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.18328", "abs": "https://arxiv.org/abs/2508.18328", "authors": ["Masudul Hasan Masud Bhuiyan", "Matteo Varvello", "Yasir Zaki", "Cristian-Alexandru Staicu"], "title": "Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective", "comment": "6 pages, 6 figures", "summary": "English is the predominant language on the web, powering nearly half of the\nworld's top ten million websites. Support for multilingual content is\nnevertheless growing, with many websites increasingly combining English with\nregional or native languages in both visible content and hidden metadata. This\nmultilingualism introduces significant barriers for users with visual\nimpairments, as assistive technologies like screen readers frequently lack\nrobust support for non-Latin scripts and misrender or mispronounce non-English\ntext, compounding accessibility challenges across diverse linguistic contexts.\nYet, large-scale studies of this issue have been limited by the lack of\ncomprehensive datasets on multilingual web content. To address this gap, we\nintroduce LangCrUX, the first large-scale dataset of 120,000 popular websites\nacross 12 languages that primarily use non-Latin scripts. Leveraging this\ndataset, we conduct a systematic analysis of multilingual web accessibility and\nuncover widespread neglect of accessibility hints. We find that these hints\noften fail to reflect the language diversity of visible content, reducing the\neffectiveness of screen readers and limiting web accessibility. We finally\npropose Kizuki, a language-aware automated accessibility testing extension to\naccount for the limited utility of language-inconsistent accessibility hints.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86LangCrUX\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u7f51\u7ad9\u6570\u636e\u96c6\uff0c\u5305\u542b12\u79cd\u975e\u62c9\u4e01\u8bed\u7cfb\u768412\u4e07\u4e2a\u6d41\u884c\u7f51\u7ad9\u3002\u7814\u7a76\u53d1\u73b0\u7f51\u9875\u53ef\u8bbf\u95ee\u6027\u63d0\u793a\u666e\u904d\u5ffd\u89c6\u8bed\u8a00\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u5c4f\u5e55\u9605\u8bfb\u5668\u6548\u679c\u4e0d\u4f73\uff0c\u5e76\u63d0\u51fa\u4e86Kizuki\u8bed\u8a00\u611f\u77e5\u81ea\u52a8\u5316\u6d4b\u8bd5\u5de5\u5177\u6765\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "motivation": "\u7f51\u7edc\u5185\u5bb9\u65e5\u76ca\u591a\u8bed\u8a00\u5316\uff0c\u4f46\u8f85\u52a9\u6280\u672f\uff08\u5982\u5c4f\u5e55\u9605\u8bfb\u5668\uff09\u5bf9\u975e\u62c9\u4e01\u8bed\u7cfb\u652f\u6301\u4e0d\u8db3\uff0c\u5bfc\u81f4\u89c6\u89c9\u969c\u788d\u7528\u6237\u9762\u4e34\u4e25\u91cd\u7684\u53ef\u8bbf\u95ee\u6027\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5168\u9762\u7684\u591a\u8bed\u8a00\u7f51\u9875\u5185\u5bb9\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u5206\u6790\u3002", "method": "\u6784\u5efaLangCrUX\u6570\u636e\u96c6\uff0812\u79cd\u975e\u62c9\u4e01\u8bed\u7cfb\u768412\u4e07\u4e2a\u6d41\u884c\u7f51\u7ad9\uff09\uff0c\u8fdb\u884c\u7cfb\u7edf\u6027\u591a\u8bed\u8a00\u7f51\u9875\u53ef\u8bbf\u95ee\u6027\u5206\u6790\uff0c\u5e76\u5f00\u53d1Kizuki\u8bed\u8a00\u611f\u77e5\u81ea\u52a8\u5316\u53ef\u8bbf\u95ee\u6027\u6d4b\u8bd5\u6269\u5c55\u5de5\u5177\u3002", "result": "\u53d1\u73b0\u53ef\u8bbf\u95ee\u6027\u63d0\u793a\u666e\u904d\u672a\u80fd\u53cd\u6620\u53ef\u89c1\u5185\u5bb9\u7684\u8bed\u8a00\u591a\u6837\u6027\uff0c\u964d\u4f4e\u4e86\u5c4f\u5e55\u9605\u8bfb\u5668\u7684\u6709\u6548\u6027\uff0c\u9650\u5236\u4e86\u7f51\u9875\u53ef\u8bbf\u95ee\u6027\u3002", "conclusion": "\u591a\u8bed\u8a00\u7f51\u9875\u53ef\u8bbf\u95ee\u6027\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff0c\u9700\u8981\u8bed\u8a00\u611f\u77e5\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5\u5de5\u5177\u6765\u6539\u5584\u8f85\u52a9\u6280\u672f\u5bf9\u975e\u62c9\u4e01\u8bed\u7cfb\u5185\u5bb9\u7684\u652f\u6301\u3002"}}
{"id": "2508.18907", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18907", "abs": "https://arxiv.org/abs/2508.18907", "authors": ["Ridwan Arefeen", "Xiaoxiao Miao", "Rong Tong", "Aik Beng Ng", "Simon See"], "title": "SegReConcat: A Data Augmentation Method for Voice Anonymization Attack", "comment": "The Paper has been accepted by APCIPA ASC 2025", "summary": "Anonymization of voice seeks to conceal the identity of the speaker while\nmaintaining the utility of speech data. However, residual speaker cues often\npersist, which pose privacy risks. We propose SegReConcat, a data augmentation\nmethod for attacker-side enhancement of automatic speaker verification systems.\nSegReConcat segments anonymized speech at the word level, rearranges segments\nusing random or similarity-based strategies to disrupt long-term contextual\ncues, and concatenates them with the original utterance, allowing an attacker\nto learn source speaker traits from multiple perspectives. The proposed method\nhas been evaluated in the VoicePrivacy Attacker Challenge 2024 framework across\nseven anonymization systems, SegReConcat improves de-anonymization on five out\nof seven systems.", "AI": {"tldr": "SegReConcat\u662f\u4e00\u79cd\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5355\u8bcd\u7ea7\u522b\u5206\u5272\u533f\u540d\u5316\u8bed\u97f3\u5e76\u91cd\u65b0\u6392\u5217\u7247\u6bb5\uff0c\u6765\u589e\u5f3a\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u7cfb\u7edf\u7684\u653b\u51fb\u80fd\u529b\uff0c\u63d0\u9ad8\u53bb\u533f\u540d\u5316\u6548\u679c\u3002", "motivation": "\u8bed\u97f3\u533f\u540d\u5316\u65e8\u5728\u9690\u85cf\u8bf4\u8bdd\u4eba\u8eab\u4efd\uff0c\u4f46\u5f80\u5f80\u4ecd\u6b8b\u7559\u8bf4\u8bdd\u4eba\u7ebf\u7d22\uff0c\u5b58\u5728\u9690\u79c1\u98ce\u9669\u3002\u9700\u8981\u5f00\u53d1\u653b\u51fb\u8005\u4fa7\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u81ea\u52a8\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u7cfb\u7edf\u7684\u53bb\u533f\u540d\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faSegReConcat\u65b9\u6cd5\uff1a1\uff09\u5728\u5355\u8bcd\u7ea7\u522b\u5206\u5272\u533f\u540d\u5316\u8bed\u97f3\uff1b2\uff09\u4f7f\u7528\u968f\u673a\u6216\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u7b56\u7565\u91cd\u65b0\u6392\u5217\u7247\u6bb5\u4ee5\u7834\u574f\u957f\u671f\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff1b3\uff09\u5c06\u91cd\u65b0\u6392\u5217\u7684\u7247\u6bb5\u4e0e\u539f\u59cb\u8bdd\u8bed\u62fc\u63a5\uff0c\u8ba9\u653b\u51fb\u8005\u80fd\u4ece\u591a\u89d2\u5ea6\u5b66\u4e60\u6e90\u8bf4\u8bdd\u4eba\u7279\u5f81\u3002", "result": "\u5728VoicePrivacy Attacker Challenge 2024\u6846\u67b6\u4e0b\u8bc4\u4f30\u4e867\u4e2a\u533f\u540d\u5316\u7cfb\u7edf\uff0cSegReConcat\u57285\u4e2a\u7cfb\u7edf\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u53bb\u533f\u540d\u5316\u6027\u80fd\u3002", "conclusion": "SegReConcat\u662f\u4e00\u79cd\u6709\u6548\u7684\u653b\u51fb\u8005\u4fa7\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u80fd\u591f\u901a\u8fc7\u7834\u574f\u8bed\u97f3\u7684\u957f\u671f\u4e0a\u4e0b\u6587\u7ed3\u6784\u6765\u589e\u5f3a\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u7cfb\u7edf\u7684\u53bb\u533f\u540d\u5316\u80fd\u529b\uff0c\u5728\u591a\u6570\u533f\u540d\u5316\u7cfb\u7edf\u4e0a\u90fd\u8868\u73b0\u51fa\u826f\u597d\u7684\u6548\u679c\u3002"}}
{"id": "2508.18381", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18381", "abs": "https://arxiv.org/abs/2508.18381", "authors": ["Yuchun Fan", "Yilin Wang", "Yongyu Mu", "Lei Huang", "Bei Li", "Xiaocheng Feng", "Tong Xiao", "Jingbo Zhu"], "title": "Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models", "comment": "Accepted by EMNLP 2025 findings", "summary": "Large vision-language models (LVLMs) have demonstrated exceptional\ncapabilities in understanding visual information with human languages but also\nexhibit an imbalance in multilingual capabilities. In this work, we delve into\nthe multilingual working pattern of LVLMs and identify a salient correlation\nbetween the multilingual understanding ability of LVLMs and language-specific\nneuron activations in shallow layers. Building on this insight, we introduce\nPLAST, a training recipe that achieves efficient multilingual enhancement for\nLVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies\nlayers involved in multilingual understanding by monitoring language-specific\nneuron activations. These layers are then precisely fine-tuned with\nquestion-translation pairs to achieve multilingual alignment. Our empirical\nresults on MM-Bench and MMMB demonstrate that PLAST effectively improves the\nmultilingual capabilities of LVLMs and achieves significant efficiency with\nonly 14% of the parameters tuned. Further analysis reveals that PLAST can be\ngeneralized to low-resource and complex visual reasoning tasks, facilitating\nthe language-specific visual information engagement in shallow layers.", "AI": {"tldr": "PLAST\u662f\u4e00\u79cd\u901a\u8fc7\u7cbe\u786e\u8bed\u8a00\u7279\u5b9a\u5c42\u5fae\u8c03\u6765\u9ad8\u6548\u63d0\u5347\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u591a\u8bed\u8a00\u80fd\u529b\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ec5\u9700\u8c03\u657414%\u53c2\u6570\u5373\u53ef\u663e\u8457\u6539\u5584\u591a\u8bed\u8a00\u7406\u89e3\u6027\u80fd", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u7814\u7a76\u53d1\u73b0\u591a\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u4e0e\u6d45\u5c42\u8bed\u8a00\u7279\u5b9a\u795e\u7ecf\u5143\u6fc0\u6d3b\u5b58\u5728\u663e\u8457\u76f8\u5173\u6027", "method": "\u9996\u5148\u901a\u8fc7\u76d1\u63a7\u8bed\u8a00\u7279\u5b9a\u795e\u7ecf\u5143\u6fc0\u6d3b\u6765\u8bc6\u522b\u53c2\u4e0e\u591a\u8bed\u8a00\u7406\u89e3\u7684\u5c42\uff0c\u7136\u540e\u4f7f\u7528\u95ee\u9898\u7ffb\u8bd1\u5bf9\u7cbe\u786e\u5fae\u8c03\u8fd9\u4e9b\u5c42\u4ee5\u5b9e\u73b0\u591a\u8bed\u8a00\u5bf9\u9f50", "result": "\u5728MM-Bench\u548cMMMB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\u63d0\u5347\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u4ec5\u8c03\u657414%\u53c2\u6570\u5373\u53ef\u5b9e\u73b0\u663e\u8457\u6548\u7387\u63d0\u5347\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u4f4e\u8d44\u6e90\u548c\u590d\u6742\u89c6\u89c9\u63a8\u7406\u4efb\u52a1", "conclusion": "PLAST\u65b9\u6cd5\u901a\u8fc7\u6d45\u5c42\u8bed\u8a00\u7279\u5b9a\u89c6\u89c9\u4fe1\u606f\u53c2\u4e0e\uff0c\u4e3a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u591a\u8bed\u8a00\u80fd\u529b\u589e\u5f3a\u65b9\u6848"}}
{"id": "2508.18655", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.18655", "abs": "https://arxiv.org/abs/2508.18655", "authors": ["Haoyu Wang", "Guangyan Zhang", "Jiale Chen", "Jingyu Li", "Yuehai Wang", "Yiwen Guo"], "title": "Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models", "comment": "5 pages, 1 figure, submitted to ICASSP 2026", "summary": "With the development of speech large language models (speech LLMs), users can\nnow interact directly with assistants via speech. However, most existing models\nsimply convert the response content into speech without fully understanding the\nrich emotional and paralinguistic cues embedded in the user's query. In many\ncases, the same sentence can have different meanings depending on the emotional\nexpression. Furthermore, emotional understanding is essential for improving\nuser experience in human-machine interaction. Currently, most speech LLMs with\nempathetic capabilities are trained on massive datasets. This approach requires\nvast amounts of data and significant computational resources. Therefore, a key\nchallenge lies in how to develop a speech LLM capable of generating empathetic\nresponses with limited data and without the need for large-scale training. To\naddress this challenge, we propose Emotion Omni, a novel model architecture\ndesigned to understand the emotional content of user speech input and generate\nempathetic speech responses. Additionally, we developed a data generation\npipeline based on an open-source TTS framework to construct a 200k emotional\ndialogue dataset, which supports the construction of an empathetic speech\nassistant. The demos are available at https://w311411.github.io/omni_demo/", "AI": {"tldr": "\u63d0\u51fa\u4e86Emotion Omni\u6a21\u578b\uff0c\u80fd\u591f\u5728\u6709\u9650\u6570\u636e\u4e0b\u7406\u89e3\u7528\u6237\u8bed\u97f3\u4e2d\u7684\u60c5\u611f\u5e76\u751f\u6210\u5171\u60c5\u8bed\u97f3\u54cd\u5e94\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u8bad\u7ec3", "motivation": "\u73b0\u6709\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u4ec5\u7b80\u5355\u8f6c\u6362\u6587\u672c\u4e3a\u8bed\u97f3\uff0c\u65e0\u6cd5\u7406\u89e3\u7528\u6237\u67e5\u8be2\u4e2d\u7684\u4e30\u5bcc\u60c5\u611f\u548c\u526f\u8bed\u8a00\u7ebf\u7d22\uff0c\u800c\u60c5\u611f\u7406\u89e3\u5bf9\u4e8e\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\u81f3\u5173\u91cd\u8981", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u6a21\u578b\u67b6\u6784Emotion Omni\uff0c\u5e76\u57fa\u4e8e\u5f00\u6e90TTS\u6846\u67b6\u5f00\u53d1\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u6784\u5efa\u4e8620\u4e07\u6761\u60c5\u611f\u5bf9\u8bdd\u6570\u636e\u96c6", "result": "\u6210\u529f\u6784\u5efa\u4e86\u652f\u6301\u5171\u60c5\u8bed\u97f3\u52a9\u624b\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u67b6\u6784\uff0c\u6f14\u793a\u6837\u4f8b\u5df2\u516c\u5f00", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f00\u53d1\u5177\u6709\u60c5\u611f\u7406\u89e3\u80fd\u529b\u7684\u8bed\u97f3\u52a9\u624b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u4e14\u8ba1\u7b97\u8d44\u6e90\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.18384", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18384", "abs": "https://arxiv.org/abs/2508.18384", "authors": ["Kellen Tan Cheng", "Anna Lisa Gentile", "Chad DeLuca", "Guang-Jie Ren"], "title": "Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails", "comment": null, "summary": "The pervasiveness of large language models (LLMs) in enterprise settings has\nalso brought forth a significant amount of risks associated with their usage.\nGuardrails technologies aim to mitigate this risk by filtering LLMs'\ninput/output text through various detectors. However, developing and\nmaintaining robust detectors faces many challenges, one of which is the\ndifficulty in acquiring production-quality labeled data on real LLM outputs\nprior to deployment. In this work, we propose backprompting, a simple yet\nintuitive solution to generate production-like labeled data for health advice\nguardrails development. Furthermore, we pair our backprompting method with a\nsparse human-in-the-loop clustering technique to label the generated data. Our\naim is to construct a parallel corpus roughly representative of the original\ndataset yet resembling real LLM output. We then infuse existing datasets with\nour synthetic examples to produce robust training data for our detector. We\ntest our technique in one of the most difficult and nuanced guardrails: the\nidentification of health advice in LLM output, and demonstrate improvement\nversus other solutions. Our detector is able to outperform GPT-4o by up to\n3.73%, despite having 400x less parameters.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86backprompting\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u751f\u4ea7\u7ea7\u522b\u6807\u6ce8\u6570\u636e\u6765\u6539\u5584\u5065\u5eb7\u5efa\u8bae\u76d1\u6d4b\u5668\u7684\u6027\u80fd\uff0c\u5728\u53c2\u6570\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\u8d85\u8fc7GPT-4o\u7684\u8868\u73b0\u3002", "motivation": "\u4f01\u4e1a\u73af\u5883\u4e2d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\u5e26\u6765\u4e86\u98ce\u9669\uff0c\u800c\u5f00\u53d1\u7a33\u5065\u7684\u76d1\u6d4b\u5668\u9762\u4e34\u751f\u4ea7\u7ea7\u522b\u6807\u6570\u636e\u83b7\u53d6\u56f0\u96be\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fabackprompting\u65b9\u6cd5\u751f\u6210\u751f\u4ea7\u7ea7\u522b\u6807\u6ce8\u6570\u636e\uff0c\u7ed3\u5408\u7a00\u758f\u4eba\u5de1\u73af\u805a\u7c7b\u6280\u672f\u8fdb\u884c\u6807\u6ce8\uff0c\u6784\u5efa\u4e0e\u539f\u59cb\u6570\u636e\u96c6\u5e76\u884c\u4f46\u7c7b\u4f3c\u771f\u5b9eLLM\u8f93\u51fa\u7684\u8bed\u6599\u5e93\u3002", "result": "\u5728\u5065\u5eb7\u5efa\u8bae\u8bc6\u522b\u8fd9\u4e2a\u96be\u9898\u4e0a\uff0c\u76d1\u6d4b\u5668\u6027\u80fd\u8d85\u8fc7GPT-4o\u8fbe\u52303.73%\uff0c\u800c\u53c2\u6570\u6570\u91cf\u53ea\u6709\u5176\u76841/400\u3002", "conclusion": "backprompting\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4e3a\u5f00\u53d1\u7a33\u5065\u7684LLM\u62e6\u622a\u5668\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2508.19205", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.19205", "abs": "https://arxiv.org/abs/2508.19205", "authors": ["Zhiliang Peng", "Jianwei Yu", "Wenhui Wang", "Yaoyao Chang", "Yutao Sun", "Li Dong", "Yi Zhu", "Weijiang Xu", "Hangbo Bao", "Zehua Wang", "Shaohan Huang", "Yan Xia", "Furu Wei"], "title": "VibeVoice Technical Report", "comment": null, "summary": "This report presents VibeVoice, a novel model designed to synthesize\nlong-form speech with multiple speakers by employing next-token diffusion,\nwhich is a unified method for modeling continuous data by autoregressively\ngenerating latent vectors via diffusion. To enable this, we introduce a novel\ncontinuous speech tokenizer that, when compared to the popular Encodec model,\nimproves data compression by 80 times while maintaining comparable performance.\nThe tokenizer effectively preserves audio fidelity while significantly boosting\ncomputational efficiency for processing long sequences. Thus, VibeVoice can\nsynthesize long-form speech for up to 90 minutes (in a 64K context window\nlength) with a maximum of 4 speakers, capturing the authentic conversational\n``vibe'' and surpassing open-source and proprietary dialogue models.", "AI": {"tldr": "VibeVoice\u662f\u4e00\u4e2a\u4f7f\u7528next-token\u6269\u6563\u6280\u672f\u5408\u6210\u591a\u8bf4\u8bdd\u4eba\u957f\u8bed\u97f3\u7684\u65b0\u6a21\u578b\uff0c\u901a\u8fc7\u65b0\u578b\u8fde\u7eed\u8bed\u97f3\u5206\u8bcd\u5668\u5b9e\u73b080\u500d\u6570\u636e\u538b\u7f29\uff0c\u80fd\u572864K\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e2d\u5408\u6210\u957f\u8fbe90\u5206\u949f\u76844\u4eba\u5bf9\u8bdd\u8bed\u97f3\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u97f3\u5408\u6210\u6a21\u578b\u5728\u5904\u7406\u957f\u5e8f\u5217\u591a\u8bf4\u8bdd\u4eba\u5bf9\u8bdd\u65f6\u9762\u4e34\u8ba1\u7b97\u6548\u7387\u548c\u6570\u636e\u538b\u7f29\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9ad8\u6548\u5904\u7406\u957f\u8bed\u97f3\u5e76\u4fdd\u6301\u97f3\u9891\u4fdd\u771f\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528next-token\u6269\u6563\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u751f\u6210\u6f5c\u5728\u5411\u91cf\u6765\u5efa\u6a21\u8fde\u7eed\u6570\u636e\uff1b\u5f15\u5165\u65b0\u578b\u8fde\u7eed\u8bed\u97f3\u5206\u8bcd\u5668\uff0c\u76f8\u6bd4Encodec\u6a21\u578b\u5b9e\u73b080\u500d\u6570\u636e\u538b\u7f29\uff1b\u652f\u630164K\u4e0a\u4e0b\u6587\u7a97\u53e3\u957f\u5ea6\u3002", "result": "\u6a21\u578b\u80fd\u591f\u5408\u6210\u957f\u8fbe90\u5206\u949f\u7684\u957f\u8bed\u97f3\uff0c\u6700\u591a\u652f\u63014\u4e2a\u8bf4\u8bdd\u4eba\uff0c\u5728\u4fdd\u6301\u97f3\u9891\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u8d85\u8d8a\u4e86\u5f00\u6e90\u548c\u4e13\u6709\u5bf9\u8bdd\u6a21\u578b\u7684\u8868\u73b0\u3002", "conclusion": "VibeVoice\u901a\u8fc7\u521b\u65b0\u7684\u5206\u8bcd\u5668\u548c\u6269\u6563\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u957f\u5e8f\u5217\u591a\u8bf4\u8bdd\u4eba\u8bed\u97f3\u5408\u6210\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u5bf9\u8bdd\u8bed\u97f3\u751f\u6210\uff0c\u6355\u6349\u4e86\u771f\u5b9e\u7684\u5bf9\u8bdd\u6c1b\u56f4\u3002"}}
{"id": "2508.18387", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18387", "abs": "https://arxiv.org/abs/2508.18387", "authors": ["Ivan Kobyzev", "Abbas Ghaddar", "Dingtao Hu", "Boxing Chen"], "title": "Integral Transformer: Denoising Attention, Not Too Much Not Too Little", "comment": "EMNLP 2025 Main", "summary": "Softmax self-attention often assigns disproportionate weight to semantically\nuninformative tokens such as special tokens and punctuation, a phenomenon known\nas attention noise. While recent methods like Cog Attention and the\nDifferential Transformer have addressed this by introducing negative attention\nscores, they risk discarding useful information. In this paper, we propose the\nIntegral Transformer, a novel self-attention mechanism that denoises attention\nby integrating signals sampled from the logit distribution. Our approach\nmitigates noise while preserving the contributions of special tokens critical\nfor model performance. Extensive experiments demonstrate that our model\noutperforms vanilla, Cog, and Differential attention variants on\nwell-established knowledge and reasoning language benchmarks. Moreover, our\nanalysis reveals that employing vanilla self-attention in the lower Transformer\nlayers enhances performance and that the Integral Transformer effectively\nbalances attention distributions and reduces rank collapse in upper layers.", "AI": {"tldr": "\u63d0\u51faIntegral Transformer\uff0c\u901a\u8fc7\u79ef\u5206\u91c7\u6837logit\u5206\u5e03\u6765\u964d\u566a\u6ce8\u610f\u529b\uff0c\u5728\u4fdd\u6301\u7279\u6b8a\u4ee4\u724c\u4f5c\u7528\u7684\u540c\u65f6\u89e3\u51b3\u6ce8\u610f\u529b\u566a\u58f0\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfsoftmax\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4f1a\u7ed9\u8bed\u4e49\u4fe1\u606f\u8f83\u5c11\u7684\u7279\u6b8a\u4ee4\u724c\u548c\u6807\u70b9\u7b26\u53f7\u5206\u914d\u8fc7\u591a\u6743\u91cd\uff0c\u5f62\u6210\u6ce8\u610f\u529b\u566a\u58f0\u3002\u73b0\u6709\u65b9\u6cd5\u5982Cog Attention\u548cDifferential Transformer\u867d\u7136\u5f15\u5165\u8d1f\u6ce8\u610f\u529b\u5206\u6570\uff0c\u4f46\u53ef\u80fd\u4e22\u5931\u6709\u7528\u4fe1\u606f\u3002", "method": "\u63d0\u51faIntegral Transformer\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u4ecelogit\u5206\u5e03\u4e2d\u79ef\u5206\u91c7\u6837\u4fe1\u53f7\u6765\u964d\u566a\u6ce8\u610f\u529b\uff0c\u540c\u65f6\u4fdd\u7559\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\u7684\u7279\u6b8a\u4ee4\u724c\u7684\u8d21\u732e\u3002", "result": "\u5728\u6210\u719f\u7684\u77e5\u8bc6\u548c\u63a8\u7406\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6a21\u578b\u4f18\u4e8evanilla\u3001Cog\u548cDifferential\u6ce8\u610f\u529b\u53d8\u4f53\u3002\u5206\u6790\u8868\u660e\uff0c\u5728\u5e95\u5c42\u4f7f\u7528vanilla\u81ea\u6ce8\u610f\u529b\u53ef\u63d0\u5347\u6027\u80fd\uff0cIntegral Transformer\u80fd\u6709\u6548\u5e73\u8861\u6ce8\u610f\u529b\u5206\u5e03\u5e76\u51cf\u5c11\u4e0a\u5c42\u79e9\u5d29\u6e83\u3002", "conclusion": "Integral Transformer\u662f\u4e00\u79cd\u6709\u6548\u7684\u81ea\u6ce8\u610f\u529b\u964d\u566a\u65b9\u6cd5\uff0c\u65e2\u80fd\u7f13\u89e3\u6ce8\u610f\u529b\u566a\u58f0\u95ee\u9898\uff0c\u53c8\u80fd\u4fdd\u6301\u5173\u952e\u4fe1\u606f\uff0c\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.18395", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18395", "abs": "https://arxiv.org/abs/2508.18395", "authors": ["Jeong-seok Oh", "Jay-yoon Lee"], "title": "Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning", "comment": null, "summary": "Probabilistic decoding in Large Language Models (LLMs) often yields\ninconsistent outputs, particularly on complex or long-form questions.\nSelf-Consistency (SC) mitigates this for short-form QA by majority voting over\nexact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram\nConsistency Score (WUCS) extend to long-form responses but lose accuracy on\nshort-form benchmarks.\n  We introduce Latent Self-Consistency (LSC), which selects the most\nsemantically consistent response using learnable token embeddings. A\nlightweight forward generation of summary tokens increases inference time by\nless than 1% and requires no changes to the model architecture.\n  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,\nTruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form\nones on average, while maintaining negligible computational overhead. These\nresults position LSC as a practical consistency-selection method that works\nreliably across answer formats. Additionally, LSC provides well-calibrated\nconfidence estimates, maintaining low Expected Calibration Error across both\nanswer formats.", "AI": {"tldr": "\u63d0\u51fa\u4e86Latent Self-Consistency (LSC)\u65b9\u6cd5\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684token\u5d4c\u5165\u9009\u62e9\u8bed\u4e49\u6700\u4e00\u81f4\u7684\u56de\u7b54\uff0c\u5728\u77ed\u5f0f\u548c\u957f\u5f0f\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6216\u957f\u5f0f\u95ee\u9898\u4e0a\u7684\u6982\u7387\u89e3\u7801\u8f93\u51fa\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5982SC\u3001USC\u548cWUCS\u5728\u77ed\u5f0f\u548c\u957f\u5f0f\u56de\u7b54\u683c\u5f0f\u4e4b\u95f4\u5b58\u5728\u51c6\u786e\u5ea6\u6743\u8861\u3002", "method": "\u4f7f\u7528\u53ef\u5b66\u4e60\u7684token\u5d4c\u5165\u6765\u9009\u62e9\u8bed\u4e49\u6700\u4e00\u81f4\u7684\u56de\u7b54\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u524d\u5411\u751f\u6210\u6458\u8981token\uff0c\u63a8\u7406\u65f6\u95f4\u589e\u52a0\u4e0d\u52301%\uff0c\u65e0\u9700\u6539\u53d8\u6a21\u578b\u67b6\u6784\u3002", "result": "\u57286\u4e2a\u77ed\u5f0f\u548c5\u4e2a\u957f\u5f0f\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982MATH\u3001MMLU\u3001TruthfulQA\uff09\u4e2d\uff0cLSC\u5728\u6240\u6709\u77ed\u5f0f\u548c\u957f\u5f0f\u6d4b\u8bd5\u4e0a\u5e73\u5747\u4f18\u4e8eSC\u3001USC\u548cWUCS\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u5ffd\u7565\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "LSC\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u8de8\u7b54\u6848\u683c\u5f0f\u4e00\u81f4\u6027\u9009\u62e9\u65b9\u6cd5\uff0c\u63d0\u4f9b\u826f\u597d\u6821\u51c6\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u5728\u4e24\u4e2a\u7b54\u6848\u683c\u5f0f\u4e0a\u90fd\u4fdd\u6301\u4f4e\u671f\u671b\u6821\u51c6\u8bef\u5dee\u3002"}}
{"id": "2508.18407", "categories": ["cs.CL", "cs.AI", "68T01, 68T07, 68T50", "I.2"], "pdf": "https://arxiv.org/pdf/2508.18407", "abs": "https://arxiv.org/abs/2508.18407", "authors": ["Michal \u0160tef\u00e1nik", "Timothee Mickus", "Marek Kadl\u010d\u00edk", "Michal Spiegel", "Josef Kucha\u0159"], "title": "Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering", "comment": "To appear in Findings of EMNLP 2025", "summary": "A majority of recent work in AI assesses models' generalization capabilities\nthrough the lens of performance on out-of-distribution (OOD) datasets. Despite\ntheir practicality, such evaluations build upon a strong assumption: that OOD\nevaluations can capture and reflect upon possible failures in a real-world\ndeployment.\n  In this work, we challenge this assumption and confront the results obtained\nfrom OOD evaluations with a set of specific failure modes documented in\nexisting question-answering (QA) models, referred to as a reliance on spurious\nfeatures or prediction shortcuts.\n  We find that different datasets used for OOD evaluations in QA provide an\nestimate of models' robustness to shortcuts that have a vastly different\nquality, some largely under-performing even a simple, in-distribution\nevaluation. We partially attribute this to the observation that spurious\nshortcuts are shared across ID+OOD datasets, but also find cases where a\ndataset's quality for training and evaluation is largely disconnected. Our work\nunderlines limitations of commonly-used OOD-based evaluations of\ngeneralization, and provides methodology and recommendations for evaluating\ngeneralization within and beyond QA more robustly.", "AI": {"tldr": "\u672c\u6587\u6316\u6398\u4e86OOD\u8bc4\u4f30\u65b9\u6cd5\u5728\u8bc4\u4f30\u95ee\u7b54\u6a21\u578b\u5065\u58ee\u6027\u65f6\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u4e0d\u540c\u6570\u636e\u96c6\u5bf9\u6a21\u578b\u504f\u597d\u77ed\u4fbf\u5f0f\u7684\u8bc4\u4f30\u8d28\u91cf\u5dee\u5f02\u5f88\u5927\uff0c\u5e76\u63d0\u51fa\u4e86\u66f4\u5065\u58ee\u7684\u8bc4\u4f30\u65b9\u6cd5\u5efa\u8bae\u3002", "motivation": "\u5f53\u524dAI\u9886\u57df\u4f9d\u8d56OOD\u6570\u636e\u96c6\u8bc4\u4f30\u6a21\u578b\u6e17\u900f\u6027\uff0c\u4f46\u8fd9\u4e2a\u5047\u8bbe\u53ef\u80fd\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\u3002\u672c\u6587\u8981\u6316\u6398OOD\u8bc4\u4f30\u5728\u8bc4\u4f30\u95ee\u7b54\u6a21\u578b\u504f\u597d\u77ed\u4fbf\u5f0f\u65f6\u7684\u6709\u6548\u6027\u95ee\u9898\u3002", "method": "\u5c06OOD\u8bc4\u4f30\u7ed3\u679c\u4e0e\u73b0\u6709\u95ee\u7b54\u6a21\u578b\u4e2d\u5df2\u6587\u6863\u5316\u7684\u5177\u4f53\u5931\u8d25\u6a21\u5f0f\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5206\u6790\u4e0d\u540cOOD\u6570\u636e\u96c6\u5bf9\u6a21\u578b\u504f\u597d\u77ed\u4fbf\u5f0f\u8bc4\u4f30\u7684\u8d28\u91cf\u5dee\u5f02\u3002", "result": "\u53d1\u73b0\u4e0d\u540cOOD\u6570\u636e\u96c6\u5bf9\u6a21\u578b\u504f\u597d\u77ed\u4fbf\u5f0f\u7684\u8bc4\u4f30\u8d28\u91cf\u5dee\u5f02\u663e\u8457\uff0c\u67d0\u4e9b\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u6548\u679c\u751a\u81f3\u8f83\u7b80\u5355\u7684\u5185\u90e8\u5206\u5e03\u8bc4\u4f30\u66f4\u5dee\u3002\u8fd9\u90e8\u5206\u5f52\u56e0\u4e8e\u77ed\u4fbf\u5f0f\u5728ID\u548cOOD\u6570\u636e\u96c6\u4e2d\u7684\u5171\u4eab\u6027\uff0c\u4ee5\u53ca\u6570\u636e\u96c6\u8bad\u7ec3\u8d28\u91cf\u4e0e\u8bc4\u4f30\u8d28\u91cf\u7684\u8131\u8282\u3002", "conclusion": "\u5e38\u89c1\u7684OOD\u57fa\u4e8e\u6e17\u900f\u6027\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u672c\u7814\u7a76\u4e3a\u5728\u95ee\u7b54\u53ca\u66f4\u5e7f\u6cdb\u9886\u57df\u66f4\u5065\u58ee\u5730\u8bc4\u4f30\u6a21\u578b\u6e17\u900f\u6027\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u548c\u5efa\u8bae\u3002"}}
{"id": "2508.18444", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18444", "abs": "https://arxiv.org/abs/2508.18444", "authors": ["Nafis Tanveer Islam", "Zhiming Zhao"], "title": "How Reliable are LLMs for Reasoning on the Re-ranking task?", "comment": "Accepted at FQAS Conference 2024. DOI will be provided in 3 weeks\n  after the conference has published the paper", "summary": "With the improving semantic understanding capability of Large Language Models\n(LLMs), they exhibit a greater awareness and alignment with human values, but\nthis comes at the cost of transparency. Although promising results are achieved\nvia experimental analysis, an in-depth understanding of the LLM's internal\nworkings is unavoidable to comprehend the reasoning behind the re-ranking,\nwhich provides end users with an explanation that enables them to make an\ninformed decision. Moreover, in newly developed systems with limited user\nengagement and insufficient ranking data, accurately re-ranking content remains\na significant challenge. While various training methods affect the training of\nLLMs and generate inference, our analysis has found that some training methods\nexhibit better explainability than others, implying that an accurate semantic\nunderstanding has not been learned through all training methods; instead,\nabstract knowledge has been gained to optimize evaluation, which raises\nquestions about the true reliability of LLMs. Therefore, in this work, we\nanalyze how different training methods affect the semantic understanding of the\nre-ranking task in LLMs and investigate whether these models can generate more\ninformed textual reasoning to overcome the challenges of transparency or LLMs\nand limited training data. To analyze the LLMs for re-ranking tasks, we utilize\na relatively small ranking dataset from the environment and the Earth science\ndomain to re-rank retrieved content. Furthermore, we also analyze the\nexplainable information to see if the re-ranking can be reasoned using\nexplainability.", "AI": {"tldr": "\\u8fd9\\u7bc7\\u8bba\\u6587\\u5206\\u6790\\u4e86\\u4e0d\\u540c\\u8bad\\u7ec3\\u65b9\\u6cd5\\u5bf9\\u5927\\u8bed\\u8a00\\u6a21\\u578b\\u5728\\u91cd\\u6392\\u540d\\u4efb\\u52a1\\u4e2d\\u8bed\\u4e49\\u7406\\u89e3\\u80fd\\u529b\\u7684\\u5f71\\u54cd\\uff0c\\u7814\\u7a76\\u6a21\\u578b\\u7684\\u53ef\\u89e3\\u91ca\\u6027\\u548c\\u5728\\u8bad\\u7ec3\\u6570\\u636e\\u6709\\u9650\\u60c5\\u51b5\\u4e0b\\u7684\\u8868\\u73b0\\u3002", "motivation": "\\u968f\\u7740\\u5927\\u8bed\\u8a00\\u6a21\\u578b\\u8bed\\u4e49\\u7406\\u89e3\\u80fd\\u529b\\u7684\\u63d0\\u5347\\uff0c\\u5b83\\u4eec\\u66f4\\u52a0\\u7b26\\u5408\\u4eba\\u7c7b\\u4ef7\\u503c\\u89c2\\uff0c\\u4f46\\u4ee5\\u900f\\u660e\\u6027\\u4e3a\\u4ee3\\u4ef7\\u3002\\u9700\\u8981\\u6df1\\u5165\\u7406\\u89e3\\u6a21\\u578b\\u5185\\u90e8\\u5de5\\u4f5c\\u673a\\u5236\\u6765\\u63d0\\u4f9b\\u53ef\\u89e3\\u91ca\\u7684\\u91cd\\u6392\\u540d\\u7406\\u7531\\uff0c\\u5e76\\u89e3\\u51b3\\u65b0\\u7cfb\\u7edf\\u4e2d\\u8bad\\u7ec3\\u6570\\u636e\\u6709\\u9650\\u7684\\u6311\\u6218\\u3002", "method": "\\u4f7f\\u7528\\u73af\\u5883\\u548c\\u5730\\u7403\\u79d1\\u5b66\\u9886\\u57df\\u7684\\u8f83\\u5c0f\\u6392\\u540d\\u6570\\u636e\\u96c6\\uff0c\\u5206\\u6790\\u4e0d\\u540c\\u8bad\\u7ec3\\u65b9\\u6cd5\\u4e0bLLM\\u5728\\u91cd\\u6392\\u540n\\u4efb\\u52a1\\u4e2d\\u7684\\u8868\\u73b0\\uff0c\\u7814\\u7a76\\u6a21\\u578b\\u751f\\u6210\\u7684\\u6587\\u672c\\u63a8\\u7406\\u548c\\u53ef\\u89e3\\u91ca\\u6027\\u4fe1\\u606f\\u3002", "result": "\\u53d1\\u73b0\\u67d0\\u4e9b\\u8bad\\u7ec3\\u65b9\\u6cd5\\u6bd4\\u5176\\u4ed6\\u65b9\\u6cd5\\u5177\\u6709\\u66f4\\u597d\\u7684\\u53ef\\u89e3\\u91ca\\u6027\\uff0c\\u8868\\u660e\\u4e0d\\u662f\\u6240\\u6709\\u8bad\\u7ec3\\u65b9\\u6cd5\\u90fd\\u80fd\\u5b66\\u4e60\\u5230\\u51c6\\u786e\\u7684\\u8bed\\u4e49\\u7406\\u89e3\\uff0c\\u800c\\u662f\\u83b7\\u5f97\\u4e86\\u62bd\\u8c61\\u77e5\\u8bc6\\u6765\\u4f18\\u5316\\u8bc4\\u4f30\\u3002", "conclusion": "\\u7814\\u7a76\\u5bf9\\u4e8e\\u7406\\u89e3LLM\\u5728\\u91cd\\u6392\\u540d\\u4efb\\u52a1\\u4e2d\\u7684\\u771f\\u5b9e\\u53ef\\u9760\\u6027\\u81f3\\u5173\\u91cd\\u8981\\uff0c\\u4e0d\\u540c\\u8bad\\u7ec3\\u65b6\\u65b9\\u6cd5\\u5bf9\\u6a21\\u578b\\u7684\\u8bed\\u4e49\\u7406\\u89e3\\u80fd\\u529b\\u548c\\u53ef\\u89e3\\u91ca\\u6027\\u6709\\u663e\\u8457\\u5f71\\u54cd\\uff0c\\u9700\\u8981\\u66f4\\u6df1\\u5165\\u7684\\u5206\\u6790\\u6765\\u786e\\u4fdd\\u6a21\\u578b\\u7684\\u900f\\u660e\\u6027\\u548c\\u53ef\\u9760\\u6027\\u3002"}}
{"id": "2508.18466", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18466", "abs": "https://arxiv.org/abs/2508.18466", "authors": ["Alina Wr\u00f3blewska", "Bartosz \u017buk"], "title": "Integrating gender inclusivity into large language models via instruction tuning", "comment": null, "summary": "Imagine a language with masculine, feminine, and neuter grammatical genders,\nyet, due to historical and political conventions, masculine forms are\npredominantly used to refer to men, women and mixed-gender groups. This is the\nreality of contemporary Polish. A social consequence of this unfair linguistic\nsystem is that large language models (LLMs) trained on Polish texts inherit and\nreinforce this masculine bias, generating gender-imbalanced outputs. This study\naddresses this issue by tuning LLMs using the IPIS dataset, a collection of\nhuman-crafted gender-inclusive proofreading in Polish and Polish-to-English\ntranslation instructions. Grounded in a theoretical linguistic framework, we\ndesign a system prompt with explicit gender-inclusive guidelines for Polish. In\nour experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and\nMistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to\nintegrate gender inclusivity as an inherent feature of these models, offering a\nsystematic solution to mitigate gender bias in Polish language generation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7IPIS\u6570\u636e\u96c6\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e3a\u6ce2\u5170\u8bed\u8bbe\u8ba1\u6027\u522b\u5305\u5bb9\u6027\u7cfb\u7edf\u63d0\u793a\uff0c\u65e8\u5728\u89e3\u51b3\u6ce2\u5170\u8bed\u4e2d\u7537\u6027\u504f\u89c1\u95ee\u9898\u3002", "motivation": "\u6ce2\u5170\u8bed\u7531\u4e8e\u5386\u53f2\u548c\u653f\u6cbb\u60ef\u4f8b\uff0c\u4e3b\u8981\u4f7f\u7528\u7537\u6027\u5f62\u5f0f\u6307\u4ee3\u6240\u6709\u4eba\uff0c\u5bfc\u81f4\u5927\u8bed\u8a00\u6a21\u578b\u7ee7\u627f\u5e76\u5f3a\u5316\u4e86\u8fd9\u79cd\u6027\u522b\u504f\u89c1\uff0c\u4ea7\u751f\u6027\u522b\u4e0d\u5e73\u8861\u7684\u8f93\u51fa\u3002", "method": "\u4f7f\u7528IPIS\u6570\u636e\u96c6\uff08\u5305\u542b\u4eba\u5de5\u5236\u4f5c\u7684\u6027\u522b\u5305\u5bb9\u6027\u6821\u5bf9\u548c\u6ce2\u5170\u8bed\u5230\u82f1\u8bed\u7ffb\u8bd1\u6307\u4ee4\uff09\u5fae\u8c03\u591a\u8bed\u8a00LLMs\u548c\u6ce2\u5170\u8bed\u4e13\u7528LLMs\uff0c\u5e76\u8bbe\u8ba1\u5305\u542b\u660e\u786e\u6027\u522b\u5305\u5bb9\u6027\u6307\u5357\u7684\u7cfb\u7edf\u63d0\u793a\u3002", "result": "\u7814\u7a76\u5bf9Llama-8B\u3001Mistral-7B\u3001Mistral-Nemo\u7b49\u591a\u8bed\u8a00\u6a21\u578b\u4ee5\u53caBielik\u3001PLLuM\u7b49\u6ce2\u5170\u8bed\u4e13\u7528\u6a21\u578b\u8fdb\u884c\u4e86IPIS\u5fae\u8c03\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e8\u5728\u5c06\u6027\u522b\u5305\u5bb9\u6027\u4f5c\u4e3a\u6a21\u578b\u7684\u5185\u5728\u7279\u5f81\uff0c\u4e3a\u7f13\u89e3\u6ce2\u5170\u8bed\u751f\u6210\u4e2d\u7684\u6027\u522b\u504f\u89c1\u63d0\u4f9b\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18473", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18473", "abs": "https://arxiv.org/abs/2508.18473", "authors": ["Jiawei Li", "Akshayaa Magesh", "Venugopal V. Veeravalli"], "title": "Principled Detection of Hallucinations in Large Language Models via Multiple Testing", "comment": "16 pages", "summary": "While Large Language Models (LLMs) have emerged as powerful foundational\nmodels to solve a variety of tasks, they have also been shown to be prone to\nhallucinations, i.e., generating responses that sound confident but are\nactually incorrect or even nonsensical. In this work, we formulate the problem\nof detecting hallucinations as a hypothesis testing problem and draw parallels\nto the problem of out-of-distribution detection in machine learning models. We\npropose a multiple-testing-inspired method to solve the hallucination detection\nproblem, and provide extensive experimental results to validate the robustness\nof our approach against state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u91cd\u68c0\u9a8c\u5047\u8bbe\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5c06LLM\u5e7b\u89c9\u68c0\u6d4b\u95ee\u9898\u5efa\u6a21\u4e3a\u5047\u8bbe\u68c0\u9a8c\u95ee\u9898\uff0c\u5e76\u4e0e\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u5916\u68c0\u6d4b\u95ee\u9898\u8fdb\u884c\u7c7b\u6bd4\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u529f\u80fd\u5f3a\u5927\uff0c\u4f46\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff08\u751f\u6210\u770b\u4f3c\u81ea\u4fe1\u4f46\u5b9e\u9645\u9519\u8bef\u6216\u65e0\u610f\u4e49\u7684\u56de\u7b54\uff09\uff0c\u9700\u8981\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u6765\u8bc6\u522b\u8fd9\u4e9b\u4e0d\u53ef\u9760\u7684\u8f93\u51fa\u3002", "method": "\u5c06\u5e7b\u89c9\u68c0\u6d4b\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u5047\u8bbe\u68c0\u9a8c\u95ee\u9898\uff0c\u501f\u9274\u591a\u91cd\u68c0\u9a8c\u7684\u601d\u60f3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u8ba1\u65b9\u6cd5\u6765\u68c0\u6d4bLLM\u751f\u6210\u7684\u56de\u7b54\u4e2d\u7684\u5e7b\u89c9\u5185\u5bb9\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522bLLM\u751f\u6210\u7684\u5e7b\u89c9\u5185\u5bb9\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u91cd\u68c0\u9a8c\u542f\u53d1\u5f0f\u65b9\u6cd5\u4e3aLLM\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7edf\u8ba1\u6846\u67b6\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.18549", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.18549", "abs": "https://arxiv.org/abs/2508.18549", "authors": ["Maike Z\u00fcfle", "Vil\u00e9m Zouhar", "Tu Anh Dinh", "Felipe Maia Polo", "Jan Niehues", "Mrinmaya Sachan"], "title": "COMET-poly: Machine Translation Metric Grounded in Other Candidates", "comment": "Maike Z\\\"ufle, Vil\\'em Zouhar, and Tu Anh Dinh contributed equally", "summary": "Automated metrics for machine translation attempt to replicate human\njudgment. Unlike humans, who often assess a translation in the context of\nmultiple alternatives, these metrics typically consider only the source\nsentence and a single translation. This discrepancy in the evaluation setup may\nnegatively impact the performance of automated metrics. We propose two\nautomated metrics that incorporate additional information beyond the single\ntranslation. COMET-polycand uses alternative translations of the same source\nsentence to compare and contrast with the translation at hand, thereby\nproviding a more informed assessment of its quality. COMET-polyic, inspired by\nretrieval-based in-context learning, takes in translations of similar source\ntexts along with their human-labeled quality scores to guide the evaluation. We\nfind that including a single additional translation in COMET-polycand improves\nthe segment-level metric performance (0.079 to 0.118 Kendall's tau-b\ncorrelation), with further gains when more translations are added.\nIncorporating retrieved examples in COMET-polyic yields similar improvements\n(0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u65b0\u7684\u673a\u5668\u7ffb\u8bd1\u81ea\u52a8\u8bc4\u4f30\u6307\u6807COMET-polycand\u548cCOMET-polyic\uff0c\u901a\u8fc7\u5f15\u5165\u591a\u4e2a\u5907\u9009\u7ffb\u8bd1\u6216\u76f8\u4f3c\u6587\u672c\u7684\u7ffb\u8bd1\u793a\u4f8b\u6765\u63d0\u5347\u8bc4\u4f30\u6027\u80fd\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528\u5355\u4e00\u7ffb\u8bd1\u7684\u4f20\u7edf\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u7ffb\u8bd1\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u53ea\u8003\u8651\u6e90\u53e5\u5b50\u548c\u5355\u4e2a\u7ffb\u8bd1\uff0c\u800c\u4eba\u7c7b\u8bc4\u4f30\u65f6\u901a\u5e38\u4f1a\u5bf9\u6bd4\u591a\u4e2a\u5907\u9009\u7ffb\u8bd1\u3002\u8fd9\u79cd\u8bc4\u4f30\u8bbe\u7f6e\u4e0a\u7684\u5dee\u5f02\u53ef\u80fd\u5f71\u54cd\u81ea\u52a8\u6307\u6807\u7684\u6027\u80fd\u3002", "method": "COMET-polycand\u4f7f\u7528\u540c\u4e00\u6e90\u53e5\u7684\u591a\u4e2a\u5907\u9009\u7ffb\u8bd1\u8fdb\u884c\u6bd4\u8f83\uff1bCOMET-polyic\u501f\u9274\u68c0\u7d22\u5f0f\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u8f93\u5165\u76f8\u4f3c\u6e90\u6587\u672c\u7684\u7ffb\u8bd1\u53ca\u5176\u4eba\u5de5\u6807\u6ce8\u8d28\u91cf\u5206\u6570\u6765\u6307\u5bfc\u8bc4\u4f30\u3002", "result": "COMET-polycand\u52a0\u5165\u5355\u4e2a\u989d\u5916\u7ffb\u8bd1\u5373\u53ef\u63d0\u5347\u6bb5\u7ea7\u6307\u6807\u6027\u80fd\uff08Kendall's tau-b\u76f8\u5173\u6027\u4ece0.079\u63d0\u5347\u81f30.118\uff09\uff0c\u52a0\u5165\u66f4\u591a\u7ffb\u8bd1\u6548\u679c\u66f4\u597d\u3002COMET-polyic\u901a\u8fc7\u68c0\u7d22\u793a\u4f8b\u83b7\u5f97\u7c7b\u4f3c\u6539\u8fdb\uff080.079\u52300.116\uff09\u3002", "conclusion": "\u5728\u81ea\u52a8\u7ffb\u8bd1\u8bc4\u4f30\u4e2d\u5f15\u5165\u591a\u4e2a\u7ffb\u8bd1\u6216\u76f8\u4f3c\u793a\u4f8b\u4fe1\u606f\u80fd\u591f\u663e\u8457\u63d0\u5347\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u5de5\u5224\u65ad\u7684\u76f8\u5173\u6027\uff0c\u6a21\u578b\u5df2\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2508.18569", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18569", "abs": "https://arxiv.org/abs/2508.18569", "authors": ["Girish A. Koushik", "Fatemeh Nazarieh", "Katherine Birch", "Shenbin Qian", "Diptesh Kanojia"], "title": "The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation", "comment": "Under Review", "summary": "Visual metaphor generation is a challenging task that aims to generate an\nimage given an input text metaphor. Inherently, it needs language understanding\nto bind a source concept with a target concept, in a way that preserves meaning\nwhile ensuring visual coherence. We propose a self-evaluating visual metaphor\ngeneration framework that focuses on metaphor alignment. Our self-evaluation\napproach combines existing metrics with our newly proposed metaphor\ndecomposition score and a meaning alignment (MA) metric. Within this setup, we\nexplore two novel approaches: a training-free pipeline that explicitly\ndecomposes prompts into source-target-meaning (S-T-M) mapping for image\nsynthesis, and a complementary training-based pipeline that improves alignment\nusing our proposed self-evaluation reward schema, without any large-scale\nretraining. On the held-out test set, the training-free approach surpasses\nstrong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores,\nwith the training-based approach close behind. We evaluate our framework output\nusing a user-facing study, and observed that participants preferred GPT-4o\noverall, while our training-free pipeline led open-source methods and edged\nImagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or\nmore abstract metaphors, with closed models excelling on short, concrete cases;\nwe also observe sensitivity to sampler settings. Overall, structured prompting\nand lightweight RL perform metaphor alignment well under modest compute, and\nremaining gaps to human preference appear driven by aesthetics and sampling.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u6211\u8bc4\u4f30\u7684\u89c6\u89c9\u9690\u55bb\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u8a00\u5206\u89e3\u548c\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u6765\u63d0\u9ad8\u9690\u55bb\u5bf9\u9f50\u6548\u679c\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86GPT-4o\u548cImagen\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u89c6\u89c9\u9690\u55bb\u751f\u6210\u9700\u8981\u540c\u65f6\u6ee1\u8db3\u8bed\u8a00\u7406\u89e3\u548c\u89c6\u89c9\u4e00\u81f4\u6027\uff0c\u5f53\u524d\u6a21\u578b\u5728\u9690\u55bb\u5bf9\u9f50\u65b9\u9762\u9047\u5230\u6311\u6218\uff0c\u9700\u8981\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u7ed1\u5b9a\u6e90\u6982\u5ff5\u4e0e\u76ee\u6807\u6982\u5ff5\u7684\u542b\u4e49\u5173\u8054\u3002", "method": "\u63d0\u51fa\u4e86\u81ea\u6211\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u65b0\u63d0\u51fa\u7684\u9690\u55bb\u5206\u89e3\u6307\u6807\u548c\u542b\u4e49\u5bf9\u9f50\u6307\u6807\u3002\u5305\u542b\u4e24\u79cd\u65b9\u6cd5\uff1a\u4e00\u79cd\u662f\u65e0\u9700\u8bad\u7ec3\u7684\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u663e\u5f0f\u5c06\u63d0\u793a\u5206\u89e3\u4e3a\u6e90-\u76ee\u6807-\u542b\u4e49\uff08S-T-M\uff09\u6620\u5c04\u8fdb\u884c\u56fe\u50cf\u5408\u6210\uff1b\u53e6\u4e00\u79cd\u662f\u57fa\u4e8e\u8bad\u7ec3\u7684\u6d41\u6c34\u7ebf\uff0c\u4f7f\u7528\u81ea\u6211\u8bc4\u4f30\u5956\u52b1\u673a\u5236\u6539\u5584\u5bf9\u9f50\u6548\u679c\u3002", "result": "\u5728\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u65e0\u9700\u8bad\u7ec3\u65b9\u6cd5\u5728\u5206\u89e3\u3001CLIP\u548c\u542b\u4e49\u5bf9\u9f50\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86GPT-4o\u548cImagen\u7b49\u5f3a\u52b2\u57fa\u7ebf\u6a21\u578b\u3002\u7528\u6237\u7814\u7a76\u663e\u793a\u7528\u6237\u66f4\u504f\u597dGPT-4o\uff0c\u4f46\u65e0\u9700\u8bad\u7ec3\u6d41\u6c34\u7ebf\u5728\u5f00\u6e90\u65b9\u6cd5\u4e2d\u9886\u5148\u5e76\u5728\u62bd\u8c61\u9690\u55bb\u4e0a\u5fae\u8d85Imagen\u3002", "conclusion": "\u7ed3\u6784\u5316\u63d0\u793a\u548c\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u9690\u55bb\u5bf9\u9f50\uff0c\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5dee\u8ddd\u4e3b\u8981\u6765\u81ea\u7f8e\u5b66\u8d28\u91cf\u548c\u91c7\u6837\u8bbe\u7f6e\u7684\u654f\u611f\u6027\u3002"}}
{"id": "2508.18598", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18598", "abs": "https://arxiv.org/abs/2508.18598", "authors": ["Colin Klein"], "title": "What do language models model? Transformers, automata, and the format of thought", "comment": null, "summary": "What do large language models actually model? Do they tell us something about\nhuman capacities, or are they models of the corpus we've trained them on? I\ngive a non-deflationary defence of the latter position. Cognitive science tells\nus that linguistic capabilities in humans rely supralinear formats for\ncomputation. The transformer architecture, by contrast, supports at best a\nlinear formats for processing. This argument will rely primarily on certain\ninvariants of the computational architecture of transformers. I then suggest a\npositive story about what transformers are doing, focusing on Liu et al.\n(2022)'s intriguing speculations about shortcut automata. I conclude with why I\ndon't think this is a terribly deflationary story. Language is not (just) a\nmeans for expressing inner state but also a kind of 'discourse machine' that\nlets us make new language given appropriate context. We have learned to use\nthis technology in one way; LLMs have also learned to use it too, but via very\ndifferent means.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u5f20\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u662f\u5bf9\u8bad\u7ec3\u8bed\u6599\u5e93\u7684\u6a21\u62df\uff0c\u800c\u975e\u4eba\u7c7b\u8ba4\u77e5\u80fd\u529b\u7684\u6a21\u578b\uff0c\u56e0\u4e3aTransformer\u67b6\u6784\u7684\u7ebf\u6027\u8ba1\u7b97\u683c\u5f0f\u4e0e\u4eba\u7c7b\u8d85\u7ebf\u6027\u8ba1\u7b97\u683c\u5f0f\u5b58\u5728\u6839\u672c\u5dee\u5f02", "motivation": "\u8fa8\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u672c\u8d28\uff0c\u660e\u786e\u5176\u662f\u5426\u771f\u6b63\u6a21\u62df\u4e86\u4eba\u7c7b\u8ba4\u77e5\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u5bf9\u8bad\u7ec3\u8bed\u6599\u5e93\u7684\u7edf\u8ba1\u5b66\u4e60", "method": "\u901a\u8fc7\u5bf9\u6bd4Transformer\u67b6\u6784\u7684\u7ebf\u6027\u8ba1\u7b97\u683c\u5f0f\u4e0e\u4eba\u7c7b\u8d85\u7ebf\u6027\u8ba1\u7b97\u683c\u5f0f\u7684\u5dee\u5f02\uff0c\u4ee5\u53ca\u57fa\u4e8eLiu\u7b49\u4eba(2022)\u5173\u4e8e\u77ed\u63a5\u81ea\u52a8\u673a\u7684\u7406\u8bba\u5206\u6790", "result": "\u8bc1\u660e\u4e86LLM\u4e3b\u8981\u662f\u5bf9\u8bed\u6599\u5e93\u7684\u6a21\u62df\uff0c\u800c\u975e\u4eba\u7c7b\u8ba4\u77e5\u80fd\u529b\u7684\u76f4\u63a5\u6a21\u578b\uff0c\u4f46\u8ba4\u4e3a\u8fd9\u5e76\u4e0d\u662f\u6c14\u837c\u7684\u89c2\u70b9", "conclusion": "\u8bed\u8a00\u4e0d\u4ec5\u4ec5\u662f\u5185\u90e8\u72b6\u6001\u7684\u8868\u8fbe\u5de5\u5177\uff0c\u66f4\u662f\u4e00\u79cd\"\u8bdd\u8bed\u673a\u5668\"\uff0cLLM\u901a\u8fc7\u4e0d\u540c\u65b9\u5f0f\u5b66\u4f1a\u4e86\u4f7f\u7528\u8fd9\u79cd\u6280\u672f"}}
{"id": "2508.18607", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18607", "abs": "https://arxiv.org/abs/2508.18607", "authors": ["Rumeng Li", "Xun Wang", "Hong Yu"], "title": "A New NMT Model for Translating Clinical Texts from English to Spanish", "comment": "This work was accepted by the Machine Learning for Health (ML4H)\n  Workshop at NeurIPS 2018", "summary": "Translating electronic health record (EHR) narratives from English to Spanish\nis a clinically important yet challenging task due to the lack of a\nparallel-aligned corpus and the abundant unknown words contained. To address\nsuch challenges, we propose \\textbf{NOOV} (for No OOV), a new neural machine\ntranslation (NMT) system that requires little in-domain parallel-aligned corpus\nfor training. NOOV integrates a bilingual lexicon automatically learned from\nparallel-aligned corpora and a phrase look-up table extracted from a large\nbiomedical knowledge resource, to alleviate both the unknown word problem and\nthe word-repeat challenge in NMT, enhancing better phrase generation of NMT\nsystems. Evaluation shows that NOOV is able to generate better translation of\nEHR with improvement in both accuracy and fluency.", "AI": {"tldr": "NOOV\u662f\u4e00\u4e2a\u65b0\u7684\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u4e13\u95e8\u7528\u4e8e\u5c06\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4ece\u82f1\u6587\u7ffb\u8bd1\u5230\u897f\u73ed\u7259\u6587\uff0c\u901a\u8fc7\u6574\u5408\u53cc\u8bed\u8bcd\u5178\u548c\u77ed\u8bed\u67e5\u627e\u8868\u6765\u89e3\u51b3\u672a\u77e5\u8bcd\u6c47\u548c\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55(EHR)\u7684\u82f1\u6587\u5230\u897f\u73ed\u7259\u6587\u7ffb\u8bd1\u5177\u6709\u91cd\u8981\u4e34\u5e8a\u610f\u4e49\uff0c\u4f46\u9762\u4e34\u7f3a\u4e4f\u5e73\u884c\u8bed\u6599\u5e93\u548c\u5927\u91cf\u672a\u77e5\u8bcd\u6c47\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faNOOV\u7cfb\u7edf\uff0c\u6574\u5408\u4ece\u5e73\u884c\u8bed\u6599\u5e93\u81ea\u52a8\u5b66\u4e60\u7684\u53cc\u8bed\u8bcd\u5178\u548c\u4ece\u5927\u578b\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u8d44\u6e90\u63d0\u53d6\u7684\u77ed\u8bed\u67e5\u627e\u8868\uff0c\u7f13\u89e3NMT\u4e2d\u7684\u672a\u77e5\u8bcd\u95ee\u9898\u548c\u8bcd\u6c47\u91cd\u590d\u6311\u6218\u3002", "result": "\u8bc4\u4f30\u663e\u793aNOOV\u80fd\u591f\u751f\u6210\u66f4\u597d\u7684EHR\u7ffb\u8bd1\uff0c\u5728\u51c6\u786e\u6027\u548c\u6d41\u7545\u6027\u65b9\u9762\u90fd\u6709\u6539\u8fdb\u3002", "conclusion": "NOOV\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86EHR\u7ffb\u8bd1\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u4e34\u5e8a\u533b\u7597\u4e2d\u7684\u8de8\u8bed\u8a00\u4fe1\u606f\u4ea4\u6d41\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18609", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18609", "abs": "https://arxiv.org/abs/2508.18609", "authors": ["Chenxi Zhou", "Pengfei Cao", "Jiang Li", "Jun Zhao", "Kang Liu"], "title": "Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models", "comment": null, "summary": "Large language models (LLMs) present significant deployment challenges due to\ntheir scale, with post-training quantization (PTQ) emerging as a practical\ncompression solution. However, a comprehensive understanding of how PTQ\nprecisely impacts diverse LLM knowledge capabilities remains elusive, and\nexisting scaling laws for quantized models often overlook crucial PTQ-specific\nparameters and task-specific sensitivities. This paper addresses these gaps by\nconducting an extensive empirical investigation to establish task-stratified\nscaling laws. We disentangle LLM knowledge into memorization and utilization\ncapabilities and develop a unified quantitative framework that incorporates\nmodel size, effective bit-width, calibration set size, and group size. Our\ncentral finding reveals that knowledge memorization exhibits markedly greater\nsensitivity to variations in effective bit-width, calibration set size, and\nmodel size compared to the more robust knowledge utilization. These findings\noffer a fine-grained understanding of PTQ's impact and provide guidance for\ndeveloping knowledge-aware quantization strategies that can better preserve\ntargeted cognitive functions.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\u5efa\u7acb\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u91cf\u5316\u7684\u4efb\u52a1\u5206\u5c42\u7f29\u653e\u5f8b\uff0c\u53d1\u73b0\u77e5\u8bc6\u8bb0\u5fc6\u6bd4\u77e5\u8bc6\u5229\u7528\u5bf9\u91cf\u5316\u53c2\u6570\u66f4\u654f\u611f\uff0c\u4e3a\u77e5\u8bc6\u611f\u77e5\u91cf\u5316\u7b56\u7565\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u5f53\u524d\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u91cf\u5316\u5982\u4f55\u51c6\u786e\u5f71\u54cd\u591a\u6837\u5316\u77e5\u8bc6\u80fd\u529b\u7684\u7406\u89e3\u4e0d\u5145\u5206\uff0c\u73b0\u6709\u7f29\u653e\u5f8b\u5ffd\u89c6\u4e86PTQ\u7279\u5b9a\u53c2\u6570\u548c\u4efb\u52a1\u7279\u5b9a\u654f\u611f\u6027\u3002", "method": "\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u5c06LLM\u77e5\u8bc6\u89e3\u6784\u4e3a\u8bb0\u5fc6\u548c\u5229\u7528\u80fd\u529b\uff0c\u5f00\u53d1\u5305\u542b\u6a21\u578b\u5927\u5c0f\u3001\u6709\u6548\u4f4d\u5bbd\u3001\u6821\u51c6\u96c6\u5927\u5c0f\u548c\u7ec4\u5927\u5c0f\u7684\u7edf\u4e00\u91cf\u5316\u6846\u67b6\u3002", "result": "\u6838\u5fc3\u53d1\u73b0\uff1a\u77e5\u8bc6\u8bb0\u5fc6\u5728\u6709\u6548\u4f4d\u5bbd\u3001\u6821\u51c6\u96c6\u5927\u5c0f\u548c\u6a21\u578b\u5927\u5c0f\u7684\u53d8\u5316\u4e0a\u663e\u793a\u51fa\u6bd4\u77e5\u8bc6\u5229\u7528\u66f4\u660e\u663e\u7684\u654f\u611f\u6027\uff0c\u800c\u77e5\u8bc6\u5229\u7528\u66f4\u52a0\u7a33\u5065\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63d0\u4f9b\u4e86\u5bf9PTQ\u5f71\u54cd\u7684\u7ec6\u7c92\u5ea6\u7406\u89e3\uff0c\u4e3a\u5f00\u53d1\u80fd\u66f4\u597d\u4fdd\u7559\u76ee\u6807\u8ba4\u77e5\u529f\u80fd\u7684\u77e5\u8bc6\u611f\u77e5\u91cf\u5316\u7b56\u7565\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2508.18648", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18648", "abs": "https://arxiv.org/abs/2508.18648", "authors": ["Cong Li", "Wenchang Chai", "Hejun Wu", "Yan Pan", "Pengxu Wei", "Liang Lin"], "title": "Thinking Before You Speak: A Proactive Test-time Scaling Approach", "comment": null, "summary": "Large Language Models (LLMs) often exhibit deficiencies with complex\nreasoning tasks, such as maths, which we attribute to the discrepancy between\nhuman reasoning patterns and those presented in the LLMs' training data. When\ndealing with complex problems, humans tend to think carefully before expressing\nsolutions. However, they often do not articulate their inner thoughts,\nincluding their intentions and chosen methodologies. Consequently, critical\ninsights essential for bridging reasoning steps may be absent in training data\ncollected from human sources. To bridge this gap, we proposes inserting\n\\emph{insight}s between consecutive reasoning steps, which review the status\nand initiate the next reasoning steps. Unlike prior prompting strategies that\nrely on a single or a workflow of static prompts to facilitate reasoning,\n\\emph{insight}s are \\emph{proactively} generated to guide reasoning processes.\nWe implement our idea as a reasoning framework, named \\emph{Thinking Before You\nSpeak} (TBYS), and design a pipeline for automatically collecting and filtering\nin-context examples for the generation of \\emph{insight}s, which alleviates\nhuman labeling efforts and fine-tuning overheads. Experiments on challenging\nmathematical datasets verify the effectiveness of TBYS. Project website:\nhttps://gitee.com/jswrt/TBYS", "AI": {"tldr": "TBYS\u6846\u67b6\u901a\u8fc7\u5728\u63a8\u7406\u6b65\u9aa4\u95f4\u63d2\u5165\u4e3b\u52a8\u751f\u6210\u7684insights\u6765\u6307\u5bfcLLM\u7684\u590d\u6742\u63a8\u7406\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u8bad\u7ec3\u6570\u636e\u4e2d\u4eba\u7c7b\u5185\u5728\u601d\u7ef4\u7f3a\u5931\u7684\u95ee\u9898", "motivation": "LLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff08\u5982\u6570\u5b66\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u4eba\u7c7b\u5728\u601d\u8003\u65f6\u4e0d\u4f1a\u8868\u8fbe\u5185\u5728\u601d\u7ef4\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6570\u636e\u4e2d\u7f3a\u5c11\u8fde\u63a5\u63a8\u7406\u6b65\u9aa4\u7684\u5173\u952einsights", "method": "\u63d0\u51faThinking Before You Speak (TBYS)\u6846\u67b6\uff0c\u5728\u8fde\u7eed\u63a8\u7406\u6b65\u9aa4\u95f4\u4e3b\u52a8\u751f\u6210insights\u6765\u56de\u987e\u72b6\u6001\u5e76\u542f\u52a8\u4e0b\u4e00\u6b65\u63a8\u7406\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\u6536\u96c6\u548c\u8fc7\u6ee4\u4e0a\u4e0b\u6587\u793a\u4f8b", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86TBYS\u7684\u6709\u6548\u6027", "conclusion": "\u901a\u8fc7\u4e3b\u52a8\u751f\u6210insights\u6765\u6307\u5bfc\u63a8\u7406\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347LLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4e14\u51cf\u5c11\u4e86\u4eba\u5de5\u6807\u6ce8\u548c\u5fae\u8c03\u7684\u5f00\u9500"}}
{"id": "2508.18651", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18651", "abs": "https://arxiv.org/abs/2508.18651", "authors": ["Chenxu Yang", "Qingyi Si", "Zheng Lin"], "title": "Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models", "comment": null, "summary": "Grounding responses in external knowledge represents an effective strategy\nfor mitigating hallucinations in Large Language Models (LLMs). However, current\nLLMs struggle to seamlessly integrate knowledge while simultaneously\nmaintaining faithfulness (or fidelity) and expressiveness, capabilities that\nhumans naturally possess. This limitation results in outputs that either lack\nsupport from external knowledge, thereby compromising faithfulness, or appear\noverly verbose and unnatural, thus sacrificing expressiveness. In this work, to\nbreak the trade-off between faithfulness and expressiveness, we propose\nCollaborative Decoding (CoDe), a novel approach that dynamically integrates\noutput probabilities generated with and without external knowledge. This\nintegration is guided by distribution divergence and model confidence, enabling\nthe selective activation of relevant and reliable expressions from the model's\ninternal parameters. Furthermore, we introduce a knowledge-aware reranking\nmechanism that prevents over-reliance on prior parametric knowledge while\nensuring proper utilization of provided external information. Through\ncomprehensive experiments, our plug-and-play CoDe framework demonstrates\nsuperior performance in enhancing faithfulness without compromising\nexpressiveness across diverse LLMs and evaluation metrics, validating both its\neffectiveness and generalizability.", "AI": {"tldr": "\u901a\u8fc7\u52a8\u6001\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u548c\u6a21\u578b\u5185\u90e8\u53c2\u6570\u7684\u8f93\u51fa\u6982\u7387\uff0c\u7a81\u7834\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fe0\u5b9e\u6027\u548c\u8868\u8fbe\u6027\u4e4b\u95f4\u7684\u4ea4\u6613\uff0c\u63d0\u5347\u4e86\u56de\u7b54\u7684\u53ef\u9760\u6027\u548c\u81ea\u7136\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u65f6\u65e0\u6cd5\u540c\u65f6\u4fdd\u6301\u5fe0\u5b9e\u6027\u548c\u8868\u8fbe\u6027\uff0c\u5bfc\u81f4\u8f93\u51fa\u6216\u7f3a\u4e4f\u77e5\u8bc6\u652f\u6301\u6216\u8fc7\u4e8e\u7e41\u7410\u4e0d\u81ea\u7136\u3002", "method": "\u63d0\u51fa\u534f\u540c\u89e3\u7801(CoDe)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5e03\u6563\u5ea6\u548c\u6a21\u578b\u4fe1\u5fc3\u6307\u5bfc\u52a8\u6001\u6574\u5408\u6709\u65e0\u5916\u90e8\u77e5\u8bc6\u7684\u8f93\u51fa\u6982\u7387\uff0c\u5e76\u4f7f\u7528\u77e5\u8bc6\u611f\u77e5\u91cd\u6392\u673a\u5236\u9632\u6b62\u8fc7\u5ea6\u4f9d\u8d56\u53c2\u6570\u77e5\u8bc6\u3002", "result": "\u5b8c\u6574\u5b9e\u9a8c\u8868\u660eCoDe\u6846\u67b6\u5728\u5404\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8bc4\u4f30\u6307\u6807\u4e0a\u90fd\u663e\u793a\u51fa\u4f18\u5f02\u6027\u80fd\uff0c\u5728\u63d0\u5347\u5fe0\u5b9e\u6027\u7684\u540c\u65f6\u4e0d\u640d\u5bb3\u8868\u8fbe\u6027\u3002", "conclusion": "CoDe\u662f\u4e00\u79cd\u63d2\u4ef6\u5f0f\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u7a81\u7834\u4e86\u5fe0\u5b9e\u6027\u4e0e\u8868\u8fbe\u6027\u7684\u4ea4\u6613\u5173\u7cfb\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u666e\u9002\u6027\u3002"}}
{"id": "2508.18687", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18687", "abs": "https://arxiv.org/abs/2508.18687", "authors": ["Songtao Jiang", "Yuxi Chen", "Sibo Song", "Yan Zhang", "Yeying Jin", "Yang Feng", "Jian Wu", "Zuozhu Liu"], "title": "Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning", "comment": null, "summary": "In high-stakes medical applications, consistent answering across diverse\nquestion phrasings is essential for reliable diagnosis. However, we reveal that\ncurrent Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility\nin Medical Visual Question Answering, as their answers fluctuate significantly\nwhen faced with semantically equivalent rephrasings of medical questions. We\nattribute this to two limitations: (1) insufficient alignment of medical\nconcepts, leading to divergent reasoning patterns, and (2) hidden biases in\ntraining data that prioritize syntactic shortcuts over semantic understanding.\nTo address these challenges, we construct RoMed, a dataset built upon original\nVQA datasets containing 144k questions with variations spanning word-level,\nsentence-level, and semantic-level perturbations. When evaluating\nstate-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarming\nperformance drops (e.g., a 40\\% decline in Recall) compared to original VQA\nbenchmarks, exposing critical robustness gaps. To bridge this gap, we propose\nConsistency and Contrastive Learning (CCL), which integrates two key\ncomponents: (1) knowledge-anchored consistency learning, aligning Med-VLMs with\nmedical knowledge rather than shallow feature patterns, and (2) bias-aware\ncontrastive learning, mitigating data-specific priors through discriminative\nrepresentation refinement. CCL achieves SOTA performance on three popular VQA\nbenchmarks and notably improves answer consistency by 50\\% on the challenging\nRoMed test set, demonstrating significantly enhanced robustness. Code will be\nreleased.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u5f53\u524d\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u4e2d\u5b58\u5728\u7b54\u6848\u4e0d\u4e00\u81f4\u7684\u8106\u5f31\u6027\u95ee\u9898\uff0c\u6784\u5efa\u4e86RoMed\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u63d0\u51fa\u4e86CCL\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u533b\u7597\u5e94\u7528\u4e2d\uff0c\u6a21\u578b\u9700\u8981\u5bf9\u8bed\u4e49\u76f8\u540c\u4f46\u8868\u8ff0\u4e0d\u540c\u7684\u533b\u5b66\u95ee\u9898\u7ed9\u51fa\u4e00\u81f4\u7684\u56de\u7b54\uff0c\u4f46\u73b0\u6709Med-VLMs\u5728\u9762\u5bf9\u95ee\u9898\u91cd\u8ff0\u65f6\u8868\u73b0\u51fa\u4ee4\u4eba\u62c5\u5fe7\u7684\u8106\u5f31\u6027\uff0c\u7b54\u6848\u6ce2\u52a8\u663e\u8457\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b14.4\u4e07\u4e2a\u95ee\u9898\u7684RoMed\u6570\u636e\u96c6\uff08\u5305\u542b\u8bcd\u6c47\u7ea7\u3001\u53e5\u5b50\u7ea7\u548c\u8bed\u4e49\u7ea7\u6270\u52a8\uff09\uff0c\u63d0\u51fa\u4e86CCL\u65b9\u6cd5\uff0c\u5305\u542b\u77e5\u8bc6\u951a\u5b9a\u4e00\u81f4\u6027\u5b66\u4e60\u548c\u504f\u5dee\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\u3002", "result": "\u5728RoMed\u4e0a\u8bc4\u4f30SOTA\u6a21\u578b\uff08\u5982LLaVA-Med\uff09\u53d1\u73b0\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff08\u5982\u53ec\u56de\u7387\u4e0b\u964d40%\uff09\uff0cCCL\u65b9\u6cd5\u5728\u4e09\u4e2a\u6d41\u884cVQA\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5e76\u5728RoMed\u6d4b\u8bd5\u96c6\u4e0a\u5c06\u7b54\u6848\u4e00\u81f4\u6027\u63d0\u9ad8\u4e8650%\u3002", "conclusion": "CCL\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u533b\u5b66\u77e5\u8bc6\u548c\u5bf9\u9f50\u8868\u793a\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86Med-VLMs\u7684\u8106\u5f31\u6027\u95ee\u9898\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.18701", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18701", "abs": "https://arxiv.org/abs/2508.18701", "authors": ["Yanfan Du", "Jun Zhang", "Bin Wang", "Jin Qiu", "Lu Huang", "Yuan Ge", "Xiaoqian Liu", "Tong Xiao", "Jingbo Zhu"], "title": "Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System", "comment": "9 pages, 4 figures, 5 tables", "summary": "Recent advances in speech large language models (SLMs) have improved speech\nrecognition and translation in general domains, but accurately generating\ndomain-specific terms or neologisms remains challenging. To address this, we\npropose Attention2Probability: attention-driven terminology probability\nestimation for robust speech-to-text system, which is lightweight, flexible,\nand accurate. Attention2Probability converts cross-attention weights between\nspeech and terminology into presence probabilities, and it further employs\ncurriculum learning to enhance retrieval accuracy. Furthermore, to tackle the\nlack of data for speech-to-text tasks with terminology intervention, we create\nand release a new speech dataset with terminology to support future research in\nthis area. Experimental results show that Attention2Probability significantly\noutperforms the VectorDB method on our test set. Specifically, its maximum\nrecall rates reach 92.57% for Chinese and 86.83% for English. This high recall\nis achieved with a latency of only 8.71ms per query. Intervening in SLMs'\nrecognition and translation tasks using Attention2Probability-retrieved terms\nimproves terminology accuracy by 6-17%, while revealing that the current\nutilization of terminology by SLMs has limitations.", "AI": {"tldr": "\u63d0\u51fa\u4e86Attention2Probability\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u6743\u91cd\u8f6c\u6362\u548c\u8bfe\u7a0b\u5b66\u4e60\u63d0\u5347\u8bed\u97f3\u5927\u6a21\u578b\u4e2d\u4e13\u4e1a\u672f\u8bed\u7684\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u5728\u4e2d\u6587\u548c\u82f1\u6587\u6d4b\u8bd5\u4e2d\u5206\u522b\u8fbe\u523092.57%\u548c86.83%\u7684\u53ec\u56de\u7387\uff0c\u5ef6\u8fdf\u4ec58.71ms\u3002", "motivation": "\u5f53\u524d\u8bed\u97f3\u5927\u6a21\u578b\u5728\u901a\u7528\u9886\u57df\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5904\u7406\u9886\u57df\u7279\u5b9a\u672f\u8bed\u548c\u65b0\u8bcd\u65f6\u51c6\u786e\u7387\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u672f\u8bed\u8bc6\u522b\u80fd\u529b\u3002", "method": "\u63d0\u51faAttention2Probability\u65b9\u6cd5\uff0c\u5c06\u8bed\u97f3\u4e0e\u672f\u8bed\u4e4b\u95f4\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u6743\u91cd\u8f6c\u6362\u4e3a\u5b58\u5728\u6982\u7387\uff0c\u5e76\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u63d0\u9ad8\u68c0\u7d22\u51c6\u786e\u6027\u3002\u540c\u65f6\u6784\u5efa\u4e86\u5305\u542b\u672f\u8bed\u7684\u8bed\u97f3\u6570\u636e\u96c6\u3002", "result": "\u5728\u6d4b\u8bd5\u96c6\u4e0a\u663e\u8457\u4f18\u4e8eVectorDB\u65b9\u6cd5\uff0c\u4e2d\u6587\u6700\u5927\u53ec\u56de\u738792.57%\uff0c\u82f1\u658786.83%\uff0c\u67e5\u8be2\u5ef6\u8fdf\u4ec58.71ms\u3002\u672f\u8bed\u5e72\u9884\u4f7f\u8bc6\u522b\u548c\u7ffb\u8bd1\u4efb\u52a1\u51c6\u786e\u7387\u63d0\u53476-17%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8f7b\u91cf\u3001\u7075\u6d3b\u4e14\u51c6\u786e\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8bed\u97f3\u5927\u6a21\u578b\u7684\u672f\u8bed\u5904\u7406\u80fd\u529b\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u672f\u8bed\u5229\u7528\u65b9\u9762\u4ecd\u5b58\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2508.18709", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18709", "abs": "https://arxiv.org/abs/2508.18709", "authors": ["Duy Le", "Kent Ziti", "Evan Girard-Sun", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "title": "Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs", "comment": null, "summary": "Multilingual riddle generation challenges large language models (LLMs) to\nbalance cultural fluency with creative abstraction. Standard prompting\nstrategies -- zero-shot, few-shot, chain-of-thought -- tend to reuse memorized\nriddles or perform shallow paraphrasing. We introduce Adaptive Originality\nFiltering (AOF), a prompting framework that filters redundant generations using\ncosine-based similarity rejection, while enforcing lexical novelty and\ncross-lingual fidelity. Evaluated across three LLMs and four language pairs,\nAOF-enhanced GPT-4o achieves \\texttt{0.177} Self-BLEU and \\texttt{0.915}\nDistinct-2 in Japanese, signaling improved lexical diversity and reduced\nredundancy compared to other prompting methods and language pairs. Our findings\nshow that semantic rejection can guide culturally grounded, creative generation\nwithout task-specific fine-tuning.", "AI": {"tldr": "\u8be5\u6587\u7ae0\u63d0\u51fa\u4e86\u9002\u5e94\u6027\u539f\u521b\u6027\u7b5b\u9009(AOF)\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u6027\u62d4\u6d88\u548c\u8bcd\u6cd5\u65b0\u9896\u6027\u7ea6\u675f\uff0c\u63d0\u5347\u591a\u8bed\u8a00\u8c1c\u8bed\u751f\u6210\u7684\u521b\u9020\u6027\u548c\u6587\u5316\u9002\u5e94\u6027\u3002", "motivation": "\u591a\u8bed\u8a00\u8c1c\u8bed\u751f\u6210\u9762\u4e34\u7684\u6311\u6218\u662f\u9700\u8981\u5728\u6587\u5316\u6d41\u5229\u6027\u548c\u521b\u9020\u6027\u62bd\u8c61\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u6807\u51c6\u63d0\u793a\u7b56\u7565\u5bb9\u6613\u91cd\u590d\u8bb0\u5fc6\u7684\u8c1c\u8bed\u6216\u8fdb\u884c\u6d45\u5c42\u7684\u53cd\u4e49\u8f6c\u6362\u3002", "method": "\u9002\u5e94\u6027\u539f\u521b\u6027\u7b5b\u9009(AOF)\u6846\u67b6\uff0c\u4f7f\u7528\u4f59\u5f26\u76f8\u4f3c\u6027\u62d4\u6d88\u6765\u7b5b\u9009\u91cd\u590d\u751f\u6210\u5185\u5bb9\uff0c\u540c\u65f6\u5f3a\u5316\u8bcd\u6cd5\u65b0\u9896\u6027\u548c\u8de8\u8bed\u8a00\u4fe1\u5ea6\u8981\u6c42\u3002", "result": "\u5728\u4e09\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u548c\u56db\u7ec4\u8bed\u8a00\u5bf9\u4e2d\u8bc4\u4f30\uff0cAOF\u589e\u5f3a\u7684GPT-4o\u5728\u65e5\u8bed\u4e2d\u8fbe\u5230\u4e860.177\u7684Self-BLEU\u548c0.915\u7684Distinct-2\u6307\u6807\uff0c\u663e\u793a\u51fa\u8bcd\u6cd5\u591a\u6837\u6027\u63d0\u5347\u548c\u91cd\u590d\u6027\u964d\u4f4e\u3002", "conclusion": "\u8bed\u4e49\u62d4\u6d88\u53ef\u4ee5\u5728\u4e0d\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u6307\u5bfc\u57fa\u4e8e\u6587\u5316\u6839\u57fa\u7684\u521b\u9020\u6027\u751f\u6210\u3002"}}
{"id": "2508.18715", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18715", "abs": "https://arxiv.org/abs/2508.18715", "authors": ["Angela Yifei Yuan", "Haoyi Li", "Soyeon Caren Han", "Christopher Leckie"], "title": "EMMM, Explain Me My Model! Explainable Machine Generated Text Detection in Dialogues", "comment": "15 pages", "summary": "The rapid adoption of large language models (LLMs) in customer service\nintroduces new risks, as malicious actors can exploit them to conduct\nlarge-scale user impersonation through machine-generated text (MGT). Current\nMGT detection methods often struggle in online conversational settings,\nreducing the reliability and interpretability essential for trustworthy AI\ndeployment. In customer service scenarios where operators are typically\nnon-expert users, explanation become crucial for trustworthy MGT detection. In\nthis paper, we propose EMMM, an explanation-then-detection framework that\nbalances latency, accuracy, and non-expert-oriented interpretability.\nExperimental results demonstrate that EMMM provides explanations accessible to\nnon-expert users, with 70\\% of human evaluators preferring its outputs, while\nachieving competitive accuracy compared to state-of-the-art models and\nmaintaining low latency, generating outputs within 1 second. Our code and\ndataset are open-sourced at\nhttps://github.com/AngieYYF/EMMM-explainable-chatbot-detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86EMMM\u6846\u67b6\uff0c\u901a\u8fc7\u5148\u89e3\u91ca\u540e\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u5728\u5ba2\u6237\u670d\u52a1\u573a\u666f\u4e2d\u5e73\u8861\u5ef6\u8fdf\u3001\u51c6\u786e\u6027\u548c\u975e\u4e13\u5bb6\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u6548\u68c0\u6d4b\u673a\u5668\u751f\u6210\u6587\u672c\u7684\u5192\u5145\u884c\u4e3a\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5ba2\u6237\u670d\u52a1\u4e2d\u7684\u5feb\u901f\u5e94\u7528\u5e26\u6765\u4e86\u65b0\u7684\u98ce\u9669\uff0c\u6076\u610f\u884c\u4e3a\u8005\u53ef\u80fd\u5229\u7528\u673a\u5668\u751f\u6210\u6587\u672c\u8fdb\u884c\u5927\u89c4\u6a21\u7528\u6237\u5192\u5145\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5728\u5728\u7ebf\u5bf9\u8bdd\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u7279\u522b\u662f\u5728\u975e\u4e13\u5bb6\u7528\u6237\u64cd\u4f5c\u7684\u5ba2\u6237\u670d\u52a1\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51faEMMM\u6846\u67b6\uff0c\u91c7\u7528\u5148\u89e3\u91ca\u540e\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u4e3a\u975e\u4e13\u5bb6\u7528\u6237\u63d0\u4f9b\u53ef\u7406\u89e3\u7684\u89e3\u91ca\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cEMMM\u4e3a\u975e\u4e13\u5bb6\u7528\u6237\u63d0\u4f9b\u4e86\u53ef\u8bbf\u95ee\u7684\u89e3\u91ca\uff0c70%\u7684\u4eba\u7c7b\u8bc4\u4f30\u8005\u504f\u597d\u5176\u8f93\u51fa\uff0c\u5728\u4fdd\u63011\u79d2\u5185\u751f\u6210\u8f93\u51fa\u7684\u4f4e\u5ef6\u8fdf\u7684\u540c\u65f6\uff0c\u8fbe\u5230\u4e86\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002", "conclusion": "EMMM\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5ba2\u6237\u670d\u52a1\u573a\u666f\u4e2d\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u7684\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u4e3a\u975e\u4e13\u5bb6\u7528\u6237\u63d0\u4f9b\u4e86\u53ef\u4fe1\u8d56\u7684AI\u90e8\u7f72\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.18739", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18739", "abs": "https://arxiv.org/abs/2508.18739", "authors": ["Chang Wang", "Siyu Yan", "Depeng Yuan", "Yuqi Chen", "Yanhua Huang", "Yuanhang Zheng", "Shuhao Li", "Yinqi Zhang", "Kedi Chen", "Mingrui Zhu", "Ruiwen Xu"], "title": "Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models", "comment": null, "summary": "The generation of ad headlines plays a vital role in modern advertising,\nwhere both quality and diversity are essential to engage a broad range of\naudience segments. Current approaches primarily optimize language models for\nheadline quality or click-through rates (CTR), often overlooking the need for\ndiversity and resulting in homogeneous outputs. To address this limitation, we\npropose DIVER, a novel framework based on large language models (LLMs) that are\njointly optimized for both diversity and quality. We first design a semantic-\nand stylistic-aware data generation pipeline that automatically produces\nhigh-quality training pairs with ad content and multiple diverse headlines. To\nachieve the goal of generating high-quality and diversified ad headlines within\na single forward pass, we propose a multi-stage multi-objective optimization\nframework with supervised fine-tuning (SFT) and reinforcement learning (RL).\nExperiments on real-world industrial datasets demonstrate that DIVER\neffectively balances quality and diversity. Deployed on a large-scale\ncontent-sharing platform serving hundreds of millions of users, our framework\nimproves advertiser value (ADVV) and CTR by 4.0% and 1.4%.", "AI": {"tldr": "DIVER\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u544a\u6807\u9898\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u591a\u76ee\u6807\u4f18\u5316\u540c\u65f6\u63d0\u5347\u6807\u9898\u8d28\u91cf\u548c\u591a\u6837\u6027\uff0c\u5728\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6548\u679c\uff0c\u90e8\u7f72\u540eADVV\u548cCTR\u5206\u522b\u63d0\u53474.0%\u548c1.4%\u3002", "motivation": "\u5f53\u524d\u5e7f\u544a\u6807\u9898\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u4f18\u5316\u8d28\u91cf\u6216\u70b9\u51fb\u7387\uff0c\u5ffd\u89c6\u4e86\u591a\u6837\u6027\u9700\u6c42\uff0c\u5bfc\u81f4\u8f93\u51fa\u540c\u8d28\u5316\u4e25\u91cd\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4e0d\u540c\u53d7\u4f17\u7fa4\u4f53\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u548c\u98ce\u683c\u611f\u77e5\u7684\u6570\u636e\u751f\u6210\u7ba1\u9053\u81ea\u52a8\u521b\u5efa\u9ad8\u8d28\u91cf\u8bad\u7ec3\u5bf9\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\uff0c\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u6837\u5316\u6807\u9898\u3002", "result": "\u5728\u771f\u5b9e\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86DIVER\u80fd\u6709\u6548\u5e73\u8861\u8d28\u91cf\u548c\u591a\u6837\u6027\uff0c\u90e8\u7f72\u5230\u4ebf\u7ea7\u7528\u6237\u7684\u5185\u5bb9\u5206\u4eab\u5e73\u53f0\u540e\uff0c\u5e7f\u544a\u4e3b\u4ef7\u503c(ADVV)\u63d0\u53474.0%\uff0c\u70b9\u51fb\u7387(CTR)\u63d0\u53471.4%\u3002", "conclusion": "DIVER\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5e7f\u544a\u6807\u9898\u751f\u6210\u4e2d\u8d28\u91cf\u4e0e\u591a\u6837\u6027\u7684\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5b9e\u73b0\u4e86\u5b9e\u9645\u4e1a\u52a1\u6307\u6807\u7684\u663e\u8457\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u591a\u76ee\u6807\u4f18\u5316\u5728\u5e7f\u544a\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.18740", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18740", "abs": "https://arxiv.org/abs/2508.18740", "authors": ["Qiao Liang", "Ying Shen", "Tiantian Chen", "Lin Zhang"], "title": "M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations", "comment": "16 pages, 8 figures. Accepted to Findings of ACL 2025", "summary": "Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has\nrecently gained significant attention in social media analysis, aiming to\nextract emotion utterances, cause utterances, and emotion categories\nsimultaneously. However, the scarcity of related datasets, with only one\npublished dataset featuring highly uniform dialogue scenarios, hinders model\ndevelopment in this field. To address this, we introduce MECAD, the first\nmultimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56\nTV series spanning a wide range of dialogue contexts. In addition, existing\nMECTEC methods fail to explicitly model emotional and causal contexts and\nneglect the fusion of semantic information at different levels, leading to\nperformance degradation. In this paper, we propose M3HG, a novel model that\nexplicitly captures emotional and causal contexts and effectively fuses\ncontextual information at both inter- and intra-utterance levels via a\nmultimodal heterogeneous graph. Extensive experiments demonstrate the\neffectiveness of M3HG compared with existing state-of-the-art methods. The\ncodes and dataset are available at https://github.com/redifinition/M3HG.", "AI": {"tldr": "\u63d0\u51fa\u4e86M3HG\u6a21\u578b\u548cMECAD\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u5bf9\u8bdd\u4e2d\u7684\u60c5\u611f\u539f\u56e0\u4e09\u5143\u7ec4\u63d0\u53d6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5f02\u6784\u56fe\u663e\u5f0f\u5efa\u6a21\u60c5\u611f\u548c\u56e0\u679c\u4e0a\u4e0b\u6587\uff0c\u5728\u8de8\u8bdd\u8bed\u548c\u8bdd\u8bed\u5185\u5c42\u9762\u878d\u5408\u8bed\u4e49\u4fe1\u606f", "motivation": "\u73b0\u6709MECTEC\u9886\u57df\u6570\u636e\u96c6\u7a00\u7f3a\u4e14\u5bf9\u8bdd\u573a\u666f\u5355\u4e00\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u663e\u5f0f\u5efa\u6a21\u60c5\u611f\u548c\u56e0\u679c\u4e0a\u4e0b\u6587\uff0c\u4e14\u5ffd\u7565\u4e86\u4e0d\u540c\u5c42\u6b21\u8bed\u4e49\u4fe1\u606f\u7684\u878d\u5408\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d", "method": "\u63d0\u51faM3HG\u6a21\u578b\uff0c\u4f7f\u7528\u591a\u6a21\u6001\u5f02\u6784\u56fe\u663e\u5f0f\u6355\u83b7\u60c5\u611f\u548c\u56e0\u679c\u4e0a\u4e0b\u6587\uff0c\u5728\u8de8\u8bdd\u8bed\u548c\u8bdd\u8bed\u5185\u4e24\u4e2a\u5c42\u9762\u6709\u6548\u878d\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eM3HG\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6548\u679c", "conclusion": "M3HG\u6a21\u578b\u548c\u591a\u6a21\u6001\u591a\u573a\u666f\u7684MECAD\u6570\u636e\u96c6\u6709\u6548\u89e3\u51b3\u4e86MECTEC\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u591a\u6a21\u6001\u5bf9\u8bdd\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.18748", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18748", "abs": "https://arxiv.org/abs/2508.18748", "authors": ["Byeongjeong Kim", "Jeonghyun Park", "Joonho Yang", "Hwanhee Lee"], "title": "Chronological Passage Assembling in RAG framework for Temporal Question Answering", "comment": "7 pages, 3 figures", "summary": "Long-context question answering over narrative tasks is challenging because\ncorrect answers often hinge on reconstructing a coherent timeline of events\nwhile preserving contextual flow in a limited context window.\nRetrieval-augmented generation (RAG) indexing methods aim to address this\nchallenge by selectively retrieving only necessary document segments. However,\nnarrative texts possess unique characteristics that limit the effectiveness of\nthese existing approaches. Specifically, understanding narrative texts requires\nmore than isolated segments, as the broader context and sequential\nrelationships between segments are crucial for comprehension. To address these\nlimitations, we propose ChronoRAG, a novel RAG framework specialized for\nnarrative texts. This approach focuses on two essential aspects: refining\ndispersed document information into coherent and structured passages, and\npreserving narrative flow by explicitly capturing and maintaining the temporal\norder among retrieved passages. We empirically demonstrate the effectiveness of\nChronoRAG through experiments on the NarrativeQA dataset, showing substantial\nimprovements in tasks requiring both factual identification and comprehension\nof complex sequential relationships, underscoring that reasoning over temporal\norder is crucial in resolving narrative QA.", "AI": {"tldr": "ChronoRAG\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u53d9\u4e8b\u6587\u672c\u7684\u65b0\u578bRAG\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u6784\u8fde\u8d2f\u6bb5\u843d\u548c\u4fdd\u6301\u65f6\u95f4\u987a\u5e8f\u6765\u6539\u8fdb\u53d9\u4e8b\u95ee\u7b54\u4efb\u52a1", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u5728\u5904\u7406\u53d9\u4e8b\u6587\u672c\u65f6\u6548\u679c\u6709\u9650\uff0c\u56e0\u4e3a\u53d9\u4e8b\u7406\u89e3\u9700\u8981\u66f4\u5e7f\u6cdb\u7684\u4e0a\u4e0b\u6587\u548c\u6bb5\u843d\u95f4\u7684\u65f6\u5e8f\u5173\u7cfb\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5b64\u7acb\u7247\u6bb5", "method": "\u63d0\u51faChronoRAG\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u4e24\u4e2a\u5173\u952e\u65b9\u9762\uff1a\u5c06\u5206\u6563\u7684\u6587\u6863\u4fe1\u606f\u91cd\u6784\u4e3a\u8fde\u8d2f\u7684\u7ed3\u6784\u5316\u6bb5\u843d\uff1b\u901a\u8fc7\u663e\u5f0f\u6355\u83b7\u548c\u7ef4\u62a4\u68c0\u7d22\u6bb5\u843d\u7684\u65f6\u95f4\u987a\u5e8f\u6765\u4fdd\u6301\u53d9\u4e8b\u6d41", "result": "\u5728NarrativeQA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cChronoRAG\u5728\u9700\u8981\u4e8b\u5b9e\u8bc6\u522b\u548c\u590d\u6742\u65f6\u5e8f\u5173\u7cfb\u7406\u89e3\u7684\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb", "conclusion": "\u65f6\u5e8f\u63a8\u7406\u5bf9\u4e8e\u89e3\u51b3\u53d9\u4e8b\u95ee\u7b54\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0cChronoRAG\u901a\u8fc7\u4e13\u95e8\u5904\u7406\u53d9\u4e8b\u6587\u672c\u7684\u65f6\u5e8f\u7279\u6027\u6709\u6548\u63d0\u5347\u4e86\u6027\u80fd"}}
{"id": "2508.18773", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18773", "abs": "https://arxiv.org/abs/2508.18773", "authors": ["Qianyu He", "Siyu Yuan", "Xuefeng Li", "Mingxuan Wang", "Jiangjie Chen"], "title": "ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models", "comment": null, "summary": "Large language models (LLMs) with chain-of-thought reasoning have\ndemonstrated remarkable problem-solving capabilities, but controlling their\ncomputational effort remains a significant challenge for practical deployment.\nRecent proprietary systems like OpenAI's gpt-oss series have introduced\ndiscrete operational modes for intuitive reasoning control, but the open-source\ncommunity has largely failed to achieve such capabilities. In this paper, we\nintroduce ThinkDial, the first open-recipe end-to-end framework that\nsuccessfully implements gpt-oss-style controllable reasoning through discrete\noperational modes. Our system enables seamless switching between three distinct\nreasoning regimes: High mode (full reasoning capability), Medium mode (50\npercent token reduction with <10 percent performance degradation), and Low mode\n(75 percent token reduction with <15 percent performance degradation). We\nachieve this through an end-to-end training paradigm that integrates\nbudget-mode control throughout the entire pipeline: budget-mode supervised\nfine-tuning that embeds controllable reasoning capabilities directly into the\nlearning process, and two-phase budget-aware reinforcement learning with\nadaptive reward shaping. Extensive experiments demonstrate that ThinkDial\nachieves target compression-performance trade-offs with clear response length\nreductions while maintaining performance thresholds. The framework also\nexhibits strong generalization capabilities on out-of-distribution tasks.", "AI": {"tldr": "ThinkDial\u662f\u9996\u4e2a\u5f00\u6e90\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u6563\u64cd\u4f5c\u6a21\u5f0f\u5b9e\u73b0GPT-style\u53ef\u63a7\u63a8\u7406\uff0c\u63d0\u4f9b\u9ad8\u4e2d\u4f4e\u4e09\u79cd\u63a8\u7406\u6a21\u5f0f\uff0c\u5728\u663e\u8457\u51cf\u5c11\u8ba1\u7b97token\u7684\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u9608\u503c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5177\u5907\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u63a7\u5236\u4ecd\u662f\u5b9e\u9645\u90e8\u7f72\u7684\u91cd\u5927\u6311\u6218\u3002\u5f00\u6e90\u793e\u533a\u7f3a\u4e4f\u50cfGPT\u7cfb\u5217\u90a3\u6837\u7684\u53ef\u63a7\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7075\u6d3b\u63a7\u5236\u8ba1\u7b97\u5f00\u9500\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u8bad\u7ec3\u8303\u5f0f\uff1a1\uff09\u9884\u7b97\u6a21\u5f0f\u76d1\u7763\u5fae\u8c03\uff0c\u5c06\u53ef\u63a7\u63a8\u7406\u80fd\u529b\u76f4\u63a5\u5d4c\u5165\u5b66\u4e60\u8fc7\u7a0b\uff1b2\uff09\u4e24\u9636\u6bb5\u9884\u7b97\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u5956\u52b1\u5851\u5f62\u6280\u672f\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e09\u79cd\u63a8\u7406\u6a21\u5f0f\uff1a\u9ad8\u6a21\u5f0f\uff08\u5b8c\u6574\u80fd\u529b\uff09\u3001\u4e2d\u6a21\u5f0f\uff08\u51cf\u5c1150%token\uff0c\u6027\u80fd\u4e0b\u964d<10%\uff09\u3001\u4f4e\u6a21\u5f0f\uff08\u51cf\u5c1175%token\uff0c\u6027\u80fd\u4e0b\u964d<15%\uff09\u3002\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ThinkDial\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u8ba1\u7b97\u6210\u672c\u63a7\u5236\u95ee\u9898\uff0c\u4e3a\u5f00\u6e90\u793e\u533a\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u53ef\u63a7\u63a8\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2508.18780", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18780", "abs": "https://arxiv.org/abs/2508.18780", "authors": ["Yilin Li", "Xunjian Yin", "Yilin Chen", "Xiaojun Wan"], "title": "Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction", "comment": "Code will be released upon publication", "summary": "Grammatical error correction is a significant task in NLP. Traditional\nmethods based on encoder-decoder models have achieved certain success, but the\napplication of LLMs in this field is still underexplored. Current research\npredominantly relies on supervised fine-tuning to train LLMs to directly\ngenerate the corrected sentence, which limits the model's powerful reasoning\nability. To address this limitation, we propose a novel framework based on\nRule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL\nframework achieves \\textbf{state-of-the-art }performance, with a notable\nincrease in \\textbf{recall}. This result clearly highlights the advantages of\nusing RL to steer LLMs, offering a more controllable and reliable paradigm for\nfuture development in GEC.", "AI": {"tldr": "\u57fa\u4e8e\u89c4\u5219\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u6846\u67b6\u5728\u4e2d\u6587\u8bed\u6cd5\u9519\u8bef\u7f16\u6b63\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u53d0\u56de\u7387", "motivation": "\u4f20\u7edf\u7684\u76f4\u63a5\u751f\u6210\u6a21\u578b\u9650\u5236\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981\u66f4\u597d\u5730\u5229\u7528RL\u6765\u5bfc\u5411LLMs\u4ee5\u83b7\u5f97\u66f4\u53ef\u63a7\u548c\u53ef\u9760\u7684\u65b9\u6848", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u5f3a\u5316\u5b66\u4e60\uff08Rule-Based RL\uff09\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u65b9\u5f0f\u6765\u5bfc\u5411\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bed\u6cd5\u9519\u8bef\u7f16\u6b63", "result": "\u5728\u4e2d\u6587\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86state-of-the-art\u6027\u80fd\uff0c\u5c24\u5176\u662f\u53ef\u53d0\u56de\u7387\uff08recall\uff09\u6709\u663e\u8457\u63d0\u5347", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u53d1\u6325\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u8bed\u6cd5\u9519\u8bef\u7f16\u6b63\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u53ef\u63a7\u548c\u53ef\u9760\u7684\u53d1\u5c55\u65b9\u5411"}}
{"id": "2508.18783", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18783", "abs": "https://arxiv.org/abs/2508.18783", "authors": ["Igor Shalyminov", "Hang Su", "Jake Vincent", "Siffi Singh", "Jason Cai", "James Gung", "Raphael Shu", "Saab Mansour"], "title": "Controllable Conversational Theme Detection Track at DSTC 12", "comment": "DSTC12@SigDial2025; data and code available at\n  https://github.com/amazon-science/dstc12-controllable-conversational-theme-detection", "summary": "Conversational analytics has been on the forefront of transformation driven\nby the advances in Speech and Natural Language Processing techniques. Rapid\nadoption of Large Language Models (LLMs) in the analytics field has taken the\nproblems that can be automated to a new level of complexity and scale. In this\npaper, we introduce Theme Detection as a critical task in conversational\nanalytics, aimed at automatically identifying and categorizing topics within\nconversations. This process can significantly reduce the manual effort involved\nin analyzing expansive dialogs, particularly in domains like customer support\nor sales. Unlike traditional dialog intent detection, which often relies on a\nfixed set of intents for downstream system logic, themes are intended as a\ndirect, user-facing summary of the conversation's core inquiry. This\ndistinction allows for greater flexibility in theme surface forms and\nuser-specific customizations. We pose Controllable Conversational Theme\nDetection problem as a public competition track at Dialog System Technology\nChallenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of\ndialog utterances, with the distinctive aspect being controllability of the\nresulting theme clusters' granularity achieved via the provided user preference\ndata. We give an overview of the problem, the associated dataset and the\nevaluation metrics, both automatic and human. Finally, we discuss the\nparticipant teams' submissions and provide insights from those. The track\nmaterials (data and code) are openly available in the GitHub repository.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5bf9\u8bdd\u5206\u6790\u4e2d\u7684\u4e3b\u9898\u68c0\u6d4b\u4efb\u52a1\uff0c\u4f5c\u4e3aDSTC 12\u7684\u7ade\u8d5b\u8d5b\u9053\uff0c\u65e8\u5728\u901a\u8fc7\u53ef\u63a7\u805a\u7c7b\u65b9\u6cd5\u81ea\u52a8\u8bc6\u522b\u548c\u5206\u7c7b\u5bf9\u8bdd\u4e3b\u9898\uff0c\u51cf\u5c11\u4eba\u5de5\u5206\u6790\u5de5\u4f5c\u91cf\u3002", "motivation": "\u968f\u7740\u8bed\u97f3\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5bf9\u8bdd\u5206\u6790\u9886\u57df\u9700\u8981\u81ea\u52a8\u5316\u5904\u7406\u66f4\u590d\u6742\u548c\u89c4\u6a21\u66f4\u5927\u7684\u95ee\u9898\u3002\u4f20\u7edf\u5bf9\u8bdd\u610f\u56fe\u68c0\u6d4b\u4f9d\u8d56\u56fa\u5b9a\u610f\u56fe\u96c6\uff0c\u800c\u4e3b\u9898\u68c0\u6d4b\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u7528\u6237\u9762\u5411\u5bf9\u8bdd\u6458\u8981\u3002", "method": "\u63d0\u51fa\u53ef\u63a7\u5bf9\u8bdd\u4e3b\u9898\u68c0\u6d4b\u95ee\u9898\uff0c\u91c7\u7528\u8054\u5408\u805a\u7c7b\u548c\u4e3b\u9898\u6807\u6ce8\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7528\u6237\u504f\u597d\u6570\u636e\u63a7\u5236\u4e3b\u9898\u7c07\u7684\u7c92\u5ea6\u3002", "result": "\u5728DSTC 12\u7ade\u8d5b\u4e2d\u8bbe\u7acb\u4e86\u516c\u5f00\u8d5b\u9053\uff0c\u63d0\u4f9b\u4e86\u76f8\u5173\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff08\u81ea\u52a8\u548c\u4eba\u5de5\uff09\uff0c\u5e76\u5206\u6790\u4e86\u53c2\u8d5b\u56e2\u961f\u7684\u63d0\u4ea4\u7ed3\u679c\u3002", "conclusion": "\u4e3b\u9898\u68c0\u6d4b\u662f\u5bf9\u8bdd\u5206\u6790\u4e2d\u7684\u5173\u952e\u4efb\u52a1\uff0c\u53ef\u63a7\u805a\u7c7b\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6ee1\u8db3\u7528\u6237\u7279\u5b9a\u7684\u7c92\u5ea6\u9700\u6c42\uff0c\u76f8\u5173\u6750\u6599\u548c\u4ee3\u7801\u5df2\u5728GitHub\u5f00\u6e90\u3002"}}
{"id": "2508.18791", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18791", "abs": "https://arxiv.org/abs/2508.18791", "authors": ["Ziming Zhu", "Chenglong Wang", "Shunjie Xing", "Yifu Huo", "Fengning Tian", "Quan Du", "Di Yang", "Chunliang Zhang", "Tong Xiao", "Jingbo Zhu"], "title": "LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination", "comment": null, "summary": "Despite the remarkable progress of modern machine translation (MT) systems on\ngeneral-domain texts, translating structured LaTeX-formatted documents remains\na significant challenge. These documents typically interleave natural language\nwith domain-specific syntax, such as mathematical equations, tables, figures,\nand cross-references, all of which must be accurately preserved to maintain\nsemantic integrity and compilability. In this paper, we introduce LaTeXTrans, a\ncollaborative multi-agent system designed to address this challenge. LaTeXTrans\nensures format preservation, structural fidelity, and terminology consistency\nthrough six specialized agents: 1) a Parser that decomposes LaTeX into\ntranslation-friendly units via placeholder substitution and syntax filtering;\n2) a Translator, Validator, Summarizer, and Terminology Extractor that work\ncollaboratively to ensure context-aware, self-correcting, and\nterminology-consistent translations; 3) a Generator that reconstructs the\ntranslated content into well-structured LaTeX documents. Experimental results\ndemonstrate that LaTeXTrans can outperform mainstream MT systems in both\ntranslation accuracy and structural fidelity, offering an effective and\npractical solution for translating LaTeX-formatted documents.", "AI": {"tldr": "LaTeXTrans\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u4e13\u95e8\u7528\u4e8e\u7ffb\u8bd1LaTeX\u683c\u5f0f\u6587\u6863\uff0c\u901a\u8fc7\u516d\u4e2a\u4e13\u4e1a\u4ee3\u7406\u786e\u4fdd\u683c\u5f0f\u4fdd\u6301\u3001\u7ed3\u6784\u4fdd\u771f\u548c\u672f\u8bed\u4e00\u81f4\u6027\uff0c\u5728\u7ffb\u8bd1\u51c6\u786e\u6027\u548c\u7ed3\u6784\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u4e3b\u6d41\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u5728\u901a\u7528\u6587\u672c\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u7ffb\u8bd1\u7ed3\u6784\u5316LaTeX\u683c\u5f0f\u6587\u6863\u65f6\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u8fd9\u4e9b\u6587\u6863\u901a\u5e38\u4ea4\u7ec7\u81ea\u7136\u8bed\u8a00\u548c\u9886\u57df\u7279\u5b9a\u8bed\u6cd5\uff08\u5982\u6570\u5b66\u516c\u5f0f\u3001\u8868\u683c\u3001\u56fe\u8868\u548c\u4ea4\u53c9\u5f15\u7528\uff09\uff0c\u9700\u8981\u51c6\u786e\u4fdd\u6301\u4ee5\u7ef4\u62a4\u8bed\u4e49\u5b8c\u6574\u6027\u548c\u53ef\u7f16\u8bd1\u6027\u3002", "method": "LaTeXTrans\u91c7\u7528\u534f\u4f5c\u5f0f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5305\u542b\u516d\u4e2a\u4e13\u95e8\u4ee3\u7406\uff1a1)\u89e3\u6790\u5668\u901a\u8fc7\u5360\u4f4d\u7b26\u66ff\u6362\u548c\u8bed\u6cd5\u8fc7\u6ee4\u5c06LaTeX\u5206\u89e3\u4e3a\u7ffb\u8bd1\u53cb\u597d\u5355\u5143\uff1b2)\u7ffb\u8bd1\u5668\u3001\u9a8c\u8bc1\u5668\u3001\u603b\u7ed3\u5668\u548c\u672f\u8bed\u63d0\u53d6\u5668\u534f\u4f5c\u786e\u4fdd\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u81ea\u6211\u7ea0\u6b63\u548c\u672f\u8bed\u4e00\u81f4\u7684\u7ffb\u8bd1\uff1b3)\u751f\u6210\u5668\u5c06\u7ffb\u8bd1\u5185\u5bb9\u91cd\u6784\u4e3a\u7ed3\u6784\u826f\u597d\u7684LaTeX\u6587\u6863\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLaTeXTrans\u5728\u7ffb\u8bd1\u51c6\u786e\u6027\u548c\u7ed3\u6784\u4fdd\u771f\u5ea6\u65b9\u9762\u80fd\u591f\u8d85\u8d8a\u4e3b\u6d41\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u3002", "conclusion": "LaTeXTrans\u4e3a\u7ffb\u8bd1LaTeX\u683c\u5f0f\u6587\u6863\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u7ed3\u6784\u5316\u6587\u6863\u7ffb\u8bd1\u4e2d\u7684\u683c\u5f0f\u4fdd\u6301\u548c\u8bed\u4e49\u5b8c\u6574\u6027\u6311\u6218\u3002"}}
{"id": "2508.18819", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.18819", "abs": "https://arxiv.org/abs/2508.18819", "authors": ["Shubham Gupta", "Shraban Kumar Chatterjee", "Suman Kundu"], "title": "LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection", "comment": null, "summary": "The proliferation of misinformation in the digital age has led to significant\nsocietal challenges. Existing approaches often struggle with capturing\nlong-range dependencies, complex semantic relations, and the social dynamics\ninfluencing news dissemination. Furthermore, these methods require extensive\nlabelled datasets, making their deployment resource-intensive. In this study,\nwe propose a novel self-supervised misinformation detection framework that\nintegrates both complex semantic relations using Abstract Meaning\nRepresentation (AMR) and news propagation dynamics. We introduce an LLM-based\ngraph contrastive loss (LGCL) that utilizes negative anchor points generated by\na Large Language Model (LLM) to enhance feature separability in a zero-shot\nmanner. To incorporate social context, we employ a multi view graph masked\nautoencoder, which learns news propagation features from social context graph.\nBy combining these semantic and propagation-based features, our approach\neffectively differentiates between fake and real news in a self-supervised\nmanner. Extensive experiments demonstrate that our self-supervised framework\nachieves superior performance compared to other state-of-the-art methodologies,\neven with limited labelled datasets while improving generalizability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u5173\u7cfb\u548c\u4f20\u64ad\u52a8\u6001\u7684\u81ea\u76d1\u7763\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u6846\u67b6\uff0c\u4f7f\u7528LLM\u751f\u6210\u5bf9\u6bd4\u635f\u5931\u548c\u591a\u91cd\u56fe\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff0c\u5728\u96f6\u6837\u672c\u548c\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u5b9e\u73b0\u4f18\u5f02\u6027\u80fd", "motivation": "\u6570\u5b57\u65f6\u4ee3\u865a\u5047\u4fe1\u606f\u6cdb\u6ee5\u5e26\u6765\u91cd\u5927\u793e\u4f1a\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56\u3001\u590d\u6742\u8bed\u4e49\u5173\u7cfb\u548c\u65b0\u95fb\u4f20\u64ad\u7684\u793e\u4f1a\u52a8\u6001\uff0c\u4e14\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u5bfc\u81f4\u90e8\u7f72\u6210\u672c\u9ad8", "method": "\u63d0\u51fa\u81ea\u76d1\u7763\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u6846\u67b6\uff1a1) \u4f7f\u7528\u62bd\u8c61\u610f\u4e49\u8868\u793a(AMR)\u6355\u83b7\u590d\u6742\u8bed\u4e49\u5173\u7cfb\uff1b2) \u57fa\u4e8eLLM\u7684\u56fe\u5bf9\u6bd4\u635f\u5931(LGCL)\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8d1f\u951a\u70b9\u589e\u5f3a\u7279\u5f81\u53ef\u5206\u6027\uff1b3) \u591a\u91cd\u56fe\u63a9\u7801\u81ea\u7f16\u7801\u5668\u4ece\u793e\u4ea4\u4e0a\u4e0b\u6587\u56fe\u4e2d\u5b66\u4e60\u65b0\u95fb\u4f20\u64ad\u7279\u5f81", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u81ea\u76d1\u7763\u6846\u67b6\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u96c6\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u4f18\u5f02\u8868\u73b0\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u6574\u5408\u4e86\u8bed\u4e49\u548c\u4f20\u64ad\u7279\u5f81\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u76d1\u7763\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u5728\u96f6\u6837\u672c\u548c\u6709\u9650\u6570\u636e\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272"}}
{"id": "2508.18824", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18824", "abs": "https://arxiv.org/abs/2508.18824", "authors": ["Sirui Chen", "Changxin Tian", "Binbin Hu", "Kunlong Chen", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou"], "title": "Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness", "comment": null, "summary": "Enhancing the mathematical reasoning of large language models (LLMs) demands\nhigh-quality training data, yet conventional methods face critical challenges\nin scalability, cost, and data reliability. To address these limitations, we\npropose a novel program-assisted synthesis framework that systematically\ngenerates a high-quality mathematical corpus with guaranteed diversity,\ncomplexity, and correctness. This framework integrates mathematical knowledge\nsystems and domain-specific tools to create executable programs. These programs\nare then translated into natural language problem-solution pairs and vetted by\na bilateral validation mechanism that verifies solution correctness against\nprogram outputs and ensures program-problem consistency. We have generated 12.3\nmillion such problem-solving triples. Experiments demonstrate that models\nfine-tuned on our data significantly improve their inference capabilities,\nachieving state-of-the-art performance on several benchmark datasets and\nshowcasing the effectiveness of our synthesis approach.", "AI": {"tldr": "\u901a\u8fc7\u7a0b\u5e8f\u8f85\u52a9\u5408\u6210\u6846\u67b6\u751f\u6210120\u4e07\u9ad8\u8d28\u91cf\u6570\u5b66\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u5b66\u8bad\u7ec3\u6570\u636e\u65f6\u9047\u5230\u7684\u53ef\u6269\u5c55\u6027\u3001\u6210\u672c\u548c\u6570\u636e\u53ef\u9760\u6027\u95ee\u9898", "method": "\u7a0b\u5e8f\u8f85\u52a9\u5408\u6210\u6846\u67b6\uff0c\u6574\u5408\u6570\u5b66\u77e5\u8bc6\u7cfb\u7edf\u548c\u9886\u57df\u7279\u5b9a\u5de5\u5177\u751f\u6210\u53ef\u6267\u884c\u7a0b\u5e8f\uff0c\u901a\u8fc7\u53cc\u5411\u9a8c\u8bc1\u673a\u5236\u786e\u4fdd\u89e3\u9898\u6b63\u786e\u6027\u548c\u7a0b\u5e8f-\u95ee\u9898\u4e00\u81f4\u6027", "result": "\u751f\u6210\u4e8612.3\u767e\u4e07\u4e2a\u95ee\u9898-\u89e3\u51b3\u4e09\u5143\u7ec4\uff0c\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u591a\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd", "conclusion": "\u8be5\u5408\u6210\u6846\u67b6\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u6570\u5b66\u8bad\u7ec3\u6570\u636e\uff0c\u6709\u6548\u63d0\u5347LLM\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b"}}
{"id": "2508.18847", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18847", "abs": "https://arxiv.org/abs/2508.18847", "authors": ["Yibo Li", "Miao Xiong", "Jiaying Wu", "Bryan Hooi"], "title": "ConfTuner: Training Large Language Models to Express Their Confidence Verbally", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in high-stakes domains\nsuch as science, law, and healthcare, where accurate expressions of uncertainty\nare essential for reliability and trust. However, current LLMs are often\nobserved to generate incorrect answers with high confidence, a phenomenon known\nas \"overconfidence\". Recent efforts have focused on calibrating LLMs'\nverbalized confidence: i.e., their expressions of confidence in text form, such\nas \"I am 80% confident that...\". Existing approaches either rely on prompt\nengineering or fine-tuning with heuristically generated uncertainty estimates,\nboth of which have limited effectiveness and generalizability. Motivated by the\nnotion of proper scoring rules for calibration in classical machine learning\nmodels, we introduce ConfTuner, a simple and efficient fine-tuning method that\nintroduces minimal overhead and does not require ground-truth confidence scores\nor proxy confidence estimates. ConfTuner relies on a new loss function,\ntokenized Brier score, which we theoretically prove to be a proper scoring\nrule, intuitively meaning that it \"correctly incentivizes the model to report\nits true probability of being correct\". ConfTuner improves calibration across\ndiverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our\nresults further show that better-calibrated confidence enables downstream gains\nin self-correction and model cascade, advancing the development of trustworthy\nLLM systems. The code is available at\nhttps://github.com/liushiliushi/ConfTuner.", "AI": {"tldr": "ConfTuner\u662f\u4e00\u79cd\u901a\u8fc7tokenized Brier score\u635f\u5931\u51fd\u6570\u5fae\u8c03LLM\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u6821\u51c6\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u8868\u8fbe\uff0c\u89e3\u51b3\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u65e0\u9700\u771f\u5b9e\u7f6e\u4fe1\u5ea6\u6807\u7b7e\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u3001\u6cd5\u5f8b\u3001\u533b\u7597\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u90e8\u7f72\u65f6\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u751f\u6210\u9519\u8bef\u7b54\u6848\u65f6\u4ecd\u8868\u73b0\u51fa\u9ad8\u7f6e\u4fe1\u5ea6\uff0c\u73b0\u6709\u6821\u51c6\u65b9\u6cd5\u6548\u679c\u6709\u9650\u4e14\u6cdb\u5316\u6027\u5dee\u3002", "method": "\u63d0\u51faConfTuner\u5fae\u8c03\u65b9\u6cd5\uff0c\u4f7f\u7528\u7406\u8bba\u8bc1\u660e\u4e3aproper scoring rule\u7684tokenized Brier score\u635f\u5931\u51fd\u6570\uff0c\u65e0\u9700\u771f\u5b9e\u7f6e\u4fe1\u5ea6\u5206\u6570\u6216\u4ee3\u7406\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3002", "result": "\u5728\u591a\u79cd\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u6539\u5584\u6821\u51c6\u6548\u679c\uff0c\u5e76\u80fd\u6cdb\u5316\u5230GPT-4o\u7b49\u9ed1\u76d2\u6a21\u578b\uff0c\u66f4\u597d\u7684\u6821\u51c6\u7f6e\u4fe1\u5ea6\u5e26\u6765\u4e86\u81ea\u6821\u6b63\u548c\u6a21\u578b\u7ea7\u8054\u7b49\u4e0b\u6e38\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "ConfTuner\u63a8\u8fdb\u4e86\u53ef\u4fe1\u8d56LLM\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u901a\u8fc7\u7b80\u5355\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u7f6e\u4fe1\u5ea6\u6821\u51c6\u95ee\u9898\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.18870", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18870", "abs": "https://arxiv.org/abs/2508.18870", "authors": ["Viktor N. Zhuravlev", "Artur R. Khairullin", "Ernest A. Dyagin", "Alena N. Sitkina", "Nikita I. Kulin"], "title": "ReflectivePrompt: Reflective evolution in autoprompting algorithms", "comment": null, "summary": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which has been gaining popularity with the rapid advancement\nof prompt engineering, driven by extensive research in the field of large\nlanguage models (LLMs). This paper presents ReflectivePrompt - a novel\nautoprompting method based on evolutionary algorithms that employs a reflective\nevolution approach for more precise and comprehensive search of optimal\nprompts. ReflectivePrompt utilizes short-term and long-term reflection\noperations before crossover and elitist mutation to enhance the quality of the\nmodifications they introduce. This method allows for the accumulation of\nknowledge obtained throughout the evolution process and updates it at each\nepoch based on the current population. ReflectivePrompt was tested on 33\ndatasets for classification and text generation tasks using open-access large\nlanguage models: t-lite-instruct-0.1 and gemma3-27b-it. The method\ndemonstrates, on average, a significant improvement (e.g., 28% on BBH compared\nto EvoPrompt) in metrics relative to current state-of-the-art approaches,\nthereby establishing itself as one of the most effective solutions in\nevolutionary algorithm-based autoprompting.", "AI": {"tldr": "ReflectivePrompt\u662f\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\u7684\u81ea\u52a8\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u77ed\u671f\u548c\u957f\u671f\u53cd\u601d\u64cd\u4f5c\u6765\u4f18\u5316\u63d0\u793a\u9009\u62e9\uff0c\u572833\u4e2a\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5e73\u5747\u63d0\u534728%\u6027\u80fd", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u81ea\u52a8\u63d0\u793a\u9009\u62e9\u6280\u672f\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u641c\u7d22\u6700\u4f18\u63d0\u793a\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u548c\u5168\u9762\u7684\u641c\u7d22\u7b56\u7565", "method": "\u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\u7684\u81ea\u53cd\u8fdb\u5316\u65b9\u6cd5\uff0c\u5728\u4ea4\u53c9\u548c\u7cbe\u82f1\u53d8\u5f02\u524d\u4f7f\u7528\u77ed\u671f\u548c\u957f\u671f\u53cd\u601d\u64cd\u4f5c\u6765\u63d0\u5347\u4fee\u6539\u8d28\u91cf\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u79ef\u7d2f\u8fdb\u5316\u8fc7\u7a0b\u4e2d\u83b7\u5f97\u7684\u77e5\u8bc6\u5e76\u5728\u6bcf\u4e2aepoch\u57fa\u4e8e\u5f53\u524d\u79cd\u7fa4\u8fdb\u884c\u66f4\u65b0", "result": "\u572833\u4e2a\u5206\u7c7b\u548c\u6587\u672c\u751f\u6210\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u4f7f\u7528t-lite-instruct-0.1\u548cgemma3-27b-it\u6a21\u578b\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5e73\u5747\u663e\u8457\u63d0\u534728%\uff08\u5728BBH\u6570\u636e\u96c6\u4e0a\uff09", "conclusion": "ReflectivePrompt\u6210\u4e3a\u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\u7684\u81ea\u52a8\u63d0\u793a\u65b9\u6cd5\u4e2d\u6700\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u4e4b\u4e00\uff0c\u901a\u8fc7\u53cd\u601d\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u63d0\u793a\u4f18\u5316\u7684\u6548\u679c"}}
{"id": "2508.18872", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18872", "abs": "https://arxiv.org/abs/2508.18872", "authors": ["Laurie Gale", "Sebastian Mateos Nicolajsen"], "title": "Empowering Computing Education Researchers Through LLM-Assisted Content Analysis", "comment": "7 pages, 2 figures", "summary": "Computing education research (CER) is often instigated by practitioners\nwanting to improve both their own and the wider discipline's teaching practice.\nHowever, the latter is often difficult as many researchers lack the colleagues,\nresources, or capacity to conduct research that is generalisable or rigorous\nenough to advance the discipline. As a result, research methods that enable\nsense-making with larger volumes of qualitative data, while not increasing the\nburden on the researcher, have significant potential within CER.\n  In this discussion paper, we propose such a method for conducting rigorous\nanalysis on large volumes of textual data, namely a variation of LLM-assisted\ncontent analysis (LACA). This method combines content analysis with the use of\nlarge language models, empowering researchers to conduct larger-scale research\nwhich they would otherwise not be able to perform. Using a computing education\ndataset, we illustrate how LACA could be applied in a reproducible and rigorous\nmanner. We believe this method has potential in CER, enabling more\ngeneralisable findings from a wider range of research. This, together with the\ndevelopment of similar methods, can help to advance both the practice and\nresearch quality of the CER discipline.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdLLM\u8f85\u52a9\u5185\u5bb9\u5206\u6790(LACA)\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u8ba1\u7b97\u6559\u80b2\u7814\u7a76\u4e2d\u7684\u5927\u89c4\u6a21\u6587\u672c\u6570\u636e\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u8fdb\u884c\u66f4\u4e25\u8c28\u548c\u53ef\u63a8\u5e7f\u7684\u7814\u7a76\u3002", "motivation": "\u8ba1\u7b97\u6559\u80b2\u7814\u7a76(CER)\u9886\u57df\u7684\u7814\u7a76\u8005\u5f80\u5f80\u7f3a\u4e4f\u8d44\u6e90\u8fdb\u884c\u5927\u89c4\u6a21\u4e25\u8c28\u7814\u7a76\uff0c\u9700\u8981\u80fd\u591f\u5904\u7406\u5927\u91cf\u5b9a\u6027\u6570\u636e\u800c\u4e0d\u589e\u52a0\u7814\u7a76\u8d1f\u62c5\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLLM\u8f85\u52a9\u5185\u5bb9\u5206\u6790(LACA)\u65b9\u6cd5\uff0c\u7ed3\u5408\u5185\u5bb9\u5206\u6790\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u7814\u7a76\u8005\u80fd\u591f\u8fdb\u884c\u539f\u672c\u65e0\u6cd5\u5b8c\u6210\u7684\u5927\u89c4\u6a21\u7814\u7a76\u3002", "result": "\u901a\u8fc7\u8ba1\u7b97\u6559\u80b2\u6570\u636e\u96c6\u5c55\u793a\u4e86LACA\u65b9\u6cd5\u7684\u5e94\u7528\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5728\u53ef\u590d\u73b0\u548c\u4e25\u8c28\u7684\u65b9\u5f0f\u4e0b\u5904\u7406\u5927\u89c4\u6a21\u6587\u672c\u6570\u636e\u3002", "conclusion": "LACA\u65b9\u6cd5\u5728CER\u9886\u57df\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u80fd\u591f\u4ece\u66f4\u5e7f\u6cdb\u7684\u7814\u7a76\u4e2d\u83b7\u5f97\u66f4\u53ef\u63a8\u5e7f\u7684\u53d1\u73b0\uff0c\u6709\u52a9\u4e8e\u63d0\u5347CER\u5b66\u79d1\u7684\u7814\u7a76\u8d28\u91cf\u548c\u5b9e\u8df5\u6c34\u5e73\u3002"}}
{"id": "2508.18916", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.18916", "abs": "https://arxiv.org/abs/2508.18916", "authors": ["Bojan Evkoski", "Igor Mozeti\u010d", "Nikola Ljube\u0161i\u0107", "Petra Kralj Novak"], "title": "Affective Polarization across European Parliaments", "comment": "6 pages, 4 figures", "summary": "Affective polarization, characterized by increased negativity and hostility\ntowards opposing groups, has become a prominent feature of political discourse\nworldwide. Our study examines the presence of this type of polarization in a\nselection of European parliaments in a fully automated manner. Utilizing a\ncomprehensive corpus of parliamentary speeches from the parliaments of six\nEuropean countries, we employ natural language processing techniques to\nestimate parliamentarian sentiment. By comparing the levels of negativity\nconveyed in references to individuals from opposing groups versus one's own, we\ndiscover patterns of affectively polarized interactions. The findings\ndemonstrate the existence of consistent affective polarization across all six\nEuropean parliaments. Although activity correlates with negativity, there is no\nobserved difference in affective polarization between less active and more\nactive members of parliament. Finally, we show that reciprocity is a\ncontributing mechanism in affective polarization between parliamentarians\nacross all six parliaments.", "AI": {"tldr": "\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u5206\u6790\u6b27\u6d32\u516d\u56fd\u8bae\u4f1a\u6f14\u8bb2\uff0c\u53d1\u73b0\u666e\u904d\u5b58\u5728\u60c5\u611f\u6781\u5316\u73b0\u8c61\uff0c\u5373\u8bae\u5458\u5bf9\u53cd\u5bf9\u6d3e\u8868\u73b0\u51fa\u66f4\u591a\u8d1f\u9762\u60c5\u7eea\uff0c\u4e14\u4e92\u60e0\u673a\u5236\u662f\u60c5\u611f\u6781\u5316\u7684\u91cd\u8981\u9a71\u52a8\u56e0\u7d20", "motivation": "\u7814\u7a76\u60c5\u611f\u6781\u5316\uff08\u5bf9\u53cd\u5bf9\u7fa4\u4f53\u8868\u73b0\u51fa\u8d1f\u9762\u60c5\u7eea\u548c\u654c\u610f\uff09\u5728\u6b27\u6d32\u8bae\u4f1a\u4e2d\u7684\u5b58\u5728\u60c5\u51b5\uff0c\u8fd9\u79cd\u6781\u5316\u73b0\u8c61\u5df2\u6210\u4e3a\u5168\u7403\u653f\u6cbb\u8bdd\u8bed\u7684\u663e\u8457\u7279\u5f81", "method": "\u6536\u96c6\u516d\u56fd\u6b27\u6d32\u8bae\u4f1a\u7684\u6f14\u8bb2\u8bed\u6599\u5e93\uff0c\u8fd0\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u4f30\u8ba1\u8bae\u5458\u60c5\u611f\uff0c\u901a\u8fc7\u6bd4\u8f83\u5bf9\u53cd\u5bf9\u6d3e\u548c\u5df1\u65b9\u4eba\u58eb\u7684\u8d1f\u9762\u60c5\u7eea\u6c34\u5e73\u6765\u8bc6\u522b\u60c5\u611f\u6781\u5316\u6a21\u5f0f", "result": "\u5728\u6240\u6709\u516d\u4e2a\u6b27\u6d32\u8bae\u4f1a\u4e2d\u90fd\u53d1\u73b0\u4e86\u4e00\u81f4\u7684\u60c5\u611f\u6781\u5316\u73b0\u8c61\uff1b\u6d3b\u52a8\u7a0b\u5ea6\u4e0e\u8d1f\u9762\u60c5\u7eea\u76f8\u5173\uff0c\u4f46\u6d3b\u8dc3\u5ea6\u4e0d\u540c\u7684\u8bae\u5458\u4e4b\u95f4\u5728\u60c5\u611f\u6781\u5316\u65b9\u9762\u6ca1\u6709\u5dee\u5f02\uff1b\u4e92\u60e0\u673a\u5236\u662f\u8de8\u8bae\u4f1a\u60c5\u611f\u6781\u5316\u7684\u8d21\u732e\u673a\u5236", "conclusion": "\u6b27\u6d32\u8bae\u4f1a\u666e\u904d\u5b58\u5728\u60c5\u611f\u6781\u5316\uff0c\u4e92\u60e0\u884c\u4e3a\u662f\u63a8\u52a8\u8fd9\u79cd\u6781\u5316\u7684\u91cd\u8981\u673a\u5236\uff0c\u7814\u7a76\u4e3a\u7406\u89e3\u653f\u6cbb\u73af\u5883\u4e2d\u7684\u60c5\u611f\u52a8\u6001\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3"}}
{"id": "2508.18929", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18929", "abs": "https://arxiv.org/abs/2508.18929", "authors": ["Ilias Driouich", "Hongliu Cao", "Eoin Thomas"], "title": "Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework", "comment": "ECAI 2025 TRUST AI workshop", "summary": "Retrieval-augmented generation (RAG) systems improve large language model\noutputs by incorporating external knowledge, enabling more informed and\ncontext-aware responses. However, the effectiveness and trustworthiness of\nthese systems critically depends on how they are evaluated, particularly on\nwhether the evaluation process captures real-world constraints like protecting\nsensitive information. While current evaluation efforts for RAG systems have\nprimarily focused on the development of performance metrics, far less attention\nhas been given to the design and quality of the underlying evaluation datasets,\ndespite their pivotal role in enabling meaningful, reliable assessments. In\nthis work, we introduce a novel multi-agent framework for generating synthetic\nQA datasets for RAG evaluation that prioritize semantic diversity and privacy\npreservation. Our approach involves: (1) a Diversity agent leveraging\nclustering techniques to maximize topical coverage and semantic variability,\n(2) a Privacy Agent that detects and mask sensitive information across multiple\ndomains and (3) a QA curation agent that synthesizes private and diverse QA\npairs suitable as ground truth for RAG evaluation. Extensive experiments\ndemonstrate that our evaluation sets outperform baseline methods in diversity\nand achieve robust privacy masking on domain-specific datasets. This work\noffers a practical and ethically aligned pathway toward safer, more\ncomprehensive RAG system evaluation, laying the foundation for future\nenhancements aligned with evolving AI regulations and compliance standards.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6bb5\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u8bed\u4e49\u591a\u6837\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u5408\u6210QA\u6570\u636e\u96c6\uff0c\u4ee5\u6539\u5584RAG\u7cfb\u7edf\u7684\u8bc4\u4f30\u8d28\u91cf\u3002", "motivation": "\u76ee\u524dRAG\u7cfb\u7edf\u8bc4\u4f30\u4e3b\u8981\u96c6\u4e2d\u5728\u6027\u80fd\u6307\u6807\u4e0a\uff0c\u5bf9\u57fa\u7840\u8bc4\u4f30\u6570\u636e\u96c6\u7684\u8bbe\u8ba1\u548c\u8d28\u91cf\u5173\u6ce8\u4e0d\u591f\uff0c\u5c24\u5176\u7f3a\u4e4f\u5bf9\u9690\u79c1\u4fdd\u62a4\u7b49\u5b9e\u9645\u7ea6\u675f\u7684\u8003\u8651\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u6bb5\u4ee3\u7406\u6846\u67b6\uff1a(1)\u591a\u6837\u6027\u6bb5\u4ee3\u7406\u5229\u7528\u805a\u7c7b\u6280\u672f\u5b8c\u5584\u4e3b\u9898\u8986\u76d6\u548c\u8bed\u4e49\u53d8\u5316\u6027\uff1b(2)\u9690\u79c1\u6bb5\u4ee3\u7406\u68c0\u6d4b\u548c\u9690\u85cf\u591a\u9886\u57df\u654f\u611f\u4fe1\u606f\uff1b(3)QA\u7cbe\u9009\u6bb5\u4ee3\u7406\u5408\u6210\u5177\u6709\u9690\u79c1\u4fdd\u62a4\u548c\u591a\u6837\u6027\u7684QA\u5bf9\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u8bc4\u4f30\u96c6\u5728\u591a\u6837\u6027\u65b9\u9762\u8d85\u8fc7\u57fa\u51c6\u65b9\u6cd5\uff0c\u5e76\u5728\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u9690\u79c1\u9690\u85cf\u6548\u679c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u4e14\u7b26\u5408\u9053\u5fb7\u89c4\u8303\u7684\u8def\u5f84\uff0c\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u66f4\u5168\u9762\u7684RAG\u7cfb\u7edf\u8bc4\u4f30\uff0c\u4e3a\u672a\u6765\u4eba\u5de5\u667a\u80fd\u76d1\u7ba1\u548c\u9075\u5faa\u6807\u51c6\u7684\u53d1\u5c55\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2508.18988", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18988", "abs": "https://arxiv.org/abs/2508.18988", "authors": ["Hung Ming Liu"], "title": "Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models", "comment": "25 pages, 9 figures. The AI Intuition Explorer dashboard is available\n  at: https://cyrilliu1974.github.io/github.io/vi.html", "summary": "We present a framework where neural models develop an AI Mother Tongue, a\nnative symbolic language that simultaneously supports intuitive reasoning,\ncompositional symbol chains, and inherent interpretability. Unlike post-hoc\nexplanation methods, our approach embeds reasoning directly into the model's\nrepresentations: symbols capture meaningful semantic patterns, chains trace\ndecision paths, and gated induction mechanisms guide selective focus, yielding\ntransparent yet flexible reasoning. We introduce complementary training\nobjectives to enhance symbol purity and decision sparsity, and employ a\nsequential specialization strategy to first build broad symbolic competence and\nthen refine intuitive judgments. Experiments on AI tasks demonstrate\ncompetitive accuracy alongside verifiable reasoning traces, showing that AI\nMother Tongue can serve as a unified mechanism for interpretability, intuition,\nand symbolic reasoning in neural models.", "AI": {"tldr": "\u63d0\u51fa\u4e86AI\u6bcd\u8bed\u6846\u67b6\uff0c\u8ba9\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u53d1\u5c55\u539f\u751f\u7b26\u53f7\u8bed\u8a00\uff0c\u540c\u65f6\u652f\u6301\u76f4\u89c9\u63a8\u7406\u3001\u7ec4\u5408\u7b26\u53f7\u94fe\u548c\u5185\u5728\u53ef\u89e3\u91ca\u6027\uff0c\u65e0\u9700\u540e\u5904\u7406\u89e3\u91ca\u65b9\u6cd5", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u7f3a\u4e4f\u5185\u5728\u53ef\u89e3\u91ca\u6027\u548c\u7b26\u53f7\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5c06\u63a8\u7406\u76f4\u63a5\u5d4c\u5165\u6a21\u578b\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u900f\u660e\u4e14\u7075\u6d3b\u7684\u53ef\u89e3\u91caAI", "method": "\u901a\u8fc7\u7b26\u53f7\u6355\u83b7\u8bed\u4e49\u6a21\u5f0f\u3001\u7b26\u53f7\u94fe\u8ffd\u8e2a\u51b3\u7b56\u8def\u5f84\u3001\u95e8\u63a7\u5f52\u7eb3\u673a\u5236\u5f15\u5bfc\u9009\u62e9\u6027\u5173\u6ce8\uff1b\u5f15\u5165\u4e92\u8865\u8bad\u7ec3\u76ee\u6807\u589e\u5f3a\u7b26\u53f7\u7eaf\u5ea6\u548c\u51b3\u7b56\u7a00\u758f\u6027\uff1b\u91c7\u7528\u987a\u5e8f\u4e13\u4e1a\u5316\u7b56\u7565\u5148\u5efa\u7acb\u5e7f\u6cdb\u7b26\u53f7\u80fd\u529b\u518d\u7cbe\u70bc\u76f4\u89c9\u5224\u65ad", "result": "\u5728AI\u4efb\u52a1\u4e0a\u5c55\u793a\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u8f68\u8ff9", "conclusion": "AI\u6bcd\u8bed\u53ef\u4ee5\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\u4e2d\u53ef\u89e3\u91ca\u6027\u3001\u76f4\u89c9\u548c\u7b26\u53f7\u63a8\u7406\u7684\u7edf\u4e00\u673a\u5236"}}
{"id": "2508.18992", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18992", "abs": "https://arxiv.org/abs/2508.18992", "authors": ["Viktor N. Zhuravlev", "Artur R. Khairullin", "Ernest A. Dyagin", "Alena N. Sitkina", "Nikita I. Kulin"], "title": "Automatic Prompt Optimization with Prompt Distillation", "comment": null, "summary": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which is gaining popularity due to the rapid development of\nprompt engineering driven by extensive research in the field of large language\nmodels (LLMs). This paper presents DistillPrompt -- a novel autoprompting\nmethod based on large language models that employs a multi-stage integration of\ntask-specific information into prompts using training data. DistillPrompt\nutilizes distillation, compression, and aggregation operations to explore the\nprompt space more thoroughly. The method was tested on different datasets for\ntext classification and generation tasks using the t-lite-instruct-0.1 language\nmodel. The results demonstrate a significant average improvement (e.g., 20.12%\nacross the entire dataset compared to Grips) in key metrics over existing\nmethods in the field, establishing DistillPrompt as one of the most effective\nnon-gradient approaches in autoprompting.", "AI": {"tldr": "DistillPrompt\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u578b\u81ea\u52a8\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u96c6\u6210\u4efb\u52a1\u7279\u5b9a\u4fe1\u606f\u5230\u63d0\u793a\u4e2d\uff0c\u5728\u6587\u672c\u5206\u7c7b\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u81ea\u52a8\u9009\u62e9\u4f18\u5316\u63d0\u793a\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u81ea\u52a8\u63d0\u793a\u65b9\u6cd5\u6765\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u6027\u80fd", "method": "\u91c7\u7528\u84b8\u998f\u3001\u538b\u7f29\u548c\u805a\u5408\u64cd\u4f5c\u7684\u591a\u9636\u6bb5\u96c6\u6210\u65b9\u6cd5\uff0c\u5c06\u4efb\u52a1\u7279\u5b9a\u4fe1\u606f\u6574\u5408\u5230\u63d0\u793a\u4e2d\uff0c\u66f4\u5168\u9762\u5730\u63a2\u7d22\u63d0\u793a\u7a7a\u95f4", "result": "\u5728\u6587\u672c\u5206\u7c7b\u548c\u751f\u6210\u4efb\u52a1\u7684\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff08\u5982Grips\uff09\u5728\u5173\u952e\u6307\u6807\u4e0a\u5e73\u5747\u63d0\u534720.12%", "conclusion": "DistillPrompt\u88ab\u8bc1\u660e\u662f\u81ea\u52a8\u63d0\u793a\u9886\u57df\u4e2d\u6700\u6709\u6548\u7684\u975e\u68af\u5ea6\u65b9\u6cd5\u4e4b\u4e00"}}
{"id": "2508.19026", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19026", "abs": "https://arxiv.org/abs/2508.19026", "authors": ["Gueter Josmy Faure", "Min-Hung Chen", "Jia-Fong Yeh", "Ying Cheng", "Hung-Ting Su", "Yung-Hao Tang", "Shang-Hong Lai", "Winston H. Hsu"], "title": "MovieCORE: COgnitive REasoning in Movies", "comment": "Accepted for EMNLP'2025 Main Conference. Project Page:\n  https://joslefaure.github.io/assets/html/moviecore.html", "summary": "This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html.", "AI": {"tldr": "MovieCORE\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u89c6\u9891\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u7535\u5f71\u5185\u5bb9\u7684\u6df1\u5c42\u8ba4\u77e5\u7406\u89e3\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5934\u8111\u98ce\u66b4\u65b9\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf\u95ee\u9898\uff0c\u5e76\u63d0\u51faACE\u6a21\u5757\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b25%\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u95ee\u7b54\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u8868\u5c42\u7406\u89e3\uff0c\u7f3a\u4e4f\u5bf9\u7535\u5f71\u5185\u5bb9\u6df1\u5c42\u8ba4\u77e5\u7406\u89e3\u7684\u8bc4\u4f30\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6fc0\u53d1\u7cfb\u7edf2\u601d\u7ef4\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002", "method": "\u91c7\u7528\u591a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u601d\u8003\u667a\u80fd\u4f53\u8fdb\u884c\u5934\u8111\u98ce\u66b4\uff0c\u751f\u6210\u548c\u7cbe\u70bc\u9ad8\u8d28\u91cf\u95ee\u7b54\u5bf9\uff1b\u5f00\u53d1\u8ba4\u77e5\u6d4b\u8bd5\u8bc4\u4f30\u6570\u636e\u96c6\u8d28\u91cf\uff1b\u63d0\u51faAgentic Choice Enhancement (ACE)\u6a21\u5757\u589e\u5f3a\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "result": "\u521b\u5efa\u4e86MovieCORE\u6570\u636e\u96c6\uff0c\u5305\u542b\u6df1\u5ea6\u8ba4\u77e5\u95ee\u9898\uff1bACE\u6a21\u5757\u4f7f\u6a21\u578b\u63a8\u7406\u80fd\u529b\u63d0\u5347\u9ad8\u8fbe25%\uff1b\u4e3a\u8bc4\u4f30VQA\u6a21\u578b\u5728\u6df1\u5c42\u8ba4\u77e5\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u5168\u9762\u65b9\u6848\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63a8\u52a8\u4e86AI\u7cfb\u7edf\u5bf9\u7535\u5f71\u7406\u89e3\u7684\u53d1\u5c55\uff0c\u63ed\u793a\u4e86\u5f53\u524dVQA\u6a21\u578b\u5728\u5904\u7406\u5177\u6709\u6311\u6218\u6027\u7684\u7535\u5f71\u5185\u5bb9\u95ee\u9898\u65f6\u7684\u80fd\u529b\u548c\u5c40\u9650\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2508.19076", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19076", "abs": "https://arxiv.org/abs/2508.19076", "authors": ["Ziyue Li", "Yuan Chang", "Gaihong Yu", "Xiaoqiu Le"], "title": "HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance", "comment": null, "summary": "Large language model (LLM)-based agents have demonstrated remarkable\ncapabilities in decision-making tasks, but struggle significantly with complex,\nlong-horizon planning scenarios. This arises from their lack of macroscopic\nguidance, causing disorientation and failures in complex tasks, as well as\ninsufficient continuous oversight during execution, rendering them unresponsive\nto environmental changes and prone to deviations. To tackle these challenges,\nwe introduce HiPlan, a hierarchical planning framework that provides adaptive\nglobal-local guidance to boost LLM-based agents'decision-making. HiPlan\ndecomposes complex tasks into milestone action guides for general direction and\nstep-wise hints for detailed actions. During the offline phase, we construct a\nmilestone library from expert demonstrations, enabling structured experience\nreuse by retrieving semantically similar tasks and milestones. In the execution\nphase, trajectory segments from past milestones are dynamically adapted to\ngenerate step-wise hints that align current observations with the milestone\nobjectives, bridging gaps and correcting deviations. Extensive experiments\nacross two challenging benchmarks demonstrate that HiPlan substantially\noutperforms strong baselines, and ablation studies validate the complementary\nbenefits of its hierarchical components.", "AI": {"tldr": "HiPlan\u662f\u4e00\u4e2a\u5206\u5c42\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40-\u5c40\u90e8\u6307\u5bfc\u63d0\u5347LLM\u667a\u80fd\u4f53\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u5728\u590d\u6742\u957f\u65f6\u7a0b\u89c4\u5212\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5", "motivation": "LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u957f\u65f6\u7a0b\u89c4\u5212\u573a\u666f\u4e2d\u7f3a\u4e4f\u5b8f\u89c2\u6307\u5bfc\u548c\u6301\u7eed\u76d1\u7763\uff0c\u5bb9\u6613\u8ff7\u5931\u65b9\u5411\u548c\u504f\u79bb\u76ee\u6807", "method": "\u5206\u5c42\u89c4\u5212\u6846\u67b6\uff1a\u79bb\u7ebf\u9636\u6bb5\u6784\u5efa\u91cc\u7a0b\u7891\u5e93\u91cd\u7528\u4e13\u5bb6\u7ecf\u9a8c\uff0c\u6267\u884c\u9636\u6bb5\u52a8\u6001\u751f\u6210\u9010\u6b65\u63d0\u793a\u6765\u5bf9\u9f50\u89c2\u6d4b\u4e0e\u76ee\u6807", "result": "\u5728\u4e24\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5927\u5e45\u8d85\u8d8a\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5206\u5c42\u7ec4\u4ef6\u7684\u4e92\u8865\u6548\u76ca", "conclusion": "HiPlan\u901a\u8fc7\u5206\u5c42\u89c4\u5212\u6709\u6548\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5bfc\u5411\u548c\u76d1\u7763\u95ee\u9898\uff0c\u4e3a\u667a\u80fd\u4f53\u51b3\u7b56\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2508.19077", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19077", "abs": "https://arxiv.org/abs/2508.19077", "authors": ["Tom R\u00f6hr", "Soumyadeep Roy", "Fares Al Mohamad", "Jens-Michalis Papaioannou", "Wolfgang Nejdl", "Felix Gers", "Alexander L\u00f6ser"], "title": "\"Where does it hurt?\" -- Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues", "comment": "Accepted at ECAI 2025", "summary": "In a doctor-patient dialogue, the primary objective of physicians is to\ndiagnose patients and propose a treatment plan. Medical doctors guide these\nconversations through targeted questioning to efficiently gather the\ninformation required to provide the best possible outcomes for patients. To the\nbest of our knowledge, this is the first work that studies physician intent\ntrajectories in doctor-patient dialogues. We use the `Ambient Clinical\nIntelligence Benchmark' (Aci-bench) dataset for our study. We collaborate with\nmedical professionals to develop a fine-grained taxonomy of physician intents\nbased on the SOAP framework (Subjective, Objective, Assessment, and Plan). We\nthen conduct a large-scale annotation effort to label over 5000 doctor-patient\nturns with the help of a large number of medical experts recruited using\nProlific, a popular crowd-sourcing platform. This large labeled dataset is an\nimportant resource contribution that we use for benchmarking the\nstate-of-the-art generative and encoder models for medical intent\nclassification tasks. Our findings show that our models understand the general\nstructure of medical dialogues with high accuracy, but often fail to identify\ntransitions between SOAP categories. We also report for the first time common\ntrajectories in medical dialogue structures that provide valuable insights for\ndesigning `differential diagnosis' systems. Finally, we extensively study the\nimpact of intent filtering for medical dialogue summarization and observe a\nsignificant boost in performance. We make the codes and data, including\nannotation guidelines, publicly available at\nhttps://github.com/DATEXIS/medical-intent-classification.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5206\u6790\u533b\u60a3\u5bf9\u8bdd\u4e2d\u7684\u533b\u751f\u610f\u56fe\u8f68\u8ff9\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8eSOAP\u6846\u67b6\u7684\u7ec6\u7c92\u5ea6\u610f\u56fe\u5206\u7c7b\u6cd5\uff0c\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684\u533b\u7597\u610f\u56fe\u5206\u7c7b\u6a21\u578b\u3002", "motivation": "\u533b\u60a3\u5bf9\u8bdd\u4e2d\u533b\u751f\u901a\u8fc7\u9488\u5bf9\u6027\u63d0\u95ee\u6765\u6536\u96c6\u4fe1\u606f\u4ee5\u8fdb\u884c\u8bca\u65ad\u548c\u6cbb\u7597\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u533b\u751f\u610f\u56fe\u8f68\u8ff9\u7684\u7cfb\u7edf\u7814\u7a76\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u5206\u7c7b\u4f53\u7cfb\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528ACI-bench\u6570\u636e\u96c6\uff0c\u4e0e\u533b\u5b66\u4e13\u5bb6\u5408\u4f5c\u5f00\u53d1\u57fa\u4e8eSOAP\u6846\u67b6\u7684\u7ec6\u7c92\u5ea6\u610f\u56fe\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7\u4f17\u5305\u5e73\u53f0\u62db\u52df\u533b\u5b66\u4e13\u5bb6\u6807\u6ce85000\u591a\u4e2a\u533b\u60a3\u5bf9\u8bdd\u8f6e\u6b21\uff0c\u8bc4\u4f30\u751f\u6210\u5f0f\u548c\u7f16\u7801\u5668\u6a21\u578b\u5728\u533b\u7597\u610f\u56fe\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u80fd\u591f\u9ad8\u7cbe\u5ea6\u7406\u89e3\u533b\u7597\u5bf9\u8bdd\u7684\u4e00\u822c\u7ed3\u6784\uff0c\u4f46\u5728\u8bc6\u522bSOAP\u7c7b\u522b\u8f6c\u6362\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff1b\u9996\u6b21\u62a5\u544a\u4e86\u533b\u7597\u5bf9\u8bdd\u7ed3\u6784\u7684\u5e38\u89c1\u8f68\u8ff9\uff1b\u610f\u56fe\u8fc7\u6ee4\u663e\u8457\u63d0\u5347\u4e86\u533b\u7597\u5bf9\u8bdd\u6458\u8981\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u533b\u7597\u5bf9\u8bdd\u5206\u6790\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u548c\u89c1\u89e3\uff0c\u5f00\u53d1\u7684\u5206\u7c7b\u4f53\u7cfb\u548c\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u8bbe\u8ba1\u5dee\u5206\u8bca\u65ad\u7cfb\u7edf\uff0c\u610f\u56fe\u8fc7\u6ee4\u6280\u672f\u53ef\u6709\u6548\u63d0\u5347\u533b\u7597\u5bf9\u8bdd\u6458\u8981\u8d28\u91cf\u3002"}}
{"id": "2508.19089", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19089", "abs": "https://arxiv.org/abs/2508.19089", "authors": ["Yue Li", "Zhixue Zhao", "Carolina Scarton"], "title": "It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs", "comment": "Accepted by EMNLP 2025", "summary": "Extremely low-resource languages, especially those written in rare scripts,\nas shown in Figure 1, remain largely unsupported by large language models\n(LLMs). This is due in part to compounding factors such as the lack of training\ndata. This paper delivers the first comprehensive analysis of whether LLMs can\nacquire such languages purely via in-context learning (ICL), with or without\nauxiliary alignment signals, and how these methods compare to\nparameter-efficient fine-tuning (PEFT). We systematically evaluate 20\nunder-represented languages across three state-of-the-art multilingual LLMs.\nOur findings highlight the limitation of PEFT when both language and its script\nare extremely under-represented by the LLM. In contrast, zero-shot ICL with\nlanguage alignment is impressively effective on extremely low-resource\nlanguages, while few-shot ICL or PEFT is more beneficial for languages\nrelatively better represented by LLMs. For LLM practitioners working on\nextremely low-resource languages, we summarise guidelines grounded by our\nresults on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning\na multilingual model on languages of unseen scripts.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5168\u9762\u5206\u6790\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u6765\u83b7\u53d6\u6781\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u6bd4\u8f83\u4e86ICL\u4e0e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u7684\u6548\u679c\u3002\u7814\u7a76\u53d1\u73b0ICL\u5728\u6781\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u800cPEFT\u5728\u8bed\u8a00\u548c\u6587\u5b57\u90fd\u6781\u5ea6\u7f3a\u4e4f\u7684\u60c5\u51b5\u4e0b\u6548\u679c\u6709\u9650\u3002", "motivation": "\u6781\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u7279\u522b\u662f\u4f7f\u7528\u7f55\u89c1\u6587\u5b57\u7684\u8bed\u8a00\uff0c\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7f3a\u4e4f\u652f\u6301\u3002\u4e3b\u8981\u539f\u56e0\u662f\u8bad\u7ec3\u6570\u636e\u532e\u4e4f\u7b49\u590d\u5408\u56e0\u7d20\u3002\u9700\u8981\u63a2\u7d22LLM\u662f\u5426\u80fd\u591f\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u6765\u83b7\u53d6\u8fd9\u4e9b\u8bed\u8a00\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e8620\u79cd\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u8bed\u8a00\uff0c\u5728\u4e09\u4e2a\u6700\u5148\u8fdb\u7684\u591a\u8bed\u8a00LLM\u4e0a\u6d4b\u8bd5\u4e86\u96f6\u6837\u672cICL\u3001\u5c11\u6837\u672cICL\u548cPEFT\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u6709\u65e0\u8f85\u52a9\u5bf9\u9f50\u4fe1\u53f7\u7684\u6548\u679c\u3002", "result": "\u5f53\u8bed\u8a00\u53ca\u5176\u6587\u5b57\u90fd\u6781\u5ea6\u7f3a\u4e4f\u65f6\uff0cPEFT\u6548\u679c\u6709\u9650\u3002\u96f6\u6837\u672cICL\u914d\u5408\u8bed\u8a00\u5bf9\u9f50\u5728\u6781\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u6548\u679c\u663e\u8457\uff0c\u800c\u5c11\u6837\u672cICL\u6216PEFT\u5bf9\u76f8\u5bf9\u8f83\u597d\u7684\u8bed\u8a00\u66f4\u6709\u6548\u3002", "conclusion": "\u4e3aLLM\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u57fa\u4e8e\u7ed3\u679c\u7684\u6307\u5bfc\u539f\u5219\uff1a\u907f\u514d\u5728\u591a\u8bed\u8a00\u6a21\u578b\u4e0a\u5bf9\u672a\u89c1\u6587\u5b57\u7684\u8bed\u8a00\u8fdb\u884c\u5fae\u8c03\uff0cICL\u662f\u5904\u7406\u6781\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u66f4\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2508.19093", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19093", "abs": "https://arxiv.org/abs/2508.19093", "authors": ["Mathew Henrickson"], "title": "Retrieval-Augmented Generation for Natural Language Art Provenance Searches in the Getty Provenance Index", "comment": null, "summary": "This research presents a Retrieval-Augmented Generation (RAG) framework for\nart provenance studies, focusing on the Getty Provenance Index. Provenance\nresearch establishes the ownership history of artworks, which is essential for\nverifying authenticity, supporting restitution and legal claims, and\nunderstanding the cultural and historical context of art objects. The process\nis complicated by fragmented, multilingual archival data that hinders efficient\nretrieval. Current search portals require precise metadata, limiting\nexploratory searches. Our method enables natural-language and multilingual\nsearches through semantic retrieval and contextual summarization, reducing\ndependence on metadata structures. We assess RAG's capability to retrieve and\nsummarize auction records using a 10,000-record sample from the Getty\nProvenance Index - German Sales. The results show this approach provides a\nscalable solution for navigating art market archives, offering a practical tool\nfor historians and cultural heritage professionals conducting historically\nsensitive research.", "AI": {"tldr": "\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u65b9\u6cd5\uff0c\u4e3a\u827a\u672f\u54c1\u6765\u6e90\u7814\u7a76\u63d0\u4f9b\u591a\u8bed\u8a00\u81ea\u7136\u8bed\u8a00\u68c0\u7d22\u548c\u8bed\u4e49\u6458\u8981\u529f\u80fd\uff0c\u89e3\u51b3\u6863\u6848\u6570\u636e\u5206\u6563\u6027\u95ee\u9898", "motivation": "\u827a\u672f\u54c1\u6765\u6e90\u7814\u7a76\u5bf9\u9a8c\u8bc1\u771f\u4f2a\u3001\u652f\u6301\u8fd8\u539f\u548c\u6cd5\u5f8b\u7eb3\u9884\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u68c0\u7d22\u5de5\u5177\u4f9d\u8d56\u7cbe\u786e\u5143\u6570\u636e\uff0c\u9650\u5236\u4e86\u63a2\u7d22\u6027\u68c0\u7d22", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u68c0\u7d22\u548c\u4e0a\u4e0b\u6587\u6458\u8981\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u548c\u591a\u8bed\u8a00\u68c0\u7d22\uff0c\u51cf\u5c11\u5bf9\u5143\u6570\u636e\u7ed3\u6784\u7684\u4f9d\u8d56", "result": "\u5728Getty\u6765\u6e90\u7d22\u5f15-\u5fb7\u56fd\u9500\u552e\u6570\u636e\u5e93\u768410,000\u6761\u8bb0\u5f55\u4e0a\u8bc4\u4f30\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u68c0\u7d22\u548c\u6458\u8981\u62cd\u5356\u8bb0\u5f55", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5bfc\u822a\u827a\u672f\u5e02\u573a\u6863\u6848\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u5386\u53f2\u5b66\u5bb6\u548c\u6587\u5316\u9057\u4ea7\u4e13\u4e1a\u4eba\u5458\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177"}}
{"id": "2508.19099", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19099", "abs": "https://arxiv.org/abs/2508.19099", "authors": ["Thomas Compton"], "title": "Beyond the Black Box: Integrating Lexical and Semantic Methods in Quantitative Discourse Analysis with BERTopic", "comment": "5 pages conference paper, 4 tables", "summary": "Quantitative Discourse Analysis has seen growing adoption with the rise of\nLarge Language Models and computational tools. However, reliance on black box\nsoftware such as MAXQDA and NVivo risks undermining methodological transparency\nand alignment with research goals. This paper presents a hybrid, transparent\nframework for QDA that combines lexical and semantic methods to enable\ntriangulation, reproducibility, and interpretability. Drawing from a case study\nin historical political discourse, we demonstrate how custom Python pipelines\nusing NLTK, spaCy, and Sentence Transformers allow fine-grained control over\npreprocessing, lemmatisation, and embedding generation. We further detail our\niterative BERTopic modelling process, incorporating UMAP dimensionality\nreduction, HDBSCAN clustering, and c-TF-IDF keyword extraction, optimised\nthrough parameter tuning and multiple runs to enhance topic coherence and\ncoverage. By juxtaposing precise lexical searches with context-aware semantic\nclustering, we argue for a multi-layered approach that mitigates the\nlimitations of either method in isolation. Our workflow underscores the\nimportance of code-level transparency, researcher agency, and methodological\ntriangulation in computational discourse studies. Code and supplementary\nmaterials are available via GitHub.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u8bcd\u6c47\u548c\u8bed\u4e49\u65b9\u6cd5\u7684\u900f\u660eQDA\u6846\u67b6\uff0c\u4f7f\u7528Python\u5de5\u5177\u94fe\u5b9e\u73b0\u53ef\u590d\u73b0\u3001\u53ef\u89e3\u91ca\u7684\u6df7\u5408\u5206\u6790\u6d41\u7a0b", "motivation": "\u89e3\u51b3\u5f53\u524d\u5b9a\u91cf\u8bdd\u8bed\u5206\u6790\u4e2d\u9ed1\u76d2\u8f6f\u4ef6\uff08\u5982MAXQDA\u3001NVivo\uff09\u5bfc\u81f4\u7684\u65b9\u6cd5\u900f\u660e\u6027\u4e0d\u8db3\u548c\u7814\u7a76\u76ee\u6807\u5bf9\u9f50\u95ee\u9898", "method": "\u4f7f\u7528Python\u5de5\u5177\u94fe\uff08NLTK\u3001spaCy\u3001Sentence Transformers\uff09\u6784\u5efa\u81ea\u5b9a\u4e49\u5904\u7406\u6d41\u7a0b\uff0c\u7ed3\u5408BERTopic\u5efa\u6a21\uff08UMAP\u964d\u7ef4\u3001HDBSCAN\u805a\u7c7b\u3001c-TF-IDF\u5173\u952e\u8bcd\u63d0\u53d6\uff09\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316", "result": "\u901a\u8fc7\u5386\u53f2\u653f\u6cbb\u8bdd\u8bed\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u8bcd\u6c47\u7cbe\u786e\u641c\u7d22\u4e0e\u8bed\u4e49\u805a\u7c7b\u76f8\u7ed3\u5408\u7684\u591a\u5c42\u6b21\u5206\u6790\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u4e3b\u9898\u4e00\u81f4\u6027\u548c\u8986\u76d6\u5ea6", "conclusion": "\u5f3a\u8c03\u4ee3\u7801\u7ea7\u900f\u660e\u5ea6\u3001\u7814\u7a76\u8005\u81ea\u4e3b\u6743\u548c\u65b9\u6cd5\u4e09\u89d2\u9a8c\u8bc1\u5728\u8ba1\u7b97\u8bdd\u8bed\u7814\u7a76\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u63d0\u5021\u6df7\u5408\u65b9\u6cd5\u4ee5\u514b\u670d\u5355\u4e00\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2508.19111", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19111", "abs": "https://arxiv.org/abs/2508.19111", "authors": ["Zhikai Ding", "Shiyu Ni", "Keping Bi"], "title": "Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs", "comment": "EMNLP2025 Findings", "summary": "Large vision-language models (LVLMs) demonstrate strong visual question\nanswering (VQA) capabilities but are shown to hallucinate. A reliable model\nshould perceive its knowledge boundaries-knowing what it knows and what it does\nnot. This paper investigates LVLMs' perception of their knowledge boundaries by\nevaluating three types of confidence signals: probabilistic confidence, answer\nconsistency-based confidence, and verbalized confidence. Experiments on three\nLVLMs across three VQA datasets show that, although LVLMs possess a reasonable\nperception level, there is substantial room for improvement. Among the three\nconfidences, probabilistic and consistency-based signals are more reliable\nindicators, while verbalized confidence often leads to overconfidence. To\nenhance LVLMs' perception, we adapt several established confidence calibration\nmethods from Large Language Models (LLMs) and propose three effective methods.\nAdditionally, we compare LVLMs with their LLM counterparts, finding that\njointly processing visual and textual inputs decreases question-answering\nperformance but reduces confidence, resulting in an improved perception level\ncompared to LLMs.", "AI": {"tldr": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u95ee\u7b54\u4e2d\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u672c\u6587\u7814\u7a76\u5176\u77e5\u8bc6\u8fb9\u754c\u611f\u77e5\u80fd\u529b\uff0c\u8bc4\u4f30\u4e09\u79cd\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\uff0c\u53d1\u73b0\u6982\u7387\u548c\u4e00\u81f4\u6027\u7f6e\u4fe1\u5ea6\u66f4\u53ef\u9760\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u95ee\u7b54\u4e2d\u8868\u73b0\u51fa\u8272\u4f46\u5b58\u5728\u5e7b\u89c9\uff0c\u9700\u8981\u53ef\u9760\u7684\u6a21\u578b\u6765\u611f\u77e5\u5176\u77e5\u8bc6\u8fb9\u754c\uff0c\u4e86\u89e3\u77e5\u9053\u4ec0\u4e48\u548c\u4e0d\u77e5\u9053\u4ec0\u4e48\u3002", "method": "\u8bc4\u4f30\u4e09\u79cd\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\uff1a\u6982\u7387\u7f6e\u4fe1\u5ea6\u3001\u7b54\u6848\u4e00\u81f4\u6027\u7f6e\u4fe1\u5ea6\u548c\u8bed\u8a00\u5316\u7f6e\u4fe1\u5ea6\uff0c\u5728\u4e09\u4e2aLVLM\u6a21\u578b\u548c\u4e09\u4e2aVQA\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u91c7\u7528LLM\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u65b9\u6cd5\u8fdb\u884c\u6539\u8fdb\u3002", "result": "LVLM\u5177\u6709\u4e00\u5b9a\u7684\u77e5\u8bc6\u8fb9\u754c\u611f\u77e5\u80fd\u529b\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u6982\u7387\u548c\u4e00\u81f4\u6027\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\u66f4\u53ef\u9760\uff0c\u8bed\u8a00\u5316\u7f6e\u4fe1\u5ea6\u5bb9\u6613\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\u3002\u89c6\u89c9\u548c\u6587\u672c\u8054\u5408\u5904\u7406\u964d\u4f4e\u4e86\u95ee\u7b54\u6027\u80fd\u4f46\u63d0\u9ad8\u4e86\u611f\u77e5\u6c34\u5e73\u3002", "conclusion": "LVLM\u7684\u77e5\u8bc6\u8fb9\u754c\u611f\u77e5\u80fd\u529b\u9700\u8981\u8fdb\u4e00\u6b65\u63d0\u5347\uff0c\u6982\u7387\u548c\u4e00\u81f4\u6027\u7f6e\u4fe1\u5ea6\u662f\u66f4\u53ef\u9760\u7684\u6307\u6807\uff0c\u63d0\u51fa\u7684\u6821\u51c6\u65b9\u6cd5\u80fd\u6709\u6548\u6539\u5584\u611f\u77e5\u80fd\u529b\uff0c\u591a\u6a21\u6001\u5904\u7406\u867d\u7136\u964d\u4f4e\u6027\u80fd\u4f46\u63d0\u9ad8\u4e86\u611f\u77e5\u51c6\u786e\u6027\u3002"}}
{"id": "2508.19202", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19202", "abs": "https://arxiv.org/abs/2508.19202", "authors": ["Alan Li", "Yixin Liu", "Arpan Sarkar", "Doug Downey", "Arman Cohan"], "title": "Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning", "comment": "28 pages, 16 figures", "summary": "Scientific problem solving poses unique challenges for LLMs, requiring both\ndeep domain knowledge and the ability to apply such knowledge through complex\nreasoning. While automated scientific reasoners hold great promise for\nassisting human scientists, there is currently no widely adopted holistic\nbenchmark for evaluating scientific reasoning, and few approaches\nsystematically disentangle the distinct roles of knowledge and reasoning in\nthese tasks. To address these gaps, we introduce SciReas, a diverse suite of\nexisting benchmarks for scientific reasoning tasks, and SciReas-Pro, a\nselective subset that requires more complex reasoning. Our holistic evaluation\nsurfaces insights about scientific reasoning performance that remain hidden\nwhen relying on individual benchmarks alone. We then propose KRUX, a probing\nframework for studying the distinct roles of reasoning and knowledge in\nscientific tasks. Combining the two, we conduct an in-depth analysis that\nyields several key findings: (1) Retrieving task-relevant knowledge from model\nparameters is a critical bottleneck for LLMs in scientific reasoning; (2)\nReasoning models consistently benefit from external knowledge added in-context\non top of the reasoning enhancement; (3) Enhancing verbalized reasoning\nimproves LLMs' ability to surface task-relevant knowledge. Finally, we conduct\na lightweight analysis, comparing our science-focused data composition with\nconcurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline\nfor scientific reasoning.", "AI": {"tldr": "SciReas\u548cSciReas-Pro\u79d1\u5b66\u63a8\u7406\u8bc4\u6d4b\u5957\u4ef6\uff0c\u7ed3\u5408KRUX\u63a2\u6d4b\u6846\u67b6\u5206\u6790LLM\u5728\u79d1\u5b66\u63a8\u7406\u4e2d\u77e5\u8bc6\u4e0e\u63a8\u7406\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u77e5\u8bc6\u68c0\u7d22\u662f\u74f6\u9888\uff0c\u5916\u90e8\u77e5\u8bc6\u80fd\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u589e\u5f3a\u63a8\u7406\u8868\u8fbe\u80fd\u6539\u5584\u77e5\u8bc6\u63d0\u53d6\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5168\u9762\u8bc4\u4f30\u79d1\u5b66\u63a8\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e14\u5f88\u5c11\u65b9\u6cd5\u7cfb\u7edf\u6027\u5730\u5206\u6790\u77e5\u8bc6\u4e0e\u63a8\u7406\u5728\u79d1\u5b66\u4efb\u52a1\u4e2d\u7684\u4e0d\u540c\u4f5c\u7528\u3002", "method": "\u5f15\u5165SciReas\u8bc4\u6d4b\u5957\u4ef6\u548cSciReas-Pro\u5b50\u96c6\uff0c\u63d0\u51faKRUX\u63a2\u6d4b\u6846\u67b6\u6765\u5206\u79bb\u77e5\u8bc6\u4e0e\u63a8\u7406\u7684\u4f5c\u7528\uff0c\u5e76\u8fdb\u884c\u6df1\u5165\u5206\u6790\u3002", "result": "\u53d1\u73b0\u77e5\u8bc6\u68c0\u7d22\u662fLLM\u79d1\u5b66\u63a8\u7406\u7684\u5173\u952e\u74f6\u9888\uff1b\u5916\u90e8\u77e5\u8bc6\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff1b\u589e\u5f3a\u63a8\u7406\u8868\u8fbe\u80fd\u6539\u5584\u76f8\u5173\u77e5\u8bc6\u63d0\u53d6\u3002", "conclusion": "\u79d1\u5b66\u63a8\u7406\u9700\u8981\u540c\u65f6\u5173\u6ce8\u77e5\u8bc6\u83b7\u53d6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u51fa\u4e86SciLit01\u4f5c\u4e3a8B\u53c2\u6570\u7684\u79d1\u5b66\u63a8\u7406\u57fa\u7ebf\u6a21\u578b\u3002"}}
{"id": "2508.19221", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19221", "abs": "https://arxiv.org/abs/2508.19221", "authors": ["Isabel Cachola", "Daniel Khashabi", "Mark Dredze"], "title": "Evaluating the Evaluators: Are readability metrics good measures of readability?", "comment": null, "summary": "Plain Language Summarization (PLS) aims to distill complex documents into\naccessible summaries for non-expert audiences. In this paper, we conduct a\nthorough survey of PLS literature, and identify that the current standard\npractice for readability evaluation is to use traditional readability metrics,\nsuch as Flesch-Kincaid Grade Level (FKGL). However, despite proven utility in\nother fields, these metrics have not been compared to human readability\njudgments in PLS. We evaluate 8 readability metrics and show that most\ncorrelate poorly with human judgments, including the most popular metric, FKGL.\nWe then show that Language Models (LMs) are better judges of readability, with\nthe best-performing model achieving a Pearson correlation of 0.56 with human\njudgments. Extending our analysis to PLS datasets, which contain summaries\naimed at non-expert audiences, we find that LMs better capture deeper measures\nof readability, such as required background knowledge, and lead to different\nconclusions than the traditional metrics. Based on these findings, we offer\nrecommendations for best practices in the evaluation of plain language\nsummaries. We release our analysis code and survey data.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u53d1\u73b0\u4f20\u7edf\u53ef\u8bfb\u6027\u6307\u6807\u5728\u666e\u901a\u8bed\u8a00\u6458\u8981\u8bc4\u4f30\u4e2d\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u5dee\uff0c\u800c\u8bed\u8a00\u6a21\u578b\u80fd\u66f4\u597d\u5730\u8bc4\u4f30\u53ef\u8bfb\u6027\uff0c\u7279\u522b\u662f\u5728\u80cc\u666f\u77e5\u8bc6\u9700\u6c42\u7b49\u6df1\u5c42\u7ef4\u5ea6\u4e0a\u3002", "motivation": "\u5f53\u524d\u666e\u901a\u8bed\u8a00\u6458\u8981\u9886\u57df\u4f7f\u7528\u4f20\u7edf\u53ef\u8bfb\u6027\u6307\u6807\uff08\u5982FKGL\uff09\u4f5c\u4e3a\u6807\u51c6\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f46\u8fd9\u4e9b\u6307\u6807\u5c1a\u672a\u4e0e\u4eba\u7c7b\u53ef\u8bfb\u6027\u5224\u65ad\u8fdb\u884c\u5bf9\u6bd4\u9a8c\u8bc1\u3002", "method": "\u8bc4\u4f308\u79cd\u4f20\u7edf\u53ef\u8bfb\u6027\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u76f8\u5173\u6027\uff0c\u5e76\u6d4b\u8bd5\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u53ef\u8bfb\u6027\u8bc4\u4f30\u5de5\u5177\u7684\u6027\u80fd\uff0c\u5206\u6790\u5176\u5728\u666e\u901a\u8bed\u8a00\u6458\u8981\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5927\u591a\u6570\u4f20\u7edf\u53ef\u8bfb\u6027\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u5dee\uff0c\u6700\u4f73\u8bed\u8a00\u6a21\u578b\u8fbe\u52300.56\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u3002\u8bed\u8a00\u6a21\u578b\u80fd\u66f4\u597d\u5730\u6355\u6349\u80cc\u666f\u77e5\u8bc6\u9700\u6c42\u7b49\u6df1\u5c42\u53ef\u8bfb\u6027\u7ef4\u5ea6\u3002", "conclusion": "\u5efa\u8bae\u5728\u666e\u901a\u8bed\u8a00\u6458\u8981\u8bc4\u4f30\u4e2d\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u66ff\u4ee3\u4f20\u7edf\u53ef\u8bfb\u6027\u6307\u6807\uff0c\u5e76\u53d1\u5e03\u4e86\u5206\u6790\u4ee3\u7801\u548c\u8c03\u67e5\u6570\u636e\u4ee5\u4f9b\u540e\u7eed\u7814\u7a76\u4f7f\u7528\u3002"}}
{"id": "2508.19227", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.19227", "abs": "https://arxiv.org/abs/2508.19227", "authors": ["Jiaqi Chen", "Yanzhe Zhang", "Yutong Zhang", "Yijia Shao", "Diyi Yang"], "title": "Generative Interfaces for Language Models", "comment": "Preprint", "summary": "Large language models (LLMs) are increasingly seen as assistants, copilots,\nand consultants, capable of supporting a wide range of tasks through natural\nconversation. However, most systems remain constrained by a linear\nrequest-response format that often makes interactions inefficient in\nmulti-turn, information-dense, and exploratory tasks. To address these\nlimitations, we propose Generative Interfaces for Language Models, a paradigm\nin which LLMs respond to user queries by proactively generating user interfaces\n(UIs) that enable more adaptive and interactive engagement. Our framework\nleverages structured interface-specific representations and iterative\nrefinements to translate user queries into task-specific UIs. For systematic\nevaluation, we introduce a multidimensional assessment framework that compares\ngenerative interfaces with traditional chat-based ones across diverse tasks,\ninteraction patterns, and query types, capturing functional, interactive, and\nemotional aspects of user experience. Results show that generative interfaces\nconsistently outperform conversational ones, with humans preferring them in\nover 70% of cases. These findings clarify when and why users favor generative\ninterfaces, paving the way for future advancements in human-AI interaction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u751f\u6210\u5f0f\u754c\u9762\u8303\u5f0f\uff0c\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u4e3b\u52a8\u751f\u6210\u7528\u6237\u754c\u9762\u6765\u54cd\u5e94\u67e5\u8be2\uff0c\u76f8\u6bd4\u4f20\u7edf\u5bf9\u8bdd\u5f0f\u754c\u9762\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4eba\u7c7b\u7528\u6237\u504f\u597d\u5ea6\u8d85\u8fc770%\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u53d7\u9650\u4e8e\u7ebf\u6027\u7684\u8bf7\u6c42-\u54cd\u5e94\u683c\u5f0f\uff0c\u5728\u591a\u8f6e\u3001\u4fe1\u606f\u5bc6\u96c6\u548c\u63a2\u7d22\u6027\u4efb\u52a1\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u66f4\u81ea\u9002\u5e94\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u754c\u9762\u6846\u67b6\uff0c\u5229\u7528\u7ed3\u6784\u5316\u754c\u9762\u7279\u5b9a\u8868\u793a\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u5c06\u7528\u6237\u67e5\u8be2\u8f6c\u6362\u4e3a\u4efb\u52a1\u7279\u5b9a\u7684\u7528\u6237\u754c\u9762\u3002", "result": "\u751f\u6210\u5f0f\u754c\u9762\u5728\u591a\u6837\u5316\u4efb\u52a1\u3001\u4ea4\u4e92\u6a21\u5f0f\u548c\u67e5\u8be2\u7c7b\u578b\u4e2d\u6301\u7eed\u4f18\u4e8e\u5bf9\u8bdd\u5f0f\u754c\u9762\uff0c\u4eba\u7c7b\u7528\u6237\u504f\u597d\u5ea6\u8d85\u8fc770%\u3002", "conclusion": "\u7814\u7a76\u9610\u660e\u4e86\u7528\u6237\u504f\u597d\u751f\u6210\u5f0f\u754c\u9762\u7684\u65f6\u673a\u548c\u539f\u56e0\uff0c\u4e3a\u672a\u6765\u4eba\u673a\u4ea4\u4e92\u7684\u8fdb\u6b65\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
