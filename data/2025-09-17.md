<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 45]
- [cs.SD](#cs.SD) [Total: 14]
- [cs.MM](#cs.MM) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Benchmarking and Improving LVLMs on Event Extraction from Multimedia Documents](https://arxiv.org/abs/2509.12876)
*Fuyu Xing,Zimu Wang,Wei Wang,Haiyang Zhang*

Main category: cs.CL

TL;DR: 本文首次系统评估了DeepSeek-VL2和Qwen-VL系列等大型视觉语言模型在多媒体事件抽取任务上的表现，发现在少样本提示和微调设置下，这些模型在视觉任务上表现良好但在文本任务上表现较差，微调能显著提升性能，多模态组合展现出强协同效应。


<details>
  <summary>Details</summary>
Motivation: 多媒体内容的激增需要开发有效的多媒体事件抽取系统，虽然大型视觉语言模型已展现出强大的跨模态能力，但它们在M2E2任务中的应用价值尚未得到充分探索。

Method: 在M2E2数据集上对代表性LVLMs进行系统评估，涵盖纯文本、纯图像和跨媒体子任务，在少样本提示和微调（使用LoRA）两种设置下进行评估，并进行详细的错误分析。

Result: 关键发现：1）少样本LVLMs在视觉任务上表现更好但在文本任务上表现较差；2）使用LoRA微调LVLMs能显著提升模型性能；3）LVLMs在多模态组合时展现出强协同效应，在跨模态设置中获得优越性能。

Conclusion: 研究揭示了在语义精度、定位和跨模态基础等领域的持续挑战，这些仍然是推进M2E2能力的关键障碍，为未来研究提供了重要见解。

Abstract: The proliferation of multimedia content necessitates the development of
effective Multimedia Event Extraction (M2E2) systems. Though Large
Vision-Language Models (LVLMs) have shown strong cross-modal capabilities,
their utility in the M2E2 task remains underexplored. In this paper, we present
the first systematic evaluation of representative LVLMs, including DeepSeek-VL2
and the Qwen-VL series, on the M2E2 dataset. Our evaluations cover text-only,
image-only, and cross-media subtasks, assessed under both few-shot prompting
and fine-tuning settings. Our key findings highlight the following valuable
insights: (1) Few-shot LVLMs perform notably better on visual tasks but
struggle significantly with textual tasks; (2) Fine-tuning LVLMs with LoRA
substantially enhances model performance; and (3) LVLMs exhibit strong synergy
when combining modalities, achieving superior performance in cross-modal
settings. We further provide a detailed error analysis to reveal persistent
challenges in areas such as semantic precision, localization, and cross-modal
grounding, which remain critical obstacles for advancing M2E2 capabilities.

</details>


### [2] [MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch](https://arxiv.org/abs/2509.12340)
*Nikolay Banar,Ehsan Lotfi,Jens Van Nooten,Cristina Arhiliuc,Marija Kliocaite,Walter Daelemans*

Main category: cs.CL

TL;DR: 本文针对荷兰语在嵌入资源中的代表性不足问题，提出了MTEB-NL基准测试、训练数据集和E5-NL嵌入模型系列，以促进荷兰语嵌入技术的发展。


<details>
  <summary>Details</summary>
Motivation: 荷兰语在多语言嵌入资源中代表性不足，通常只占已发布资源的一小部分，需要专门的资源来支持荷兰语嵌入模型的开发和评估。

Method: 1) 创建MTEB-NL基准测试，包含现有荷兰语数据集和新创建的数据集；2) 提供训练数据集，结合可用检索数据集和LLM生成的合成数据；3) 开发紧凑高效的E5-NL嵌入模型系列。

Result: 发布了完整的荷兰语嵌入评估和生成资源，包括基准测试、训练数据和预训练模型，所有资源都通过Hugging Face Hub和MTEB包公开提供。

Conclusion: 该研究填补了荷兰语嵌入资源的空白，为荷兰语NLP社区提供了重要的基础设施，有望推动荷兰语嵌入技术的进一步发展。

Abstract: Recently, embedding resources, including models, benchmarks, and datasets,
have been widely released to support a variety of languages. However, the Dutch
language remains underrepresented, typically comprising only a small fraction
of the published multilingual resources. To address this gap and encourage the
further development of Dutch embeddings, we introduce new resources for their
evaluation and generation. First, we introduce the Massive Text Embedding
Benchmark for Dutch (MTEB-NL), which includes both existing Dutch datasets and
newly created ones, covering a wide range of tasks. Second, we provide a
training dataset compiled from available Dutch retrieval datasets, complemented
with synthetic data generated by large language models to expand task coverage
beyond retrieval. Finally, we release a series of E5-NL models compact yet
efficient embedding models that demonstrate strong performance across multiple
tasks. We make our resources publicly available through the Hugging Face Hub
and the MTEB package.

</details>


### [3] [MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables](https://arxiv.org/abs/2509.12371)
*Matteo Marcuzzo,Alessandro Zangari,Andrea Albarelli,Jose Camacho-Collados,Mohammad Taher Pilehvar*

Main category: cs.CL

TL;DR: MORABLES是一个基于寓言和短篇故事构建的道德推理基准测试，通过多项选择题评估LLM的深层理解能力，发现大模型虽然表现更好但仍易受对抗性攻击影响，存在显著的自相矛盾问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在标准阅读理解基准上的优异表现，需要评估它们在复杂抽象推理和推断方面的能力，特别是道德推理等深层理解技能。

Method: 构建MORABLES基准测试，使用历史文学中的寓言和短篇故事，设计多项选择题针对道德推理，并创建对抗性变体来测试模型鲁棒性。

Result: 大模型表现优于小模型，但对对抗性操作敏感，经常依赖表面模式而非真正的道德推理，最佳模型在约20%的情况下会自相矛盾地反驳自己的答案。

Conclusion: 规模而非推理能力是性能的主要驱动因素，推理增强的模型未能弥合这一差距，表明当前LLM在道德推理方面仍存在脆弱性。

Abstract: As LLMs excel on standard reading comprehension benchmarks, attention is
shifting toward evaluating their capacity for complex abstract reasoning and
inference. Literature-based benchmarks, with their rich narrative and moral
depth, provide a compelling framework for evaluating such deeper comprehension
skills. Here, we present MORABLES, a human-verified benchmark built from fables
and short stories drawn from historical literature. The main task is structured
as multiple-choice questions targeting moral inference, with carefully crafted
distractors that challenge models to go beyond shallow, extractive question
answering. To further stress-test model robustness, we introduce adversarial
variants designed to surface LLM vulnerabilities and shortcuts due to issues
such as data contamination. Our findings show that, while larger models
outperform smaller ones, they remain susceptible to adversarial manipulation
and often rely on superficial patterns rather than true moral reasoning. This
brittleness results in significant self-contradiction, with the best models
refuting their own answers in roughly 20% of cases depending on the framing of
the moral choice. Interestingly, reasoning-enhanced models fail to bridge this
gap, suggesting that scale - not reasoning ability - is the primary driver of
performance.

</details>


### [4] [LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.12382)
*Anu Pradhan,Alexandra Ortan,Apurv Verma,Madhavan Seshadri*

Main category: cs.CL

TL;DR: 本文研究在法学推荐系统中使用LLM作为评估者的可行性，发现传统评估指标在AI系统评估中存在误导性，提出了更可靠的评估指标和统计方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的兴起，传统评估指标无法捕捉法学等专业领域推荐系统的细微质量差异，需要探索LLM作为评估者的可靠性。

Method: 通过系统性实验，比较不同评估者间可靠性指标，分析传统指标在AI系统评估中的局限性，并测试Gwet's AC2和秩相关系数等替代指标的有效性。

Result: 发现Krippendorff's alpha等传统指标在AI系统评估的偏态分布中会产生误导，而Gwet's AC2和秩相关系数是更可靠的评估者选择指标，Wilcoxon Signed-Rank Test配合Benjamini-Hochberg校正确保了系统比较的统计严谨性。

Conclusion: LLM作为评估者提供了一条可扩展、成本效益高的评估路径，能够满足法学应用所需的精确度，将人力密集型瓶颈转变为自动化且统计严谨的评估框架。

Abstract: The evaluation bottleneck in recommendation systems has become particularly
acute with the rise of Generative AI, where traditional metrics fall short of
capturing nuanced quality dimensions that matter in specialized domains like
legal research. Can we trust Large Language Models to serve as reliable judges
of their own kind? This paper investigates LLM-as-a-Judge as a principled
approach to evaluating Retrieval-Augmented Generation systems in legal
contexts, where the stakes of recommendation quality are exceptionally high.
  We tackle two fundamental questions that determine practical viability: which
inter-rater reliability metrics best capture the alignment between LLM and
human assessments, and how do we conduct statistically sound comparisons
between competing systems? Through systematic experimentation, we discover that
traditional agreement metrics like Krippendorff's alpha can be misleading in
the skewed distributions typical of AI system evaluations. Instead, Gwet's AC2
and rank correlation coefficients emerge as more robust indicators for judge
selection, while the Wilcoxon Signed-Rank Test with Benjamini-Hochberg
corrections provides the statistical rigor needed for reliable system
comparisons.
  Our findings suggest a path toward scalable, cost-effective evaluation that
maintains the precision demanded by legal applications, transforming what was
once a human-intensive bottleneck into an automated, yet statistically
principled, evaluation framework.

</details>


### [5] [SENTRA: Selected-Next-Token Transformer for LLM Text Detection](https://arxiv.org/abs/2509.12385)
*Mitchell Plyler,Yilun Zhang,Alexander Tuzhilin,Saoud Khalifah,Sen Tian*

Main category: cs.CL

TL;DR: SENTRA是一种基于Transformer的监督式LLM文本检测器，通过选择下一个token概率序列和对比预训练，在跨域检测任务中显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着LLM能力增强和广泛应用，其被滥用的风险也在增加，需要检测未明确声明的LLM生成文本。

Method: 提出SENTRA模型，基于Transformer编码器，利用选择下一个token概率序列，并在大量无标签数据上进行对比预训练。

Result: 在三个流行公共数据集的24个文本域上实验表明，SENTRA在跨域设置下显著优于主流基线方法。

Conclusion: SENTRA是一个通用的分类器，能有效检测LLM生成的未声明文本，在跨域场景中表现优异。

Abstract: LLMs are becoming increasingly capable and widespread. Consequently, the
potential and reality of their misuse is also growing. In this work, we address
the problem of detecting LLM-generated text that is not explicitly declared as
such. We present a novel, general-purpose, and supervised LLM text detector,
SElected-Next-Token tRAnsformer (SENTRA). SENTRA is a Transformer-based encoder
leveraging selected-next-token-probability sequences and utilizing contrastive
pre-training on large amounts of unlabeled data. Our experiments on three
popular public datasets across 24 domains of text demonstrate SENTRA is a
general-purpose classifier that significantly outperforms popular baselines in
the out-of-domain setting.

</details>


### [6] [MORQA: Benchmarking Evaluation Metrics for Medical Open-Ended Question Answering](https://arxiv.org/abs/2509.12405)
*Wen-wai Yim,Asma Ben Abacha,Zixuan Yu,Robert Doerning,Fei Xia,Meliha Yetisgen*

Main category: cs.CL

TL;DR: MORQA是一个新的多语言医学问答基准测试，包含专家编写的多个参考答案和人工评分，用于评估NLG系统在医学领域的性能。研究发现基于LLM的评估方法显著优于传统指标。


<details>
  <summary>Details</summary>
Motivation: 医学领域的自然语言生成评估面临独特挑战，传统自动评估指标（如BLEU、ROUGE）在区分高质量输出方面存在不足，特别是在开放式医学问答任务中可能存在多个有效回答。

Method: 创建MORQA多语言基准测试，包含英语和中文的医学视觉和文本问答数据集，每个问题有2-4+个医学专家编写的参考答案和专家人工评分。比较传统指标和基于LLM（如GPT-4、Gemini）的评估方法。

Result: 基于LLM的评估方法在相关性方面显著优于传统指标，LLM对语义细微差别更敏感，对参考答案的变异性更具鲁棒性。

Conclusion: 这是医学领域首个全面的多语言NLG评估定性研究，强调了需要与人类判断对齐的评估方法。所有数据集和标注将公开发布以支持未来研究。

Abstract: Evaluating natural language generation (NLG) systems in the medical domain
presents unique challenges due to the critical demands for accuracy, relevance,
and domain-specific expertise. Traditional automatic evaluation metrics, such
as BLEU, ROUGE, and BERTScore, often fall short in distinguishing between
high-quality outputs, especially given the open-ended nature of medical
question answering (QA) tasks where multiple valid responses may exist. In this
work, we introduce MORQA (Medical Open-Response QA), a new multilingual
benchmark designed to assess the effectiveness of NLG evaluation metrics across
three medical visual and text-based QA datasets in English and Chinese. Unlike
prior resources, our datasets feature 2-4+ gold-standard answers authored by
medical professionals, along with expert human ratings for three English and
Chinese subsets. We benchmark both traditional metrics and large language model
(LLM)-based evaluators, such as GPT-4 and Gemini, finding that LLM-based
approaches significantly outperform traditional metrics in correlating with
expert judgments. We further analyze factors driving this improvement,
including LLMs' sensitivity to semantic nuances and robustness to variability
among reference answers. Our results provide the first comprehensive,
multilingual qualitative study of NLG evaluation in the medical domain,
highlighting the need for human-aligned evaluation methods. All datasets and
annotations will be publicly released to support future research.

</details>


### [7] [MedFact: Benchmarking the Fact-Checking Capabilities of Large Language Models on Chinese Medical Texts](https://arxiv.org/abs/2509.12440)
*Jiayi He,Yangmin Huang,Qianyun Du,Xiangying Zhou,Zhiyang He,Jiaxue Hu,Xiaodong Tao,Lixian Lai*

Main category: cs.CL

TL;DR: MedFact是一个新的中文医疗事实核查基准，包含2,116个专家标注实例，涵盖13个医学专业和8种错误类型，用于评估LLMs在医疗领域的真实可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基准在医疗领域的数据范围有限，无法捕捉真实世界医疗信息的复杂性，需要更全面的评估工具来确保LLMs在医疗应用中的事实可靠性。

Method: 采用混合AI-人类框架构建基准，通过迭代专家反馈优化AI驱动的多标准筛选过程，确保数据质量和难度。评估了20个领先LLMs在真实性分类和错误定位方面的性能。

Result: 研究发现模型虽然能判断文本是否包含错误，但精确定位错误仍具挑战性，顶级模型表现仍不及人类专家。还发现模型存在"过度批评"现象，常将正确信息误判为错误。

Conclusion: MedFact突显了在医疗应用中部署LLMs的关键挑战，为开发更事实可靠和医学感知的模型提供了强大资源。

Abstract: The increasing deployment of Large Language Models (LLMs) in healthcare
necessitates a rigorous evaluation of their factual reliability. However,
existing benchmarks are often limited by narrow domains of data, failing to
capture the complexity of real-world medical information. To address this
critical gap, we introduce MedFact, a new and challenging benchmark for Chinese
medical fact-checking. MedFact comprises 2,116 expert-annotated instances
curated from diverse real-world texts, spanning 13 medical specialties, 8
fine-grained error types, 4 writing styles, and multiple difficulty levels. Its
construction employs a hybrid AI-human framework where iterative expert
feedback refines an AI-driven, multi-criteria filtering process, ensuring both
high data quality and difficulty. We conduct a comprehensive evaluation of 20
leading LLMs, benchmarking their performance on veracity classification and
error localization against a human expert baseline. Our results reveal that
while models can often determine if a text contains an error, precisely
localizing it remains a substantial challenge, with even top-performing models
falling short of human performance. Furthermore, our analysis uncovers a
frequent ``over-criticism'' phenomenon, a tendency for models to misidentify
correct information as erroneous, which is exacerbated by advanced reasoning
techniques such as multi-agent collaboration and inference-time scaling. By
highlighting these critical challenges for deploying LLMs in medical
applications, MedFact provides a robust resource to drive the development of
more factually reliable and medically aware models.

</details>


### [8] [Topic Coverage-based Demonstration Retrieval for In-Context Learning](https://arxiv.org/abs/2509.12451)
*Wonbin Kweon,SeongKu Kang,Runchu Tian,Pengcheng Jiang,Jiawei Han,Hwanjo Yu*

Main category: cs.CL

TL;DR: TopicK是一个基于主题覆盖的检索框架，通过识别测试输入的主题需求并评估模型在这些主题上的知识水平，迭代选择能够覆盖模型知识不足主题的演示样本，提升上下文学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅基于嵌入相似度或生成概率检索演示样本，导致选择不相关或冗余的示例，无法全面覆盖测试输入所需的细粒度知识需求。

Method: 提出TopicK框架：1）估计输入所需的主题；2）评估模型在这些主题上的知识水平；3）迭代选择能够引入模型知识不足且尚未覆盖的主题的演示样本。

Result: 在多个数据集和开源/闭源大语言模型上的广泛实验验证了TopicK的有效性。

Conclusion: TopicK通过主题覆盖的方式选择演示样本，能够更全面地满足测试输入的知识需求，显著提升上下文学习性能。

Abstract: The effectiveness of in-context learning relies heavily on selecting
demonstrations that provide all the necessary information for a given test
input. To achieve this, it is crucial to identify and cover fine-grained
knowledge requirements. However, prior methods often retrieve demonstrations
based solely on embedding similarity or generation probability, resulting in
irrelevant or redundant examples. In this paper, we propose TopicK, a topic
coverage-based retrieval framework that selects demonstrations to
comprehensively cover topic-level knowledge relevant to both the test input and
the model. Specifically, TopicK estimates the topics required by the input and
assesses the model's knowledge on those topics. TopicK then iteratively selects
demonstrations that introduce previously uncovered required topics, in which
the model exhibits low topical knowledge. We validate the effectiveness of
TopicK through extensive experiments across various datasets and both open- and
closed-source LLMs. Our source code is available at
https://github.com/WonbinKweon/TopicK_EMNLP2025.

</details>


### [9] [Does Language Model Understand Language?](https://arxiv.org/abs/2509.12459)
*Suvojit Acharjee,Utathya Aich,Asfak Ali*

Main category: cs.CL

TL;DR: 本文评估了SOTA语言模型在英语和孟加拉语中对时态、否定、语态等精细语言现象的理解能力，发现Compound-Beta模型在跨语言场景中与人类判断最为一致。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在教育技术中的广泛应用，需要确保它们与人类语言理解保持一致，特别是在联合国可持续发展目标4的背景下，语言清晰度至关重要。

Method: 引入RECISE评估指南和LUCID数据集，使用皮尔逊相关系数、斯皮尔曼相关系数、平均绝对误差和新型HCE准确率等指标评估多个SOTA模型。

Result: Compound-Beta表现最均衡，在英语中获得最高皮尔逊相关系数，在混合语言数据上表现稳健，显示出与人类判断的强一致性。

Conclusion: 研究强调了语言模型在精细语言理解方面的挑战，Compound-Beta在跨语言场景中展现出最佳的人类对齐性能。

Abstract: Despite advances in natural language generation and understanding, LM still
struggle with fine grained linguistic phenomena such as tense, negation, voice,
and modality which are the elements central to effective human communication.
In the context of the United Nations SDG 4, where linguistic clarity is
critical, the deployment of LMs in educational technologies demands careful
scrutiny. As LMs are increasingly powering applications like tutoring systems,
automated grading, and translation, their alignment with human linguistic
interpretation becomes essential for effective learning. In this study, we
conduct a evaluation of SOTA language models across these challenging contexts
in both English and Bengali. To ensure a structured assessment, we introduce a
new Route for Evaluation of Cognitive Inference in Systematic Environments
guidelines. Our proposed LUCID dataset, composed of carefully crafted sentence
pairs in English and Bengali, specifically challenges these models on critical
aspects of language comprehension, including negation, tense, voice variations.
We assess the performance of SOTA models including MISTRAL-SABA-24B,
LLaMA-4-Scout-17B, LLaMA-3.3-70B, Gemma2-9B, and Compound-Beta using standard
metrics like Pearson correlation, Spearman correlation, and Mean Absolute
Error, as well as novel, linguistically inspired metric the HCE accuracy. The
HCE accuracy measures how often model predictions fall within one standard
deviation of the mean human rating, thus capturing human like tolerance for
variability in language interpretation. Our findings highlight Compound-Beta as
the most balanced model, consistently achieving high correlations and low MAEs
across diverse language conditions. It records the highest Pearson correlation
in English and demonstrates robust performance on mixed-language data,
indicating a strong alignment with human judgments in cross lingual scenarios.

</details>


### [10] [Audited Reasoning Refinement: Fine-Tuning Language Models via LLM-Guided Step-Wise Evaluation and Correction](https://arxiv.org/abs/2509.12476)
*Sumanta Bhattacharyya,Sara Riaz,Pedram Rooshenas*

Main category: cs.CL

TL;DR: R2tA是一种通过精炼大语言模型的推理轨迹来训练任务特定推理模型的方法，在数据稀缺领域提供可扩展的LLM适应方案


<details>
  <summary>Details</summary>
Motivation: 当直接人工监督或高质量标签稀缺时，训练任务特定的小型推理模型具有挑战性，但具备推理能力的LLMs产生的大量中间推理轨迹可以被系统精炼来创建有效的监督信号

Method: Reason-Refine-then-Align (R2tA)：首先生成基础模型在任务特定输入上的初始推理和响应，然后精炼这些轨迹以修复幻觉和不一致性，形成高质量数据集；进行两阶段对齐（监督微调SFT和直接偏好优化DPO）来校准模型的中间推理

Result: 在数据库系统设计的扩展实体关系图(EERD)评估任务上，R2tA相比仅提示方法能更好地检测错误而不产生幻觉，在包含600个EERD变体的数据集上表现出色

Conclusion: R2tA为数据稀缺领域的可扩展LLM适应提供了实用且成本效益高的路径，为教育等领域的可重现AI工具开发提供了可能

Abstract: Training a task-specific small reasoning model is challenging when direct
human supervision or high-quality labels are scarce. However, LLMs with
reasoning capabilities produce abundant intermediate reasoning traces that can
be systematically refined to create effective supervision signals. We propose
Reason-Refine-then-Align (R2tA), which turns refined model rationales into
supervision for training task-specific reasoning models. Our method generates
initial reasoning and responses from an open-source base model on task-specific
inputs, then refines these traces, fixing hallucinations and inconsistencies,
to form a high-fidelity dataset. We perform a two-stage alignment, supervised
fine-tuning (SFT), followed by direct preference optimization (DPO) to
calibrate the model's intermediate reasoning with human-validated conceptual
preferences and then condition the final output on that aligned reasoning. As a
case study, we apply R2tA to evaluate extended entity relationship diagrams
(EERDs) in database system design, a structurally complex task where
prompt-only methods miss or hallucinate errors. We curated a dataset of 600
EERD variants (train/test split of 450/150, respectively) with induced mistakes
spanning 11 categories. Empirical evaluation suggests R2tA provides a
practical, cost-effective path to scalable LLM adaptation in data-scarce
domains, enabling reproducible AI tools for education and beyond.

</details>


### [11] [FunAudio-ASR Technical Report](https://arxiv.org/abs/2509.12508)
*Keyu An,Yanni Chen,Chong Deng,Changfeng Gao,Zhifu Gao,Bo Gong,Xiangang Li,Yabin Li,Xiang Lv,Yunjie Ji,Yiheng Jiang,Bin Ma,Haoneng Luo,Chongjia Ni,Zexu Pan,Yiping Peng,Zhendong Peng,Peiyao Wang,Hao Wang,Wen Wang,Wupeng Wang,Biao Tian,Zhentao Tan,Nan Yang,Bin Yuan,Jieping Ye,Jixing Yu,Qinglin Zhang,Kun Zou,Han Zhao,Shengkui Zhao,Jingren Zhou*

Main category: cs.CL

TL;DR: FunAudio-ASR是一个基于大语言模型的大规模语音识别系统，通过数据扩展、模型规模扩展、LLM深度集成和强化学习的协同组合，在多样化复杂语音识别场景中实现最先进性能，并针对实际部署进行了专门优化。


<details>
  <summary>Details</summary>
Motivation: 虽然ASR技术近年来取得了显著进展，但大语言模型容易产生幻觉问题，这会严重影响实际ASR应用中的用户体验。现有LLM-based ASR系统在开源基准上表现良好，但在真实工业评估集上往往表现不佳。

Method: 结合海量数据、大模型容量、LLM集成和强化学习，专门优化了流式处理能力、噪声鲁棒性、语码转换、热词定制等实际应用需求。

Result: 实验结果显示，FunAudio-ASR在实际应用数据集上实现了SOTA性能，证明了其在真实场景中的有效性和鲁棒性。

Conclusion: FunAudio-ASR通过生产导向的优化，成功解决了LLM-based ASR系统在实际部署中的性能下降问题，为实际应用提供了高效可靠的语音识别解决方案。

Abstract: In recent years, automatic speech recognition (ASR) has witnessed
transformative advancements driven by three complementary paradigms: data
scaling, model size scaling, and deep integration with large language models
(LLMs). However, LLMs are prone to hallucination, which can significantly
degrade user experience in real-world ASR applications. In this paper, we
present FunAudio-ASR, a large-scale, LLM-based ASR system that synergistically
combines massive data, large model capacity, LLM integration, and reinforcement
learning to achieve state-of-the-art performance across diverse and complex
speech recognition scenarios. Moreover, FunAudio-ASR is specifically optimized
for practical deployment, with enhancements in streaming capability, noise
robustness, code-switching, hotword customization, and satisfying other
real-world application requirements. Experimental results show that while most
LLM-based ASR systems achieve strong performance on open-source benchmarks,
they often underperform on real industry evaluation sets. Thanks to
production-oriented optimizations, FunAudio-ASR achieves SOTA performance on
real application datasets, demonstrating its effectiveness and robustness in
practical settings.

</details>


### [12] [A comparison of pipelines for the translation of a low resource language based on transformers](https://arxiv.org/abs/2509.12514)
*Chiara Bonfanti,Michele Colombino,Giulia Coucourde,Faeze Memari,Stefano Pinardi,Rosa Meo*

Main category: cs.CL

TL;DR: 比较三种Bambara语机器翻译训练管道的性能，简单Transformer模型在低资源环境下表现最佳，达到10% BLEU和21% chrF分数


<details>
  <summary>Details</summary>
Motivation: 为非洲曼德语系的Bambara语（约1418万使用者）开发有效的机器翻译系统，解决低资源语言的翻译挑战

Method: 1) 简单Transformer法语到Bambara翻译；2) LLaMA3指导模型微调；3) LaBSE语言蒸馏+BERT扩展的双神经网络方法

Result: 简单Transformer管道在Bayelemagaba数据集上获得10% BLEU和21% chrF，在Yiri数据集上达到33.81% BLEU和41% chrF，表现最佳；指导模型在单一数据集上表现更好

Conclusion: 对于低资源语言翻译，简单的Transformer架构比复杂的指导模型和语言蒸馏方法更有效，指导模型更适合捕获特定数据集模式

Abstract: This work compares three pipelines for training transformer-based neural
networks to produce machine translators for Bambara, a Mand\`e language spoken
in Africa by about 14,188,850 people. The first pipeline trains a simple
transformer to translate sentences from French into Bambara. The second
fine-tunes LLaMA3 (3B-8B) instructor models using decoder-only architectures
for French-to-Bambara translation. Models from the first two pipelines were
trained with different hyperparameter combinations to improve BLEU and chrF
scores, evaluated on both test sentences and official Bambara benchmarks. The
third pipeline uses language distillation with a student-teacher dual neural
network to integrate Bambara into a pre-trained LaBSE model, which provides
language-agnostic embeddings. A BERT extension is then applied to LaBSE to
generate translations. All pipelines were tested on Dokotoro (medical) and
Bayelemagaba (mixed domains). Results show that the first pipeline, although
simpler, achieves the best translation accuracy (10% BLEU, 21% chrF on
Bayelemagaba), consistent with low-resource translation results. On the Yiri
dataset, created for this work, it achieves 33.81% BLEU and 41% chrF.
Instructor-based models perform better on single datasets than on aggregated
collections, suggesting they capture dataset-specific patterns more
effectively.

</details>


### [13] [MAGIC-Enhanced Keyword Prompting for Zero-Shot Audio Captioning with CLIP Models](https://arxiv.org/abs/2509.12591)
*Vijay Govindarajan,Pratik Patel,Sahil Tripathi,Md Azizul Hoque,Gautam Siddharth Kashyap*

Main category: cs.CL

TL;DR: 提出了一个零样本自动音频字幕生成系统，利用预训练模型避免大量训练，通过音频CLIP模型提取特征并生成结构化提示来指导LLM生成字幕，相比传统方法在NLG评分上提升35%


<details>
  <summary>Details</summary>
Motivation: 自动音频字幕生成面临数据集有限的问题，相比图像字幕数据集规模较小，需要克服这一限制

Method: 使用预训练音频CLIP模型提取听觉特征并生成结构化提示，指导大型语言模型生成字幕，通过音频CLIP模型优化token选择以确保与音频内容对齐

Result: 实验结果显示使用MAGIC搜索和WavCaps模型时，NLG平均得分从4.7提升到7.3，提升了35%。性能受音频-文本匹配模型和关键词选择影响很大，使用单个关键词提示效果最佳，无关键词列表时性能下降50%

Conclusion: 零样本方法在自动音频字幕生成中表现优异，预训练模型的结合使用显著提升了性能，关键词选择和音频-文本匹配是影响效果的关键因素

Abstract: Automated Audio Captioning (AAC) generates captions for audio clips but faces
challenges due to limited datasets compared to image captioning. To overcome
this, we propose the zero-shot AAC system that leverages pre-trained models,
eliminating the need for extensive training. Our approach uses a pre-trained
audio CLIP model to extract auditory features and generate a structured prompt,
which guides a Large Language Model (LLM) in caption generation. Unlike
traditional greedy decoding, our method refines token selection through the
audio CLIP model, ensuring alignment with the audio content. Experimental
results demonstrate a 35% improvement in NLG mean score (from 4.7 to 7.3) using
MAGIC search with the WavCaps model. The performance is heavily influenced by
the audio-text matching model and keyword selection, with optimal results
achieved using a single keyword prompt, and a 50% performance drop when no
keyword list is used.

</details>


### [14] [EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving](https://arxiv.org/abs/2509.12603)
*Mukai Li,Linfeng Song,Zhenwen Liang,Jiahao Xu,Shansan Gong,Qi Liu,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 本文提出EconProver方法，通过动态CoT切换机制和多样化并行强化学习，在保持定理证明性能的同时大幅降低计算成本至基线的12%


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在自动定理证明中采用反射式思维链推理和增加采样次数等策略，虽然性能提升但计算开销巨大，且现有成本分析忽略了不同扩展策略带来的采样成本差异

Method: 提出两种互补方法：1）动态CoT切换机制减少不必要的token消耗；2）多样化并行强化学习与可训练前缀，在有限采样次数下提高通过率。这些方法可集成到统一的EconRL流程中

Result: 在miniF2F和ProofNet数据集上的实验表明，EconProver仅用基线方法12%的计算成本就达到了可比性能

Conclusion: 这项工作为部署轻量级自动定理证明模型提供了可行方案，在不牺牲性能的前提下显著降低计算开销

Abstract: Large Language Models (LLMs) have recently advanced the field of Automated
Theorem Proving (ATP), attaining substantial performance gains through widely
adopted test-time scaling strategies, notably reflective Chain-of-Thought (CoT)
reasoning and increased sampling passes. However, they both introduce
significant computational overhead for inference. Moreover, existing cost
analyses typically regulate only the number of sampling passes, while
neglecting the substantial disparities in sampling costs introduced by
different scaling strategies. In this paper, we systematically compare the
efficiency of different test-time scaling strategies for ATP models and
demonstrate the inefficiency of the current state-of-the-art (SOTA) open-source
approaches. We then investigate approaches to significantly reduce token usage
and sample passes while maintaining the original performance. Specifically, we
propose two complementary methods that can be integrated into a unified EconRL
pipeline for amplified benefits: (1) a dynamic Chain-of-Thought (CoT) switching
mechanism designed to mitigate unnecessary token consumption, and (2) Diverse
parallel-scaled reinforcement learning (RL) with trainable prefixes to enhance
pass rates under constrained sampling passes. Experiments on miniF2F and
ProofNet demonstrate that our EconProver achieves comparable performance to
baseline methods with only 12% of the computational cost. This work provides
actionable insights for deploying lightweight ATP models without sacrificing
performance.

</details>


### [15] [Positional Encoding via Token-Aware Phase Attention](https://arxiv.org/abs/2509.12635)
*Yu,Wang,Sheng Shen,Rémi Munos,Hongyuan Zhan,Yuandong Tian*

Main category: cs.CL

TL;DR: RoPE存在距离依赖偏差限制长上下文建模能力，本文提出TAPA方法通过可学习相位函数改进位置编码，实现更好的长上下文处理效果。


<details>
  <summary>Details</summary>
Motivation: Rotary Positional Embedding (RoPE) 在注意力分数中引入了固有的距离依赖偏差，限制了其建模长上下文的能力。现有的RoPE扩展方法通常需要在预训练后进行后处理调整，如重新缩放或超参数调优。

Method: 提出Token-Aware Phase Attention (TAPA)，一种新的位置编码方法，将可学习的相位函数整合到注意力机制中。TAPA通过直接且轻量的微调即可扩展到更长上下文，并能外推到未见过的长度。

Result: TAPA在长上下文上获得了比RoPE系列方法显著更低的困惑度(perplexity)，能够保持长距离上的token交互。

Conclusion: TAPA是一种有效的长上下文位置编码解决方案，无需复杂的后处理调整，通过可学习相位函数显著提升了长序列建模性能。

Abstract: We prove under practical assumptions that Rotary Positional Embedding (RoPE)
introduces an intrinsic distance-dependent bias in attention scores that limits
RoPE's ability to model long-context. RoPE extension methods may alleviate this
issue, but they typically require post-hoc adjustments after pretraining, such
as rescaling or hyperparameters retuning. This paper introduces Token-Aware
Phase Attention (TAPA), a new positional encoding method that incorporates a
learnable phase function into the attention mechanism. TAPA preserves token
interactions over long range, extends to longer contexts with direct and light
fine-tuning, extrapolates to unseen lengths, and attains significantly lower
perplexity on long-context than RoPE families.

</details>


### [16] [PAC: Pronunciation-Aware Contextualized Large Language Model-based Automatic Speech Recognition](https://arxiv.org/abs/2509.12647)
*Li Fu,Yu Xin,Sunlu Zeng,Lu Fan,Youzheng Wu,Xiaodong He*

Main category: cs.CL

TL;DR: 提出发音感知上下文框架(PAC)，通过两阶段学习方法解决LLM-based ASR中的发音建模和同音词区分问题，显著降低词错误率


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在自动语音识别中发音建模不足和同音词区分困难的问题，特别是对生僻词和长尾词的识别

Method: 两阶段学习范式：1) 发音引导的上下文学习方法，采用交错的字素-音素上下文建模策略；2) 发音区分性强化学习方法，使用扰动标签采样

Result: 在Librispeech和AISHELL-1数据集上，WER相对降低30.2%和53.8%，长尾词的偏置WER相对降低31.8%和60.5%

Conclusion: PAC框架有效提升了LLM-based ASR系统的发音建模能力和同音词区分能力，显著改善了语音识别性能

Abstract: This paper presents a Pronunciation-Aware Contextualized (PAC) framework to
address two key challenges in Large Language Model (LLM)-based Automatic Speech
Recognition (ASR) systems: effective pronunciation modeling and robust
homophone discrimination. Both are essential for raw or long-tail word
recognition. The proposed approach adopts a two-stage learning paradigm. First,
we introduce a pronunciation-guided context learning method. It employs an
interleaved grapheme-phoneme context modeling strategy that incorporates
grapheme-only distractors, encouraging the model to leverage phonemic cues for
accurate recognition. Then, we propose a pronunciation-discriminative
reinforcement learning method with perturbed label sampling to further enhance
the model\'s ability to distinguish contextualized homophones. Experimental
results on the public English Librispeech and Mandarin AISHELL-1 datasets
indicate that PAC: (1) reduces relative Word Error Rate (WER) by 30.2% and
53.8% compared to pre-trained LLM-based ASR models, and (2) achieves 31.8% and
60.5% relative reductions in biased WER for long-tail words compared to strong
baselines, respectively.

</details>


### [17] [Don't Change My View: Ideological Bias Auditing in Large Language Models](https://arxiv.org/abs/2509.12652)
*Paul Kröger,Emilio Barkett*

Main category: cs.CL

TL;DR: 开发了一种检测大型语言模型意识形态偏见的方法，无需访问模型内部，通过分析主题相关提示下的输出分布变化来识别潜在的意识形态操控


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在数百万用户产品中的广泛应用，其输出可能影响个人信念和公共舆论。如果模型可以被有意引导到特定意识形态立场，控制者可能获得对公共话语的不当影响力

Method: 采用先前提出的统计方法，适应意识形态偏见审计的新背景。该方法与模型无关，不需要访问语言模型内部，而是通过分析主题相关提示下模型输出的分布变化来识别潜在的意识形态操控

Result: 通过一系列实验验证了该方法的有效性，证明了其实际适用性和支持对大语言模型行为进行独立事后审计的潜力

Conclusion: 该方法特别适合审计专有黑盒系统，为检测大语言模型意识形态操控提供了重要的第一步，有助于确保AI系统的透明度和问责制

Abstract: As large language models (LLMs) become increasingly embedded in products used
by millions, their outputs may influence individual beliefs and, cumulatively,
shape public opinion. If the behavior of LLMs can be intentionally steered
toward specific ideological positions, such as political or religious views,
then those who control these systems could gain disproportionate influence over
public discourse. Although it remains an open question whether LLMs can
reliably be guided toward coherent ideological stances and whether such
steering can be effectively prevented, a crucial first step is to develop
methods for detecting when such steering attempts occur. In this work, we adapt
a previously proposed statistical method to the new context of ideological bias
auditing. Our approach carries over the model-agnostic design of the original
framework, which does not require access to the internals of the language
model. Instead, it identifies potential ideological steering by analyzing
distributional shifts in model outputs across prompts that are thematically
related to a chosen topic. This design makes the method particularly suitable
for auditing proprietary black-box systems. We validate our approach through a
series of experiments, demonstrating its practical applicability and its
potential to support independent post hoc audits of LLM behavior.

</details>


### [18] [Mitigating Strategy Preference Bias in Emotional Support Conversation via Uncertainty Estimations](https://arxiv.org/abs/2509.12661)
*Yougen Zhou,Qin Chen,Ningning Zhou,Jie Zhou,Xingjiao Wu,Liang He*

Main category: cs.CL

TL;DR: 本文提出了一种通过强化学习缓解大语言模型在情感支持对话中策略规划偏好偏差的方法，通过双奖励函数优化策略规划的准确性和置信度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在情感支持对话中存在策略规划准确性低和特定策略偏好偏差的问题，现有方法未能深入分析这种偏差的根本原因。

Method: 首先揭示LLMs在策略规划中的知识边界，然后提出基于强化学习的双奖励函数方法，通过准确性和基于熵的置信度来优化策略规划。

Result: 在ESCov和ExTES数据集上的实验表明，该方法在多个LLM骨干网络上均优于基线方法。

Conclusion: 该方法有效缓解了LLMs在情感支持对话中的策略偏好偏差，提高了策略规划的效果。

Abstract: Emotional support conversation (ESC) aims to alleviate distress through
empathetic dialogue, yet large language models (LLMs) face persistent
challenges in delivering effective ESC due to low accuracy in strategy
planning. Moreover, there is a considerable preference bias towards specific
strategies. Prior methods using fine-tuned strategy planners have shown
potential in reducing such bias, while the underlying causes of the preference
bias in LLMs have not well been studied. To address these issues, we first
reveal the fundamental causes of the bias by identifying the knowledge
boundaries of LLMs in strategy planning. Then, we propose an approach to
mitigate the bias by reinforcement learning with a dual reward function, which
optimizes strategy planning via both accuracy and entropy-based confidence for
each region according to the knowledge boundaries. Experiments on the ESCov and
ExTES datasets with multiple LLM backbones show that our approach outperforms
the baselines, confirming the effectiveness of our approach.

</details>


### [19] [Chat-Driven Text Generation and Interaction for Person Retrieval](https://arxiv.org/abs/2509.12662)
*Zequn Xie,Chuxin Wang,Sihang Cai,Yeqiang Wang,Shulei Wang,Tao Jin*

Main category: cs.CL

TL;DR: 提出了MTG和MTI两个模块，通过多轮对话生成伪标签和优化查询，实现无需人工标注的文本行人搜索系统


<details>
  <summary>Details</summary>
Motivation: 解决文本行人搜索中高质量文本标注获取困难、成本高的问题，提升系统的可扩展性和实际部署能力

Method: 使用多轮文本生成(MTG)模块通过与大语言模型对话生成丰富的伪标签，以及多轮文本交互(MTI)模块在推理时通过对话式推理优化用户查询

Result: 在无需人工标注的情况下取得了竞争性或更优的检索结果，显著提升了检索准确性、鲁棒性和可用性

Conclusion: 该方法为文本行人搜索系统的规模化实际部署铺平了道路，解决了人工标注依赖的限制

Abstract: Text-based person search (TBPS) enables the retrieval of person images from
large-scale databases using natural language descriptions, offering critical
value in surveillance applications. However, a major challenge lies in the
labor-intensive process of obtaining high-quality textual annotations, which
limits scalability and practical deployment. To address this, we introduce two
complementary modules: Multi-Turn Text Generation (MTG) and Multi-Turn Text
Interaction (MTI). MTG generates rich pseudo-labels through simulated dialogues
with MLLMs, producing fine-grained and diverse visual descriptions without
manual supervision. MTI refines user queries at inference time through dynamic,
dialogue-based reasoning, enabling the system to interpret and resolve vague,
incomplete, or ambiguous descriptions - characteristics often seen in
real-world search scenarios. Together, MTG and MTI form a unified and
annotation-free framework that significantly improves retrieval accuracy,
robustness, and usability. Extensive evaluations demonstrate that our method
achieves competitive or superior results while eliminating the need for manual
captions, paving the way for scalable and practical deployment of TBPS systems.

</details>


### [20] [Towards Inclusive Toxic Content Moderation: Addressing Vulnerabilities to Adversarial Attacks in Toxicity Classifiers Tackling LLM-generated Content](https://arxiv.org/abs/2509.12672)
*Shaz Furniturewala,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: 本文提出基于机制解释性技术的新策略，通过识别毒性分类器中的脆弱组件来改善对抗攻击下的性能，发现不同人口群体存在不同的脆弱头，为开发更具包容性的毒性检测模型提供见解。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型生成的在线内容激增，传统基于人类文本训练的毒性分类器面临误分类问题，现有防御策略被动且依赖对抗训练或外部检测模型，需要更主动的解决方案。

Method: 使用机制解释性技术分析微调的BERT和RoBERTa分类器，通过对抗攻击技术识别脆弱电路，然后抑制这些脆弱电路来提升对抗攻击性能，并在多样化数据集上进行测试。

Result: 发现模型存在对性能关键或易受攻击的特定头，抑制脆弱头能提高对抗输入的性能，不同人口群体有不同的脆弱头负责漏洞。

Conclusion: 该方法能有效提升毒性分类器在对抗攻击下的鲁棒性，揭示了模型训练中的公平性和鲁棒性差距，为开发更具包容性的毒性检测模型提供了重要见解。

Abstract: The volume of machine-generated content online has grown dramatically due to
the widespread use of Large Language Models (LLMs), leading to new challenges
for content moderation systems. Conventional content moderation classifiers,
which are usually trained on text produced by humans, suffer from
misclassifications due to LLM-generated text deviating from their training data
and adversarial attacks that aim to avoid detection. Present-day defence
tactics are reactive rather than proactive, since they rely on adversarial
training or external detection models to identify attacks. In this work, we aim
to identify the vulnerable components of toxicity classifiers that contribute
to misclassification, proposing a novel strategy based on mechanistic
interpretability techniques. Our study focuses on fine-tuned BERT and RoBERTa
classifiers, testing on diverse datasets spanning a variety of minority groups.
We use adversarial attacking techniques to identify vulnerable circuits.
Finally, we suppress these vulnerable circuits, improving performance against
adversarial attacks. We also provide demographic-level insights into these
vulnerable circuits, exposing fairness and robustness gaps in model training.
We find that models have distinct heads that are either crucial for performance
or vulnerable to attack and suppressing the vulnerable heads improves
performance on adversarial input. We also find that different heads are
responsible for vulnerability across different demographic groups, which can
inform more inclusive development of toxicity detection models.

</details>


### [21] [Case-Based Decision-Theoretic Decoding with Quality Memories](https://arxiv.org/abs/2509.12677)
*Hiroyuki Deguchi,Masaaki Nagata*

Main category: cs.CL

TL;DR: 提出了基于案例的决策理论解码(CBDT)方法，通过使用领域数据样本来估计期望效用，解决了MBR解码在领域外知识获取上的局限性。


<details>
  <summary>Details</summary>
Motivation: MBR解码依赖于从文本生成模型中采样的文本，难以正确捕获领域外知识或信息，需要一种更好的方法来估计期望效用。

Method: 提出CBDT解码方法，利用领域数据示例来估计期望效用，并与MBR解码相结合使用。

Result: 在7个领域De-En和Ja↔En翻译任务以及MSCOCO和nocaps数据集的图像描述任务中，MBR与CBDT解码的组合优于单独的MBR解码。

Conclusion: CBDT解码不仅比MAP解码生成更高质量的文本，而且与MBR解码结合使用时能进一步提升性能，特别是在处理领域外知识时表现更优。

Abstract: Minimum Bayes risk (MBR) decoding is a decision rule of text generation,
which selects the hypothesis that maximizes the expected utility and robustly
generates higher-quality texts than maximum a posteriori (MAP) decoding.
However, it depends on sample texts drawn from the text generation model; thus,
it is difficult to find a hypothesis that correctly captures the knowledge or
information of out-of-domain. To tackle this issue, we propose case-based
decision-theoretic (CBDT) decoding, another method to estimate the expected
utility using examples of domain data. CBDT decoding not only generates
higher-quality texts than MAP decoding, but also the combination of MBR and
CBDT decoding outperformed MBR decoding in seven domain De--En and
Ja$\leftrightarrow$En translation tasks and image captioning tasks on MSCOCO
and nocaps datasets.

</details>


### [22] [HistoryBankQA: Multilingual Temporal Question Answering on Historical Events](https://arxiv.org/abs/2509.12720)
*Biswadip Mandal,Anant Khandelwal,Manish Gupta*

Main category: cs.CL

TL;DR: HistoryBank是一个多语言历史事件数据库，包含1000万+事件，覆盖10种语言，并构建了全面的时间推理问答基准来评估大语言模型的历史事件理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有时间推理数据集规模有限、缺乏多语言覆盖且主要关注当代事件，需要构建更全面的历史事件数据库来评估大语言模型的时间推理能力。

Method: 从维基百科时间线页面和文章信息框中提取历史事件，构建包含10种语言的1000万+事件数据库，并设计6种时间问答推理任务的基准测试。

Result: GPT4o在所有答案类型和语言中表现最佳，Gemma-2在小型语言模型中表现最优，验证了基准的有效性和模型性能差异。

Conclusion: HistoryBank为推进多语言和时间感知的自然语言理解提供了全面资源，代码和数据集将在论文接受后公开以促进进一步研究。

Abstract: Temporal reasoning about historical events is a critical skill for NLP tasks
like event extraction, historical entity linking, temporal question answering,
timeline summarization, temporal event clustering and temporal natural language
inference. Yet efforts on benchmarking temporal reasoning capabilities of large
language models (LLMs) are rather limited. Existing temporal reasoning datasets
are limited in scale, lack multilingual coverage and focus more on contemporary
events. To address these limitations, we present HistoryBank, a multilingual
database of 10M+ historical events extracted from Wikipedia timeline pages and
article infoboxes. Our database provides unprecedented coverage in both
historical depth and linguistic breadth with 10 languages. Additionally, we
construct a comprehensive question answering benchmark for temporal reasoning
across all languages. This benchmark covers a diverse set of 6 temporal QA
reasoning tasks, and we evaluate a suite of popular language models
(LLaMA-3-8B, Mistral-7B, Gemma-2-9b, Qwen3-8B, GPT4o) to assess their
performance on these tasks. As expected GPT4o performs best across all answer
types and languages; Gemma-2 outperforms the other small language models. Our
work aims to provide a comprehensive resource for advancing multilingual and
temporally-aware natural language understanding of historical events. To
facilitate further research, we will make our code and datasets publicly
available upon acceptance of this paper.

</details>


### [23] [Contrastive Learning with Enhanced Abstract Representations using Grouped Loss of Abstract Semantic Supervision](https://arxiv.org/abs/2509.12771)
*Omri Suissa,Muhiim Ali,Shengmai Chen,Yinuo Cai,Shekhar Pradhan*

Main category: cs.CL

TL;DR: 本文提出CLEAR GLASS模型，通过分组对比损失函数训练VLM模型，使其具备概念抽象能力，能够在未接触高层概念的情况下识别图像中的抽象概念。


<details>
  <summary>Details</summary>
Motivation: 研究视觉语言模型(VLM)是否具备人类般的概念抽象能力，以及如何通过编码高层概念信息来增强这种能力。

Method: 构建MAGIC分组图像-标题数据集，使用新颖的分组对比损失技术（外部对比损失和内部损失），迫使模型为每个图像-标题组创建接近高层概念语义表示的特征。

Result: 实验表明该方法训练的模型在抽象概念识别方面相比最先进模型有所提升。

Conclusion: 通过分组对比学习可以激发VLM的概念抽象能力，使其在没有直接接触高层概念的情况下学会识别抽象概念。

Abstract: Humans can recognize an image as an instance of a general concept, beyond
simply identifying its objects and their relationships. In this paper, we
investigate 1. The extent to which VLMs have this concept abstraction capacity,
and 2. Strategies for encoding the sort of higher-concept information in images
that would enable the resulting VLM model (CLEAR GLASS model) to have this
capability to a greater degree. To this end, we introduce a grouped
image-caption dataset (MAGIC), which consists of several groups of image
captions and for each group a set of associated images and higher-level
conceptual labels. We use a novel contrastive loss technique to induce the
model to encode in the representation of each image (caption) in a group the
information that is common to all members of the image-caption group. Our main
contribution is a grouped contrastive loss function based on text-image
contrastive groups (outer contrastive loss) as well as an inner loss which
measures the distances between image-caption instances in the group. Our
training methodology results in the CLEAR GLASS model having the concept
abstraction capacity as an emergent capacity because the model is not exposed
to the higher-level concepts associated with each group. Instead, the training
forces the model to create for each image-caption group a semantic
representation that brings it closer to the semantic representation of the
higher-level concepts in the latent semantic space. Our experiments show that
this training methodology results in a model which shows improvement in
abstract concept recognition compared to SOTA models.

</details>


### [24] [ConvergeWriter: Data-Driven Bottom-Up Article Construction](https://arxiv.org/abs/2509.12811)
*Binquan Ji,Jiaqi Wang,Ruiting Li,Xingchen Han,Yiyang Qi,Shichao Wang,Yifei Lu,Yuantao Han,Feiliang Ren*

Main category: cs.CL

TL;DR: 提出了一种新颖的"自下而上"数据驱动框架，通过"检索优先、聚类构建结构"策略，先建立知识边界再进行生成规划，有效解决长文本生成中的事实准确性和内容碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 现有"自上而下"方法在生成假设或大纲后再检索证据，容易导致模型计划与可用知识脱节，产生内容碎片化和事实不准确的问题。

Method: 采用"检索优先获取知识，聚类构建结构"策略，首先从知识库进行详尽迭代检索，然后使用无监督聚类算法将检索到的文档组织成不同的"知识簇"，这些簇为后续层次化大纲和最终文档内容的生成提供客观的数据驱动基础。

Result: 在14B和32B参数模型上的实验结果表明，该方法达到或超过了最先进基线的性能，在需要高保真度和结构连贯性的知识约束场景中展现出独特优势。

Conclusion: 该工作为生成可靠、结构化的长文档提供了有效范式，为在高风险、知识密集型领域中构建更稳健的LLM应用铺平了道路。

Abstract: Large Language Models (LLMs) have shown remarkable prowess in text
generation, yet producing long-form, factual documents grounded in extensive
external knowledge bases remains a significant challenge. Existing "top-down"
methods, which first generate a hypothesis or outline and then retrieve
evidence, often suffer from a disconnect between the model's plan and the
available knowledge, leading to content fragmentation and factual inaccuracies.
To address these limitations, we propose a novel "bottom-up," data-driven
framework that inverts the conventional generation pipeline. Our approach is
predicated on a "Retrieval-First for Knowledge, Clustering for Structure"
strategy, which first establishes the "knowledge boundaries" of the source
corpus before any generative planning occurs. Specifically, we perform
exhaustive iterative retrieval from the knowledge base and then employ an
unsupervised clustering algorithm to organize the retrieved documents into
distinct "knowledge clusters." These clusters form an objective, data-driven
foundation that directly guides the subsequent generation of a hierarchical
outline and the final document content. This bottom-up process ensures that the
generated text is strictly constrained by and fully traceable to the source
material, proactively adapting to the finite scope of the knowledge base and
fundamentally mitigating the risk of hallucination. Experimental results on
both 14B and 32B parameter models demonstrate that our method achieves
performance comparable to or exceeding state-of-the-art baselines, and is
expected to demonstrate unique advantages in knowledge-constrained scenarios
that demand high fidelity and structural coherence. Our work presents an
effective paradigm for generating reliable, structured, long-form documents,
paving the way for more robust LLM applications in high-stakes,
knowledge-intensive domains.

</details>


### [25] [Data Augmentation for Maltese NLP using Transliterated and Machine Translated Arabic Data](https://arxiv.org/abs/2509.12853)
*Kurt Micallef,Nizar Habash,Claudia Borg*

Main category: cs.CL

TL;DR: 阿拉伯语资源可以通过跨语言增强技术显著提升马耳他语NLP任务性能，包括新的音译方案和机器翻译方法


<details>
  <summary>Details</summary>
Motivation: 马耳他语作为受罗曼语和日耳曼语影响的闪族语言，与其最接近的阿拉伯语亲属存在正字法差异，研究如何利用阿拉伯语资源支持马耳他语NLP

Method: 探索多种阿拉伯语文本数据与马耳他语对齐策略，包括不同音译方案和机器翻译方法，并引入新的能更好表示马耳他语正字法的音译系统

Result: 阿拉伯语为基础的增强技术能显著提升马耳他语NLP任务性能

Conclusion: 阿拉伯语资源通过适当的跨语言增强技术可以有效支持马耳他语的自然语言处理

Abstract: Maltese is a unique Semitic language that has evolved under extensive
influence from Romance and Germanic languages, particularly Italian and
English. Despite its Semitic roots, its orthography is based on the Latin
script, creating a gap between it and its closest linguistic relatives in
Arabic. In this paper, we explore whether Arabic-language resources can support
Maltese natural language processing (NLP) through cross-lingual augmentation
techniques. We investigate multiple strategies for aligning Arabic textual data
with Maltese, including various transliteration schemes and machine translation
(MT) approaches. As part of this, we also introduce novel transliteration
systems that better represent Maltese orthography. We evaluate the impact of
these augmentations on monolingual and mutlilingual models and demonstrate that
Arabic-based augmentation can significantly benefit Maltese NLP tasks.

</details>


### [26] [The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via Hidden Representations](https://arxiv.org/abs/2509.12886)
*Yubo Zhu,Dongrui Liu,Zecheng Lin,Wei Tong,Sheng Zhong,Jing Shao*

Main category: cs.CL

TL;DR: 提出了一种基于LLM隐藏状态的新颖难度估计方法，无需生成输出token即可准确评估问题难度，并在自适应推理策略中实现更高效率


<details>
  <summary>Details</summary>
Motivation: 现有LLM难度估计方法依赖重复采样、辅助模型或微调，计算成本高且可能影响通用性，需要更高效准确的估计方案

Method: 将token级生成过程建模为马尔可夫链，定义价值函数来估计给定隐藏状态的预期输出质量，仅基于初始隐藏状态进行难度估计

Result: 在文本和多模态任务上的广泛实验表明，该方法在难度估计方面持续优于现有基线，并在自适应推理策略中实现了更高的推理效率

Conclusion: 该方法提供了一种计算高效且通用的难度估计方案，能够显著提升LLM推理效率，为自适应推理策略提供了有效指导

Abstract: Estimating the difficulty of input questions as perceived by large language
models (LLMs) is essential for accurate performance evaluation and adaptive
inference. Existing methods typically rely on repeated response sampling,
auxiliary models, or fine-tuning the target model itself, which may incur
substantial computational costs or compromise generality. In this paper, we
propose a novel approach for difficulty estimation that leverages only the
hidden representations produced by the target LLM. We model the token-level
generation process as a Markov chain and define a value function to estimate
the expected output quality given any hidden state. This allows for efficient
and accurate difficulty estimation based solely on the initial hidden state,
without generating any output tokens. Extensive experiments across both textual
and multimodal tasks demonstrate that our method consistently outperforms
existing baselines in difficulty estimation. Moreover, we apply our difficulty
estimates to guide adaptive reasoning strategies, including Self-Consistency,
Best-of-N, and Self-Refine, achieving higher inference efficiency with fewer
generated tokens.

</details>


### [27] [Conan-Embedding-v2: Training an LLM from Scratch for Text Embeddings](https://arxiv.org/abs/2509.12892)
*Shiyu Li,Yang Tang,Ruijie Liu,Shi-Zhe Chen,Xi Chen*

Main category: cs.CL

TL;DR: Conan-embedding-v2是一个14亿参数的语言模型，通过从零开始训练和专门针对嵌入任务的优化，在跨语言文本嵌入任务上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用LoRA微调现有LLMs，但存在数据和训练目标之间的差距问题。LLMs使用因果掩码和token级损失，而嵌入模型需要双向掩码和句子级损失，这种训练差距使得全微调效果不如LoRA。

Method: 1) 添加新闻数据和多语言对进行LLM预训练以弥合数据差距；2) 提出跨语言检索数据集；3) 引入软掩码机制在两种掩码类型之间平滑过渡；4) 提出动态硬负样本挖掘方法。

Result: 仅使用约14亿参数，Conan-embedding-v2在Massive Text Embedding Benchmark (MTEB)和中文MTEB上均取得了最先进的性能（截至2025年5月19日）。

Conclusion: 通过从零开始训练、软掩码机制和动态硬负样本挖掘等方法，可以有效解决LLMs与嵌入模型之间的数据和训练差距问题，实现高效的文本嵌入性能。

Abstract: Large language models (LLMs) have recently demonstrated excellent performance
in text embedding tasks. Previous work usually use LoRA to fine-tune existing
LLMs, which are limited by the data and training gap between LLMs and embedding
models. In this work, we introduce Conan-embedding-v2, a new 1.4B-parameter LLM
trained from scratch and fine-tuned as a text embedder. First, we add news data
and multilingual pairs for LLM pretraining to bridge the data gap. Based on
this, we propose a cross-lingual retrieval dataset that enables the LLM to
better integrate embeddings across different languages. Second, whereas LLMs
use a causal mask with token-level loss, embedding models use a bidirectional
mask with sentence-level loss. This training gap makes full fine-tuning less
effective than LoRA. We introduce a soft-masking mechanism to gradually
transition between these two types of masks, enabling the model to learn more
comprehensive representations. Based on this, we propose a dynamic hard
negative mining method that exposes the model to more difficult negative
examples throughout the training process. Being intuitive and effective, with
only approximately 1.4B parameters, Conan-embedding-v2 achieves SOTA
performance on both the Massive Text Embedding Benchmark (MTEB) and Chinese
MTEB (May 19, 2025).

</details>


### [28] [All Roads Lead to Rome: Graph-Based Confidence Estimation for Large Language Model Reasoning](https://arxiv.org/abs/2509.12908)
*Caiqi Zhang,Chang Shu,Ehsan Shareghi,Nigel Collier*

Main category: cs.CL

TL;DR: 提出基于图的无训练置信度估计方法，专门针对LLM推理任务，通过建模推理路径为有向图并利用图属性来提升置信度估计效果


<details>
  <summary>Details</summary>
Motivation: 现有置信度估计方法主要针对事实性QA任务，在推理任务上泛化能力不足，需要专门针对推理任务设计置信度估计方法

Method: 将推理路径建模为有向图，利用图的中心性、路径收敛性和路径权重等属性进行置信度估计，无需训练

Result: 在三个推理数据集和两个LLM上的实验表明，该方法在置信度估计和下游任务性能上均有提升

Conclusion: 图基置信度估计方法能有效提升LLM在推理任务中的可靠性部署

Abstract: Confidence estimation is essential for the reliable deployment of large
language models (LLMs). Existing methods are primarily designed for factual QA
tasks and often fail to generalize to reasoning tasks. To address this gap, we
propose a set of training-free, graph-based confidence estimation methods
tailored to reasoning tasks. Our approach models reasoning paths as directed
graphs and estimates confidence by exploiting graph properties such as
centrality, path convergence, and path weighting. Experiments with two LLMs on
three reasoning datasets demonstrate improved confidence estimation and
enhanced performance on two downstream tasks.

</details>


### [29] [Automated Generation of Research Workflows from Academic Papers: A Full-text Mining Framework](https://arxiv.org/abs/2509.12955)
*Heng Zhang,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 提出端到端框架，通过挖掘全文学术论文生成结构化研究流程，在NLP领域实现高精度工作流识别和可视化


<details>
  <summary>Details</summary>
Motivation: 现有方法只能提取碎片化流程组件，无法捕获完整研究流程，需要自动化生成研究流程以提高研究可重复性和加速"AI for Science"范式

Method: 段落中心方法：1) PU学习+SciBERT识别工作流描述段落 2) Flan-T5+提示学习生成工作流短语 3) ChatGPT+少样本学习分类短语 4) 映射到文档位置生成可视化流程图

Result: 工作流段落识别F1=0.9772；工作流短语生成ROUGE-1=0.4543/ROUGE-2=0.2877/ROUGE-L=0.4427；短语分类精度=0.958；成功分析NLP领域20年方法论变迁

Conclusion: 提供了经过验证的自动化工作流生成技术框架，为实证研究演化科学范式提供了新颖的过程导向视角

Abstract: The automated generation of research workflows is essential for improving the
reproducibility of research and accelerating the paradigm of "AI for Science".
However, existing methods typically extract merely fragmented procedural
components and thus fail to capture complete research workflows. To address
this gap, we propose an end-to-end framework that generates comprehensive,
structured research workflows by mining full-text academic papers. As a case
study in the Natural Language Processing (NLP) domain, our paragraph-centric
approach first employs Positive-Unlabeled (PU) Learning with SciBERT to
identify workflow-descriptive paragraphs, achieving an F1-score of 0.9772.
Subsequently, we utilize Flan-T5 with prompt learning to generate workflow
phrases from these paragraphs, yielding ROUGE-1, ROUGE-2, and ROUGE-L scores of
0.4543, 0.2877, and 0.4427, respectively. These phrases are then systematically
categorized into data preparation, data processing, and data analysis stages
using ChatGPT with few-shot learning, achieving a classification precision of
0.958. By mapping categorized phrases to their document locations in the
documents, we finally generate readable visual flowcharts of the entire
research workflows. This approach facilitates the analysis of workflows derived
from an NLP corpus and reveals key methodological shifts over the past two
decades, including the increasing emphasis on data analysis and the transition
from feature engineering to ablation studies. Our work offers a validated
technical framework for automated workflow generation, along with a novel,
process-oriented perspective for the empirical investigation of evolving
scientific paradigms. Source code and data are available at:
https://github.com/ZH-heng/research_workflow.

</details>


### [30] [Investigating ReLoRA: Effects on the Learning Dynamics of Small Language Models](https://arxiv.org/abs/2509.12960)
*Yuval Weiss,David Demitri Africa,Paula Buttery,Richard Diehl Martinez*

Main category: cs.CL

TL;DR: ReLoRA在小型语言模型(11M-66M参数)预训练中表现不如标准训练，性能差距随模型增大而扩大，低秩更新策略在SLM预训练中可能不适用


<details>
  <summary>Details</summary>
Motivation: 研究ReLoRA在小型语言模型预训练中的表现和学习动态，探索参数高效方法在低计算成本场景下的适用性

Method: 通过对11M-66M参数的小型语言模型进行系统性的ReLoRA预训练实验，评估损失、Paloma困惑度和BLiMP性能，并分析学习动态

Result: ReLoRA在所有评估指标上均表现不如标准训练，且性能差距随模型规模增大而扩大；分析显示ReLoRA会加剧小模型的秩缺陷问题

Conclusion: 低秩更新策略可能无法轻易迁移到小型语言模型预训练中，需要在低计算领域进行更多研究

Abstract: Parameter-efficient methods such as LoRA have revolutionised the fine-tuning
of LLMs. Still, their extension to pretraining via ReLoRA is less well
understood, especially for small language models (SLMs), which offer lower
computational and environmental costs. This work is the first systematic study
of ReLoRA in SLMs (11M-66M parameters), evaluating both performance and
learning dynamics. Through ablation experiments, we find that ReLoRA generally
performs worse than standard training on loss, Paloma perplexity and BLiMP,
with the gap widening for the larger models. Further analysis of the learning
dynamics of the models indicates that ReLoRA reinforces the rank deficiencies
found in smaller models. These results indicate that low-rank update strategies
may not transfer easily to SLM pretraining, highlighting the need for more
research in the low-compute regime.

</details>


### [31] [Do LLMs Understand Wine Descriptors Across Cultures? A Benchmark for Cultural Adaptations of Wine Reviews](https://arxiv.org/abs/2509.12961)
*Chenye Zou,Xingyue Wen,Tianyi Hu,Qian Janice Wang,Daniel Hershcovich*

Main category: cs.CL

TL;DR: 该研究提出了跨文化葡萄酒评论适应问题，构建了首个中英平行语料库，评估了机器翻译和LLM在文化适应性方面的表现，发现现有模型难以捕捉文化细微差别。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的发展为文化感知语言任务提供了可能，需要超越字面翻译，融入地区口味偏好和文化特定的风味描述符。

Method: 构建包含8k中文和16k英文评论的平行语料库，使用神经机器翻译基线和最先进LLM进行基准测试，提出三个文化导向评估标准进行人工评估。

Result: 分析表明当前模型难以捕捉文化细微差别，特别是在跨文化翻译葡萄酒描述时，凸显了翻译模型在处理文化内容方面的挑战和局限性。

Conclusion: 跨文化葡萄酒评论适应是一个具有挑战性的问题，现有翻译模型在文化内容处理方面存在明显局限，需要进一步改进文化感知能力。

Abstract: Recent advances in large language models (LLMs) have opened the door to
culture-aware language tasks. We introduce the novel problem of adapting wine
reviews across Chinese and English, which goes beyond literal translation by
incorporating regional taste preferences and culture-specific flavor
descriptors. In a case study on cross-cultural wine review adaptation, we
compile the first parallel corpus of professional reviews, containing 8k
Chinese and 16k Anglophone reviews. We benchmark both
neural-machine-translation baselines and state-of-the-art LLMs with automatic
metrics and human evaluation. For the latter, we propose three culture-oriented
criteria -- Cultural Proximity, Cultural Neutrality, and Cultural Genuineness
-- to assess how naturally a translated review resonates with target-culture
readers. Our analysis shows that current models struggle to capture cultural
nuances, especially in translating wine descriptions across different cultures.
This highlights the challenges and limitations of translation models in
handling cultural content.

</details>


### [32] [SitLLM: Large Language Models for Sitting Posture Health Understanding via Pressure Sensor Data](https://arxiv.org/abs/2509.12994)
*Jian Gao,Fufangchen Zhao,Yiyang Zhang,Danfeng Yan*

Main category: cs.CL

TL;DR: SitLLM是一个轻量级多模态框架，结合压力传感和大型语言模型，实现细粒度坐姿理解和个性化健康反馈生成。


<details>
  <summary>Details</summary>
Motivation: 现有坐姿监测系统识别粒度粗糙，缺乏语义表达能力，无法提供个性化反馈。不良坐姿是导致长期肌肉骨骼疾病和生理功能障碍的关键因素。

Method: 包含三个核心组件：1) 高斯鲁棒传感器嵌入模块，对压力图进行空间分块并注入局部噪声扰动；2) 提示驱动的跨模态对齐模块，通过多头交叉注意力将传感器嵌入重编程到LLM语义空间；3) 多上下文提示模块，融合特征级、结构级、统计级和语义级上下文信息。

Result: 论文提出了SitLLM框架，但未在摘要中明确说明具体实验结果。

Conclusion: SitLLM通过整合压力传感和LLM，能够实现更精细的坐姿理解和个性化的健康导向响应生成，解决了现有系统的局限性。

Abstract: Poor sitting posture is a critical yet often overlooked factor contributing
to long-term musculoskeletal disorders and physiological dysfunctions. Existing
sitting posture monitoring systems, although leveraging visual, IMU, or
pressure-based modalities, often suffer from coarse-grained recognition and
lack the semantic expressiveness necessary for personalized feedback. In this
paper, we propose \textbf{SitLLM}, a lightweight multimodal framework that
integrates flexible pressure sensing with large language models (LLMs) to
enable fine-grained posture understanding and personalized health-oriented
response generation. SitLLM comprises three key components: (1) a
\textit{Gaussian-Robust Sensor Embedding Module} that partitions pressure maps
into spatial patches and injects local noise perturbations for robust feature
extraction; (2) a \textit{Prompt-Driven Cross-Modal Alignment Module} that
reprograms sensor embeddings into the LLM's semantic space via multi-head
cross-attention using the pre-trained vocabulary embeddings; and (3) a
\textit{Multi-Context Prompt Module} that fuses feature-level, structure-level,
statistical-level, and semantic-level contextual information to guide
instruction comprehension.

</details>


### [33] [Multi-Model Synthetic Training for Mission-Critical Small Language Models](https://arxiv.org/abs/2509.13047)
*Nolan Platt,Pragyansmita Nayak*

Main category: cs.CL

TL;DR: 使用LLM作为一次性教师生成合成数据，而非直接用于推理，实现了261倍成本降低，使微调后的小模型在专业领域达到与大模型相当的准确率


<details>
  <summary>Details</summary>
Motivation: 解决专业领域训练数据稀缺和复杂的问题，降低大语言模型在专业领域应用的成本

Method: 利用GPT-4o和o3-mini多模型生成方法，将32亿条船舶跟踪记录转化为21,543个合成问答对，用于微调Qwen2.5-7B模型

Result: 微调后的模型在海事任务上达到75%准确率，成本比使用大模型推理大幅降低

Conclusion: 通过合成数据生成和适当微调，小模型可以在专业领域提供与大模型相似的准确性，为手动标注不可行的领域提供了可复现框架

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
many domains, yet their application to specialized fields remains constrained
by the scarcity and complexity of domain-specific training data. We present a
novel approach that achieves a 261x cost reduction for maritime intelligence by
using LLMs as one-time teachers rather than using them directly for inference.
Our method transforms 3.2 billion Automatic Identification System (AIS) vessel
tracking records into 21,543 synthetic question and answer pairs through
multi-model generation (GPT-4o and o3-mini), preventing overfitting and
ensuring accurate reasoning. The resulting fine-tuned Qwen2.5-7B model achieves
75% accuracy on maritime tasks, while being substantially cheaper than using a
larger model for inference. We show that smaller, cheaper models -- when fine
tuned properly -- can provide similar accuracy compared to larger models that
are prohibitively expensive. Our work contributes to the growing field of
synthetic dataset generation for specialized AI applications and presents a
highly reproducible framework for domains where manual annotation is
infeasible. Beyond expanding research in the growing field of specialized small
language models, our approach has immediate applications in maritime safety,
security operations, and vessel traffic management systems in various
industries.

</details>


### [34] [Shaping Explanations: Semantic Reward Modeling with Encoder-Only Transformers for GRPO](https://arxiv.org/abs/2509.13081)
*Francesco Pappone,Ruggero Marino Lazzaroni,Federico Califano,Niccolò Gentile,Roberto Marras*

Main category: cs.CL

TL;DR: 提出在GRPO框架中使用小型编码器变换器作为语义奖励模型，通过余弦相似度提供密集语义奖励信号，改善LLM生成解释的忠实性和清晰度


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成符合复杂定性目标（如教学合理性）的输出方面存在挑战，传统强化学习方法依赖缓慢的LLM评估或脆弱的基于关键词的指标，无法捕捉高质量解释的语义本质

Method: 在GRPO框架中使用小型高效的仅编码器变换器作为语义奖励模型，基于生成解释与真实参考之间的余弦相似度提供密集语义奖励信号，应用于意大利医学院入学考试任务

Result: 与强SFT基线相比，使用提出的语义奖励的GRPO显著提高了解释的忠实性和清晰度

Conclusion: 在复杂生成任务中使用轻量级编码器模型进行细致奖励塑造具有强大效果

Abstract: While Large Language Models (LLMs) excel at generating human-like text,
aligning their outputs with complex, qualitative goals like pedagogical
soundness remains a significant challenge. Standard reinforcement learning
techniques often rely on slow and expensive LLM-as-a-judge evaluations or on
brittle, keyword-based metrics like ROUGE, which fail to capture the semantic
essence of a high-quality explanation. In this work, we introduce a novel
approach to reward shaping within the Group Relative Policy Optimisation (GRPO)
framework. Our central contribution is the use of a small, efficient
encoder-only transformer as a semantic reward model. This model provides a
dense, semantically rich reward signal based on the cosine similarity between a
generated explanation and a ground-truth reference, guiding the policy towards
explanations that are not just factually correct but also structurally and
conceptually aligned with expert reasoning. We apply this method to the task of
training a model for the Italian medical-school entrance examinations,
following standard domain-adaptive continued pre-training (CPT) and supervised
fine-tuning (SFT). Our results demonstrate that GRPO with our proposed semantic
reward significantly improves explanation faithfulness and clarity over a
strong SFT baseline, showcasing the power of using lightweight encoder models
for nuanced reward shaping in complex generation tasks

</details>


### [35] [Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon Planning](https://arxiv.org/abs/2509.13127)
*Sijia Cui,Shuai Xu,Aiyao He,Yanna Wang,Bo Xu*

Main category: cs.CL

TL;DR: PLAP框架通过结合语言规划和参数化技能执行，解决了LLM智能体在长时域对抗环境中的落地问题，在MicroRTS游戏中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体方法要么难以生成可靠的低层动作，要么过度依赖专家经验来转换高层任务。需要一种新方法来有效解决长时域环境中的智能体落地问题。

Method: 提出PLAP（Plan with Language, Act with Parameter）框架，包含三个核心组件：环境特定的参数化技能库、LLM驱动的技能规划器、将参数化技能转换为可执行动作序列的技能执行器。

Result: GPT-4o驱动的PLAP在零样本设置下超越80%基线智能体，Qwen2-72B驱动的PLAP在精心设计的少样本示例下超越了顶级脚本智能体CoacAI。还建立了LLM长时域技能规划能力排行榜。

Conclusion: PLAP框架有效解决了LLM智能体在复杂长时域环境中的落地挑战，为LLM智能体的实际应用提供了可行的解决方案。

Abstract: Recent advancements in Large Language Models(LLMs) have led to the
development of LLM-based AI agents. A key challenge is the creation of agents
that can effectively ground themselves in complex, adversarial long-horizon
environments. Existing methods mainly focus on (1) using LLMs as policies to
interact with the environment through generating low-level feasible actions,
and (2) utilizing LLMs to generate high-level tasks or language guides to
stimulate action generation. However, the former struggles to generate reliable
actions, while the latter relies heavily on expert experience to translate
high-level tasks into specific action sequences. To address these challenges,
we introduce the Plan with Language, Act with Parameter (PLAP) planning
framework that facilitates the grounding of LLM-based agents in long-horizon
environments. The PLAP method comprises three key components: (1) a skill
library containing environment-specific parameterized skills, (2) a skill
planner powered by LLMs, and (3) a skill executor converting the parameterized
skills into executable action sequences. We implement PLAP in MicroRTS, a
long-horizon real-time strategy game that provides an unfamiliar and
challenging environment for LLMs. The experimental results demonstrate the
effectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot setting
outperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefully
crafted few-shot examples, surpasses the top-tier scripted agent, CoacAI.
Additionally, we design comprehensive evaluation metrics and test 6
closed-source and 2 open-source LLMs within the PLAP framework, ultimately
releasing an LLM leaderboard ranking long-horizon skill planning ability. Our
code is available at https://github.com/AI-Research-TeamX/PLAP.

</details>


### [36] [LLM Hallucination Detection: A Fast Fourier Transform Method Based on Hidden Layer Temporal Signals](https://arxiv.org/abs/2509.13154)
*Jinxin Li,Gang Tu,ShengYu Cheng,Junjie Hu,Jinting Wang,Rui Chen,Zhilong Zhou,Dongbo Shan*

Main category: cs.CL

TL;DR: HSAD是一种基于隐藏信号分析的幻觉检测框架，通过建模自回归生成过程中隐藏表示的时间动态，在频域分析中提取最强非直流频率分量作为特征，在多个基准测试中比现有方法提升10个百分点以上。


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉检测方法存在局限性：事实性检查受外部知识覆盖范围限制，静态隐藏状态分析无法捕捉推理动态中的偏差。需要一种更有效和鲁棒的检测方法。

Method: HSAD框架通过采样各层激活构建隐藏层信号，应用快速傅里叶变换(FFT)获得频域表示，提取最强非直流频率分量作为频谱特征，并利用LLMs的自回归特性确定最佳观测点。

Result: 在包括TruthfulQA在内的多个基准测试中，HSAD相比现有最先进方法实现了超过10个百分点的改进。

Conclusion: 通过将推理过程建模与频域分析相结合，HSAD为LLMs中的鲁棒幻觉检测建立了新范式。

Abstract: Hallucination remains a critical barrier for deploying large language models
(LLMs) in reliability-sensitive applications. Existing detection methods
largely fall into two categories: factuality checking, which is fundamentally
constrained by external knowledge coverage, and static hidden-state analysis,
that fails to capture deviations in reasoning dynamics. As a result, their
effectiveness and robustness remain limited. We propose HSAD (Hidden Signal
Analysis-based Detection), a novel hallucination detection framework that
models the temporal dynamics of hidden representations during autoregressive
generation. HSAD constructs hidden-layer signals by sampling activations across
layers, applies Fast Fourier Transform (FFT) to obtain frequency-domain
representations, and extracts the strongest non-DC frequency component as
spectral features. Furthermore, by leveraging the autoregressive nature of
LLMs, HSAD identifies optimal observation points for effective and reliable
detection. Across multiple benchmarks, including TruthfulQA, HSAD achieves over
10 percentage points improvement compared to prior state-of-the-art methods. By
integrating reasoning-process modeling with frequency-domain analysis, HSAD
establishes a new paradigm for robust hallucination detection in LLMs.

</details>


### [37] [The Few-shot Dilemma: Over-prompting Large Language Models](https://arxiv.org/abs/2509.13196)
*Yongjian Tang,Doruk Tuncel,Christian Koerner,Thomas Runkler*

Main category: cs.CL

TL;DR: 研究发现过度提示（over-prompting）现象：在大型语言模型的少样本学习中，过多的领域特定示例反而会降低性能，挑战了传统认为更多相关示例总是有益的认知。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在少样本学习中的过提示现象，挑战传统认为更多相关示例总是有益的观点，特别是在软件工程需求分析等实际应用中。

Method: 使用三种标准少样本选择方法（随机采样、语义嵌入、TF-IDF向量）构建提示框架，在多个LLM（GPT-4o、GPT-3.5-turbo等）上进行评估，通过逐步增加TF-IDF选择的分层少样本示例数量来寻找最优数量。

Result: 实验发现过度添加领域特定示例会降低某些LLM性能，通过组合方法用更少示例实现更好性能，在功能和非功能需求分类任务上超越现有最佳方法1%。

Conclusion: 过提示是LLM少样本学习中的重要问题，需要谨慎选择示例数量，TF-IDF分层选择方法能有效避免此问题并提升性能。

Abstract: Over-prompting, a phenomenon where excessive examples in prompts lead to
diminished performance in Large Language Models (LLMs), challenges the
conventional wisdom about in-context few-shot learning. To investigate this
few-shot dilemma, we outline a prompting framework that leverages three
standard few-shot selection methods - random sampling, semantic embedding, and
TF-IDF vectors - and evaluate these methods across multiple LLMs, including
GPT-4o, GPT-3.5-turbo, DeepSeek-V3, Gemma-3, LLaMA-3.1, LLaMA-3.2, and Mistral.
Our experimental results reveal that incorporating excessive domain-specific
examples into prompts can paradoxically degrade performance in certain LLMs,
which contradicts the prior empirical conclusion that more relevant few-shot
examples universally benefit LLMs. Given the trend of LLM-assisted software
engineering and requirement analysis, we experiment with two real-world
software requirement classification datasets. By gradually increasing the
number of TF-IDF-selected and stratified few-shot examples, we identify their
optimal quantity for each LLM. This combined approach achieves superior
performance with fewer examples, avoiding the over-prompting problem, thus
surpassing the state-of-the-art by 1% in classifying functional and
non-functional requirements.

</details>


### [38] [Evaluating LLM Alignment on Personality Inference from Real-World Interview Data](https://arxiv.org/abs/2509.13244)
*Jianfeng Zhu,Julina Maharjan,Xinyu Li,Karin G. Coifman,Ruoming Jin*

Main category: cs.CL

TL;DR: LLM在人格特质评估方面表现有限，与真实人格评估的相关性低于0.26，提示当前模型在心理理解方面仍有挑战


<details>
  <summary>Details</summary>
Motivation: 随着LLM在情感支持和决策辅助等需要心理理解的应用中部署，需要评估其在生态有效对话环境中解释人类人格特质的能力

Method: 使用包含半结构化访谈记录和连续大五人格特质分数的基准数据集，评估了三种范式：零样本和思维链提示、LoRA微调、以及使用预训练嵌入的回归分析

Result: 所有模型预测与真实人格特质之间的皮尔逊相关系数均低于0.26，思维链提示相比零样本提示仅有微小改进

Conclusion: 当前LLM与验证心理构念的对齐程度有限，人格推断更依赖于潜在语义表示而非显式推理，需要未来在特质特定提示、情境感知建模和对齐导向微调方面的工作

Abstract: Large Language Models (LLMs) are increasingly deployed in roles requiring
nuanced psychological understanding, such as emotional support agents,
counselors, and decision-making assistants. However, their ability to interpret
human personality traits, a critical aspect of such applications, remains
unexplored, particularly in ecologically valid conversational settings. While
prior work has simulated LLM "personas" using discrete Big Five labels on
social media data, the alignment of LLMs with continuous, ground-truth
personality assessments derived from natural interactions is largely
unexamined. To address this gap, we introduce a novel benchmark comprising
semi-structured interview transcripts paired with validated continuous Big Five
trait scores. Using this dataset, we systematically evaluate LLM performance
across three paradigms: (1) zero-shot and chain-of-thought prompting with
GPT-4.1 Mini, (2) LoRA-based fine-tuning applied to both RoBERTa and Meta-LLaMA
architectures, and (3) regression using static embeddings from pretrained BERT
and OpenAI's text-embedding-3-small. Our results reveal that all Pearson
correlations between model predictions and ground-truth personality traits
remain below 0.26, highlighting the limited alignment of current LLMs with
validated psychological constructs. Chain-of-thought prompting offers minimal
gains over zero-shot, suggesting that personality inference relies more on
latent semantic representation than explicit reasoning. These findings
underscore the challenges of aligning LLMs with complex human attributes and
motivate future work on trait-specific prompting, context-aware modeling, and
alignment-oriented fine-tuning.

</details>


### [39] [ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking Guided Attention Refinement](https://arxiv.org/abs/2509.13282)
*Ali Salamatian,Amirhossein Abaskohi,Wan-Cyuan Fan,Mir Rayat Imtiaz Hossain,Leonid Sigal,Giuseppe Carenini*

Main category: cs.CL

TL;DR: ChartGaze是一个新的眼动追踪数据集，通过比较人类和模型注意力模式，发现大型视觉语言模型在图表问答任务中注意力常偏离人类注视模式，导致准确性和可解释性下降。提出了基于人类注视的注意力优化方法，在多个模型上实现了最高2.56个百分点的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 图表是重要的信息可视化媒介，但现有大型视觉语言模型在图表问答任务中仍面临挑战，特别是模型注意力常集中在图表的不相关区域，导致性能受限。

Method: 构建ChartGaze眼动追踪数据集记录人类在图表推理任务中的注视模式；通过系统比较人类和模型注意力差异；提出基于人类注视的注意力优化方法，使图像-文本注意力与人类注视点对齐。

Result: 方法显著提高了答案准确性和注意力对齐度，在多个模型上实现了最高2.56个百分点的准确率提升，同时增强了模型的可解释性。

Conclusion: 研究表明融入人类注视信息可以有效提升图表导向大型视觉语言模型的推理质量和可解释性，为未来模型优化提供了有前景的方向。

Abstract: Charts are a crucial visual medium for communicating and representing
information. While Large Vision-Language Models (LVLMs) have made progress on
chart question answering (CQA), the task remains challenging, particularly when
models attend to irrelevant regions of the chart. In this work, we present
ChartGaze, a new eye-tracking dataset that captures human gaze patterns during
chart reasoning tasks. Through a systematic comparison of human and model
attention, we find that LVLMs often diverge from human gaze, leading to reduced
interpretability and accuracy. To address this, we propose a gaze-guided
attention refinement that aligns image-text attention with human fixations. Our
approach improves both answer accuracy and attention alignment, yielding gains
of up to 2.56 percentage points across multiple models. These results
demonstrate the promise of incorporating human gaze to enhance both the
reasoning quality and interpretability of chart-focused LVLMs.

</details>


### [40] [WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents](https://arxiv.org/abs/2509.13309)
*Zile Qiao,Guoxin Chen,Xuanzhong Chen,Donglei Yu,Wenbiao Yin,Xinyu Wang,Zhen Zhang,Baixuan Li,Huifeng Yin,Kuan Li,Rui Min,Minpeng Liao,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: WebResearcher是一个通过马尔可夫决策过程构建AI研究代理的新框架，包含迭代深度研究范式和可扩展数据合成引擎，在6个基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 解决现有单上下文方法中的上下文窒息和噪声污染问题，构建能够自主发现和合成知识的AI代理

Method: 1) WebResearcher：将深度研究重新构建为马尔可夫决策过程，定期整合发现到演进报告中；2) WebFrontier：通过工具增强的复杂性升级生成高质量训练数据

Result: 在6个具有挑战性的基准测试中实现最先进性能，甚至超越前沿专有系统，训练数据显著增强工具使用能力

Conclusion: 该框架通过并行思维自然扩展，支持并发多智能体探索以获得更全面的结论，为构建自主研究代理提供了有效解决方案

Abstract: Recent advances in deep-research systems have demonstrated the potential for
AI agents to autonomously discover and synthesize knowledge from external
sources. In this paper, we introduce WebResearcher, a novel framework for
building such agents through two key components: (1) WebResearcher, an
iterative deep-research paradigm that reformulates deep research as a Markov
Decision Process, where agents periodically consolidate findings into evolving
reports while maintaining focused workspaces, overcoming the context
suffocation and noise contamination that plague existing mono-contextual
approaches; and (2) WebFrontier, a scalable data synthesis engine that
generates high-quality training data through tool-augmented complexity
escalation, enabling systematic creation of research tasks that bridge the gap
between passive knowledge recall and active knowledge construction. Notably, we
find that the training data from our paradigm significantly enhances tool-use
capabilities even for traditional mono-contextual methods. Furthermore, our
paradigm naturally scales through parallel thinking, enabling concurrent
multi-agent exploration for more comprehensive conclusions. Extensive
experiments across 6 challenging benchmarks demonstrate that WebResearcher
achieves state-of-the-art performance, even surpassing frontier proprietary
systems.

</details>


### [41] [Scaling Agents via Continual Pre-training](https://arxiv.org/abs/2509.13310)
*Liangcai Su,Zhen Zhang,Guangyu Li,Zhuo Chen,Chenxi Wang,Maojia Song,Xinyu Wang,Kuan Li,Jialong Wu,Xuanzhong Chen,Zile Qiao,Zhongwang Zhang,Huifeng Yin,Shihao Cai,Runnan Fang,Zhengwei Tao,Wenbiao Yin,Chenxiong Qian,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 论文提出Agentic Continual Pre-training方法，通过构建强大的智能体基础模型来解决现有方法在智能体任务中表现不佳的问题，开发了AgentFounder模型并在多个基准测试中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于通用基础模型的训练后方法在智能体任务中表现不佳，主要原因是缺乏强大的智能体基础模型，导致模型需要同时学习多样化的智能体行为并与专家演示对齐，造成优化冲突。

Method: 提出Agentic Continual Pre-training方法，将其整合到深度研究智能体训练流程中，构建强大的智能体基础模型，并开发了AgentFounder-30B模型。

Result: 在10个基准测试中实现最先进性能，BrowseComp-en达到39.9%，BrowseComp-zh达到43.3%，HLE Pass@1达到31.5%，同时保持强大的工具使用能力。

Conclusion: Agentic CPT方法有效解决了智能体基础模型缺失的问题，AgentFounder模型在多个智能体任务基准上展现了卓越性能，为智能体系统发展提供了新的解决方案。

Abstract: Large language models (LLMs) have evolved into agentic systems capable of
autonomous tool use and multi-step reasoning for complex problem-solving.
However, post-training approaches building upon general-purpose foundation
models consistently underperform in agentic tasks, particularly in open-source
implementations. We identify the root cause: the absence of robust agentic
foundation models forces models during post-training to simultaneously learn
diverse agentic behaviors while aligning them to expert demonstrations, thereby
creating fundamental optimization tensions. To this end, we are the first to
propose incorporating Agentic Continual Pre-training (Agentic CPT) into the
deep research agents training pipeline to build powerful agentic foundational
models. Based on this approach, we develop a deep research agent model named
AgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve
state-of-the-art performance while retains strong tool-use ability, notably
39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.

</details>


### [42] [Towards General Agentic Intelligence via Environment Scaling](https://arxiv.org/abs/2509.13311)
*Runnan Fang,Shihao Cai,Baixuan Li,Jialong Wu,Guangyu Li,Wenbiao Yin,Xinyu Wang,Xiaobin Wang,Liangcai Su,Zhen Zhang,Shibin Wu,Zhengwei Tao,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 提出了一个可扩展的环境构建框架AgentScaler，通过自动生成异构模拟环境来增强大语言模型的函数调用能力，采用两阶段微调策略显著提升了智能体性能


<details>
  <summary>Details</summary>
Motivation: 现实世界API需要精确、鲁棒的函数调用智能，而智能体能力的广度与其训练环境的多样性密切相关。为了解决环境规模化构建和有效训练两大核心挑战

Method: 设计可扩展框架自动构建完全模拟的异构环境，采用两阶段微调策略：先赋予基础智能体能力，再进行领域特化

Result: 在tau-bench、tau2-Bench和ACEBench等智能体基准测试中，AgentScaler模型显著增强了模型的函数调用能力

Conclusion: 通过规模化环境构建和两阶段训练策略，能够有效提升大语言模型在实际应用中的智能体能力

Abstract: Advanced agentic intelligence is a prerequisite for deploying Large Language
Models in practical, real-world applications. Diverse real-world APIs demand
precise, robust function-calling intelligence, which needs agents to develop
these capabilities through interaction in varied environments. The breadth of
function-calling competence is closely tied to the diversity of environments in
which agents are trained. In this work, we scale up environments as a step
towards advancing general agentic intelligence. This gives rise to two central
challenges: (i) how to scale environments in a principled manner, and (ii) how
to effectively train agentic capabilities from experiences derived through
interactions with these environments. To address these, we design a scalable
framework that automatically constructs heterogeneous environments that are
fully simulated, systematically broadening the space of function-calling
scenarios. We further adapt a two-phase agent fine-tuning strategy: first
endowing agents with fundamental agentic capabilities, then specializing them
for domain-specific contexts. Extensive experiments on agentic benchmarks,
tau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model,
AgentScaler, significantly enhances the function-calling capability of models.

</details>


### [43] [WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research](https://arxiv.org/abs/2509.13312)
*Zijian Li,Xin Guan,Bo Zhang,Shen Huang,Houquan Zhou,Shaopeng Lai,Ming Yan,Yong Jiang,Pengjun Xie,Fei Huang,Jun Zhang,Jingren Zhou*

Main category: cs.CL

TL;DR: WebWeaver是一个双智能体框架，通过模拟人类研究过程解决开放深度研究问题，采用动态规划和分层检索写作方法，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 当前开放深度研究方法存在静态研究流程和一次性生成范式的双重限制，导致长上下文失败问题和幻觉问题，需要更接近人类研究过程的解决方案。

Method: 提出双智能体框架：规划器动态循环迭代证据获取和提纲优化，生成基于证据的全面提纲；写作者执行分层检索和写作过程，逐部分撰写报告。

Result: 在DeepResearch Bench、DeepConsult和DeepResearchGym等主要OEDR基准测试中建立了新的最先进水平。

Conclusion: 自适应规划和聚焦合成对于生成高质量、可靠且结构良好的报告至关重要，验证了以人为中心的迭代方法论的优越性。

Abstract: This paper tackles open-ended deep research (OEDR), a complex challenge where
AI agents must synthesize vast web-scale information into insightful reports.
Current approaches are plagued by dual-fold limitations: static research
pipelines that decouple planning from evidence acquisition and one-shot
generation paradigms that easily suffer from long-context failure issues like
"loss in the middle" and hallucinations. To address these challenges, we
introduce WebWeaver, a novel dual-agent framework that emulates the human
research process. The planner operates in a dynamic cycle, iteratively
interleaving evidence acquisition with outline optimization to produce a
comprehensive, source-grounded outline linking to a memory bank of evidence.
The writer then executes a hierarchical retrieval and writing process,
composing the report section by section. By performing targeted retrieval of
only the necessary evidence from the memory bank for each part, it effectively
mitigates long-context issues. Our framework establishes a new state-of-the-art
across major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and
DeepResearchGym. These results validate our human-centric, iterative
methodology, demonstrating that adaptive planning and focused synthesis are
crucial for producing high-quality, reliable, and well-structured reports.

</details>


### [44] [ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization](https://arxiv.org/abs/2509.13313)
*Xixi Wu,Kuan Li,Yida Zhao,Liwen Zhang,Litu Ou,Huifeng Yin,Zhongwang Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Minhao Cheng,Shuai Wang,Hong Cheng,Jingren Zhou*

Main category: cs.CL

TL;DR: ReSum是一种新的LLM网络代理范式，通过周期性上下文摘要解决ReAct等方法的上下文窗口限制问题，实现无限探索能力


<details>
  <summary>Details</summary>
Motivation: 基于LLM的网络代理在知识密集型任务上表现良好，但受到上下文窗口限制。复杂查询涉及多个实体、交织关系和高度不确定性，需要大量搜索周期，在达到完整解决方案之前就耗尽了上下文预算

Method: 引入ReSum范式，通过定期上下文摘要将不断增长的交互历史转换为紧凑的推理状态，保持对先前发现的认知同时绕过上下文约束。提出ReSum-GRPO，整合GRPO与分段轨迹训练和优势广播，使代理适应基于摘要的推理

Result: 在三个基准测试中，ReSum相比ReAct平均绝对提升4.5%，经过ReSum-GRPO训练后进一步提升达8.2%。WebResummer-30B在BrowseComp-zh上达到33.3% Pass@1，在BrowseComp-en上达到18.3%，超越现有开源网络代理

Conclusion: ReSum通过上下文摘要机制有效解决了LLM网络代理的上下文窗口限制问题，实现了更好的性能和无限探索能力，为复杂网络任务提供了新的解决方案

Abstract: Large Language Model (LLM)-based web agents demonstrate strong performance on
knowledge-intensive tasks but are hindered by context window limitations in
paradigms like ReAct. Complex queries involving multiple entities, intertwined
relationships, and high uncertainty demand extensive search cycles that rapidly
exhaust context budgets before reaching complete solutions. To overcome this
challenge, we introduce ReSum, a novel paradigm that enables indefinite
exploration through periodic context summarization. ReSum converts growing
interaction histories into compact reasoning states, maintaining awareness of
prior discoveries while bypassing context constraints. For paradigm adaptation,
we propose ReSum-GRPO, integrating GRPO with segmented trajectory training and
advantage broadcasting to familiarize agents with summary-conditioned
reasoning. Extensive experiments on web agents of varying scales across three
benchmarks demonstrate that ReSum delivers an average absolute improvement of
4.5\% over ReAct, with further gains of up to 8.2\% following ReSum-GRPO
training. Notably, with only 1K training samples, our WebResummer-30B (a
ReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\% Pass@1 on
BrowseComp-zh and 18.3\% on BrowseComp-en, surpassing existing open-source web
agents.

</details>


### [45] [Do Natural Language Descriptions of Model Activations Convey Privileged Information?](https://arxiv.org/abs/2509.13316)
*Millicent Li,Alberto Mario Ceballos Arroyo,Giordano Rogers,Naomi Saphra,Byron C. Wallace*

Main category: cs.CL

TL;DR: 该研究批判性评估了LLM激活解释方法，发现现有基准测试存在缺陷，解释结果往往反映了解释器LLM的参数知识而非目标模型的真实内部表示。


<details>
  <summary>Details</summary>
Motivation: 验证激活解释方法是否能真正揭示目标LLM的内部工作机制，还是仅仅反映了输入信息或解释器LLM的固有知识。

Method: 在先前工作使用的数据集上评估流行解释方法，进行对照实验分析解释结果的信息来源。

Result: 解释方法在无目标模型内部信息的情况下也能通过基准测试，解释结果主要反映解释器LLM的参数知识而非目标模型的激活模式。

Conclusion: 需要针对性的基准测试和实验控制来严格评估解释方法是否真正提供了对LLM操作的有意义洞察。

Abstract: Recent interpretability methods have proposed to translate LLM internal
representations into natural language descriptions using a second verbalizer
LLM. This is intended to illuminate how the target model represents and
operates on inputs. But do such activation verbalization approaches actually
provide privileged knowledge about the internal workings of the target model,
or do they merely convey information about its inputs? We critically evaluate
popular verbalization methods across datasets used in prior work and find that
they succeed at benchmarks without any access to target model internals,
suggesting that these datasets are not ideal for evaluating verbalization
methods. We then run controlled experiments which reveal that verbalizations
often reflect the parametric knowledge of the verbalizer LLM which generated
them, rather than the activations of the target LLM being decoded. Taken
together, our results indicate a need for targeted benchmarks and experimental
controls to rigorously assess whether verbalization methods provide meaningful
insights into the operations of LLMs.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [46] [A Traditional Approach to Symbolic Piano Continuation](https://arxiv.org/abs/2509.12267)
*Christian Zhou-Zheng,John Backsund,Dun Li Chan,Alex Coventry,Avid Eslami,Jyotin Goel,Xingwen Han,Danysh Soomro,Galen Wei*

Main category: cs.SD

TL;DR: 本文提出了一种传统的符号钢琴音乐延续方法，使用简单的下一个token预测目标来处理原始MIDI数据，旨在在单乐器任务中超越大型基础模型。


<details>
  <summary>Details</summary>
Motivation: 虽然当前计算音乐生成主要关注开发大型基础模型和复杂架构改进，但作者认为对于受限的单乐器任务，简单方法可能更有效。

Method: 采用简单的、未经增强的下一个token预测目标，对token化的原始MIDI数据进行处理，注重更好的数据和基础技术。

Result: 开发了模型并发布了权重和代码，但具体性能结果未在摘要中明确说明。

Conclusion: 对于符号钢琴音乐生成这样的特定任务，简单传统方法可能比复杂的大型基础模型更有效，强调了数据和基础技术的重要性。

Abstract: We present a traditional approach to symbolic piano music continuation for
the MIREX 2025 Symbolic Music Generation challenge. While computational music
generation has recently focused on developing large foundation models with
sophisticated architectural modifications, we argue that simpler approaches
remain more effective for constrained, single-instrument tasks. We thus return
to a simple, unaugmented next-token-prediction objective on tokenized raw MIDI,
aiming to outperform large foundation models by using better data and better
fundamentals. We release model weights and code at
https://github.com/christianazinn/mirex2025.

</details>


### [47] [An Adaptive CMSA for Solving the Longest Filled Common Subsequence Problem with an Application in Audio Querying](https://arxiv.org/abs/2509.12261)
*Marko Djukanovic,Christian Blum,Aleksandar Kartelj,Ana Nikolikj,Guenther Raidl*

Main category: cs.SD

TL;DR: 本文针对NP难问题LFCS，提出了自适应CMSA框架，在更大规模基准测试中实现最优性能，解决了1486/1510个已知最优解实例，并展示了在歌曲识别等实际应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有LFCS算法主要在小型实例上测试，缺乏对算法可扩展性的有效评估。需要更大规模的基准数据集和更高效的算法来解决大规模实例。

Method: 采用自适应Construct, Merge, Solve, Adapt (CMSA)框架，通过基于组件的构建迭代生成有前景的子问题，并利用外部黑盒求解器进行优化。

Result: 在1510个已知最优解的问题实例中解决了1486个，达到99.9%的最优解质量，显著优于5种领先方法，展现出卓越的可扩展性。

Conclusion: 自适应CMSA框架在LFCS问题上实现了state-of-the-art性能，新的大规模基准数据集提供了更有意义的算法评估，同时展示了LFCS在音频歌曲识别等实际应用中的价值。

Abstract: This paper addresses the Longest Filled Common Subsequence (LFCS) problem, a
challenging NP-hard problem with applications in bioinformatics, including gene
mutation prediction and genomic data reconstruction. Existing approaches,
including exact, metaheuristic, and approximation algorithms, have primarily
been evaluated on small-sized instances, which offer limited insights into
their scalability. In this work, we introduce a new benchmark dataset with
significantly larger instances and demonstrate that existing datasets lack the
discriminative power needed to meaningfully assess algorithm performance at
scale. To solve large instances efficiently, we utilize an adaptive Construct,
Merge, Solve, Adapt (CMSA) framework that iteratively generates promising
subproblems via component-based construction and refines them using feedback
from prior iterations. Subproblems are solved using an external black-box
solver. Extensive experiments on both standard and newly introduced benchmarks
show that the proposed adaptive CMSA achieves state-of-the-art performance,
outperforming five leading methods. Notably, on 1,510 problem instances with
known optimal solutions, our approach solves 1,486 of them -- achieving over
99.9% optimal solution quality and demonstrating exceptional scalability. We
additionally propose a novel application of LFCS for song identification from
degraded audio excerpts as an engineering contribution, using real-world
energy-profile instances from popular music. Finally, we conducted an empirical
explainability analysis to identify critical feature combinations influencing
algorithm performance, i.e., the key problem features contributing to success
or failure of the approaches across different instance types are revealed.

</details>


### [48] [Omni-CLST: Error-aware Curriculum Learning with guided Selective chain-of-Thought for audio questuin answering](https://arxiv.org/abs/2509.12275)
*Jinghua Zhao,Hang Su,Lichun Fan,Zhenbo Luo,Jian Luan,Hui Wang,Haoqin Sun,Yong Qin*

Main category: cs.SD

TL;DR: Omni-CLST是一个错误感知的课程学习框架，通过难度分级和引导式思维丢弃机制，在音频问答任务中实现了优异的性能表现。


<details>
  <summary>Details</summary>
Motivation: 为了解决音频问答任务中模型难以有效利用高质量数据集的问题，作者提出了一种能够根据样本难度进行智能学习的框架，以提高模型在复杂情况下的推理能力。

Method: 采用错误感知课程学习策略，按难度组织样本；使用引导式思维丢弃机制专注于挑战性案例；结合GRPO训练方法，使模型能够更有效地从信息丰富的样本中学习。

Result: 在MMAU-mini数据集上达到73.80%的准确率，在MMAR数据集上创下64.30%的新state-of-the-art结果，展现了出色的鲁棒性和泛化能力。

Conclusion: Omni-CLST框架通过创新的课程学习和选择性思维链机制，在多模态音频-语言理解任务中表现出强大的性能和泛化能力，为音频问答领域提供了有效的解决方案。

Abstract: We propose Omni-CLST, an error-aware Curriculum Learning framework with
guided Selective Chain-of-Thought for audio question answering. The framework
efficiently leverages existing high-quality dataset through two key strategies:
an error-aware curriculum that organizes samples by difficulty, and a guided
thought dropout mechanism that focuses reasoning on challenging cases.
Integrated with GRPO training, these strategies enable the model to learn more
effectively from informative samples. Experiments on MMAU-mini and MMAR
demonstrate that Omni-CLST achieves competitive accuracy (73.80% on MMAU-mini)
and establishes a new state of the art (64.30% on MMAR), highlighting its
robustness and generalization capability in multimodal audio-language
understanding.

</details>


### [49] [More Similar than Dissimilar: Modeling Annotators for Cross-Corpus Speech Emotion Recognition](https://arxiv.org/abs/2509.12295)
*James Tavernor,Emily Mower Provost*

Main category: cs.SD

TL;DR: 提出利用标注者相似性，通过预训练模型识别相似标注者，实现低成本个性化语音情感识别


<details>
  <summary>Details</summary>
Motivation: 传统语音情感识别系统只能预测多个标注者的共识值，无法预测单个标注者的标注，且新标注者需要大量标注数据才能适配

Method: 利用预训练在大规模标注者群体上的模型，通过有限的新标注者注册数据识别相似的历史标注者，使用相似标注者的预测结果

Result: 该方法显著优于其他现成方法，为轻量级情感适配铺平道路

Conclusion: 该方法实现了极低成本的个性化，适合实际部署应用

Abstract: Speech emotion recognition systems often predict a consensus value generated
from the ratings of multiple annotators. However, these models have limited
ability to predict the annotation of any one person. Alternatively, models can
learn to predict the annotations of all annotators. Adapting such models to new
annotators is difficult as new annotators must individually provide sufficient
labeled training data. We propose to leverage inter-annotator similarity by
using a model pre-trained on a large annotator population to identify a
similar, previously seen annotator. Given a new, previously unseen, annotator
and limited enrollment data, we can make predictions for a similar annotator,
enabling off-the-shelf annotation of unseen data in target datasets, providing
a mechanism for extremely low-cost personalization. We demonstrate our approach
significantly outperforms other off-the-shelf approaches, paving the way for
lightweight emotion adaptation, practical for real-world deployment.

</details>


### [50] [Osu2MIR: Beat Tracking Dataset Derived From Osu! Data](https://arxiv.org/abs/2509.12667)
*Ziyun Liu,Chris Donahue*

Main category: cs.SD

TL;DR: 使用Osu!节奏游戏的社区制作谱面作为节拍和重拍标注的替代来源，通过提取和处理流程获得高质量音乐信息研究数据


<details>
  <summary>Details</summary>
Motivation: 传统音乐标注数据稀缺且缺乏多样性，Osu!社区拥有大量涵盖动漫、Vocaloid等小众音乐类型的谱面，可作为规模化、多样化的音乐信息研究资源

Method: 开发了从Osu!谱面提取标注的流程，将谱面按时间点间距分类（单时间点、≥5秒间距、<5秒间距），并通过人工分析验证标注质量

Result: 发现单时间点或间距≥5秒的谱面提供可靠标注，间距<5秒的谱面需要额外处理，同一歌曲的多重标注具有高度一致性

Conclusion: Osu!数据可作为音乐信息研究的可扩展、多样化社区驱动资源，发布了处理流程和高质量数据集osu2beat2025

Abstract: In this work, we explore the use of Osu!, a community-based rhythm game, as
an alternative source of beat and downbeat annotations. Osu! beatmaps are
created and refined by a large, diverse community and span underrepresented
genres such as anime, Vocaloid, and video game music. We introduce a pipeline
for extracting annotations from Osu! beatmaps and partition them into
meaningful subsets. Through manual analysis, we find that beatmaps with a
single timing point or widely spaced multiple timing points (>=5 seconds apart)
provide reliable annotations, while closely spaced timing points (<5 seconds
apart) often require additional curation. We also observe high consistency
across multiple annotations of the same song. This study demonstrates the
potential of Osu! data as a scalable, diverse, and community-driven resource
for MIR research. We release our pipeline and a high-quality subset
osu2beat2025 to support further exploration:
https://github.com/ziyunliu4444/osu2mir.

</details>


### [51] [Timbre-Adaptive Transcription: A Lightweight Architecture with Associative Memory for Dynamic Instrument Separation](https://arxiv.org/abs/2509.12712)
*Ruigang Li,Yongxu Zhu*

Main category: cs.SD

TL;DR: 提出轻量级深度聚类解决方案，包含音色无关主干网络和联想记忆机制，实现少样本多音色音乐转录和分离


<details>
  <summary>Details</summary>
Motivation: 现有多音色转录模型泛化能力有限，无法处理预训练乐器之外的音色，且受限于固定的音源数量约束

Method: 使用音色无关主干网络（参数量减半）+ 基于注意力的联想记忆聚类机制（模仿人类听觉认知）+ 新的合成数据集方法

Result: 音色无关转录模型在公开基准测试中优于现有模型，分离模块展现出良好的音色辨别能力，仅需12.5分钟训练数据

Conclusion: 提供了一个高效的音乐转录框架，通过认知启发架构探索了音色感知分离的新方向

Abstract: Existing multi-timbre transcription models struggle with generalization
beyond pre-trained instruments and rigid source-count constraints. We address
these limitations with a lightweight deep clustering solution featuring: 1) a
timbre-agnostic backbone achieving state-of-the-art performance with only half
the parameters of comparable models, and 2) a novel associative memory
mechanism that mimics human auditory cognition to dynamically encode unseen
timbres via attention-based clustering. Our biologically-inspired framework
enables adaptive polyphonic separation with minimal training data (12.5
minutes), supported by a new synthetic dataset method offering cost-effective,
high-precision multi-timbre generation. Experiments show the timbre-agnostic
transcription model outperforms existing models on public benchmarks, while the
separation module demonstrates promising timbre discrimination. This work
provides an efficient framework for timbre-related music transcription and
explores new directions for timbre-aware separation through cognitive-inspired
architectures.

</details>


### [52] [Beyond Bars: Distribution of Edit Operations in Historical Prints](https://arxiv.org/abs/2509.12786)
*Adrian Nachtwey,Fabian C. Moss,Anna Viktoria Katrin Plaksin*

Main category: cs.SD

TL;DR: 提出了一种通过采样小节而非完整编码来简化音乐学比较语料库研究的方法，以贝多芬作品为案例评估了三种采样方法的效果


<details>
  <summary>Details</summary>
Motivation: 减少音乐学研究中耗时的数字化过程，使大规模分析和统计上可靠的结果成为可能，同时促进对19世纪编辑实践的理解

Method: 建议从音乐资料中采样小节而非完整编码整个语料库，评估了三种不同的采样方法，以贝多芬的Bagatelles Op. 33作为案例研究

Result: 找到了在寻找具有代表性差异样本方面效果最佳的方法

Conclusion: 这种方法为音乐学研究提供了重要价值，是理解19世纪编辑实践和丰富历史音乐作品学术编辑领域的重要一步

Abstract: In this paper, we present a method for conducting comparative corpus studies
in musicology that reduces the time-consuming digitization process. Instead of
encoding whole corpora of musical sources, we suggest sampling bars from these
sources. We address the challenge of selecting representative samples and
evaluate three different sampling methods. We used Beethoven's Bagatelles Op.
33 as a case study to find the method that works best in finding samples
representative with respect to differences. We believe that this approach
offers significant value to musicological research by enabling large-scale
analyses and thereby statistically sound results. Moreover, we believe our work
to be a valuable step toward understanding nineteenth-century editorial
practices and enriching the field of scholarly editing of historical musical
works.

</details>


### [53] [A Lightweight Pipeline for Noisy Speech Voice Cloning and Accurate Lip Sync Synthesis](https://arxiv.org/abs/2509.12831)
*Javeria Amir,Farwa Attaria,Mah Jabeen,Umara Noor,Zahid Rashid*

Main category: cs.SD

TL;DR: 提出了一种模块化语音克隆和唇语同步管道，使用Tortoise TTS进行零样本语音克隆，结合轻量级GAN实现实时唇语同步，适用于噪声环境和低资源场景。


<details>
  <summary>Details</summary>
Motivation: 当前语音克隆和唇语同步方法需要大规模数据集和计算密集型处理，在噪声或低资源环境中不可行，需要开发更轻量、鲁棒的解决方案。

Method: 使用基于Transformer的潜在扩散模型Tortoise TTS进行零样本语音克隆，结合轻量级生成对抗网络实现实时唇语同步，采用模块化管道设计。

Result: 能够实现高保真度的零样本语音克隆和鲁棒的实时唇语同步，在噪声和无约束场景下表现良好，减少对大规模预训练的依赖。

Conclusion: 该模块化管道为情感表达语音生成和唇语同步提供了实用解决方案，具有扩展到多模态和文本引导语音调制的潜力，适用于现实世界系统。

Abstract: Recent developments in voice cloning and talking head generation demonstrate
impressive capabilities in synthesizing natural speech and realistic lip
synchronization. Current methods typically require and are trained on large
scale datasets and computationally intensive processes using clean studio
recorded inputs that is infeasible in noisy or low resource environments. In
this paper, we introduce a new modular pipeline comprising Tortoise text to
speech. It is a transformer based latent diffusion model that can perform high
fidelity zero shot voice cloning given only a few training samples. We use a
lightweight generative adversarial network architecture for robust real time
lip synchronization. The solution will contribute to many essential tasks
concerning less reliance on massive pre training generation of emotionally
expressive speech and lip synchronization in noisy and unconstrained scenarios.
The modular structure of the pipeline allows an easy extension for future multi
modal and text guided voice modulation and it could be used in real world
systems.

</details>


### [54] [Improving Anomalous Sound Detection with Attribute-aware Representation from Domain-adaptive Pre-training](https://arxiv.org/abs/2509.12845)
*Xin Fang,Guirui Zhong,Qing Wang,Fan Chu,Lei Wang,Mengui Qian,Mingqi Cai,Jiangzhao Wu,Jianqing Gao,Jun Du*

Main category: cs.SD

TL;DR: 本文提出了一种基于层次聚类的伪属性标签分配方法，通过领域自适应预训练模型提取表征来捕获机器属性特征，然后在机器属性分类任务上进行监督微调，实现了新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 异常声音检测通常被表述为机器属性分类任务，但完整收集机器属性标签既费力又不切实际。为了解决属性标签缺失的挑战，需要开发无需完整标注的方法。

Method: 使用领域自适应预训练模型提取表征，通过凝聚层次聚类方法分配伪属性标签，然后对预训练模型进行监督微调以进行机器属性分类。

Result: 在DCASE 2025挑战数据集上的评估表明，该方法带来了显著的性能提升，最终超越了之前在挑战中排名第一的系统。

Conclusion: 所提出的伪属性标签分配和监督微调方法有效解决了机器属性标签缺失问题，在异常声音检测任务上实现了新的最先进性能。

Abstract: Anomalous Sound Detection (ASD) is often formulated as a machine attribute
classification task, a strategy necessitated by the common scenario where only
normal data is available for training. However, the exhaustive collection of
machine attribute labels is laborious and impractical. To address the challenge
of missing attribute labels, this paper proposes an agglomerative hierarchical
clustering method for the assignment of pseudo-attribute labels using
representations derived from a domain-adaptive pre-trained model, which are
expected to capture machine attribute characteristics. We then apply model
adaptation to this pre-trained model through supervised fine-tuning for machine
attribute classification, resulting in a new state-of-the-art performance.
Evaluation on the Detection and Classification of Acoustic Scenes and Events
(DCASE) 2025 Challenge dataset demonstrates that our proposed approach yields
significant performance gains, ultimately outperforming our previous
top-ranking system in the challenge.

</details>


### [55] [The CCF AATC 2025: Speech Restoration Challenge](https://arxiv.org/abs/2509.12974)
*Junan Zhang,Mengyao Zhu,Xin Xu,Hui Bu,Zhenhua Ling,Zhizheng Wu*

Main category: cs.SD

TL;DR: CCF AATC 2025语音修复挑战赛旨在解决现实场景中多种失真同时存在的复杂语音增强问题，包括声学失真、信号链伪影和预处理模型引入的二次伪影。


<details>
  <summary>Details</summary>
Motivation: 现实语音通信常受多种失真影响，现有算法在多种失真共存时效果不佳，需要推动更全面的语音修复研究。

Method: 设计包含三类失真的复合退化任务：复杂声学失真（非平稳噪声和混响）、信号链伪影（如MP3压缩）和其他预处理模型引入的二次伪影。创建综合数据集并制定详细评估协议。

Result: 提出了一个全面的语音修复挑战框架，包括任务设计、数据集构建方法和评估标准，为相关研究提供基准平台。

Conclusion: 该挑战赛为多失真语音修复领域提供了重要的研究推动力，通过综合评估协议促进算法在性能和复杂度间的平衡发展。

Abstract: Real-world speech communication is often hampered by a variety of distortions
that degrade quality and intelligibility. While many speech enhancement
algorithms target specific degradations like noise or reverberation, they often
fall short in realistic scenarios where multiple distortions co-exist and
interact. To spur research in this area, we introduce the Speech Restoration
Challenge as part of the China Computer Federation (CCF) Advanced Audio
Technology Competition (AATC) 2025. This challenge focuses on restoring speech
signals affected by a composite of three degradation types: (1) complex
acoustic degradations including non-stationary noise and reverberation; (2)
signal-chain artifacts such as those from MP3 compression; and (3) secondary
artifacts introduced by other pre-processing enhancement models. We describe
the challenge's background, the design of the task, the comprehensive dataset
creation methodology, and the detailed evaluation protocol, which assesses both
objective performance and model complexity. Homepage: https://ccf-aatc.org.cn/.

</details>


### [56] [GLAD: Global-Local Aware Dynamic Mixture-of-Experts for Multi-Talker ASR](https://arxiv.org/abs/2509.13093)
*Yujie Guo,Jiaming Zhou,Yuhang Jia,Shiwan Zhao,Yong Qin*

Main category: cs.SD

TL;DR: 提出了GLAD混合专家模型，通过全局-局部融合策略动态选择专家，在多人语音识别任务中显著提升重叠语音的识别准确率


<details>
  <summary>Details</summary>
Motivation: 解决端到端多人语音识别在高重叠场景下面临的挑战，需要更准确地转录重叠语音

Method: GLAD混合专家模型，动态融合说话人感知的全局信息和细粒度局部特征来指导专家选择，利用全局上下文和局部声学线索实现说话人特定路由

Result: 在LibriSpeechMix数据集上实验表明，GLAD优于现有MTASR方法，特别是在具有挑战性的多人场景中

Conclusion: 这是首个将混合专家模型应用于端到端多人语音识别的工作，采用全局-局部融合策略，取得了显著性能提升

Abstract: End-to-end multi-talker automatic speech recognition (MTASR) faces
significant challenges in accurately transcribing overlapping speech,
especially under high-overlap conditions. To address these challenges, we
proposed Global-Local Aware Dynamic (GLAD) Mixture-of-Experts, which
dynamically fuse speaker-aware global information and fine-grained local
features to guide expert selection. This mechanism enables speaker-specific
routing by leveraging both global context and local acoustic cues. Experiments
on LibriSpeechMix show that GLAD outperforms existing MTASR approaches,
particularly in challenging multi-talker scenarios. To our best knowledge, this
is the first work to apply Mixture-of-Experts (MoE) to end-to-end MTASR with a
global-local fusion strategy. Our code and train dataset can be found at
https://github.com/NKU-HLT/GLAD.

</details>


### [57] [UTI-LLM: A Personalized Articulatory-Speech Therapy Assistance System Based on Multimodal Large Language Model](https://arxiv.org/abs/2509.13145)
*Yudong Yang,Xiaokang Liu,Shaofeng zhao,Rongfeng Su,Nan Yan,Lan Wang*

Main category: cs.SD

TL;DR: 基于多模态大语言模型的语音康复辅助系统，结合超声舌成像和语音信号提供精准的发音反馈


<details>
  <summary>Details</summary>
Motivation: 传统语音治疗方法在实时可及性和发音运动反馈方面存在局限，而现有MLLM在发音信息获取融合、器官运动轨迹解析等方面存在不足，阻碍了在语音治疗中的应用

Method: 构建高质量领域特定数据集（UTI-语音对话对），采用超声视频和语音信号的时空融合训练策略，实现细粒度发音障碍分析和可操作反馈生成

Result: 开发了一个能够提供精确交互式发音反馈的MLLM语音康复辅助系统

Conclusion: 该方法通过多模态数据融合和领域特定数据集构建，有效解决了MLLM在语音治疗应用中的关键挑战，为发音障碍康复提供了新的技术方案

Abstract: Speech therapy plays a critical role in training speech disorders caused by
neurological impairments such as stroke. However, traditional manual and
computer-assisted systems are limited in real-time accessibility and
articulatory motion feedback, constraining their practical utility. Recent
advances in multimodal large language models (MLLMs) have demonstrated
significant potential in healthcare, particularly through their ability to
integrate multimodal data for adaptive assessment and therapeutic feedback.
Nevertheless, challenges including insufficient acquisition and fusion of
articulatory information, inadequate parsing of articulatory organ motion
trajectories, and the scarcity of high-quality domain-specific datasets hinder
the application of MLLMs in speech therapy. To address these limitations, we
propose an MLLM-based speech rehabilitation assistance system that
synergistically leverages ultrasound tongue imaging and speech signals to
deliver precise, interactive articulatory feedback. We construct a high-quality
domain-specific dataset comprising UTI-speech dialogue pairs. This dataset
facilitates fine-tuning to enhance the model's clinical adaptability. Building
on this dataset, our methods achieves spatiotemporal fusion training strategy
of ultrasound videos and speech signals, enabling fine-grained articulatory
impairment analysis and ultimately generating actionable feedback.

</details>


### [58] [Can Large Audio Language Models Understand Audio Well? Speech, Scene and Events Understanding Benchmark for LALMs](https://arxiv.org/abs/2509.13148)
*Han Yin,Jung-Woo Choi*

Main category: cs.SD

TL;DR: SSEU-Bench是首个考虑语音与非语音音频能量差异的通用音频理解基准，包含独立和联合理解设置，并通过思维链方法提升大音频语言模型的联合理解性能。


<details>
  <summary>Details</summary>
Motivation: 现有音频理解基准在真实世界交互的关键方面探索不足，特别是音频信号通常同时包含语音和非语音成分，且这些成分的能量水平在不同场景中差异显著。大多数基准没有考虑同一音频片段中语音、场景和事件的联合理解。

Method: 提出SSEU-Bench基准，明确考虑语音与非语音音频的能量差异，设置独立和联合理解任务。针对联合理解性能问题，引入思维链（Chain-of-Thought）方法，将复杂任务分解为更简单的推理步骤。

Result: 研究表明某些大音频语言模型在联合理解设置下的某些任务中表现不佳。思维链方法有效提升了模型的联合音频理解性能。

Conclusion: SSEU-Bench填补了现有音频理解基准的空白，为评估大音频语言模型的真实世界交互能力提供了更全面的测试平台，思维链方法为解决联合理解难题提供了有效解决方案。

Abstract: Recently, Large Audio Language Models (LALMs) have progressed rapidly,
demonstrating their strong efficacy in universal audio understanding through
cross-modal integration. To evaluate the LALM's audio understanding
performance, researchers have proposed different benchmarks. However, key
aspects for real-world interactions are underexplored in existing benchmarks,
i.e., audio signals typically contain both speech and non-speech components,
and energy levels of these components can vary significantly across different
scenarios. Moreover, most benchmarks do not consider the joint understanding of
speech, scene, and events within the same audio clip. In this work, we
introduce SSEU-Bench, the first versatile audio understanding benchmark that
explicitly accounts for energy differences between speech and non-speech audio,
with both independent and joint understanding settings for speech, scene, and
events. Furthermore, we demonstrate that some LALMs tend to underperform on
certain tasks in a joint understanding setting. To address this issue, we
introduce Chain-of-Thought, which effectively improves the LALM's joint audio
understanding performance by decomposing complex tasks into simpler reasoning
steps

</details>


### [59] [Contrastive timbre representations for musical instrument and synthesizer retrieval](https://arxiv.org/abs/2509.13285)
*Gwendal Le Vaillant,Yannick Molle*

Main category: cs.SD

TL;DR: 本文提出了一种基于对比学习的乐器检索框架，能够直接从音频混合中检索特定乐器音色，在单乐器和多乐器检索任务中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 数字音乐制作中从音频混合中高效检索特定乐器音色仍然是一个挑战，现有方法在数据增强和多乐器检索方面存在局限性。

Method: 采用对比学习框架，为虚拟乐器生成真实的正面/负面声音对，使用单一模型处理单乐器和多乐器声音检索。

Result: 在3884种乐器数据集上，单乐器检索性能与基于分类预训练的方法相当；多乐器检索中，三乐器混合的top-1准确率达到81.7%，top-5准确率达到95.7%，优于相关工作。

Conclusion: 提出的对比学习框架在乐器检索任务中表现优异，特别是在多乐器混合检索方面展现出显著优势，为音乐制作中的音色检索提供了有效解决方案。

Abstract: Efficiently retrieving specific instrument timbres from audio mixtures
remains a challenge in digital music production. This paper introduces a
contrastive learning framework for musical instrument retrieval, enabling
direct querying of instrument databases using a single model for both single-
and multi-instrument sounds. We propose techniques to generate realistic
positive/negative pairs of sounds for virtual musical instruments, such as
samplers and synthesizers, addressing limitations in common audio data
augmentation methods.
  The first experiment focuses on instrument retrieval from a dataset of 3,884
instruments, using single-instrument audio as input. Contrastive approaches are
competitive with previous works based on classification pre-training. The
second experiment considers multi-instrument retrieval with a mixture of
instruments as audio input. In this case, the proposed contrastive framework
outperforms related works, achieving 81.7\% top-1 and 95.7\% top-5 accuracies
for three-instrument mixtures.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [60] [Evaluation of Objective Image Quality Metrics for High-Fidelity Image Compression](https://arxiv.org/abs/2509.13150)
*Shima Mohammadi,Mohsen Jenadeleh,Jon Sneyers,Dietmar Saupe,João Ascenso*

Main category: cs.MM

TL;DR: 本文评估了当前图像质量评估指标在高保真压缩场景下的性能，特别是在JND阈值附近的失真检测能力，提出了新的评估方法并发布了公开数据集。


<details>
  <summary>Details</summary>
Motivation: 随着图像压缩技术向高保真质量范围发展，需要能够检测和量化细微压缩伪影的质量评估指标，但现有指标在此范围内的性能尚未得到充分研究。

Method: 使用JPEG AIC-3高保真图像压缩数据集，提出包含主观评分不确定性的Z-RMSE评估方法，应用新颖的统计测试来评估指标间的显著差异。

Result: 研究全面评估了质量指标在整个JPEG AIC-3范围及其高保真和中保真子集的性能，分析了主观测试中裁剪的影响。

Conclusion: 研究提供了高保真图像压缩质量评估的系统方法，发布了包含基准和评估工具的公开数据集以支持进一步研究。

Abstract: Nowadays, image compression solutions are increasingly designed to operate
within high-fidelity quality ranges, where preserving even the most subtle
details of the original image is essential. In this context, the ability to
detect and quantify subtle compression artifacts becomes critically important,
as even slight degradations can impact perceptual quality in professional or
quality sensitive applications, such as digital archiving, professional editing
and web delivery. However, the performance of current objective image quality
assessment metrics in this range has not been thoroughly investigated. In
particular, it is not well understood how reliably these metrics estimate
distortions at or below the threshold of Just Noticeable Difference (JND). This
study directly addresses this issue by proposing evaluation methodologies for
assessing the performance of objective quality metrics and performing a
comprehensive evaluation using the JPEG AIC-3 dataset which is designed for
high-fidelity image compression. Beyond conventional criteria, the study
introduces Z-RMSE to incorporate subjective score uncertainty and applies novel
statistical tests to assess significant differences between metrics. The
analysis spans the full JPEG AIC-3 range and its high- and medium-fidelity
subsets, examines the impact of cropping in subjective tests, and a public
dataset with benchmarks and evaluation tools is released to support further
research.

</details>
